{
    "abstract_sentences": {
        "1": "This paper presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words.", 
        "2": "The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment.", 
        "3": "In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments.", 
        "4": "We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks.", 
        "5": "Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1799\u20131809 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1165  1 Introduction  Since the seminal works of Hofmann (1999) and Blei et al.", 
        "2": "(2003), there have been several developments in probabilistic topic models.", 
        "3": "Many extensions have indeed been proposed for different applications, including ad-hoc information retrieval (Wei and Croft, 2006), clustering search results (Zeng et al., 2004) and driving faceted browsing (Mimno and McCallum, 2007).", 
        "4": "In most of these studies, the initial exchangeability assumptions of\nPLSA and LDA, stipulating that words within a document are interdependent, has led to incoherent topic assignments within semantically meaningful text units, even though the importance of having topically coherent phrases is generally admitted (Griffiths et al., 2005).", 
        "5": "More recently, (Balikas et al., 2016b) has shown that binding topics, so as to obtain more coherent topic assignments, within such text segments as noun phrases improves the performance (e.g.", 
        "6": "in terms of perplexity) of LDAbased models.", 
        "7": "The question nevertheless remains as to which segmentation one should rely on.", 
        "8": "Furthermore, text segments can refer to topics that are barely present in other parts of the document.", 
        "9": "For example, the segment \u201cthe Kurdish regional capital\u201d in the sentence1 \u201cA thousand protesters took to the main street in Erbil, the Kurdish regional capital, to condemn a new law requiring all public demonstrations to have government permits.\u201d refers to geography in a document that is mainly devoted to politics.", 
        "10": "Relying on a single topic distribution, as done in most previous studies including (Balikas et al., 2016b), may prevent one from capturing those segment specific topics.", 
        "11": "In this paper, we propose a novel LDA-based model that automatically segments documents into topically coherent sequences of words.", 
        "12": "The coherence between topics is ensured through copulas (Elidan, 2013) that bind the topics associated to the words of a segment.", 
        "13": "In addition, this model relies on both document and segment specific topic distri-\n1This sentence is taken from New York Times news (NYT) collection described in Section 4.", 
        "14": "1799\nbutions so as to capture fine grained differences in topic assignments.", 
        "15": "A simple switching mechanism is used to select the appropriate distribution (document or segment specific) for assigning a topic to a word.", 
        "16": "We show that this model naturally encompasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, Normalized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with documents, and the Micro F1-measure in a text classification context.", 
        "17": "2 Related work  Probabilistic Latent Semantic Analysis (PLSA) proposed by (Hofmann, 1999) is the first probabilistic model that explains the generation of cooccurrence data using latent random topics and, the EM algorithm for parameter estimation.", 
        "18": "The model was found more flexible and scalable than the Latent Semantic Analysis (Deerwester et al., 1990), which is based on the singular value decomposition of the document-term matrix, however PLSA is not a generative model as parameter estimation should be performed at each addition of new documents.", 
        "19": "To overcome this drawback, Blei et al.", 
        "20": "(2003) proposed the Latent Dirichlet Allocation (LDA) by assuming that the latent topics are random variables sampled from a Dirichlet distribution and that the generated words, occurring in a document, are exchangeable.", 
        "21": "The interdependence assumption allows the parameter estimation and the inference of the LDA model to be carried out efficiently, but it is not realistic in the sense that topics assigned to similar words of a text span are generally incoherent.", 
        "22": "Different studies, presented in the following sections, attempted to remedy this problem and they can be grouped in two broad families depending on whether they make use of external knowledgebased tools or not in order to exhibit text structure for word-topic assignment.", 
        "23": "2.1 Knowledge-based topic assignments  The main assumption behind these models are that text-spans such as sentences, phrases or segments are related in their content.", 
        "24": "Therefore, the integration of these dependent structures can help to discover coherent latent topics for words.", 
        "25": "Different attempts to combine LDA-based models with sta-\ntistical tools to discover document structures have been successfully proposed, such as the study of Griffiths et al.", 
        "26": "(2005) who investigated the effect of combining a Hidden Markov Model with LDA to capture long and short distance dependencies.", 
        "27": "Similarly, (Boyd-Graber and Blei, 2008; Balikas et al., 2016a,b) integrated text structure exhibited by a parser or a chunker in their topic models.", 
        "28": "In this line, Du et al.", 
        "29": "(2013) following (Du et al., 2010) presented a hierarchical Bayesian model for unsupervised topic segmentation.", 
        "30": "This model integrates a boundary sampling method used in a Bayesian segmentation model introduced by Purver et al.", 
        "31": "(2006) to the topic model.", 
        "32": "For inference, a non-parametric Markov Chain inference is used that splits and merges the segments while a PitmanYor process (Teh, 2006) binds the topics.", 
        "33": "Recently, Tamura and Sumita (2016) extended this idea to the bilingual setting.", 
        "34": "They assume that documents consist of segments and the topic distribution of each segment is generated using a Pitman-Yor process (Teh, 2006).", 
        "35": "Though, the topic assignments follow the structure of the text; these models suffer from the bias of statistical or linguistic tools they rely on.", 
        "36": "To overpass this limitation, other systems integrated automatically the extraction of text structure, in the form of phrases, in their process.", 
        "37": "2.2 Knowledge-free topic assignments  This type of models extract text-spans using ngram counts and word collections and use bigrams to integrate the order of words as well as to capture the topical content of a phrase (Lau et al., 2013).", 
        "38": "In (Wang et al., 2007), depending on the topic a particular bigram can be either considered as a single token or as two unigrams.", 
        "39": "Further, Wang et al.", 
        "40": "(2009) merged topic models with a unigram model over sentences that assigns topics to the sentences instead of the words.", 
        "41": "Our proposed approach also does not make use of external statistical tools to find text segments.", 
        "42": "The main difference with the previous knowledgefree topic model approaches is that the proposed approach assigns topics to words based on two, segment-specific and document-specific distributions selected from a Bernoulli law.", 
        "43": "Topics within segments are then constrained using copulas that bind their distributions.", 
        "44": "In this way, segmentation is embedded in the model and it naturally comes along with the topic assignment.", 
        "45": "3 Joint latent model for topics and segments  We define here a segment as a topically coherent sequence of contiguous words.", 
        "46": "By topically coherent, we mean that, even though words in a segment can be associated to different topics, these topics are usually related.", 
        "47": "This view is in line with the one expressed in (Balikas et al., 2016b), in which a latent topic model, referred to as copLDA in the remainder, includes a binding mechanism between topics within coherent text spans, defined in their study as noun phrases (NPs).", 
        "48": "The relation between topics is captured through a copula that provides a joint probability for all the topics used in a segment.", 
        "49": "That is, to generate words in a segment, one first jointly generates all the word specific topics z via a copula, and then generates each word in the segment from its word specific topic and the word-topic distribution \u03c6.", 
        "50": "Figure 1(a) illustrates this.", 
        "51": "Copulas are particularly useful when modeling dependencies between random variables, as the joint cumulative distribution function (CDF) FX1,\u00b7\u00b7\u00b7 ,Xn of any random vector X = (X1, \u00b7 \u00b7 \u00b7 , Xn) can be written as a function of its marginals, according to Sklar\u2019s Theorem (Nelsen, 2006):\nFX1,\u00b7\u00b7\u00b7 ,Xn(x1, \u00b7 \u00b7 \u00b7 , xp) = C(FX1(x1), \u00b7 \u00b7 \u00b7 , FXn(xn))\nwhere C is a copula.", 
        "52": "For latent topic models, as discussed in (Amoualian et al., 2016), Frank\u2019s copula is particularly interesting as (a) it is invariant by permutations and associative, as are the words and topics z in each segment due to the exchangeability assumption, and (b) it relies on a single parameter (denoted \u03bb here) that controls the strength of dependence between the variables and is thus easy to implement.", 
        "53": "In Frank\u2019s copula, when the parameter \u03bb approaches 0, the variables are independent of each other, whereas when \u03bb approaches +\u221e, the variables take the same value.", 
        "54": "For further details on copulas, we refer the reader to (Nelsen, 2006).", 
        "55": "One important problem, however, with copLDA is its reliance on a predefined segmentation.", 
        "56": "Although the information brought by the segmentation based on NPs helps to improve topic assignment, it may not be flexible enough to capture all the possible segments of a text.", 
        "57": "It is easy to correct this problem by considering all possible segmentations of a document and by choosing the most appropriate one at the same time that topics are assigned to words.", 
        "58": "This is illustrated in Figure 1(b), where a segmentation S is chosen from the set Sd of possible segmentations for a document d, and where each segment in S are generated in turn.", 
        "59": "We refer to the associated model as segLDAcopp=0 for reasons that will become clear later.", 
        "60": "Another point to be noted about copLDA (and\nsegLDAcopp=0) is that the topics used in each segment come from the same document specific topic distribution \u03b8d.", 
        "61": "This entails that, in these models, one cannot differentiate the main topics of a document from potential segment specific topics that can explain some parts of it.", 
        "62": "Indeed, some text segments can refer to topics that are barely present in other parts of the document; relying on a single topic distribution may prevent one from capturing those segment specific topics.", 
        "63": "It is possible to overcome this difficulty by generating a segment specific topic distribution as illustrated in Figure 1(c) (this model is referred to as segLDAcop\u03bb=0, again for reasons that will become clear later).", 
        "64": "However, as some words in a segment can be associated to the general topics of a document, we introduce a mechanism to choose, for each word in a segment, a topic either from the segment specific topic distribution \u03b8s or from the document specific topic distribution \u03b8d (this mechanism is similar to the one used for routes and levels in (Paul and Girju, 2010)).", 
        "65": "The choice between them is based on the Bernoulli variable f , as explained in the generative story given below.", 
        "66": "The above developments can be combined in a single, complete model, illustrated in Figure 1(d) and detailed below.", 
        "67": "We will simply refer to this model as segLDAcop.", 
        "68": "3.1 Complete generative model  As in standard LDA based models, with V denoting the size of the vocabulary of the collection and K the number of latent topics, \u03b2 and \u03c6k, 1 \u2264 k \u2264 K, are V dimensional vectors, \u03b1 and \u03b8 (i.e., \u03b8d, \u03b8s, \u03b8d,s,n) are K dimensional vectors, whereas zn takes value in {1, \u00b7 \u00b7 \u00b7 ,K}.", 
        "69": "Lower indices are used to denote coordinates of the above vectors.", 
        "70": "Lastly, Dir denotes the Dirichlet distribution, Cat the categorical distribution (which is a multinomial distribution with one draw) and we omit, as is usual, the generation of the length of the document.", 
        "71": "The complete model segLDAcop is then based on the following generative process:\n1.", 
        "72": "Generate, for each topic k, 1 \u2264 k \u2264 K, a distribution over the words: \u03c6k \u223c Dir(\u03b2);\n2.", 
        "73": "For each document d, 1 \u2264 d \u2264 D: (a) Choose a document specific topic distribu-\ntion: \u03b8d \u223c Dir(\u03b1); (b) Choose a segmentation S of the document\nuniformly from the set of all possible\nsegmentations Sd: P (S) = 1|Sd| ; (c) For each segment s in S:\n(i) Choose a segment specific topic distribution: \u03b8s \u223c Dir(\u03b1); (ii) For each position n in s, choose fn \u223c Ber(p) and set:\n\u03b8d,s,n = { \u03b8s if fn = 1 \u03b8d otherwise\n(iii) Choose topics Zs = {z1, .", 
        "74": ".", 
        "75": ".", 
        "76": ", zn} from Frank\u2019s copula with parameter \u03bb and marginals Cat(\u03b8d,s,n);\n(iv) For each position n in s, choose word wn: wn \u223c Cat(\u03c6zn).", 
        "77": "As on can note, the generative process relies on a segmentation uniformly chosen from the set of possible segmentations (step 2.b) to generate related topics within each segment (Frank\u2019s copula in step 2.c.", 
        "78": "(iii)), the distribution underlying each word specific topic zn being either specific to the segment or general to the document (steps 2.c.", 
        "79": "(i) and 2.c.(ii)).", 
        "80": "The other steps are similar to the standard LDA steps.", 
        "81": "As in almost all previous studies on LDA, \u03b1 and \u03b2 are considered fixed and symmetric, each coordinate of the vector being equal: \u03b11 = \u00b7 \u00b7 \u00b7 = \u03b1K .", 
        "82": "The hyperparameters p (\u2208 [0, 1]) of the Bernoulli distribution and \u03bb (\u2208 [0,+\u221e]) of Frank\u2019s copula respectively regulate the choice between the segment specific and the document specific topic distributions and the strength of the dependence between topics in a segment.", 
        "83": "As for the other hyperparameters, we consider them fixed here (the values for all hyperparameters are given in Section 4).", 
        "84": "As mentioned before, all the models presented in Figure 1 are special cases of the complete model segLDAcop: hence segLDAcop\u03bb=0 is obtained by dropping the topic dependencies, which amounts to setting \u03bb to (a value close to) 0, segLDAcopp=0 is obtained by relying only on the topic distribution obtained for the document, which amounts to setting p to 0, and the previously introduced copLDA model is obtained by setting p to 0, and fixing the segmentation.", 
        "85": "3.2 Inference with Gibbs sampling  The parameters of the complete model can be directly estimated through Gibbs sampling.", 
        "86": "The Gibbs updates for the parameters \u03c6 and \u03b8 are the same as the ones for standard LDA (Blei et al.,\n2003).", 
        "87": "The parameters fn are directly estimated through: fn \u223c Ber(p).", 
        "88": "Lastly, for the variables z, we follow the same strategy as the one described in (Balikas et al., 2016b) and based on (Amoualian et al., 2016), leading to:\nP (Zs|Z\u2212s,W,\u0398,\u03a6, \u03bb) = p(Zs|\u0398, \u03bb) \u220f\nn\n\u03c6znwn\nwhere W denotes the document collection, and \u0398 and \u03a6 the sets of all \u03b8 and \u03c6k, 1 \u2264 k \u2264 K, vectors.", 
        "89": "p(Zs|\u0398, \u03bb) is obtained by Frank\u2019s copula with parameter \u03bb and marginals Cat(\u03b8d,s,n).", 
        "90": "As is standard in topic models, the notation \u2212s means excluding the information from s.\nFrom the above equation, one can formulate an acceptance/rejection algorithm based on the following steps: (a) sample Zs from p(Zs|\u0398, \u03bb) using Frank\u2019s copula, and (b) accept the sample with probability \u220f n \u03c6 zn wn , where n runs over all the positions in segment s.  3.3 Efficient segmentation  As topics may change from one sentence to another, we assume here that segments cannot overlap sentence boundaries.", 
        "91": "The different segmentations of a document are thus based on its sentence segmentations.", 
        "92": "In the remainder, we use L to denote the maximum length of a segment and g(M ;L) to denote the number of segmentations in a sentence of length M , each segment comprising at most L words.", 
        "93": "Generating all possible segmentations of a sentence and then selecting one at random is not an efficient process as the number of segments rapidly grows with the length of the sentence.", 
        "94": "In practice, however, one can define an efficient segmentation on the basis of the following proposition, the proof of which is given in Appendix A:\nProposition 3.1.", 
        "95": "Let lsi be the random variable associated to the length of the segment starting at position i in a sentence of length M (positions go from 1 to M and lsi takes value in {1, \u00b7 \u00b7 \u00b7 , L}).", 
        "96": "Then P (lsi = l) := g(M+1\u2212i\u2212l);L) g(M+1\u2212i;L) defines a probability distribution over lsi .", 
        "97": "Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations.", 
        "98": "From pos.", 
        "99": "1, repeat till end of sentence: (a) Generate segment length acc.", 
        "100": "to P; (b) Add segment to current segmentation; (c) Move to position after the segment.", 
        "101": "In practice, we thus replace steps 2.b and 2.c of the generative story by a loop over all sentences, and in each sentence use the process described in Prop, 3.1.", 
        "102": "Furthermore, as described in Appendix A, the values of g needed to compute P (lsi = l) can be efficiently computed by recurrence.", 
        "103": "4 Experiments  We conducted a number of experiments aimed at studying the impact of simultaneously segmenting and assigning topics to words within segments using the proposed segLDAcop model.", 
        "104": "Datasets: We considered six publicly available datasets derived from Pubmed2 (Tsatsaronis et al., 2015), Wikipedia (Partalas et al., 2015), Reuters3 and New York Times (NYT)4 (Yao et al., 2016).", 
        "105": "The first two collections were considered in (Balikas et al., 2016a), we followed their setup by considering 3 subsets of Wikipedia with different number of classes (namely, Wiki0, Wiki1 and Wiki2).", 
        "106": "The Reuters dataset comes from Reuters-21578, Distribution 1.0 as investigated in (Bird et al., 2009) and the NYT dataset is collected from full text of New York Times global news, from January 1st to December 31st, 2011.", 
        "107": "These collections were processed following (Blei et al., 2003) by removing a standard list of 50 stop words, lemmatizing, lowercasing and keeping only words made of letters.", 
        "108": "To deal with relatively homogeneous collections, we also removed documents that are too long.", 
        "109": "The statistics of these datasets, as well as the admissible maximal length for documents, in terms of the number of words they contain, can be found in Table 1.", 
        "110": "Settings: We compared our models (segLDAcopp=0, segLDAcop\u03bb=0, segLDAcop) with three models, namely the standard LDA model, and two previously introduced models aiming at binding topics within segments:\n1.", 
        "111": "LDA: Standard Latent Dirichlet Allocation implemented using collapsed Gibbs sampling inference (Griffiths and Steyvers, 2004)5.", 
        "112": "Note\n2https://github.com/balikasg/ topicModelling/tree/master/data\n3https://archive.ics.uci.edu/ ml/datasets/Reuters-21578+Text+ Categorization+Collection\n4https://github.com/yao8839836/COT/ tree/master/data\n5http://gibbslda.sourceforge.net\nthat there are neither segmentation nor topic binding mechanisms in this model;\n2. senLDA: Sentence LDA, introduced in (Balikas et al., 2016a), which forces all words within a sentence to be assigned to the same topic.", 
        "113": "The segments considered thus correspond to sentences, and the binding between topics within segments is maximal as all word specific topics are equal;\n3. copLDA: Copula LDA, introduced in (Balikas et al., 2016b) already discussed before, which relies on two types of segments, namely NPs (extracted with the nltk.chunk package (Bird et al., 2009)) and single words.", 
        "114": "In addition, a copula is also used to bind topics within NPs, from the document specific topic distribution.", 
        "115": "Both senLDA and copLDA implementations, can be found in https://github.com/ balikasg/topicModelling.", 
        "116": "In all models \u03b1 and \u03b2 play a symmetric role and are respectively fixed to 1/K, following (Asuncion et al., 2009).", 
        "117": "For copula based models, \u03bb is set to 5, following (Balikas et al., 2016b).", 
        "118": "As already discussed, p is set to 0 for segLDAcopp=0; it is set to 0.5 for segLDAcop so as not to privilege a priori one topic distribution (document or segment specific) over the other.", 
        "119": "For sampling from Frank\u2019s copula, we relied on the R copula package (Hofert and Maechler, 2011) 6.", 
        "120": "We chose L (the maximum length of a segment) using line search forL \u2208 [2, 5] and used L = 3 in all our experiments.", 
        "121": "Finally, to illustrate the behaviors of the different models with different number of topics, we present here the results obtained with K = 20 and K = 100.", 
        "122": "We now compare the different models along three main dimensions: perplexity, use of topic\n6Our complete code will be available for research purposes.", 
        "123": "representations for classification and topic coherence.", 
        "124": "4.1 Perplexity  We first randomly split here all the collections, using 75% of them for training, and 25% for testing.", 
        "125": "In order to see how well the models fit the data and following (Blei et al., 2003), we first evaluated the methods in terms of perplexity defined as:\nPerplexity = exp\n( \u2212\u2211d\u2208D \u2211 w\u2208d log \u2211K k=1 \u03b8 d k\u03c6\nk w\u2211\nd\u2208D |d|\n) ,\nwhere d is a test document from the test set D, and |d| is the total number of words in d, and K is the total number of topics.", 
        "126": "The lower the perplexity is, the better the model fits the test data.", 
        "127": "Table 2 shows perplexities of different methods for K = 20 and K = 100 topics.", 
        "128": "From Table 2, it comes out that the best performing model in terms of perplexity over all datasets and for different number of topics is segLDAcop.", 
        "129": "Further, segLDAcop\u03bb=0, that uses both document and segment specific topic distributions, performs better than segLDAcopp=0, which in turn outperforms copLDA, bringing evidence that using all possible segmentations rather than only NPs unit extracted using a chunker yields a more flexible and natural topic assignment.", 
        "130": "segLDAcop also converges faster than the other methods to its minimum as it is shown in Figure 2, depicting the evolution of perplexity of different models over the number of iterations on the NYT collection (a similar behavior is observed on the other collections).", 
        "131": "4.2 Topical induced representation for classification  Some studies compare topic models using extrinsic tasks such as document classification.", 
        "132": "In this case, it is possible to reduce the dimensionality of the representation space by using the induced topics (Blei et al., 2003).", 
        "133": "In this study, we first randomly splitted the datasets, except NYT that does not contain class information, into training (75%) and test (25%) sets.", 
        "134": "We then applied SVMs with a linear kernel; the value of the hyperparameter C was found by cross-validation over the training set {0.01, 0.1, 1, 10, 100}.", 
        "135": "For datasets where certain documents have more than one label (Pubmed, Reuters), we used the one-versus-all approach for performing multi-label classification.", 
        "136": "In Table 3, we report the Micro F1 (MiF) score of different models on the test sets.", 
        "137": "Again, the best results are obtained with segLDAcop, followed by segLDAcop\u03bb=0.", 
        "138": "This shows the importance of relying on both document and segment specific topic distributions.", 
        "139": "As conjectured before, our model is able to captures fine grained topic assignments within documents.", 
        "140": "In addition, all models relying on an inferred segmentation (segLDAcopp=0, segLDAcop\u03bb=0, segLDAcop) outperform the models relying on fixed segmentations (sentences or NPs).", 
        "141": "This shows the importance of being able to discover flexible segmentations for assigning topics within documents.", 
        "142": "4.3 Topic coherence  Another common way to evaluate topic models is by examining how coherent the produced topics\nare.", 
        "143": "Doing this manually is a time consuming process and cannot scale.", 
        "144": "To overcome this limitation the task of automatically evaluating the coherence of topics produced by topic models received a lot of attention (Mimno et al., 2011).", 
        "145": "It has been found that scoring the topics using co-occurrence measures, such as the pointwise mutual information (PMI) between the top-words of a topic, correlates well with human judgments (Newman et al., 2010).", 
        "146": "For this purpose an external, large corpus is used as a meta-document where the PMI scores of pairs of words are estimated using a sliding window.", 
        "147": "As discussed above, calculating the co-occurrence measures requires selecting the top-N words of a topic and performing the manual or automatic evaluation.", 
        "148": "Hence, N is a hyper-parameter to be chosen and its value can impact the results.", 
        "149": "Very recently, Lau and Baldwin (2016) showed that N actually impacts the quality of the obtained results and, in particular, the correlation with human judgments.", 
        "150": "In their work, they found that aggregating the topic coherence scores over several topic cardinalities leads to a substantially more stable and robust evaluation.", 
        "151": "Following the findings of Lau and Baldwin (2016) and using (Newman et al., 2010)\u2019s equation, we present in Figure 3 the topic coherence scores as measured by the Normalized Pointwise Mutual Information (NPMI) .", 
        "152": "Their values are in [- 1,1], where in the limit of -1 two words w1 and w2 never occur together, while in the limit of +1 they always occur together (complete co-occurrence).", 
        "153": "For the reported scores, we aggregate the topic coherence scores over three different topic cardinalities: N \u2208 {5, 10, 15}.", 
        "154": "segLDAcop model which\nuses copulas and segmentation together, shows the best score for the given reference meta-data (Wikipedia) in all of the datasets.", 
        "155": "It should be noted that segLDAcop\u03bb=0 which has not copula binder inside the model has less improvement against the segLDAcopp=0 which has the copula.", 
        "156": "This means using copula has more effect on the topic coherence than only the segment-specific topic distribution.", 
        "157": "4.4 Visualization  In order to illustrate the results obtained by segLDAcop, we display in Figure 4 the top 10 most probable words over 5 topics (K = 20) for the Reuters dataset, for both segLDAcop and LDA.", 
        "158": "In segLDAcop, topic 1, the top-ranked words are mostly relevant to the topic \u201cdate\u201d (e.g.,\nmarch, january, year, fall, february, week).", 
        "159": "However, a similar topic learned by LDA appears to involve less such words (year, january, february), indicating a less coherent topic.", 
        "160": "Figure 5 illustrates another aspect of our model, namely the possibility to detect topically coherent segments.", 
        "161": "In particular, as one can note, the sentence is segmented in six parts by our model, the first one is a NP, Ralph Borsodi where one single topic is assigned to both words.", 
        "162": "We observe a similar coherence in topic assignments on other NPs and segments, in which a single topic is used for the words involved.", 
        "163": "The data-driven approach we have adopted here can discover such fine grained differences, something the approaches based on fixed segmentations (either based on sentences or NPs), are less likely to achieve.", 
        "164": "5 Discussion  In this paper, we have introduced an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words.", 
        "165": "The coherence between topics is ensured through Frank\u2019s copula, that binds the topics associated to the words of a segment.", 
        "166": "In addition, this model relies on both document and segment specific topic distributions so as to capture fine grained differences in topic assignments.", 
        "167": "We have shown that this model naturally encompasses other state-of-the-art LDA-based models proposed to accomplish the same task, and that it outperforms these models over six publicly available collections in terms of perplexity, Normalized Pointwise Mutual Information (NPMI), a measure used to assess the coherence of topics with documents, and the Micro F1-measure in a text classification context.", 
        "168": "Our results confirm the importance of a flexible segmentation as well as a\nbinding mechanism to produce topically coherent segments.", 
        "169": "As regards complexity, it is true that more complex models, as the one we are considering, are more prone to underfitting (when data is scarce) and overfitting than simpler models.", 
        "170": "This said, the experimental results on perplexity (in which the word-topic distributions are fixed) and on classification (based on the topical induced representations) suggest that our model neither underfits nor overfits compared to simpler models.", 
        "171": "We believe that this is due to the fact that the main additional parameters in our model (the segment specific topic distribution) do not really add complexity as they are drawn from the same distribution as the standard document specific topics.", 
        "172": "Furthermore, the parameters p and f are simple parameters to choose between these two distributions.", 
        "173": "The comparison with other segmentation methods is also an important point.", 
        "174": "While state-of-theart supervised segmentation models can be used before applying the LDA model, we note such a pipeline approach comes with several limitations.", 
        "175": "The approach requires external annotated data to train the segmentation models, where certain domain and language specific information need to be captured.", 
        "176": "By contrast, our unsupervised approach learns both segmentations and topics jointly in a domain and language independent manner.", 
        "177": "Furthermore, existing supervised segmentation models are largely designed for a very different purpose with strong linguistic motivations, which may not align well with our main goal in this paper which is improving topic coherence in topic modeling.", 
        "178": "Similarly, unsupervised approaches, used for example in the TDT (Topic Detection and Tracking) campaigns or more recently in Du et al.", 
        "179": "(2013), usually consider coarse-grained topics, that can encompass several sentences.", 
        "180": "In contrast, our approach aims at identifying fine-grained topics associated with coherent segments that do not overlap sentence boundaries.", 
        "181": "These considerations, explain the choice of the baselines retained: they are based on segments of different granularities (words, NPs, sentences) that do not overlap sentence boundaries.", 
        "182": "In the future, we plan on relying on other inference approaches, based for example on variational Bayes known to yield better estimates for perplexity (Asuncion et al., 2009); it is however not certain that the gain in perplexity one can expect from the use of variational Bayes approaches will nec-\nessarily result in a gain in, say, topic coherence.", 
        "183": "Indeed, the impact of the inference approach on the different usages of latent topic models for text collections remains to be better understood.", 
        "184": "Acknowledgments  We would like to thank the reviewers for their helpful comments.", 
        "185": "Most of this work was done when Hesam Amoualian was visiting Singapore University of Technology and Design.", 
        "186": "This work is supported by MOE Tier 1 grant SUTDT12015008, also partly supported by the LabEx PERSYVAL-Lab ANR-11-LABX-0025.", 
        "187": "A Efficient segmentation  Let us recall the property presented before:\nProposition A.1.", 
        "188": "Let lsi be the random variable associated to the length of the segment starting at position i in a sentence of length M (positions go from 1 to M and lsi takes value in {1, \u00b7 \u00b7 \u00b7 , L}).", 
        "189": "Then P (lsi = l) := g(M+1\u2212i\u2212l);L) g(M+1\u2212i;L) defines a probability distribution over lsi .", 
        "190": "Furthermore, the following process is equivalent to choosing sentence segmentations uniformly from the set of possible segmentations.", 
        "191": "From pos.", 
        "192": "1, repeat till end of sentence: (a) Generate segment length acc.", 
        "193": "to P; (b) Add segment to current segmentation; (c) Move to position after the segment.", 
        "194": "Proof Any segmentation of the sentence of length M starts with either a segment of length 1, a segment of length 2, \u00b7 \u00b7 \u00b7 , or a segment of length L. Thus, g(M ;L) can be defined through the following recurrence relation:\ng(M ;L) = L\u2211\nl=1\ng(M \u2212 l;L) (1)\ntogether with the initial values g(1;L), g(2;L), \u00b7 \u00b7 \u00b7 , g(L;L), which can be computed offline (for example, for L = 3, one has: g(1; 3) = 1, g(2; 3) = 2, g(3; 3) = 4).", 
        "195": "Note that g(1;L) = 1 for all L.\nThus:\nL\u2211\nl=1\nP (lsi = l) = L\u2211\nl=1\ng(M + 1\u2212 i\u2212 l);L) g(M + 1\u2212 i;L) = 1\ndue to the recurrence relation on g. This proves the first part of the proposition.", 
        "196": "Using the process described above where segments are generated one after another according to P , for a segmentation S, comprising |S| segments, let us denote by l1, l2, \u00b7 \u00b7 \u00b7 , l|S| the lengths of each segment and by i1, i2, \u00b7 \u00b7 \u00b7 , i|S| the starting positions of each segment (with i1 = 1).", 
        "197": "One has, as segments are independent of each other:\nP (S) =\n|S|\u220f\nj=1\nP (lsij = lj) =\n|S|\u220f\nj=1\ng(M + 1\u2212 (ij + lj);L) g(M + 1\u2212 ij ;L)\n= g(M \u2212 l1;L) g(M ;L) g(M \u2212 l1 \u2212 l2;L) g(M \u2212 l1;L) \u00b7 \u00b7 \u00b7 = 1 g(M ;L)\nas g(1;L) = 1.", 
        "198": "This concludes the proof of the proposition.", 
        "199": "2\nFurthermore, as one can note from Eq.", 
        "200": "1, the various elements needed to compute P (lsi = l) can be efficiently computed, the time complexity being equal to O(M).", 
        "201": "In addition, as the number of different sentence lengths is limited, one can store the values of g to reuse them during the segmentation phase."
    }, 
    "document_id": "P17-1165.pdf.json"
}
