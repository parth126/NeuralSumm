{
    "abstract_sentences": {
        "1": "Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements.", 
        "2": "Specifically, we linearize parse trees of source sentences to obtain structural label sequences.", 
        "3": "On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed.", 
        "4": "Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy.", 
        "5": "It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points.", 
        "6": "Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688\u2013697 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1064  1 Introduction  Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT)\n\u2217Work done at Huawei Noah\u2019s Ark Lab, HongKong.", 
        "2": "on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).", 
        "3": "However, Shi et al.", 
        "4": "(2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus.", 
        "5": "Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT.", 
        "6": "As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax.", 
        "7": "In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy.", 
        "8": "In principle, syntax is a promising avenue for translation modeling.", 
        "9": "This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008;\n688\nShen et al., 2008; Li et al., 2013).", 
        "10": "While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax.", 
        "11": "Figure 1 (a) shows a Chinese-to-English translation example of NMT.", 
        "12": "In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., \u65b0 \u751f/xinsheng \u94f6\u884c/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence.", 
        "13": "Statistics on our development set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT translation.1 Figure 1 (b) shows another example with over translation, where the noun phrase \u4e24/liang \u4e2a/ge \u5973\u5b69/nvhai is translated twice in English.", 
        "14": "Similar to discontinuous translation, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence.", 
        "15": "In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation.", 
        "16": "Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general.", 
        "17": "Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information.", 
        "18": "On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model.", 
        "19": "Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy.", 
        "20": "2 Attention-based NMT  As a background and a baseline, in this section, we briefly describe the NMT model with an attention mechanism by Bahdanau et al.", 
        "21": "(2015), which mainly consists of an encoder and a decoder, as shown in Figure 2.", 
        "22": "Encoder The encoding of a source sentence is for1Manually examining 200 random such discontinuously translated noun phrases, we find that 90% of them should be continuously translated according to the reference translation.", 
        "23": "mulated using a pair of neural networks, i.e., two recurrent neural networks (denoted bi-RNN): one reads an input sequence x = (x1, ..., xm) from left to right and outputs a forward sequence of hidden states ( \u2212\u2192 h1, ..., \u2212\u2192 hm), while the other operates from right to left and outputs a backward sequence ( \u2190\u2212 h1, ..., \u2190\u2212 hm).", 
        "24": "Each source word xj is represented as hj (also referred to as word annotation vector): the concatenation of hidden states \u2212\u2192 hj and \u2190\u2212 hj .", 
        "25": "Such bi-RNN encodes not only the word itself but also its left and right context, which can provide important evidence for its translation.", 
        "26": "Decoder The decoder is also an RNN that predicts a target sequence y = (y1, ..., yn).", 
        "27": "Each target word yi is predicted via a multi-layer perceptron (MLP) component which is based on a recurrent hidden state si, the previous predicted word yi\u22121, and a source-side context vector ci.", 
        "28": "Here, ci is calculated as a weighted sum over source annotation vectors (h1, ..., hm).", 
        "29": "The weight vector \u03b1i \u2208 Rm over source annotation vectors is obtained by an attention model, which captures the correspondences between the source and the target languages.", 
        "30": "The attention weight \u03b1ij is computed based on the previous recurrent hidden state si\u22121 and source annotation vector hj .", 
        "31": "3 NMT with Source Syntax  The conventional NMT models treat a sentence as a sequence of words and ignore external knowledge, failing to effectively capture various kinds of inherent structure of the sentence.", 
        "32": "To leverage external knowledge, specifically the syntax in the source side, we focus on the parse tree of a sentence and propose three different NMT models that explicitly consider the syntactic structure into encoding.", 
        "33": "Our purpose is to inform the NMT model the structural context of each word in its corresponding parse tree with the goal that the learned annotation vectors (h1, ..., hm) encode not\nI love dogs\nonly the information of words and their surroundings, but also structural context in the parse tree.", 
        "34": "In the rest of this section, we use English sentences as examples to explain our methods.", 
        "35": "3.1 Syntax Representation  To obtain the structural context of a word in its parse tree, ideally the model should not only capture and remember the whole parse tree structure, but also discriminate the contexts of any two different words.", 
        "36": "However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence.", 
        "37": "For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order.", 
        "38": "Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2\n2We have also tried to include the ending brackets in the structural label sequence, as what (Vinyals et al., 2015; Choe\nhwj and\n\u2190\u2212\u2212\nhwj are the forward and\nbackward hidden states for word wj ,\n\u2212\u2192\nhli and\n\u2190\u2212\nhli are for structural label li, ewj is the word embedding for word wj , and \u2295 is for concatenation operator.", 
        "39": "There is no doubt that the structural label sequence is much longer than its word sequence.", 
        "40": "In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi\u2019s part-of-speech (POS) tag in the label sequence and view the tag\u2019s annotation vector as wi\u2019s label annotation vector.", 
        "41": "This is because wi\u2019s POS tag location can also represent wi\u2019s location in the parse tree.", 
        "42": "For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1.", 
        "43": "Likewise, w2 maps to l5 and w3 to l7.", 
        "44": "That is to say, we use l3\u2019s learned annotation vector as w1\u2019s label annotation vector.", 
        "45": "and Charniak, 2016) do.", 
        "46": "However, the performance gap is very small by adding the ending brackets or not.", 
        "47": "3.2 RNN Encoders with Source Syntax  In the next, we first propose two different encoders to augment word annotation vector with its corresponding label annotation vector, each of which consists of two RNNs 3: in one encoder, the two RNNs work independently (i.e., Parallel RNN Encoder) while in another encoder the two RNNs work in a hierarchical way (i.e., Hierarchical RNN Encoder).", 
        "48": "The difference between the two encoders lies in how the two RNNs interact.", 
        "49": "Then, we propose the third encoder with a single RNN, which learns word and label annotation vectors stitchingly (i.e., Mixed RNN Encoder).", 
        "50": "Since any of the above three approaches focuses only on the encoder as to generate source annotation vectors along with structural information, we keep the rest part of the NMT models unchanged.", 
        "51": "Parallel RNN Encoder Figure 4 (a) illustrates our Parallel RNN encoder, which includes two parallel RNNs: i.e., a word RNN and a structural label RNN.", 
        "52": "On the one hand, the word RNN, as in conventional NMT models, takes a word sequence as input and output a word annotation vector for each word.", 
        "53": "On the other hand, the structural label RNN takes the structural label sequence of the word sequence as input and obtains a label annotation vector for each label.", 
        "54": "Besides, we concatenate each word\u2019s word annotation vector and its POS tag\u2019s label annotation vector as the final annotation vector for the word.", 
        "55": "For example, the final annotation vector for word love in Figure 4 (a) is [ \u2212\u2212\u2192 hw2; \u2190\u2212\u2212 hw2; \u2212\u2192 hl5; \u2190\u2212 hl5], where the first two subitems [ \u2212\u2212\u2192 hw2; \u2190\u2212\u2212 hw2] are the word annotation vector and the rest two subitems [ \u2212\u2192 hl5; \u2190\u2212 hl5] are its POS tag VBP\u2019s label annotation vector.", 
        "56": "Hierarchical RNN Encoder Partially inspired by the model architecture of GNMT (Wu et al., 2016) which consists of multiple layers of LSTM RNNs, we propose a two-layer model architecture in which the lower layer is the structural label RNN while the upper layer is the word RNN, as shown in Figure 4 (b).", 
        "57": "We put the word RNN in the upper layer because each item in the word sequence can map into an item in the structural label sequence, while this does not hold if the order of the two RNNs is reversed.", 
        "58": "As shown in Figure 4 (b), for example, the POS tag VBP\u2019s label annotation vector [ \u2212\u2192 hl5, \u2190\u2212 hl5] is concatenated with word\n3Hereafter, we simplify bi-RNN as RNN.", 
        "59": "love\u2019s word embedding ew2 to feed as the input to the word RNN.", 
        "60": "Mixed RNN Encoder Figure 5 presents our Mixed RNN encoder.", 
        "61": "Similarly, the sequence of input is the linearization of its parse tree (as in Figure 3 (b)) following a depth-first traversal order, but being mixed with both words and structural labels in a stitching way.", 
        "62": "It shows that the RNN learns annotation vectors for both the words and the structural labels, though only the annotation vectors of words are further fed to decoding (e.g., ([ \u2212\u2192 h4, \u2190\u2212 h4], [ \u2212\u2192 h7, \u2190\u2212 h7], [ \u2212\u2192 h10, \u2190\u2212 h10])).", 
        "63": "Even though the annotation vectors of structural labels are not directly fed forward for decoding, the error signal is back propagated along the word sequence and allows the annotation vectors of structural labels being updated accordingly.", 
        "64": "3.3 Comparison of RNN Encoders with Source Syntax  Though all the three encoders model both word sequence and structural label sequence, the differences lie in their respective model architecture with respect to the degree of coupling the two sequences:\n\u2022 In the Parallel RNN encoder, the word RNN and structural label RNN work in a parallel way.", 
        "65": "That is to say, the error signal back propagated from the word sequence would not affect the structural label RNN, and vice versa.", 
        "66": "In contrast, in the Hierarchical RNN encoder, the error signal back propagated from the word sequence has a direct impact on the structural label annotation vectors, and thus on the structural label embeddings.", 
        "67": "Finally, the Mixed RNN encoder ties the structural label sequence and word sequence together in the closest way.", 
        "68": "Therefore, the degrees of coupling the word and structural\nlabel sequences in these three encoders are like this: Mixed RNN encoder > Hierarchical RNN encoder > Parallel RNN encoder.", 
        "69": "\u2022 Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest.", 
        "70": "Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence.", 
        "71": "Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders.", 
        "72": "4 Experimentation  We have presented our approaches to incorporating the source syntax into NMT encoders.", 
        "73": "In this section, we evaluate their effectiveness on Chinese-to-English translation.", 
        "74": "4.1 Experimental Settings  Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4 We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5 To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6 (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005).", 
        "75": "We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.", 
        "76": "For efficient training of neural networks, we limit the maximum sentence length on both source and target sides to 50.", 
        "77": "We also limit both the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8% and 98.2% of the two corpora respectively.", 
        "78": "All the out-of-vocabulary words are mapped to a special token UNK.", 
        "79": "Besides, the word embedding dimension is 620 and the size of a hidden layer is 1000.", 
        "80": "All the other settings are the same as in Bahdanau et al.(2015).", 
        "81": "4The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.", 
        "82": "5http://www.itl.nist.gov/iad/mig/ tests/mt/\n6https://github.com/slavpetrov/ berkeleyparser\nThe inventory of structural labels includes 16 phrase labels and 32 POS tags.", 
        "83": "In both our Parallel RNN encoder and Hierarchical RNN encoder, we set the embedding dimension of these labels as 100 and the size of a hidden layer as 100.", 
        "84": "Besides, the maximum structural label sequence length is set to 100.", 
        "85": "In our Mixed RNN encoder, since we only have one input sequence, we equally treat the structural labels and words (i.e., a structural label is also initialized with 620 dimension embedding).", 
        "86": "Compared to the baseline NMT model, the only different setting is that we increase the maximum sentence length on source-side from 50 to 150.", 
        "87": "We compare our method with two state-of-theart models of SMT and NMT:\n\u2022 cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7\n\u2022 RNNSearch: a re-implementation of the attentional NMT system (Bahdanau et al., 2015) with slight changes taken from dl4mt tutorial.8 For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b).", 
        "88": "It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly generated word.", 
        "89": "We use AdaDelta (Zeiler, 2012) to optimize model parameters in training with the mini-batch size of 80.", 
        "90": "For translation, a beam search with size 10 is employed.", 
        "91": "4.2 Experiment Results  Table 1 shows the translation performances measured in BLEU score.", 
        "92": "Clearly, all the proposed NMT models with source syntax improve the translation accuracy over all test sets, although there exist considerable differences among different variants.", 
        "93": "Parameters The three proposed models introduce new parameters in different ways.", 
        "94": "As a baseline model, RNNSearch has 60.6M parameters.", 
        "95": "Due to the infrastructure similarity, the Parallel RNN system and the Hierarchical RNN system introduce\n7https://github.com/redpony/cdec 8https://github.com/nyu-dl/\ndl4mt-tutorial\nthe similar size of additional parameters, resulting from the RNN model for structural label sequences (about 0.1M parameters) and catering either the augmented annotation vectors (as shown in Figure 4 (a)) or the augmented word embeddings (as shown in Figure 4 (b)) (the remain parameters).", 
        "96": "It is not surprising that the Mixed RNN system does not require any additional parameters since though the input sequence becomes longer, we keep the vocabulary size unchanged, resulting in no additional parameters.", 
        "97": "Speed Introducing the source syntax slightly slows down the training speed.", 
        "98": "When running on a single GPU GeForce GTX 1080, the baseline model speeds 153 minutes per epoch with 14K updates while the proposed structural label RNNs in both Parallel RNN and Hierarchical RNN systems only increases the training time by about 6% (thanks to the small size of structural label embeddings and annotation vectors), and the Mixed RNN system spends 26% more training time to cater the triple sized input sequence.", 
        "99": "Comparison with the baseline NMT model (RNNSearch) While all the three proposed NMT models outperform RNNSearch, the Parallel RNN system and the Hierarchical RNN system achieve similar accuracy (e.g., 36.6 v.s.", 
        "100": "36.7).", 
        "101": "Besides, the Mixed RNN system achieves the best accuracy overall test sets with the only exception of NIST MT 02.", 
        "102": "Over all test sets, it outperforms RNNSearch by 1.4 BLEU points and outperforms the other two improved NMT models by 0.3\u223c0.4 BLEU points, suggesting the benefits of high degree of coupling the word sequence and the structural label sequence.", 
        "103": "This is very encouraging since the Mixed RNN encoder is the simplest, without introducing new parameters and with only slight additional training time.", 
        "104": "Comparison with the SMT model (cdec) Table 1 also shows that all NMT systems outperform the SMT system.", 
        "105": "This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).", 
        "106": "5 Analysis  As the proposed Mixed RNN system achieves the best performance, we further look at the RNNSearch system and the Mixed RNN system to explore more on how syntactic information helps in translation.", 
        "107": "5.1 Effects on Long Sentences  Following Bahdanau et al.", 
        "108": "(2015), we group sentences of similar lengths together and compute BLEU scores.", 
        "109": "Figure 6 presents the BLEU scores over different lengths of input sentences.", 
        "110": "It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths.", 
        "111": "It also shows that the performance drops substantially\nwhen the length of input sentences increases.", 
        "112": "This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a).", 
        "113": "We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec).", 
        "114": "We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences.", 
        "115": "5.2 Analysis on Word Alignment  Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment.", 
        "116": "To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs.", 
        "117": "We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations.", 
        "118": "To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2.", 
        "119": "Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word.", 
        "120": "5.3 Analysis on Phrase Alignment  The above subsection examines the alignment performance at the word level.", 
        "121": "In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit.", 
        "122": "Given a source phrase XP, we use word alignments to examine if the phrase is translated continuously (Cont.", 
        "123": "), or dis-\n9Though the maximum source length limit in Mixed RNN system is set to 150, it approximately contains 50 words in maximum.", 
        "124": "continuously (Dis.", 
        "125": "), or if it is not translated at all (Un.).", 
        "126": "There are some phrases, such as noun phrases (NPs), prepositional phrases (PPs) that we usually expect to have a continuous translation.", 
        "127": "With respect to several such types of phrases, Table 3 shows how these phrases are translated.", 
        "128": "From the table, we see that translations of RNNSearch system do not respect source syntax very well.", 
        "129": "For example, in RNNSearch translations, 57.3%, 33.6%, and 9.1% of PPs are translated continuously, discontinuously, and untranslated, respectively.", 
        "130": "Fortunately, our Mixed RNN system is able to have more continuous translation for those phrases.", 
        "131": "Table 3 also suggests that there is still much room for NMT to show more respect to syntax.", 
        "132": "5.4 Analysis on Over Translation  To estimate the over translation generated by NMT, we propose ratio of over translation (ROT):\nROT =\n\u2211 wi t(wi)\n|w| (1)\nwhere |w| is the number of words in consideration, t(wi) is the times of over translation for word wi.", 
        "133": "Given a word w and its translation e = e1e2 .", 
        "134": ".", 
        "135": ".", 
        "136": "en, we have:\nt(w) = |e| \u2212 |uniq(e)| (2)\nwhere |e| is the number of words in w\u2019s translation e, while |uniq(e)| is the number of unique words in e. For example, if a source word \u9999\n\u6e2f/xiangkang is translated as hong kong hong kong, we say it being over translated 2 times.", 
        "137": "Table 4 presents ROT grouped by some typical POS tags.", 
        "138": "It is not surprising that RNNSearch system has high ROT with respect to POS tags of NR (proper noun) and CD (cardinal number): this is due to the fact that the two POS tags include high percentage of unknown words which tend to be translated multiple times in translation.", 
        "139": "Words of DT (determiner) are another source of over translation since they are usually translated to multiple the in English.", 
        "140": "It also shows that by introducing source syntax, Mixed RNN system alleviates the over translation issue by 18%: ROT drops from 5.5% to 4.5%.", 
        "141": "5.5 Analysis on Rare Word Translation  We analyze the translation of source-side rare words that are mapped to a special token UNK.", 
        "142": "Given a rare word w, we examine if it is translated into a non-UNK word (non-UNK), UNK (UNK), or if it is not translated at all (Un.).", 
        "143": "Table 5 shows how source-side rare words are translated.", 
        "144": "The four POS tags listed in the table account for about 90% of all rare words in the test sets.", 
        "145": "It shows that in Mixed RNN system is more likely to translate source-side rare words into UNK on the target side.", 
        "146": "This is reasonable since the source side rare words tends to be translated into rare words in the target side.", 
        "147": "Moreover, it is hard to obtain its correct non-UNK translation when a source-side rare word is replaced as UNK.", 
        "148": "Note that our approach is compatible with with approaches of open vocabulary.", 
        "149": "Taking the sub-\nword approach (Sennrich et al., 2016) as an example, for a word on the source side which is divided into several subword units, we can synthesize subPOS nodes that cover these units.", 
        "150": "For example, if misunderstand/VB is divided into units of mis and understand, we construct substructure (VB (VB-F mis) (VB-I understand)).", 
        "151": "6 Related Work  While there has been substantial work on linguistically motivated SMT, approaches that leverage syntax for NMT start to shed light very recently.", 
        "152": "Generally speaking, NMT can provide a flexible mechanism for adding linguistic knowledge, thanks to its strong capability of automatically learning feature representations.", 
        "153": "Eriguchi et al.", 
        "154": "(2016) propose a tree-tosequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes.", 
        "155": "They also allow the attention model to align target words to non-terminal nodes.", 
        "156": "Our approach is similar to theirs by using source-side phrase parse tree.", 
        "157": "However, our Mixed RNN system, for example, incorporates syntax information by learning annotation vectors of syntactic labels and words stitchingly, but is still a sequenceto-sequence model, with no extra parameters and with less increased training time.", 
        "158": "Sennrich and Haddow (2016) define a few linguistically motivated features that are attached to each individual words.", 
        "159": "Their features include lemmas, subword tags, POS tags, dependency labels, etc.", 
        "160": "They concatenate feature embeddings with word embeddings and feed the concatenated em-\nbeddings into the NMT encoder.", 
        "161": "On the contrast, we do not specify any feature, but let the model implicitly learn useful information from the structural label sequence.", 
        "162": "Shi et al.", 
        "163": "(2016) design a few experiments to investigate if the NMT system without external linguistic input is capable of learning syntactic information on the source-side as a by-product of training.", 
        "164": "However, their work is not focusing on improving NMT with linguistic input.", 
        "165": "Moreover, we analyze what syntax is disrespected in translation from several new perspectives.", 
        "166": "Garc\u0131\u0301a-Mart\u0131\u0301nez et al.", 
        "167": "(2016) generalize NMT outputs as lemmas and morphological factors in order to alleviate the issues of large vocabulary and out-of-vocabulary word translation.", 
        "168": "The lemmas and corresponding factors are then used to generate final words in target language.", 
        "169": "Though they use linguistic input on the target side, they are limited to the word level features.", 
        "170": "Phrase level, or even sentence level linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time.", 
        "171": "7 Conclusion  In this paper, we have investigated whether and how source syntax can explicitly help NMT to improve its translation accuracy.", 
        "172": "To obtain syntactic knowledge, we linearize a parse tree into a structural label sequence and let the model automatically learn useful information through it.", 
        "173": "Specifically, we have described three different models to capture the syntax knowledge, i.e., Parallel RNN, Hierarchical RNN, and Mixed RNN.", 
        "174": "Experimentation on Chinese-to-English translation shows that all proposed models yield improvements over a state-ofthe-art baseline NMT system.", 
        "175": "It is also interesting to note that the simplest model (i.e., Mixed RNN) achieves the best performance, resulting in obtaining significant improvements of 1.4 BLEU points on NIST MT 02 to 05.", 
        "176": "In this paper, we have also analyzed the translation behavior of our improved system against the state-of-the-art NMT baseline system from several perspectives.", 
        "177": "Our analysis shows that there is still much room for NMT translation to be consistent with source syntax.", 
        "178": "In our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syn-\ntactic features (e.g., features showing the syntactic role that a word is playing) for NMT, and employing the source syntax to constrain and guild the attention models.", 
        "179": "Acknowledgments  The authors would like to thank three anonymous reviewers for providing helpful comments, and also acknowledge Xing Wang, Xiangyu Duan, Zhengxian Gong for useful discussions.", 
        "180": "This work was supported by National Natural Science Foundation of China (Grant No.", 
        "181": "61525205, 61331011, 61401295)."
    }, 
    "document_id": "P17-1064.pdf.json"
}
