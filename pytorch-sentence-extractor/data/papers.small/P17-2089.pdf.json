{
    "abstract_sentences": {
        "1": "Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance.", 
        "2": "Recently Neural Machine Translation (NMT) has become prominent in the field.", 
        "3": "However, most of the existing domain adaptation methods only focus on phrase-based machine translation.", 
        "4": "In this paper, we exploit the NMT\u2019s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data.", 
        "5": "The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560\u2013566 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2089  1 Introduction  Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016).", 
        "2": "An ever increasing amount of data is becoming available for NMT training.", 
        "3": "However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance.", 
        "4": "Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015).", 
        "5": "To the best of our knowledge, there are only\na few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016).", 
        "6": "Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows.", 
        "7": "For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained.", 
        "8": "These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015).", 
        "9": "Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation.", 
        "10": "Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed.", 
        "11": "The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data.", 
        "12": "Empirical results show their method can improve NMT performance, and this approach provides a natural baseline.", 
        "13": "For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores.", 
        "14": "A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima\u2019an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016).", 
        "15": "These methods select useful sentences from the whole corpus, so they can be directly applied to NMT.", 
        "16": "However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural\n560\ntranslation process.", 
        "17": "For NMT sentences selection, our hypothesis is that the NMT system itself can be used to score each sentence in the training data.", 
        "18": "Specifically, an NMT system embeds the source sentence into a vector representation1 and we can use these vectors to measure a sentence pair\u2019s similarity to the in-domain corpus.", 
        "19": "In comparison with the CNN or other sentence embedding methods, this method can directly make use of information induced by the NMT system information itself.", 
        "20": "In addition, the proposed sentence selection method can be used in conjunction with the NMT further training method (Luong and Manning, 2015).", 
        "21": "2 NMT Background  An attention-based NMT system uses a Bidirectional RNN (BiRNN) as an encoder and a decoder that emulates searching through a source sentence during decoding (Bahdanau et al., 2015).", 
        "22": "The encoder\u2019s BiRNN consists of forward and backward RNNs.", 
        "23": "Each word xi is represented by concatenating the forward hidden state \u2212\u2192 hi and the backward one \u2190\u2212 hi as hi = [ \u2212\u2192 hi ; \u2190\u2212 hi ] >.", 
        "24": "In this way, the source sentence X = {x1, ..., xTx} can be represented as annotations H = {h1, ..., hTx}.", 
        "25": "In the decoder, an RNN hidden state sj for time j is computed by:\nsj = f(sj\u22121, yj\u22121, cj).", 
        "26": "(1)\nThe context vector cj is then, computed as a weighted sum of these annotations H = {h1, ..., hTx}, by using alignment weight \u03b1ji:\ncj =\nTx\u2211\nj=1\n\u03b1jihi.", 
        "27": "(2)  3 Sentence Embedding and Selection    3.1 Sentence Embedding  A source sentence can be represented as the annotations H. However the length of H depends on the sentence length Tx.", 
        "28": "To represent a sentence as a fixed-length vector, we adopt the initial hidden\n1Li et al.", 
        "29": "(2016)\u2019s fine-tuned NMT systems apply a similar sentence representation.", 
        "30": "In comparison, we adopt a transition layer between the source and target layers and don\u2019t use test data.", 
        "31": "layer state sinit for the decoder as this vector:\nsinit(X) = tanh(W \u2211Tx i=1 hi Tx\n+ b), hi \u2208 H, (3)\nwhere an average pooling layer averages the annotation hi for each source word into a fixedlength source sentence vector, and a nonlinear transition layer (weights W and bias b are jointly trained with all the other components of NMT system) transforms this embedded source sentence vector into the initial hidden state sinit for the decoder (Bahdanau et al., 2015).", 
        "32": "3.2 Sentence Selection  We employ the data selection method, which is inspired by (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013).", 
        "33": "As Axelrod et al.", 
        "34": "(2011) mentioned, there are some pseudo in-domain data in out-of-domain data, which are close to in-domain data.", 
        "35": "Our intuition is to select the sentences whose embeddings are similar to the average in-domain ones, while being dis-similar to the average out-of-domain ones:\n\u2022 1) We train a French-to-English NMT system NFE using the in-domain and out-of-domain data together as training data.2\n\u2022 2) Each sentence f in the training data F (both in-domain Fin and out-of-domain Fout) is embedded as a vector vf = sinit(f) by using NFE.", 
        "36": "\u2022 3) The sentence pairs (f, e) in the outof-domain corpus Fout are classified into two sets: the sentences close to in-domain sentences, and those that are distant.", 
        "37": "That is, we firstly calculate the vector centers of in-domain CFin and out-of-domain CFout corpora, respectively.", 
        "38": "CFin = \u2211 f\u2208Fin vf |Fin| ,\nCFout = \u2211 f\u2208Fout vf |Fout| .", 
        "39": "(4)\nThen we measure the Euclidean distance d between each sentence vector vf and in-domain\n2It is possible to use a sample of the out-of-domain data.", 
        "40": "In this paper, we use all of them.", 
        "41": "vector center CFin as d(vf , CFin) and out-ofdomain vector center CFout as d(vf , CFout), respectively.", 
        "42": "We use the difference \u03b4 of these two distances to classify each sentence:\n\u03b4f = d(vf , CFin)\u2212 d(vf , CFout).", 
        "43": "(5)\nBy using an English-to-French NMT system NEF, we can obtain a target sentence embedding ve, in-domain target vector center CEin and out-of-domain target vector center CEout .", 
        "44": "Corresponding distance difference \u03b4e is,\n\u03b4e = d(ve, CEin)\u2212 d(ve, CEout).", 
        "45": "(6)\n\u03b4f , \u03b4e and \u03b4fe = \u03b4f + \u03b4e can be used to select sentences.", 
        "46": "That is, the sentence pairs (f, e) with \u03b4f (or \u03b4e, \u03b4fe) less than a threshold are the new selected in-domain corpus.", 
        "47": "This threshold is tuned by using the development data.", 
        "48": "4 Experiments    4.1 Data sets  The proposed methods were evaluated on two data sets as shown in Table 1.", 
        "49": "\u2022 IWSLT 2014 English (EN) to French (FR) corpus3 was used as in-domain training data and dev2010 and test2010/2011 (Cettolo et al., 2014), were selected as development (dev) and test data, respectively.", 
        "50": "Outof-domain corpora contained Common Crawl, Europarl v7, News Commentary v10 and United Nation (UN) EN-FR parallel corpora.4\n\u2022 NIST 2006 Chinese (ZH) to English corpus5 was used as the in-domain training corpus, following the settings of (Wang et al., 2014).", 
        "51": "Chinese-to-English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data set were used as out-ofdomain data.", 
        "52": "NIST MT 2002-2004 and NIST MT 2005/2006 were used as the development and test data, respectively.", 
        "53": "We are aware of that there are additional NIST corpora in a similar domain, but because this task was for domain adaptation, we only selected a small subset, which is mainly focused on news and\n3https://wit3.fbk.eu/mt.php?release=2014-01 4http://statmt.org/wmt15/translation-task.html 5http://www.itl.nist.gov/iad/mig/tests/mt/2006/\nblog texts.", 
        "54": "The statistics on data sets were shown in Table 1.", 
        "55": "These adaptation corpora settings were nearly the same as that used in (Wang et al., 2016).", 
        "56": "The differences were:\n\u2022 For IWSLT, they chose FR-EN translation task, which is popular in PBSMT.", 
        "57": "We chose EN-FR, which is more popular in NMT;\n\u2022 For NIST, they chose 02-05 as dev set, and we chose 02-04.", 
        "58": "Because we would report results on two test sets (MT05 and MT06) in comparison with only one (MT06).", 
        "59": "4.2 NMT System  We implemented the proposed method in Groundhog6 (Bahdanau et al., 2015), which is one of the state-of-the-art NMT frameworks.", 
        "60": "The default settings of Groundhog were applied for all NMT systems: the word embedding dimension was 620 and the size of a hidden layer was 1000, the batch size was 64, the source and target side vocabulary sizes were 30K, the maximum sequence length were 50, and the beam size for decoding was 10.", 
        "61": "Default dropout were applied.", 
        "62": "We used a mini-batch Stochastic Gradient Descent (SGD) algorithm together with ADADELTA optimizer (Zeiler, 2012).", 
        "63": "Training was conducted on a single Tesla K80 GPU.", 
        "64": "Each NMT model was trained for 500K batches, taking 7-10 days.", 
        "65": "For sentence embedding and selection, it only took several hours to process all of sentences in the training data, because decoding was not necessary.", 
        "66": "6https://github.com/lisa-groundhog/ GroundHog  4.3 Baselines  Along with the standard NMT baseline system, we also compared the proposed methods to the recent state-of-the-art NMT adaptation method of Luong and Manning (2015)7 as described in Section 1.", 
        "67": "Two typical sentence selection methods for PBSMT were also used as baselines: Axelrod et al.", 
        "68": "(2011) used language model-based crossentropy difference as criterion; Chen et al.", 
        "69": "(2016) used a CNN to classify the sentences as either in-domain or out-of-domain.", 
        "70": "In addition, we randomly sampled out-of-domain data to create a corpus the same size as that used for the best performing proposed system.", 
        "71": "We tried our best to re-implement the baseline methods using the same basic NMT setting as the proposed method.", 
        "72": "4.4 Results and Analyses  In Tables 2 and 3, the in, out and in + out indicate that the in-domain, out-of-domain and their mixture were used as the NMT training corpora.", 
        "73": "\u03b4f , \u03b4e and \u03b4fe indicate that corresponding criterion was used to select sentences, and these selected sentences were added to in-domain corpus to construct the new training corpora.", 
        "74": "+fur indicates that the selected sentences were used to train an initial NMT system, and then this initial system was further trained by in-domain data (Luong and Manning, 2015).", 
        "75": "The threshold for the sentence selection method was selected on development data.", 
        "76": "That is, we selected the top ranked 10%, 20%,...,90% out-of-domain data to be added into the in-domain data, and the best performing models on development data were used in the evaluation on test data.", 
        "77": "The vocabulary was built by using the selected corpus and in-domain corpus.8 Translation performance was measured by case-insensitive BLEU (Papineni et al., 2002).", 
        "78": "Since the proposed method is a sentence selection approach, we can also show the effect on standard PBSMT (Koehn et al., 2007).", 
        "79": "In the IWSLT task, the observations were as follows:\n\u2022 Adding out-of-domain to in-domain data, or directly using out-of-domain data, degraded\n7Freitag and Al-Onaizan (2016)\u2019s method is quite similar to Luong and Manning (2015)\u2019s, so we did not compare to them.", 
        "80": "8According to our empirical comparison, the performance did not significantly change if we used in + out to build the vocabulary for all of the systems.", 
        "81": "PBSMT and NMT performance.", 
        "82": "\u2022 Adding data selected by \u03b4f , \u03b4e and \u03b4fe substantially improved NMT performance (3.9 to 6.6 BLEU points), and gave rise to a modest improvement in PBSMT performance (0.4 to 3.1 BLEU points).", 
        "83": "This method also outperformed the best existing baselines by up to 1.1 BLEU points for NMT and 0.8 BLEU for PBSMT.", 
        "84": "\u2022 The proposed method worked synergistically with Luong\u2019s further training method, and the combination was able to add up to an additional 2-3 BLEU points, indicating that the proposed method and Luong\u2019s method are essentially orthogonal.", 
        "85": "\u2022 The performance by using both sides of sentence embeddings \u03b4fe was slightly better\nthan using monolingual sentence embedding \u03b4f and \u03b4e.", 
        "86": "In the NIST task, the observations were similar to the IWSLT task, except:\n\u2022 Adding out-of-domain slightly improved PBSMT and NMT performance.", 
        "87": "\u2022 The proposed method improved both PBSMT and NMT performance, but not as substantially as in IWSLT.", 
        "88": "These observations suggest that the out-ofdomain data was closer to the in-domain than in IWSLT.", 
        "89": "5 Discussions    5.1 Selected Size Effect  We show experimental results on varying the size of additional data selected from the out-of-domain dataset, in Figure 1.", 
        "90": "It shows that the proposed method \u03b4fe reached the highest performance on dev set, when top 30% out-of-domain sentences are selected as pseudo in-domain data.", 
        "91": "\u03b4fe outperforms the other methods in most of the cases on development data.", 
        "92": "5.2 Training Time Effect  We also show the relationship between BLEU and batches of training in Figure 2.", 
        "93": "Most of the methods (without further training) converged after similar batches training.", 
        "94": "Specifically, in researched the highest BLEU performance on dev faster than other methods (without further training), then decreased and finally converged.", 
        "95": "The further training methods, which firstly trained the models using out-of-domain data and then in-domain data, converged very soon after\n0\n5\n10\n15\n20\n25\n30\n0 1 10 30 50 100 200 300 400 500\nB L\nE U\n( d\nev )\nBatches (K)\nLearning Curves\nin out in+out fur \u03b4fe \u03b4fe+fur\nFigure 2: Training time on IWSLT.", 
        "96": "in-domain data were introduced.", 
        "97": "In further training, the out-of-domain trained system could be considered as a pre-trained NMT system.", 
        "98": "Then the in-domain data training help NMT system overfit at in-domain data and gained around two BLEU improvement.", 
        "99": "6 Conclusion and Future Work  In this paper, we proposed a straightforward sentence selection method for NMT domain adaptation.", 
        "100": "Instead of the existing external selection criteria, we applied the internal NMT sentence embedding similarity as the criterion.", 
        "101": "Empirical results on IWSLT and NIST tasks showed that the proposed method can substantially improve NMT performances and outperform state-of-the-art existing NMT adaptation methods on NMT (even PBSMT) performances.", 
        "102": "In addition, we found that the combination of sentence selection and further training has an additional effect, with a fast convergence.", 
        "103": "In our further work, we will investigate the effect of training data order and batch data selection on NMT training.", 
        "104": "Acknowledgments  Thanks a lot for the helpful discussions with Dr. Lemao Liu, Kehai Chen and Dr. Atsushi Fujita.", 
        "105": "We also appreciate the insightful comments from three anonymous reviewers."
    }, 
    "document_id": "P17-2089.pdf.json"
}
