{
    "abstract_sentences": {
        "1": "We evaluate feature hashing for language identification (LID), a method not previously used for this task.", 
        "2": "Using a standard dataset, we first show that while feature performance is high, LID data is highly dimensional and mostly sparse (>99.5%) as it includes large vocabularies for many languages; memory requirements grow as languages are added.", 
        "3": "Next we apply hashing using various hash sizes, demonstrating that there is no performance loss with dimensionality reductions of up to 86%.", 
        "4": "We also show that using an ensemble of low-dimension hash-based classifiers further boosts performance.", 
        "5": "Feature hashing is highly useful for LID and holds great promise for future work in this area."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 399\u2013403 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2063  1 Introduction  Language Identification (LID) is the task of determining the language of a text, at the document, sub-document or even sentence level.", 
        "2": "LID is a fundamental preprocessing task in NLP and is also used in lexicography, machine translation and information retrieval.", 
        "3": "It is widely used for filtering to select documents in a specific language; e.g.", 
        "4": "LID can filter webpages or tweets by language.", 
        "5": "Although LID has been widely studied, several open issues remain (Hughes et al., 2006).", 
        "6": "Current goals include developing models that can identify thousands of languages; extending the task to more fine-grained dialect identification; and making LID functionality more readily available to users/developers.", 
        "7": "A common challenge among these goals is dealing with high dimensional feature spaces.", 
        "8": "LID differs from traditional text categorization tasks in some important aspects.", 
        "9": "Standard tasks, such as topic classification, are usually\nperformed within a single language, and the maximum feature space size is a function of the single language\u2019s vocabulary.", 
        "10": "However, LID must deal with vocabulary from many languages and the feature space grows prodigiously.", 
        "11": "This raises immediate concerns about memory requirements for such systems and portends implementation issues for applying the systems to dozens, hundreds or even thousands of languages.", 
        "12": "Recent LID work has reported results on datasets including over 1,300 languages (Brown, 2014), albeit using small samples.", 
        "13": "Such models are going to include an extraordinarily large feature space, and individual vectors for each sample are going to be extremely sparse.", 
        "14": "LID is usually done using n-grams and as the number of languages and/or n gets larger, the feature space will become prohibitively large or impractical for real-world use.", 
        "15": "For high dimensional input, traditional dimensionality reduction methods (e.g.", 
        "16": "PCA, LDA) can be computationally expensive.", 
        "17": "Feature selection methods, e.g.", 
        "18": "those using entropy, are simpler but still expensive.", 
        "19": "Recently, feature hashing has been shown to be a very effective dimensionality reduction method (Weinberger et al., 2009).", 
        "20": "It has proven to be useful in numerous machine learning applications, particularly for handling extremely high dimensional data.", 
        "21": "It also provides numerous other benefits, which we describe in \u00a72.1.", 
        "22": "Although hashing could be tremendously useful for LID, to our knowledge no such experiments have been reported to date.", 
        "23": "It is unclear how collisions of features from different languages would affect its application for LID.", 
        "24": "Accordingly, the aims of the present work are to: (1) evaluate the effectiveness of hashing for LID; (2) compare its performance to the standard n-gram approach; (3) assess the role of hash size (and collision rate) on accuracy for different feature types; and (4) determine if ensemble methods can boost performance.", 
        "25": "399  2 Related Work    2.1 Language and Dialect Identification  Work in language identification (LID) dates back to the seminal work of Beesley (1988), Dunning (1994) and Cavnar and Trenkle (1994).", 
        "26": "Automatic LID methods have since been widely used in NLP research and applications.", 
        "27": "Recently, attention has turned to discriminating between close languages, such as Malay-Indonesian and Croatian-Serbian (Ljubes\u030cic\u0301 et al., 2007), or even dialects/varieties of one language, e.g.", 
        "28": "Arabic dialects (Malmasi et al., 2015).", 
        "29": "This has been the focus of the \u201cDiscriminating Similar Language\u201d (DSL) shared task series in recent years.", 
        "30": "In this work we use data from the 2016 task (Malmasi et al., 2016).", 
        "31": "The 2016 task used data from 12 different languages/dialects.", 
        "32": "A training and development set consisting of 20,000 sentences from each language and an unlabelled test set of 1,000 sentences per language was used for evaluation.", 
        "33": "Most participants relied on multi-class discriminative classifiers trained with word unigrams and character n-grams (Malmasi et al., 2016).", 
        "34": "2.2 Feature Hashing  Feature hashing is a method for mapping a highdimensional input to a low-dimensional space using hashing (Weinberger et al., 2009).", 
        "35": "Hashing has proven to be simple, efficient and effective.", 
        "36": "It has been applied to various tasks including protein sequence classification (Caragea et al., 2012), sentiment analysis (Da Silva et al., 2014), and malware detection (Jang et al., 2011).", 
        "37": "This method uses a hash function h(x) to arbitrarily map input to a hash key of a specified size.", 
        "38": "The hash size, e.g.", 
        "39": "218, determines the size of the mapped feature space.", 
        "40": "Hash functions are manyto-one mappings.", 
        "41": "Collision occur when distinct inputs yield the same output, i.e.", 
        "42": "h(a) = h(b).", 
        "43": "The collision rate is affected by the hash size.", 
        "44": "From a learning perspective, collisions cause random clustering of features and introduce noise; unrelated features map to the same vector index and may degrade the learner\u2019s accuracy.", 
        "45": "However, it has been shown that \u201cthe interference between independently hashed subspaces is negligible with high probability\u201d (Weinberger et al., 2009).", 
        "46": "A positive by-product of hashing is that it eliminates the need for a feature dictionary.", 
        "47": "In NLP, bag-of-words models require a full pass over the data to identify the vocabulary for each feature type (e.g.", 
        "48": "n-grams) and build a feature index.", 
        "49": "Eliminating this has many benefits: it simplifies implementation of feature extraction methods, reduces memory overhead, and facilitates distributed computing.", 
        "50": "Global statistics, e.g.", 
        "51": "totals and per-class feature counts, are usually required for feature selection and dimensionality reduction methods.", 
        "52": "Feature hashing may eliminate the need for full processing of the data to calculate these.", 
        "53": "3 Data and Experimental Setup  Our methodology is based on the results of 2016 DSL Shared Task (Malmasi et al., 2016) and we use their dataset.", 
        "54": "The DSL task is performed at the sentence level, making it more challenging.", 
        "55": "3.1 Data  A key shortcoming in LID research has been the absence of a common dataset for evaluation (Hughes et al., 2006), a need that has been met by the corpora released as part of the DSL shared task series.", 
        "56": "We use the DSLCC 3.0 corpus from the 2016 DSL task.1 This allows us to compare our findings to that of the 17 participants.", 
        "57": "Using a standard, publicly available dataset also facilitates replicability of our results.", 
        "58": "The 2016 task used data from 12 different languages and varieties,2 including training/development sets composed of 20,000 sentences per language.", 
        "59": "An unlabelled test set of 1,000 sentences per language was used for evaluation.", 
        "60": "The total sentences for training and testing are 240k and 12k, respectively.", 
        "61": "We report our results on the standard test set.", 
        "62": "3.2 Classifier and Evaluation  Participants applied various methods, but the task organizers note that linear classifiers, particularly SVMs, were the most successful (Malmasi et al., 2016).", 
        "63": "This is unsurprising as SVMs have been very successful for text classification and we adopt this method.", 
        "64": "The data is balanced across classes, so accuracy is used as the evaluation metric.", 
        "65": "3.3 Features  Most DSL entries use surface features, with words and high-order character n-grams being particularly successful.", 
        "66": "We apply character n-grams of order 1\u20136 (CH1-6) and word unigrams (WD1).", 
        "67": "1http://ttg.uni-saarland.de/resources/DSLCC/ 2Bosnian (BS), Argentine Spanish (ES AR), Peninsular Spanish (ES ES), Mexican Spanish (ES MX), Canadian French (FR CA), Hexagonal French (FR FR), Croatian (HR), Indonesian (ID), Malay (MY), Brazilian Portuguese (PT BR), European Portuguese (PT PT) and Serbian (SR).", 
        "68": "4 Feature Performance & Dimensionality  In our first experiment we examine the feature space in our dataset and establish the memory requirements and accuracy of the feature types we use (character 1\u20136 n-grams and word unigrams).", 
        "69": "For each feature type we extract the training vectors and use it to train a linear SVM model which is used to classify the standard test set.", 
        "70": "We report the feature\u2019s accuracy on the test set along with some statistics about the data: the number of features in the training data, the number of out-ofvocabulary (OOV) features3 in the test data, and the sparsity4 of the training data matrix.", 
        "71": "These results are shown in Figure 1.", 
        "72": "These results reveal a number of interesting patterns.", 
        "73": "Character n-grams perform better than word unigrams.", 
        "74": "The winner of the 2016 DSL task combined various character n-grams into an SVM model to obtain 89.4% accuracy on the test set.", 
        "75": "We obtain the best result of 89.2% using character 6-grams alone.", 
        "76": "The character n-gram feature grow rapidly, from 61k trigrams to 3.7m 6-grams.", 
        "77": "While accuracy plateaus at 6-grams, there is only a 1% improvement from 4- to 6-grams, but a huge feature space increase.", 
        "78": "The number of OOV test features also increases, but is relatively small.", 
        "79": "The sparsity analysis is also revealing, showing that for many features only around 0.1% of the training matrix contains non-zero values.", 
        "80": "We can expect that this sparsity will rapidly grow with the addition of more classes to the dataset.", 
        "81": "This poses a huge problem for practical applications of LID for discriminating large number of languages.", 
        "82": "In this regard, we can also assess how the dimensionality cumulatively increases as languages 3i.e.", 
        "83": "features present in the test set but not the training set.", 
        "84": "4Matrix sparsity is the proportion of zero-valued elements.", 
        "85": "are added, shown in Figure 2.", 
        "86": "Features increase even as similar languages are added; we expect this trend to continue if more classes are added.", 
        "87": "5 Feature Hashing Performance  Having established baseline performance for our features using the standard approach, we now experiment with applying feature hashing to the same data in order to evaluate its effectiveness for this task and compare it to the standard approach.", 
        "88": "We also assess the effect of hash size on the feature collision rate, and in turn, on classification accuracy.", 
        "89": "To do this we test each feature5 using hash sizes in the range 210 (1024) to 222 (2.1m) features, which covers most of our feature types.", 
        "90": "Our hash function is implemented using the signed 32-bit version of MurmurHash3.6\nWe report each feature\u2019s accuracy at every hash size, with the smallest hash that yields maximal accuracy considered to be the best result.", 
        "91": "Each feature is compared against its performance using the full feature space (baseline).", 
        "92": "These results, along with the reduction in the feature space for the best results, are listed in Table 1.", 
        "93": "Our first observation is that every feature matches baseline performance at a hash size smaller than its full feature space.", 
        "94": "This demonstrates that feature hashing is useful for LID.", 
        "95": "We can also assess the effect of feature collisions using the results, which we plot in Figure 3.", 
        "96": "We note that at the same hash size, features with a larger space perform worse.", 
        "97": "That is, with a 212 hash size, CHAR4 outperforms 5- and 6-grams.", 
        "98": "This is evidence of performance degradation due to hash collision.", 
        "99": "However, we see that when using an appropriately sized hash, feature collisions between languages do not degrade performance.", 
        "100": "5Except character unigrams which only have 272 features.", 
        "101": "6https://github.com/aappleby/smhasher\nWe also analyze the memory reduction achieved via hashing by calculating the relative difference in dimensionality between the best result and the full feature set, listed in the last column of Table 1.", 
        "102": "We see very significant reductions of up to 86% in dimensionality without any performance loss.", 
        "103": "Character 4-grams yield very competitive results (0.88) with a large feature space reduction of 82% using a hash size of 216.", 
        "104": "6 Hashing-based Ensemble Classifier  Ensemble classifiers combine multiple learners with the aim of improving accuracy through enhanced decision making.", 
        "105": "They have been applied to many tasks and shown to achieve better results compared to single-classifier methods (Oza and Tumer, 2008).", 
        "106": "By aggregating the outputs of multiple classifiers their outputs are generally considered to be more robust.", 
        "107": "Ensembles have been successfully used for LID, e.g.", 
        "108": "winning\nthe 2015 task (Malmasi and Dras, 2015a).", 
        "109": "They also achieve state-of-the-art performance for Native Language Identification (Malmasi and Dras, 2017).", 
        "110": "Could an ensemble composed of lowdimension hash-based classifiers achieve competitive performance?", 
        "111": "In order to assess this we created an ensemble of our features with a hash size of 216.", 
        "112": "Evaluating against the test set, a hard voting ensemble achieved 88.7% accuracy while a probabilitybased combination obtained 89.2%.", 
        "113": "Comparing to the winning shared task accuracy of 89.4%, this is an excellent result given that only 65,536 features were used by our system.", 
        "114": "Ensemble combination boosted our best single-model 216 hash size result by 1.1%.", 
        "115": "This highlights the utility of ensemble methods for hashing-based feature spaces.", 
        "116": "It also shows that model combination can compensate for small performance losses caused by hashing.", 
        "117": "7 Discussion and Conclusion  We presented the first application of feature hashing for language identification.", 
        "118": "Results show that hashing is highly effective for LID and can ameliorate the dimensionality issues that can impose prohibitive memory requirements for some LID tasks.", 
        "119": "We further showed that reduced feature spaces with as few as 65k features can be combined in ensemble classifiers to boost performance.", 
        "120": "We also demonstrated the effect of hash collision on accuracy, and outlined the type of analysis needed to choose the correct hash size for a given feature.", 
        "121": "Hashing provided dimensionality reductions of up to 86% without performance degradation.", 
        "122": "This is impressive considering that no feature selection or analysis was performed, making it highly effi-\ncient.", 
        "123": "This reduction facilitates model loading and training as we also showed that LID data is extremely sparse, with over 99% of our training matrix cells containing zeros.", 
        "124": "This is particularly useful for limited memory scenarios (e.g.", 
        "125": "handheld or embedded devices).", 
        "126": "It may also enable the use of methods requiring dense data representations, something often infeasible for large datasets.", 
        "127": "Another key advantage of hashing is that it eliminates the need for maintaining a feature dictionary, making it easy to develop feature extraction modules.", 
        "128": "This greatly simplifies parallelization, lending itself to online learning and distributed systems, which are important issues for LID systems in our experience.", 
        "129": "Hashing also holds promise for facilitating the use of deep learning methods for LID.", 
        "130": "In the 2016 DSL task, such systems performed \u201cpoorly compared to traditional classifiers\u201d; participants cited \u201cmemory requirements and long training times\u201d spanning several days (Malmasi et al., 2016).", 
        "131": "Feature hashing has recently been used to compress neural networks (Chen et al., 2015) and its application for deep learning-based text classification may provide insightful results.", 
        "132": "There are also downsides to hashing, including the inability to interpret feature weights and model parameters, and some minor performance loss.", 
        "133": "Future work in this area includes evaluation on larger datasets, as well as cross-corpus experiments, which may also be insightful.", 
        "134": "The application of these methods to other text classification tasks, particularly those dealing with language varieties such as Native Language Identification (Malmasi and Dras, 2015b), could also provide a deeper understanding about how they work."
    }, 
    "document_id": "P17-2063.pdf.json"
}
