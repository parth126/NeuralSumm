{
    "abstract_sentences": {
        "1": "We present a framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance of state-ofthe-art models.", 
        "2": "While most successful approaches for reading comprehension rely on recurrent neural networks (RNNs), running them over long documents is prohibitively slow because it is difficult to parallelize over sequences.", 
        "3": "Inspired by how people first skim the document, identify relevant parts, and carefully read these parts to produce an answer, we combine a coarse, fast model for selecting relevant sentences and a more expensive RNN for producing the answer from those sentences.", 
        "4": "We treat sentence selection as a latent variable trained jointly from the answer only using reinforcement learning.", 
        "5": "Experiments demonstrate the state of the art performance on a challenging subset of the WIKIREADING dataset (Hewlett et al., 2016) and on a new dataset, while speeding up the model by 3.5x-6.7x."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 209\u2013220 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1020  1 Introduction  Reading a document and answering questions about its content are among the hallmarks of natural language understanding.", 
        "2": "Recently, interest in question answering (QA) from unstructured documents has increased along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016; Nguyen et al., 2016; Trischler et al., 2016a).", 
        "3": "Current state-of-the-art approaches for QA over documents are based on recurrent neural networks \u2020Work done while the authors were at Google.", 
        "4": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016; Xiong et al., 2016).", 
        "5": "While such models have access to all the relevant information, they are slow because the model needs to be run sequentially over possibly thousands of tokens, and the computation is not parallelizable.", 
        "6": "In fact, such models usually truncate the documents and consider only a limited number of tokens (Miller et al., 2016; Hewlett et al., 2016).", 
        "7": "Inspired by studies on how people answer questions by first skimming the document, identifying relevant parts, and carefully reading these parts to produce an answer (Masson, 1983), we propose a coarse-to-fine model for question answering.", 
        "8": "Our model takes a hierarchical approach (see Figure 1), where first a fast model is used to select a few sentences from the document that are relevant for answering the question (Yu et al., 2014; Yang et al., 2016a).", 
        "9": "Then, a slow RNN is employed to produce the final answer from the selected sentences.", 
        "10": "The RNN is run over a fixed number of tokens, regardless of the length of the document.", 
        "11": "Empirically, our model encodes the\n209\ntext up to 6.7 times faster than the base model, which reads the first few paragraphs, while having access to four times more tokens.", 
        "12": "A defining characteristic of our setup is that an answer does not necessarily appear verbatim in the input (the genre of a movie can be determined even if not mentioned explicitly).", 
        "13": "Furthermore, the answer often appears multiple times in the document in spurious contexts (the year \u20182012\u2019 can appear many times while only once in relation to the question).", 
        "14": "Thus, we treat sentence selection as a latent variable that is trained jointly with the answer generation model from the answer only using reinforcement learning.", 
        "15": "Treating sentence selection as a latent variable has been explored in classification (Yessenalina et al., 2010; Lei et al., 2016), however, to our knowledge, has not been applied for question answering.", 
        "16": "We find that jointly training sentence selection and answer generation is especially helpful when locating the sentence containing the answer is hard.", 
        "17": "We evaluate our model on the WIKIREADING dataset (Hewlett et al., 2016), focusing on examples where the document is long and sentence selection is challenging, and on a new dataset called WIKISUGGEST that contains more natural questions gathered from a search engine.", 
        "18": "To conclude, we present a modular framework and learning procedure for QA over long text.", 
        "19": "It captures a limited form of document structure such as sentence boundaries and deals with long documents or potentially multiple documents.", 
        "20": "Experiments show improved performance compared to the state of the art on the subset of WIKIREADING, comparable performance on other datasets, and a 3.5x-6.7x speed up in document encoding, while allowing access to much longer documents.", 
        "21": "2 Problem Setting  Given a training set of question-document-answer triples {x(i), d(i), y(i)}Ni=1, our goal is to learn a model that produces an answer y for a questiondocument pair (x, d).", 
        "22": "A document d is a list of sentences s1, s2, .", 
        "23": ".", 
        "24": ".", 
        "25": ", s|d|, and we assume that the answer can be produced from a small latent subset of the sentences.", 
        "26": "Figure 2 illustrates a training example in which sentence s5 is in this subset.", 
        "27": "3 Data  We evaluate on WIKIREADING, WIKIREADING LONG, and a new dataset, WIKISUGGEST.", 
        "28": "WIKIREADING (Hewlett et al., 2016) is a QA dataset automatically generated from Wikipedia and Wikidata: given a Wikipedia page about an entity and a Wikidata property, such as PROFESSION, or GENDER, the goal is to infer the target value based on the document.", 
        "29": "Unlike other recently released large-scale datasets (Rajpurkar et al., 2016; Trischler et al., 2016a), WIKIREADING does not annotate answer spans, making sentence selection more challenging.", 
        "30": "Due to the structure and short length of most Wikipedia documents (median number of sentences: 9), the answer can usually be inferred from the first few sentences.", 
        "31": "Thus, the data is not ideal for testing a sentence selection model compared to a model that uses the first few sentences.", 
        "32": "Table 1 quantifies this intuition: We consider sentences containing the answer y\u2217 as a proxy for sentences that should be selected, and report how often y\u2217 appears in the document.", 
        "33": "Additionally, we report how frequently this proxy oracle sentence is the first sentence.", 
        "34": "We observe that in WIKIREADING, the answer appears verbatim in 47.1% of the examples, and in 75% of them the match is in the first sentence.", 
        "35": "Thus, the importance of modeling sentence selection is limited.", 
        "36": "To remedy that, we filter WIKIREADING and ensure a more even distribution of answers throughout the document.", 
        "37": "We prune short docu-\nments with less than 10 sentences, and only consider Wikidata properties for which Hewlett et al.", 
        "38": "(2016)\u2019s best model obtains an accuracy of less than 60%.", 
        "39": "This prunes out properties such as GENDER, GIVEN NAME, and INSTANCE OF.1 The resulting WIKIREADING LONG dataset contains 1.97M examples, where the answer appears in 50.4% of the examples, and appears in the first sentence only 31% of the time.", 
        "40": "On average, the documents in WIKIREADING LONG contain 1.2k tokens, more tokens than those of SQuAD (average 122 tokens) or CNN (average 763 tokens) datasets (see Table 2).", 
        "41": "Table 1 shows that the exact answer string is often missing from the document in WIKIREADING.", 
        "42": "This is since Wikidata statements include properties such as NATIONALITY, which are not explicitly mentioned, but can still be inferred.", 
        "43": "A drawback of this dataset is that the queries, Wikidata properties, are not natural language questions and are limited to 858 properties.", 
        "44": "To model more realistic language queries, we collect the WIKISUGGEST dataset as follows.", 
        "45": "We use the Google Suggest API to harvest natural language questions and submit them to Google Search.", 
        "46": "Whenever Google Search returns a box with a short answer from Wikipedia (Figure 3), we create an example from the question, answer, and the Wikipedia document.", 
        "47": "If the answer string is missing from the document this often implies a spurious question-answer pair, such as (\u2018what time is half time in rugby\u2019, \u201880 minutes, 40 minutes\u2019).", 
        "48": "Thus, we pruned question-answer pairs without the exact answer string.", 
        "49": "We examined fifty examples after filtering and found that 54% were well-formed question-answer pairs where we can ground answers in the document, 20% contained answers without textual evidence in the document (the answer string exists in an irreleveant context), and 26% contain incorrect QA pairs such as the last two examples in Figure 3.", 
        "50": "The data collection was performed in May 2016.", 
        "51": "1These three relations alone account for 33% of the data.", 
        "52": "4 Model  Our model has two parts (Figure 1): a fast sentence selection model (Section 4.1) that defines a distribution p(s | x, d) over sentences given the input question (x) and the document (d), and a more costly answer generation model (Section 4.3) that generates an answer y given the question and a document summary, d\u0302 (Section 4.2), that focuses on the relevant parts of the document.", 
        "53": "4.1 Sentence Selection Model  Following recent work on sentence selection (Yu et al., 2014; Yang et al., 2016b), we build a feed-forward network to define a distribution over the sentences s1, s2, .", 
        "54": ".", 
        "55": ".", 
        "56": ", s|d|.", 
        "57": "We consider three simple sentence representations: a bag-of-words (BoW) model, a chunking model, and a (parallelizable) convolutional model.", 
        "58": "These models are efficient at dealing with long documents, but do not fully capture the sequential nature of text.", 
        "59": "BoW Model Given a sentence s, we denote by BoW(s) the bag-of-words representation that averages the embeddings of the tokens in s. To define a distribution over the document sentences, we employ a standard attention model (e.g., (Hermann et al., 2015)), where the BoW representation of the query is concatenated to the BoW representation of each sentence sl, and then passed through a single layer feed-forward network:\nhl = [BoW(x);BoW(sl)] vl = v >ReLU(Whl),\np(s = sl | x, d) = softmax(vl),\nwhere [; ] indicates row-wise concatenation, and the matrix W , the vector v, and the word embeddings are learned parameters.", 
        "60": "Chunked BoW Model To get more fine-grained granularity, we split sentences into fixed-size smaller chunks (seven tokens per chunk) and score each chunk separately (Miller et al., 2016).", 
        "61": "This is beneficial if questions are answered with subsentential units, by allowing to learn attention over different chunks.", 
        "62": "We split a sentence sl into a fixed number of chunks (cl,1, cl,2 .", 
        "63": ".", 
        "64": ".", 
        "65": ", cl,J ), generate a BoW representation for each chunk, and score it exactly as in the BoW model.", 
        "66": "We obtain a distribution over chunks, and compute sentence probabilities by marginalizing over chunks from the same sentence.", 
        "67": "Let p(c = cl,j | x, d) be the distribution over chunks from all sentences, then:\np(s = sl | x, d) = J\u2211\nj=1\np(c = cl,j | x, d),\nwith the same parameters as in the BoW model.", 
        "68": "Convolutional Neural Network Model While our sentence selection model is designed to be fast, we explore a convolutional neural network (CNN) that can compose the meaning of nearby words.", 
        "69": "A CNN is still efficient, since all filters can be computed in parallel.", 
        "70": "Following previous work (Kim, 2014; Kalchbrenner et al., 2014), we concatenate the embeddings of tokens in the query x and the sentence sl, and run a convolutional layer with F filters and width w over the concatenated embeddings.", 
        "71": "This results in F features for every span of length w, and we employ max-over-time-pooling (Collobert et al., 2011) to get a final representation hl \u2208 RF .", 
        "72": "We then compute p(s = sl | x, d) by passing hl through a single layer feed-forward network as in the BoW model.", 
        "73": "4.2 Document Summary  After computing attention over sentences, we create a summary that focuses on the document parts related to the question using deterministic soft attention or stochastic hard attention.", 
        "74": "Hard attention is more flexible, as it can focus on multiple sentences, while soft attention is easier to optimize and retains information from multiple sentences.", 
        "75": "Hard Attention We sample a sentence s\u0302 \u223c p(s | x, d) and fix the document summary d\u0302 = s\u0302 to be that sentence during training.", 
        "76": "At test time,\nwe choose the most probable sentence.", 
        "77": "To extend the document summary to contain more information, we can sample without replacement K sentences from the document and define the summary to be the concatenation of the sampled sentences d\u0302 = [s\u03021; s\u03022; .", 
        "78": ".", 
        "79": ".", 
        "80": "; s\u0302K ].", 
        "81": "Soft Attention In the soft attention model (Bahdanau et al., 2015) we compute a weighted average of the tokens in the sentences according to p(s | x, d).", 
        "82": "More explicitly, let d\u0302m be the mth token of the document summary.", 
        "83": "Then, by fixing the length of every sentence toM tokens,2 the blended tokens are computed as follows:\nd\u0302m =\n|d|\u2211\nl=1\np(s = sl | x, d) \u00b7 sl,m,\nwhere sl,m is the mth word in the lth sentence (m \u2208 {1, .", 
        "84": ".", 
        "85": ".", 
        "86": ",M}).", 
        "87": "As the answer generation models (Section 4.3) take a sequence of vectors as input, we average the tokens at the word level.", 
        "88": "This gives the hard attention an advantage since it samples a \u201creal\u201d sentence without mixing words from different sentences.", 
        "89": "Conversely, soft attention is trained more easily, and has the capacity to learn a low-entropy distribution that is similar to hard attention.", 
        "90": "4.3 Answer Generation Model  State-of-the-art question answering models use RNN models to encode the document and question and selects the answer.", 
        "91": "We focus on a hierarchical model with fast sentence selection, and do not subscribe to a particular answer generation architecture.", 
        "92": "Here we implemented the state-of-the-art wordlevel sequence-to-sequence model with placeholders, described by Hewlett et al.", 
        "93": "(2016).", 
        "94": "This models can produce answers that does not appear in the sentence verbatim.", 
        "95": "This model takes the query tokens, and the document (or document summary) tokens as input and encodes them with a Gated Recurrent Unit (GRU; Cho et al.", 
        "96": "(2014)).", 
        "97": "Then, the answer is decoded with another GRU model, defining a distribution over answers p(y | x, d\u0302).", 
        "98": "In this work, we modified the original RNN: the word embeddings for the RNN decoder input, output and original word embeddings are shared.", 
        "99": "2Long sentences are truncated and short ones are padded.", 
        "100": "5 Learning  We consider three approaches for learning the model parameters (denoted by \u03b8): (1) We present a pipeline model, where we use distant supervision to train a sentence selection model independently from an answer generation model.", 
        "101": "(2) The hard attention model is optimized with REINFORCE (Williams, 1992) algorithm.", 
        "102": "(3) The soft attention model is fully differentiable and is optimized end-to-end with backpropagation.", 
        "103": "Distant Supervision While we do not have an explicit supervision for sentence selection, we can define a simple heuristic for labeling sentences.", 
        "104": "We define the gold sentence to be the first sentence that has a full match of the answer string, or the first sentence in the document if no full match exists.", 
        "105": "By labeling gold sentences, we can train sentence selection and answer generation independently with standard supervised learning, maximizing the log-likelihood of the gold sentence and answer, given the document and query.", 
        "106": "Let y\u2217 and s\u2217 be the target answer and sentence , where s\u2217 also serves as the document summary.", 
        "107": "The objective is to maximize:\nJ(\u03b8) = log p\u03b8(y \u2217, s\u2217 | x, d)\n= log p\u03b8(s \u2217 | x, d) + log p\u03b8(y\u2217 | s\u2217, x).", 
        "108": "Since at test time we do not have access to the target sentence s\u2217 needed for answer generation, we replace it by the model prediction argmaxsl\u2208d p\u03b8(s = sl | d, x).", 
        "109": "Reinforcement Learning Because the target sentence is missing, we use reinforcement learning where our action is sentence selection, and our goal is to select sentences that lead to a high reward.", 
        "110": "We define the reward for selecting a sentence as the log probability of the correct answer given that sentence, that is, R\u03b8(sl) = log p\u03b8(y = y\u2217 | sl, x).", 
        "111": "Then the learning objective is to maximize the expected reward:\nJ(\u03b8) = \u2211\nsl\u2208d p\u03b8(s=sl | x, d) \u00b7R\u03b8(sl)\n= \u2211\nsl\u2208d p\u03b8(s=sl | x, d) \u00b7 log p\u03b8(y=y\u2217 | sl, x).", 
        "112": "Following REINFORCE (Williams, 1992), we approximate the gradient of the objective with a\nsample, s\u0302 \u223c p\u03b8(s | x, d):\n\u2207J(\u03b8) \u2248 \u2207 log p\u03b8(y | s\u0302, x) + log p\u03b8(y | s\u0302, x) \u00b7 \u2207 log p\u03b8(s\u0302 | x, d).", 
        "113": "Sampling K sentences is similar and omitted for brevity.", 
        "114": "Training with REINFORCE is known to be unstable due to the high variance induced by sampling.", 
        "115": "To reduce variance, we use curriculum learning, start training with distant supervision and gently transition to reinforcement learning, similar to DAGGER (Ross et al., 2011).", 
        "116": "Given an example, we define the probability of using the distant supervision objective at each step as re, where r is the decay rate and e is the index of the current training epoch.3\nSoft Attention We train the soft attention model by maximizing the log likelihood of the correct answer y\u2217 given the input question and document log p\u03b8(y\n\u2217 | d, x).", 
        "117": "Recall that the answer generation model takes as input the query x and document summary d\u0302, and since d\u0302 is an average of sentences weighted by sentence selection, the objective is differentiable and is trained end-to-end.", 
        "118": "6 Experiments  Experimental Setup We used 70% of the data for training, 10% for development, and 20% for testing in all datasets.", 
        "119": "We used the first 35 sentences in each document as input to the hierarchical models, where each sentence has a maximum length of 35 tokens.", 
        "120": "Similar to Miller et al.", 
        "121": "(2016), we add the first five words in the document (typically the title) at the end of each sentence sequence for WIKISUGGEST.", 
        "122": "We add the sentence index as a one hot vector to the sentence representation.", 
        "123": "We coarsely tuned and fixed most hyperparameters for all models.", 
        "124": "The word embedding dimension is set to 256 for both sentence selection and answer generation models.", 
        "125": "We used the decay rate of 0.8 for curriculum learning.", 
        "126": "Hidden dimension is fixed at 128, batch size at 128, GRU state cell at 512, and vocabulary size at 100K.", 
        "127": "For CNN sentence selection model, we used 100 filters and set filter width as five.", 
        "128": "The initial learning rate and gradient clipping coefficients for each model are tuned on the development set.", 
        "129": "The ranges for learning rates were 0.00025, 0.0005, 0.001, 0.002, 0.004 and 0.5, 1.0 for gradient clipping coefficient.", 
        "130": "3 We tuned r \u2208 [0.3, 1] on the development set.", 
        "131": "We halved the learning rate every 25k steps.", 
        "132": "We use the Adam (Kingma and Ba, 2015) optimizer and TensorFlow framework (Abadi et al., 2015).", 
        "133": "Evaluation Metrics Our main evaluation metric is answer accuracy, the proportion of questions answered correctly.", 
        "134": "For sentence selection, since we do not know which sentence contains the answer, we report approximate accuracy by matching sentences that contain the answer string (y\u2217).", 
        "135": "For the soft attention model, we treat the sentence with the highest probability as the predicted sentence.", 
        "136": "Models and Baselines The models PIPELINE, REINFORCE, and SOFTATTEND correspond to the learning objectives in Section 5.", 
        "137": "We compare these models against the following baselines:\nFIRST always selects the first sentence of the document.", 
        "138": "The answer appears in the first sentence in 33% and 15% of documents in WIKISUGGEST and WIKIREADING LONG.", 
        "139": "BASE is the re-implementation of the best model by Hewlett et al.", 
        "140": "(2016), consuming the first 300 tokens.", 
        "141": "We experimented with providing additional tokens to match the length of document available to hierarchical models, but this performed poorly.4 ORACLE selects the first sentence with the answer string if it exists, or otherwise the first sentence in the document.", 
        "142": "4Our numbers on WIKIREADING outperform previously reported numbers due to modifications in implementation and better optimization.", 
        "143": "Answer Accuracy Results Table 3 summarizes answer accuracy on all datasets.", 
        "144": "We use BOW encoder for sentence selection as it is the fastest.", 
        "145": "The proposed hierarchical models match or exceed the performance of BASE, while reducing the number of RNN steps significantly, from 300 to 35 (or 70 for K=2), and allowing access to later parts of the document.", 
        "146": "Figure 4 reports the speed gain of our system.", 
        "147": "While throughput at training time can be improved by increasing the batch size, at test time real-life QA systems use batch size 1, where REINFORCE obtains a 3.5x-6.7x speedup (for K=2 or K=1).", 
        "148": "In all settings, REINFORCE was at least three times faster than the BASE model.", 
        "149": "All models outperform the FIRST baseline, and utilizing the proxy oracle sentence (ORACLE) improves performance on WIKISUGGEST and WIKIREADNG LONG.", 
        "150": "In WIKIREADING, where the proxy oracle sentence is often missing and documents are short, BASE outperforms ORACLE.", 
        "151": "Jointly learning answer generation and sentence selection, REINFORCE outperforms PIPELINE, which relies on a noisy supervision signal for sentence selection.", 
        "152": "The improvement is larger in WIKIREADING LONG, where the approximate supervision for sentence selection is missing for 51% of examples compared to 22% of examples in WIKISUGGEST.5\nOn WIKIREADING LONG, REINFORCE outper-\n5The number is lower than in Table 1 because we cropped sentences and documents, as mentioned above.", 
        "153": "forms all other models (excluding ORACLE, which has access to gold labels at test time).", 
        "154": "In other datasets, BASE performs slightly better than the proposed models, at the cost of speed.", 
        "155": "In these datasets, the answers are concentrated in the first few sentences.", 
        "156": "BASE is advantageous in categorical questions (such as GENDER), gathering bits of evidence from the whole document, at the cost of speed.", 
        "157": "Encouragingly, our system almost reaches the performance of ORACLE in WIKIREADING, showing strong results in a limited token setting.", 
        "158": "Sampling an additional sentence into the document summary increased performance in all datasets, illustrating the flexibility of hard attention compared to soft attention.", 
        "159": "Additional sampling allows recovery from mistakes in WIKIREADING LONG, where sentence selection is challenging.6 Comparing hard attention to soft attention, we observe that REINFORCE performed better than SOFTATTEND.", 
        "160": "The attention distribution learned by the soft attention model was often less peaked, generating noisier summaries.", 
        "161": "Sentence Selection Results Table 4 reports sentence selection accuracy by showing the proportion of times models selects the proxy gold sentence when it is found by ORACLE.", 
        "162": "In WIKIREADING LONG, REINFORCE finds the approximate gold sentence in 74.4% of the examples where the the answer is in the document.", 
        "163": "In WIKISUGGEST performance is at 67.5%, mostly due to noise in the data.", 
        "164": "PIPELINE performs slightly better as it is directly trained towards our noisy eval-\n6Sampling more help pipeline methods less.", 
        "165": "uation.", 
        "166": "However, not all sentences that contain the answer are useful to answer the question (first example in Table 6).", 
        "167": "REINFORCE learned to choose sentences that are likely to generate a correct answer rather than proxy gold sentences, improving the final answer accuracy.", 
        "168": "On WIKIREADING LONG, complex models (CNN and CHUNKBOW) outperform the simple BOW, while on WIKISUGGEST BOW performed best.", 
        "169": "Qualitative Analysis We categorized the primary reasons for the errors in Table 5 and present an example for each error type in Table 6.", 
        "170": "All examples are from REINFORCE with BOW sentence selection.", 
        "171": "The most frequent source of error for WIKIREADING LONG was lack of evidence in the document.", 
        "172": "While the dataset does not contain false answers, the document does not always provide supporting evidence (examples of properties without clues are ELEVATION ABOVE SEA LEVEL and SISTER).", 
        "173": "Interestingly, the answer string can still appear in the document as in the first example in Table 6: \u2018Saint Petersburg\u2019 appears in the document (4th sentence).", 
        "174": "Answer generation at times failed to generate the answer even when the correct sentence was selected.", 
        "175": "This was pronounced especially in long answers.", 
        "176": "For the automatically collected WIKISUGGEST dataset, noisy question-answer pairs were problematic, as discussed in Section 3.", 
        "177": "However, the models frequently guessed the spurious answer.", 
        "178": "We attribute higher proxy performance in sentence selection for WIKISUGGEST to noise.", 
        "179": "In manual analysis, sentence selection was harder in WIKIREADING LONG, explaining why sampling two sentences improved performance.", 
        "180": "In the first correct prediction (Table 6), the model generates the answer, even when it is not in the document.", 
        "181": "The second example shows when our model spots the relevant sentence without obvious clues.", 
        "182": "In the last example the model spots a sentence far from the head of the document.", 
        "183": "Figure 5 contains a visualization of the atten-\ntion distribution over sentences, p(sl | d, x), for different learning procedures.", 
        "184": "The increased frequency of the answer string in WIKISUGGEST vs. WIKIREADING LONG is evident in the leftmost plot.", 
        "185": "SOFTATTEND and CHUNKBOW clearly distribute attention more evenly across the sentences compared to BOW and CNN.", 
        "186": "7 Related Work  There has been substantial interest in datasets for reading comprehension.", 
        "187": "MCTest (Richardson et al., 2013) is a smaller-scale datasets focusing on common sense reasoning; bAbi (Weston et al., 2015) is a synthetic dataset that captures various aspects of reasoning; and SQuAD (Rajpurkar et al., 2016; Wang et al., 2016; Xiong\net al., 2016) and NewsQA (Trischler et al., 2016a) are QA datasets where the answer is a span in the document.", 
        "188": "Compared to Wikireading, some datasets covers shorter passages (average 122 words for SQuAD).", 
        "189": "Cloze-style question answering datasets (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2015) assess machine comprehension but do not form questions.", 
        "190": "The recently released MS MARCO dataset (Nguyen et al., 2016) consists of query logs, web documents and crowd-sourced answers.", 
        "191": "Answer sentence selection is studied with the TREC QA (Voorhees and Tice, 2000), WikiQA (Yang et al., 2016b) and SelQA (Jurczyk et al., 2016) datasets.", 
        "192": "Recently, neural networks models (Wang and Nyberg, 2015; Severyn and\nMoschitti, 2015; dos Santos et al., 2016) achieved improvements on TREC datsaet.", 
        "193": "Sultan et al.", 
        "194": "(2016) optimized the answer sentence extraction and the answer extraction jointly, but with gold labels for both parts.", 
        "195": "Trischler et al.", 
        "196": "(2016b) proposed a model that shares the intuition of observing inputs at multiple granularities (sentence, word), but deals with multiple choice questions.", 
        "197": "Our model considers answer sentence selection as latent and generates answer strings instead of selecting text spans, and we found that WIKIREADING dataset suits our purposes best with some pruning, which still provided 1.97 million examples compared to 2K questions for TREC dataset.", 
        "198": "Hierarchical models which treats sentence selection as a latent variable have been applied text categorization (Yang et al., 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al., 2014) and sentiment analysis (Yessenalina et al., 2010; Lei et al., 2016).", 
        "199": "To the best of our knowledge, we are the first to use the hierarchical nature of a document for QA.", 
        "200": "Finally, our work is related to the reinforcement learning literature.", 
        "201": "Hard and soft attention were examined in the context of caption generation (Xu et al., 2015).", 
        "202": "Curriculum learning was investigated in Sachan and Xing (2016), but they focused on the ordering of training examples while we com-\nbine supervision signals.", 
        "203": "Reinforcement learning recently gained popularity in tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al., 2015; He et al., 2016).", 
        "204": "8 Conclusion  We presented a coarse-to-fine framework for QA over long documents that quickly focuses on the relevant portions of a document.", 
        "205": "In future work we would like to deepen the use of structural clues and answer questions over multiple documents, using paragraph structure, titles, sections and more.", 
        "206": "Incorporating coreference resolution would be another important direction for future work.", 
        "207": "We argue that this is necessary for developing systems that can efficiently answer the information needs of users over large quantities of text.", 
        "208": "Acknowledgement  We appreciate feedbacks from Google colleagues.", 
        "209": "We also thank Yejin Choi, Kenton Lee, Mike Lewis, Mark Yatskar and Luke Zettlemoyer for comments on the earlier draft of the paper.", 
        "210": "The last author is partially supported by Israel Science Foundation, grant 942/16."
    }, 
    "document_id": "P17-1020.pdf.json"
}
