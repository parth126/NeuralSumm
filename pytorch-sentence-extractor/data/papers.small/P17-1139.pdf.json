{
    "abstract_sentences": {
        "1": "Although neural machine translation has made significant progress recently, how to integrate multiple overlapping, arbitrary prior knowledge sources remains a challenge.", 
        "2": "In this work, we propose to use posterior regularization to provide a general framework for integrating prior knowledge into neural machine translation.", 
        "3": "We represent prior knowledge sources as features in a log-linear model, which guides the learning process of the neural translation model.", 
        "4": "Experiments on ChineseEnglish translation show that our approach leads to significant improvements."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1514\u20131523 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1139  1 Introduction  The past several years have witnessed the rapid development of neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015), which aims to model the translation process using neural networks in an end-to-end manner.", 
        "2": "With the capability of capturing long-distance dependencies due to the gating (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) and attention (Bahdanau et al., 2015) mechanisms, NMT has shown remarkable superiority over conventional statistical machine translation (SMT) across a variety of natural languages (Junczys-Dowmunt et al., 2016).", 
        "3": "Despite the apparent success, NMT still suffers from one significant drawback: it is difficult to integrate prior knowledge into neural networks.", 
        "4": "On one hand, neural networks use continuous realvalued vectors to represent all language structures involved in the translation process.", 
        "5": "While these vector representations prove to be capable of capturing translation regularities implicitly (Sutskever\n\u2217Corresponding author: Yang Liu.", 
        "6": "et al., 2014), it is hard to interpret each hidden state in neural networks from a linguistic perspective.", 
        "7": "On the other hand, prior knowledge in machine translation is usually represented in discrete symbolic forms such as dictionaries and rules (Nirenburg, 1989) that explicitly encode translation regularities.", 
        "8": "It is difficult to transform prior knowledge represented in discrete forms to continuous representations required by neural networks.", 
        "9": "Therefore, a number of authors have endeavored to integrate prior knowledge into NMT in recent years, either by modifying model architectures (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016) or by modifying training objectives (Cohn et al., 2016; Feng et al., 2016; Cheng et al., 2016).", 
        "10": "For example, to address the over-translation and under-translation problems widely observed in NMT, Tu et al.", 
        "11": "(2016) directly extend standard NMT to model the coverage constraint that each source phrase should be translated into exactly one target phrase (Koehn et al., 2003).", 
        "12": "Alternatively, Cohn et al.", 
        "13": "(2016) and Feng et al.", 
        "14": "(2016) propose to control the fertilities of source words by appending additional additive terms to training objectives.", 
        "15": "Although these approaches have demonstrated clear benefits of incorporating prior knowledge into NMT, how to combine multiple overlapping, arbitrary prior knowledge sources still remains a major challenge.", 
        "16": "It is difficult to achieve this end by directly modifying model architectures because neural networks usually impose strong independence assumptions between hidden states.", 
        "17": "As a result, extending a neural model requires that the interdependence of information sources be modeled explicitly (Tu et al., 2016; Tang et al., 2016), making it hard to extend.", 
        "18": "While this drawback can be partly alleviated by appending additional additive terms to training objectives (Cohn et al., 2016; Feng et al., 2016), these terms are restricted to a\n1514\nlimited number of simple constraints.", 
        "19": "In this work, we propose a general framework for integrating multiple overlapping, arbitrary prior knowledge sources into NMT using posterior regularization (Ganchev et al., 2010).", 
        "20": "Our framework is capable of incorporating indirect supervision via posterior distributions of neural translation models.", 
        "21": "To represent prior knowledge sources as arbitrary real-valued features, we define the posterior distribution as a loglinear model instead of a constrained posterior set (Ganchev et al., 2010).", 
        "22": "This treatment not only leads to a simpler and more efficient training algorithm but also achieves better translation performance.", 
        "23": "Experiments show that our approach is able to incorporate a variety of features and achieves significant improvements over posterior regularization using constrained posterior sets on NIST Chinese-English datasets.", 
        "24": "2 Background    2.1 Neural Machine Translation  Given a source sentence x = x1, .", 
        "25": ".", 
        "26": ".", 
        "27": ", xi, .", 
        "28": ".", 
        "29": ".", 
        "30": ", xI and a target sentence y = y1, .", 
        "31": ".", 
        "32": ".", 
        "33": ", yj , .", 
        "34": ".", 
        "35": ".", 
        "36": ", yJ , a neural translation model (Sutskever et al., 2014; Bahdanau et al., 2015) is usually factorized as a product of word-level translation probabilities:\nP (y|x;\u03b8) = J\u220f\nj=1\nP (yj |x,y<j ;\u03b8), (1)\nwhere \u03b8 is a set of model parameters and y<j = y1, .", 
        "37": ".", 
        "38": ".", 
        "39": ", yj\u22121 denotes a partial translation.", 
        "40": "The word-level translation probability is defined using a softmax function:\nP (yj |x,y<j ;\u03b8) \u221d exp ( f(vyj ,vx,vy<j ,\u03b8) ) , (2)\nwhere f(\u00b7) is a non-linear function, vyj is a vector representation of the j-th target word yj , vx is a vector representation of the source sentence x that encodes the context on the source side, and vy<j is a vector representation of the partial translation y<j that encodes the context on the target side.", 
        "41": "Given a training set {\u3008x(n),y(n)\u3009}Nn=1, the standard training objective is to maximize the loglikelihood of the training set:\n\u03b8\u0302MLE = argmax \u03b8\n{ L(\u03b8) } , (3)\nwhere\nL(\u03b8) = N\u2211\nn=1\nlogP (y(n)|x(n);\u03b8).", 
        "42": "(4)\nAlthough the introduction of vector representations into machine translation has resulted in substantial improvements in terms of translation quality (Junczys-Dowmunt et al., 2016), it is difficult to incorporate prior knowledge represented in discrete symbolic forms into NMT.", 
        "43": "For example, given a Chinese-English dictionary containing ground-truth translational equivalents such as \u3008baigong, the White House\u3009, it is non-trivial to leverage the dictionary to guide the learning process of NMT.", 
        "44": "To address this problem, Tang et al.", 
        "45": "(2016) propose a new architecture called phraseNet on top of RNNsearch (Bahdanau et al., 2015) that equips standard NMT with an external memory storing phrase tables.", 
        "46": "Another important prior knowledge source is the coverage constraint (Koehn et al., 2003): each source phrase should be translated into exactly one target phrase.", 
        "47": "To encode this linguistic intuition into NMT, Tu et al.", 
        "48": "(2016) extend standard NMT with a coverage vector to keep track of the attention history.", 
        "49": "While these approaches are capable of incorporating individual prior knowledge sources separately, how to combine multiple overlapping, arbitrary knowledge sources still remains a major challenge.", 
        "50": "This can be hardly addressed by modifying model architectures because of the lack of interpretability in NMT and the incapability of neural networks in modeling arbitrary knowledge sources.", 
        "51": "Although modifying training objectives to include additional knowledge sources as additive terms can partially alleviate this problem, these terms have been restricted to a limited number of simple constraints (Cheng et al., 2016; Cohn et al., 2016; Feng et al., 2016) and incapable of combining arbitrary knowledge sources.", 
        "52": "Therefore, it is important to develop a new framework for integrating arbitrary prior knowledge sources into NMT.", 
        "53": "2.2 Posterior Regularization  Ganchev et al.", 
        "54": "(2010) propose posterior regularization for incorporating indirect supervision via constraints on posterior distributions of structured latent-variable models.", 
        "55": "The basic idea is to penalize the log-likelihood of a neural translation model\nwith the KL divergence between a desired distribution that incorporates prior knowledge and the model posteriors.", 
        "56": "The posterior regularized likelihood is defined as\nF (\u03b8, q)\n= \u03bb1L(\u03b8)\u2212\n\u03bb2\nN\u2211\nn=1\nmin q\u2208Q\nKL ( q(y) \u2223\u2223\u2223 \u2223\u2223\u2223P (y|x(n);\u03b8), ) (5)\nwhere \u03bb1 and \u03bb2 are hyper-parameters to balance the preference between likelihood and posterior regularization,Q is a set of constrained posteriors:\nQ = {q(y) : Eq[\u03c6(x,y)] \u2264 b}, (6) where \u03c6(x,y) is constraint feature and b is the bound of constraint feature expectations.", 
        "57": "Ganchev et al.", 
        "58": "(2010) use constraint features to encode structural bias and define the set of valid distributions with respect to the expectations of constraint features to facilitate inference.", 
        "59": "As maximizing F (\u03b8, q) involves minimizing the KL divergence, Ganchev et al.", 
        "60": "(2010) present a minorization-maximization algorithm akin to EM at sentence level:\nE : q(t+1) = argmin q\nKL ( q(y) \u2223\u2223\u2223 \u2223\u2223\u2223P (y|x(n);\u03b8(t)) )\nM : \u03b8(t+1) = argmax \u03b8\nEq(t+1) [ logP (y|x(n);\u03b8) ]\nHowever, directly applying posterior regularization to neural machine translation faces a major difficulty: it is hard to specify the hyper-parameter b to effectively bound the expectation of features, which are usually real-valued in translation (Och and Ney, 2002; Koehn et al., 2003; Chiang, 2005).", 
        "61": "For example, the coverage penalty constraint (Wu et al., 2016) proves to be an essential feature for controlling the length of a translation in NMT.", 
        "62": "As the value of coverage penalty varies significantly over different sentences, it is difficult to set an appropriate bound for all sentences on the training data.", 
        "63": "In addition, the minorization-maximization algorithm involves an additional step to find q(t+1) as compared with standard NMT, which increases training time significantly.", 
        "64": "3 Posterior Regularization for Neural Machine Translation    3.1 Modeling  In this work, we propose to adapt posterior regularization (Ganchev et al., 2010) to neural ma-\nchine translation.", 
        "65": "The major difference is that we represent the desired distribution as a log-linear model (Och and Ney, 2002) rather than a constrained posterior set as described in (Ganchev et al., 2010):\nJ (\u03b8,\u03b3) = \u03bb1L(\u03b8)\u2212\n\u03bb2\nN\u2211\nn=1\nKL ( Q(y|x(n);\u03b3) \u2223\u2223\u2223 \u2223\u2223\u2223P (y|x(n);\u03b8) ) , (7)\nwhere the desired distribution that encodes prior knowledge is defined as: 1\nQ(y|x;\u03b3) = exp\n( \u03b3 \u00b7 \u03c6(x,y) )\n\u2211 y\u2032 exp ( \u03b3 \u00b7 \u03c6(x,y\u2032) ) .", 
        "66": "(8)\nAs compared to previous work on integrating prior knowledge into NMT (Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016), our approach provides a general framework for combining arbitrary knowledge sources.", 
        "67": "This is due to log-linear models that offer sufficient flexibility to represent arbitrary prior knowledge sources as features.", 
        "68": "We tackle the representation discrepancy problem by associating the Q distribution that encodes discrete representations of prior knowledge with neural models using continuous representations learned from data in the KL divergence.", 
        "69": "Another advantage of our approach is the transparency to model architectures.", 
        "70": "In principle, our approach can be applied to any neural models for natural language processing.", 
        "71": "Our approach also differs from the original version of posterior regularization (Ganchev et al., 2010) in the definition of desired distribution.", 
        "72": "We resort to log-linear models (Och and Ney, 2002) to incorporate features that have proven effective in SMT.", 
        "73": "Another benefit of using log-linear models is the differentiability of our training objective (see Eq.", 
        "74": "(7)).", 
        "75": "It is easy to leverage standard stochastic gradient descent algorithms to optimize model parameters (Section 3.3).", 
        "76": "3.2 Feature Design  In this section, we introduce how to design features to encode prior knowledge in the desired dis-\n1Ideally, the desired distribution Q should be fixed to guide the learning process of P .", 
        "77": "However, it is hard to manually specify the feature weights \u03b3.", 
        "78": "Therefore, we propose to train both \u03b8 and \u03bb jointly (see Section 3.3).", 
        "79": "We find that joint training results in significant improvements in practice (see Table 1).", 
        "80": "tribution.", 
        "81": "Note that not all features in SMT can be adopted to our framework.", 
        "82": "This is because features in SMT are defined on latent structures such as phrase pairs and synchronous CFG rules, which are not accessible to the decoding process of NMT.", 
        "83": "Fortunately, we can still leverage internal information in neural models that is linguistically meaningful such as the attention matrix a (Bahdanau et al., 2015).", 
        "84": "We will introduce a number of features used in our experiments as follows.", 
        "85": "3.2.1 Bilingual Dictionary  It is natural to leverage a bilingual dictionary D to improve neural machine translation.", 
        "86": "Arthur et al.", 
        "87": "(2016) propose to incorporate discrete translation lexicons into NMT by using the attention vector to select lexical probabilities on which to be focused.", 
        "88": "In our work, for each entry \u3008x, y\u3009 \u2208 D in the dictionary, a bilingual dictionary (BD) feature is defined at the sentence level:\n\u03c6BD\u3008x,y\u3009(x,y) = { 1 if x \u2208 x \u2227 y \u2208 y 0 otherwise .", 
        "89": "(9)\nNote that number of bilingual dictionary features depends on the vocabulary of the neural translation model.", 
        "90": "Entries containing out-of-vocabulary words has to be discarded.", 
        "91": "3.2.2 Phrase Table  Phrases, which are sequences of consecutive words, are capable of memorizing local context to deal with word ordering within phrases and translation of short idioms, word insertions or deletions (Koehn et al., 2003; Chiang, 2005).", 
        "92": "As a result, phrase tables that specify phrase-level correspondences between the source and target languages also prove to be an effective knowledge source in NMT (Tang et al., 2016).", 
        "93": "Similar to the bilingual dictionary features, we define a phrase table (PT) feature for each entry \u3008x\u0303, y\u0303\u3009 in a phrase table P:\n\u03c6PT\u3008x\u0303,y\u0303\u3009(x,y) = { 1 if x\u0303 \u2208 x \u2227 y\u0303 \u2208 y 0 otherwise .", 
        "94": "(10)\nThe number of phrase table features also depends on the vocabulary of the neural translation model.", 
        "95": "3.2.3 Coverage Penalty  To overcome the over-translation and undertranslation problems widely observed in NMT, a\nnumber of authors have proposed to model the fertility (Brown et al., 1993) and converge constraint (Koehn et al., 2003) to improve the adequacy of translation (Tu et al., 2016; Cohn et al., 2016; Feng et al., 2016; Wu et al., 2016; Mi et al., 2016).", 
        "96": "We follow Wu et al.", 
        "97": "(2016) to define a coverage penalty (CP) feature to penalize source words with lower sum of attention weights: 2\n\u03c6CP(x,y) =\n|x|\u2211\ni=1\nlog ( min ( |y|\u2211\nj=1\nai,j , 1.0 )) , (11)\nwhere ai,j is the attention probability of the j-th target word on the i-th source word.", 
        "98": "Note that the value of coverage penalty feature varies significantly over sentences of different lengths.", 
        "99": "3.2.4 Length Ratio  Controlling the length of translations is very important in NMT as neural models tend to generate short translations for long sentences, which deteriorates the translation performance of NMT for long sentences as compared with SMT (Shen et al., 2016).", 
        "100": "Therefore, we define the length ratio (LR) feature to encourage the length of a translation to fall in a reasonable range:\n\u03c6LR(x,y) = { (\u03b2|x|)/|y| if \u03b2|x| < |y| |y|/(\u03b2|x|) otherwise , (12)\nwhere \u03b2 is a hyper-parameter for penalizing too long or too short translations.", 
        "101": "For example, to convey the same meaning, an English sentence is usually about 1.2 times longer than a Chinese sentence.", 
        "102": "As a result, we can set \u03b2 = 1.2.", 
        "103": "If the length of a Chinese sentence |x| is 10 and the length of an English sentence |y| is 12, then, \u03c6LR(x,y) = 1.", 
        "104": "If the translation is too long (e.g., |y| = 100), then the feature value is 0.12.", 
        "105": "If the translation is too short (e.g., |y| = 6), the feature value is 0.5.", 
        "106": "3.3 Training  In training, our goal is to find a set of model parameters that maximizes the posterior regularized likelihood:\n\u03b8\u0302, \u03b3\u0302 = argmax \u03b8,\u03b3\n{ J (\u03b8,\u03b3) } .", 
        "107": "(13)\n2For simplicity, we omit the attention matrix a in the input of the coverage feature function.", 
        "108": "Note that unlike the original version of posterior regularization (Ganchev et al., 2010) that relies on a minorization-maximization algorithm to optimize model parameters, our training objective is differentiable with respect to model parameters.", 
        "109": "Therefore, it is easy to use standard stochastic gradient descent algorithms to train our model.", 
        "110": "However, a major difficulty in calculating gradients is that the algorithm needs to sum over all candidate translations in an exponential search space for KL divergence.", 
        "111": "For example, the partial derivative of J (\u03b8,\u03b3) with respect to \u03b3 is given by\n\u2202J (\u03b8,\u03b3) \u2202\u03b3\n= \u2212\u03bb2 \u00d7 N\u2211\nn=1\n\u2202 \u2202\u03b3 KL ( Q(y|x(n);\u03b3) \u2223\u2223\u2223 \u2223\u2223\u2223P (y|x(n);\u03b8) ) .", 
        "112": "(14)\nThe KL divergence is defined as\nKL ( Q(y|x(n);\u03b3) \u2223\u2223\u2223 \u2223\u2223\u2223P (y|x(n);\u03b8) )\n= \u2211 y\u2208Y(x(n)) Q(y|x(n);\u03b3) log Q(y|x (n);\u03b3) P (y|x(n);\u03b8) , (15)\nwhere Y(x(n)) is a set of all possible candidate translations for the source sentence x(n).", 
        "113": "To alleviate this problem, we follow Shen et al.", 
        "114": "(2016) to approximate the full search space Y(x(n)) with a sampled sub-space S(x(n)).", 
        "115": "Therefore, the KL divergence can be approximated as\nKL ( Q(y|x(n);\u03b3) \u2223\u2223\u2223 \u2223\u2223\u2223P (y|x(n);\u03b8) )\n\u2248 \u2211 y\u2208S(x(n)) Q\u0303(y|x(n);\u03b3) log Q\u0303(y|x (n);\u03b3) P\u0303 (y|x(n);\u03b8) .", 
        "116": "(16)\nNote that the Q distribution is also approximated on the sub-space:\nQ\u0303(y|x(n);\u03b3)\n= exp(\u03b3 \u00b7 \u03c6(x(n),y))\u2211\ny\u2032\u2208S(x(n)) exp(\u03b3 \u00b7 \u03c6(x(n),y\u2032)) .", 
        "117": "(17)\nWe follow Shen et al.", 
        "118": "(2016) to control the sharpness of approximated neural translation distribution normalized on the sampled sub-space:\nP\u0303 (y|x(n);\u03b8) = P (y|x (n);\u03b8)\u03b1\u2211\ny\u2032\u2208S(x(n)) P (y \u2032|x(n);\u03b8)\u03b1 .", 
        "119": "(18)  3.4 Search  Given learned model parameters \u03b8\u0302 and \u03b3\u0302, the decision rule for translating an unseen source sentence x is given by\ny\u0302 = argmax Y(x)\n{ P (y|x; \u03b8\u0302) } .", 
        "120": "(19)\nThe search process can be factorized at the word level:\ny\u0302j = argmax y\u2208Vy\n{ P (y|x, y\u0302<j ; \u03b8\u0302) } , (20)\nwhere Vy is the target language vocabulary.", 
        "121": "Although this decision rule shares the same efficiency and simplicity with standard NMT (Bahdanau et al., 2015), it does not involve prior knowledge in decoding.", 
        "122": "Previous studies reveal that incorporating prior knowledge in decoding also significantly boosts translation performance (Arthur et al., 2016; He et al., 2016; Wang et al., 2016).", 
        "123": "As directly incorporating prior knowledge into the decoding process of NMT depends on both model structure and the locality of features, we resort to a coarse-to-fine approach instead to keep the architecture transparency of our approach.", 
        "124": "Given a source sentence x in the test set, we first use the neural translation model P (y|x; \u03b8\u0302) to generate a k-best list of candidate translation C(x).", 
        "125": "Then, the algorithm decides on the most probable candidate translation using the following decision rule:\ny\u0302 = argmax y\u2208C(x)\n{ logP (y|x; \u03b8\u0302) + \u03b3\u0302 \u00b7 \u03c6(x,y) } .", 
        "126": "(21)  4 Experiments    4.1 Setup  We evaluate our approach on Chinese-English translation.", 
        "127": "The evaluation metric is caseinsensitive BLEU calculated by the multibleu.perl script.", 
        "128": "Our training set3 consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words.", 
        "129": "We use the NIST 2002 dataset as validation set and the NIST 2003, 2004, 2005, 2006, 2008 datasets as test sets.", 
        "130": "In the experiments, we compare our approach with the following two baseline approaches:\n3The training set includes LDC2002E18, LDC2003E07, LDC2003E14, part of LDC2004T07, LDC2004T08 and LDC2005T06.", 
        "131": "1.", 
        "132": "RNNSEARCH (Bahdanau et al., 2015): a standard attention-based neural machine translation model,\n2.", 
        "133": "CPR (Wu et al., 2016): extending RNNSEARCH by introducing coverage penalty refinement (Eq.", 
        "134": "(11)) in decoding,\n3.", 
        "135": "POSTREG (Ganchev et al., 2010): extending RNNSEARCH with posterior regularization using constrained posterior set.", 
        "136": "For RNNSEARCH, we use an in-house attention-based NMT system that achieves comparable translation performance with GROUNDHOG (Bahdanau et al., 2015), which serves as a baseline approach in our experiments.", 
        "137": "We limit vocabulary size to 30K for both languages.", 
        "138": "The word embedding dimension is set to 620.", 
        "139": "The dimension of hidden layer is set to 1,000.", 
        "140": "In training, the batch size is set to 80.", 
        "141": "We use the AdaDelta algorithm (Zeiler, 2012) for optimizing model parameters.", 
        "142": "In decoding, the beam size is set to 10.", 
        "143": "For CPR, we simply follow Wu et al.", 
        "144": "(2016) to incorporate the coverage penalty into the beam\nsearch algorithm of RNNSEARCH.", 
        "145": "For POSTREG, we adapt the original version of posterior regularization (Ganchev et al., 2010) to NMT on top of RNNSEARCH.", 
        "146": "Following Ganchev et al.", 
        "147": "(2010), we use a ten-step projected gradient descent algorithm to search for an approximate desired distribution in the E step and a one-step gradient descent for the M step.", 
        "148": "Our approach extends RNNSEARCH by incorporating prior knowledge.", 
        "149": "For each source sentence, we sample 80 candidate translations to approximate the P\u0303 and Q\u0303 distributions.", 
        "150": "The hyperparameter \u03b1 is set to 0.2.", 
        "151": "The batch size is 1.", 
        "152": "The hyper-parameters \u03bb1 and \u03bb2 are set to 8\u00d710\u22125 and 2.5 \u00d7 10\u22124.", 
        "153": "Note that they not only balance the preference between likelihood and posterior regularization, but also control the values of gradients to fall in a reasonable range for optimization.", 
        "154": "We construct bilingual dictionary and phrase table in an automatic way.", 
        "155": "First, we run the statistical machine translation system MOSES (Koehn and Hoang, 2007) to obtain probabilistic bilingual dictionary and phrase table.", 
        "156": "For the bilingual dictionary, we retain entries with probabilities higher than 0.1 in both source-to-target and\ntarget-to-source directions.", 
        "157": "For phrase table, we first remove phrase pairs that occur less than 10 times and then retain entries with probabilities higher than 0.5 in both directions.", 
        "158": "As a result, both bilingual dictionary and phrase table contain highquality translation correspondences.", 
        "159": "We estimate the length ratio on Chinese-English data and set the hyper-parameter \u03b2 to 1.236.", 
        "160": "By default, both POSTREG and our approach use reranking to search for the most probable translations (Section 3.4).", 
        "161": "4.2 Main Results  Table 1 shows the BLEU scores obtained by RNNSEARCH, POSTREG, and our approach on the Chinese-English datasets.", 
        "162": "We find POSTREG achieves significant improvements over RNNSEARCH by adding features that encode prior knowledge.", 
        "163": "The most effective single feature for POSTREG seems to be the length ratio (LR) feature, suggesting that it is important for NMT to control the length of translation to improve translation quality.", 
        "164": "Note that POSTREG is unable to include the coverage penalty (CP) feature because the feature value varies significantly over different sentences.", 
        "165": "It is hard to specify an appropriate bound b for constraining the expected feature value.", 
        "166": "We observe that a loose bound often makes the training process very unstable and fail to converge.", 
        "167": "Combining features obtains further modest improvements.", 
        "168": "Our approach outperforms both RNNSEARCH and POSTREG significantly.", 
        "169": "The bilingual dictio-\nnary (BD) feature turns out to make the most contribution.", 
        "170": "Compared with CPR that imposes coverage penalty during decoding, our approach that using a single CP feature obtains a significant improvement (i.e., 30.76 over 29.72), suggesting that incorporating prior knowledge sources in modeling might be more beneficial than in decoding.", 
        "171": "We find that combining features only results in modest improvements for our approach.", 
        "172": "One possible reason is that the bilingual dictionary and phrase table features overlap on single word pairs.", 
        "173": "4.3 Effect of Reranking  Table 2 shows the effect of reranking on translation quality.", 
        "174": "We find that using prior knowledge features to rescore the k-best list produced by the neural translation model usually leads to improvements.", 
        "175": "This finding confirms that adding prior knowledge is beneficial for NMT, either in the training or decoding process.", 
        "176": "4.4 Training Speed  Initialized with the best RNNSEARCH model trained for 300K iterations, our model converges after about 100K iterations.", 
        "177": "For each iteration, our approach is 1.5 times slower than RNNSEARCH.", 
        "178": "On a single GPU device Tesla M40, it takes four days to train the RNNSEARCH model and three extra days to train our model.", 
        "179": "4.5 Example Translations  Table 3 gives four examples to demonstrate the benefits of adding features.", 
        "180": "In the first example, source words \u201cfenzhan\u201d (fighting), \u201cyuangong\u201d (staff), and \u201cyinglai\u201d (welcomed) are untranslated in the output of RNNSEARCH.", 
        "181": "Adding the bilingual dictionary (BD) feature encourages the model to translate these words if they occur in the dictionary.", 
        "182": "In the second example, while RNNSEARCH fails to capture phrase cohesion, adding the phrase table (PT) feature is beneficial for translating short idioms, word insertions or deletions that are sensitive to local context.", 
        "183": "In the third example, RNNSEARCH tends to omit many source content words such as \u201cchuyuan\u201d (discharged from the hospital), \u201cjianchi\u201d (insisted on), and \u201cdaobie\u201d (say farewell).", 
        "184": "The coverage penalty (CP) feature\nhelps to alleviate the word omission problem.", 
        "185": "In the fourth example, the translation produced by RNNSEARCH is too long and \u201cthe golan heights\u201d occurs twice.", 
        "186": "The length ratio (LR) feature is capable of controlling the sentence length in a reasonable range.", 
        "187": "5 Related Work  Our work is directly inspired by posterior regularization (Ganchev et al., 2010).", 
        "188": "The major difference is that we use a log-linear model to represent the desired distribution rather than a constrained posterior set.", 
        "189": "Using log-linear models not only enables our approach to incorporate arbitrary knowledge sources as real-valued features, but also is differentiable to be jointly trained with\nneural translation models efficiently.", 
        "190": "Our work is closely related to recent work on injecting prior knowledge into NMT (Arthur et al., 2016; Tu et al., 2016; Cohn et al., 2016; Tang et al., 2016; Feng et al., 2016; Wang et al., 2016).", 
        "191": "The major difference is that our approach aims to provide a general framework for incorporating arbitrary prior knowledge sources while keeping the neural translation model unchanged.", 
        "192": "He et al.", 
        "193": "(2016) also propose to combine the strengths of neural networks on learning representations and log-linear models on encoding prior knowledge.", 
        "194": "But they treat neural translation models as a feature in the log-linear model.", 
        "195": "In contrast, we connect the two models via KL divergence to keep the transparency of our approach to model architectures.", 
        "196": "This enables our approach to be easily applied to other neural models in NLP.", 
        "197": "6 Conclusion  We have presented a general framework for incorporating prior knowledge into end-to-end neural machine translation based on posterior regularization (Ganchev et al., 2010).", 
        "198": "The basic idea is to guide NMT models towards desired behavior using a log-linear model that encodes prior knowledge.", 
        "199": "Experiments show that incorporating prior knowledge leads to significant improvements over both standard NMT and posterior regularization using constrained posterior sets.", 
        "200": "Acknowledgments  We thank Shiqi Shen for useful discussions and anonymous reviewers for insightful comments.", 
        "201": "This work is supported by the National Natural Science Foundation of China (No.61432013), the 973 Program (2014CB340501), and the National Natural Science Foundation of China (No.61522204).", 
        "202": "This research is also supported by Sogou Inc. and the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme."
    }, 
    "document_id": "P17-1139.pdf.json"
}
