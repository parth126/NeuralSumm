{
    "abstract_sentences": {
        "1": "We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing.", 
        "2": "Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models.", 
        "3": "Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work.", 
        "4": "To address this problem, we propose embeddings of character strings, in addition to words.", 
        "5": "Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing.", 
        "6": "We also explore bi-LSTM models with fewer features."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204\u20131214 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1111  1 Introduction  Dependency parsers have been enhanced by the use of neural networks and embedding vectors (Chen and Manning, 2014; Weiss et al., 2015; Zhou et al., 2015; Alberti et al., 2015; Andor et al., 2016; Dyer et al., 2015).", 
        "2": "When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate.", 
        "3": "However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations.", 
        "4": "In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined.", 
        "5": "Hence, the pipeline of word segmentation, POS tagging and dependency parsing always suffers from word seg-\nmentation errors.", 
        "6": "Once words have been wronglysegmented, word embeddings and traditional onehot word features, used in dependency parsers, will mistake the precise meanings of the original sentences.", 
        "7": "As a result, pipeline models achieve dependency scores of around 80% for Chinese.", 
        "8": "A traditional solution to this error propagation problem is to use joint models.", 
        "9": "Many Chinese words play multiple grammatical roles with only one grammatical form.", 
        "10": "Therefore, determining the word boundaries and the subsequent tagging and dependency parsing are closely correlated.", 
        "11": "Transition-based joint models for Chinese word segmentation, POS tagging and dependency parsing are proposed by Hatori et al.", 
        "12": "(2012) and Zhang et al.", 
        "13": "(2014).", 
        "14": "Hatori et al.", 
        "15": "(2012) state that dependency information improves the performances of word segmentation and POS tagging, and develop the first transition-based joint word segmentation, POS tagging and dependency parsing model.", 
        "16": "Zhang et al.", 
        "17": "(2014) expand this and find that both the inter-word dependencies and intraword dependencies are helpful in word segmentation and POS tagging.", 
        "18": "Although the models of Hatori et al.", 
        "19": "(2012) and Zhang et al.", 
        "20": "(2014) perform better than pipeline models, they rely on the one-hot representation of characters and words, and do not assume the similarities among characters and words.", 
        "21": "In addition, not only words and characters but also many incomplete tokens appear in the transitionbased joint parsing process.", 
        "22": "Such incomplete or unknown words (UNK) could become important cues for parsing, but they are not listed in dictionaries or pre-trained word embeddings.", 
        "23": "Some recent studies show that character-based embeddings are effective in neural parsing (Ballesteros et al., 2015; Zheng et al., 2015), but their models could not be directly applied to joint models because they use given word segmentations.", 
        "24": "To solve\n1204\nthese problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing.", 
        "25": "We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens.", 
        "26": "Another problem in the models of Hatori et al.", 
        "27": "(2012) and Zhang et al.", 
        "28": "(2014) is that they rely on detailed feature engineering.", 
        "29": "Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016).", 
        "30": "In their models, the bi-LSTM is used to represent the tokens including their context.", 
        "31": "Indeed, such neural networks can observe whole sentence through the bi-LSTM.", 
        "32": "This biLSTM is similar to that of neural machine translation models of Bahdanau et al.", 
        "33": "(2014).", 
        "34": "As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models.", 
        "35": "We also develop joint models with ngram character string bi-LSTM.", 
        "36": "In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better dependency scores than the previous joint models.", 
        "37": "To the best of our knowledge, this is the first model to use embeddings and neural networks for Chinese full joint parsing.", 
        "38": "Our contributions are summarized as follows: (1) we propose the first embedding-based fully joint parsing model, (2) we use character string embeddings for UNK and incomplete tokens.", 
        "39": "(3) we also explore bi-LSTM models to avoid the detailed feature engineering in previous approaches.", 
        "40": "(4) in experiments using Chinese corpus, we achieve state-of-the-art scores in word segmentation, POS tagging and dependency parsing.", 
        "41": "2 Model  All full joint parsing models we present in this paper use the transition-based algorithm in Section 2.1 and the embeddings of character strings in Section 2.2.", 
        "42": "We present two neural networks: the feed-forward neural network models in Section 2.3 and the bi-LSTM models in Section 2.4.", 
        "43": "2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing  Based on Hatori et al.", 
        "44": "(2012), we use a modified arc-standard algorithm for character transi-\n\u6280\u672f\u6709\u4e86\u65b0\u7684\u8fdb\u5c55\u3002\nTechnology have made new progress.", 
        "45": "tions (Figure 1).", 
        "46": "The model consists of one buffer and one stack.", 
        "47": "The buffer contains characters in the input sentence, and the stack contains words shifted from the buffer.", 
        "48": "The stack words may have their child nodes.", 
        "49": "The words in the stack are formed by the following transition operations.", 
        "50": "\u2022 SH(t) (shift): Shift the first character of the buffer to the top of the stack as a new word.", 
        "51": "\u2022 AP (append): Append the first character of the buffer to the end of the top word of the stack.", 
        "52": "\u2022 RR (reduce-right): Reduce the right word of the top two words of the stack, and make the right child node of the left word.", 
        "53": "\u2022 RL (reduce-left): Reduce the left word of the top two words of the stack, and make the left child node of the right word.", 
        "54": "The RR and RL operations are the same as those of the arc-standard algorithm (Nivre, 2004a).", 
        "55": "SH makes a new word whereas AP makes the current word longer by adding one character.", 
        "56": "The POS tags are attached with the SH(t) transition.", 
        "57": "In this paper, we explore both greedy models and beam decoding models.", 
        "58": "This parsing algorithm works in both types.", 
        "59": "We also develop a joint model of word segmentation and POS tagging, along with a dependency parsing model.", 
        "60": "The joint model of word segmentation and POS tagging does not have RR and RL transitions.", 
        "61": "2.2 Embeddings of Character Strings  First, we explain the embeddings used in the neural networks.", 
        "62": "Later, we explain details of the neural networks in Section 2.3 and 2.4.", 
        "63": "Both meaningful words and incomplete tokens appear during transition-based joint parsing.", 
        "64": "Although embeddings of incomplete tokens are not used in previous work, they could become useful features in several cases.", 
        "65": "For example, \u201c\u5357\u4eac \u4e1c\u8def\u201d (Nanjing East Road, the famous shopping street of Shanghai) is treated as a single Chinese word in the Penn Chinese Treebank (CTB) corpus.", 
        "66": "There are other named entities of this form in CTB, e.g, \u201c\u5317\u4eac\u897f\u8def\u201d (Beijing West Road) and \u201c\u6e58\u897f\u8def\u201d (Hunan West Road).", 
        "67": "In these cases, \u201c\u5357\u4eac\u201d (Nanjing) and \u201c\u5317\u4eac\u201d (Beijing) are location words, while \u201c\u4e1c\u8def\u201d (East Road) and \u201c\u897f \u8def\u201d (West Road) are sub-words.", 
        "68": "\u201c\u4e1c\u8def\u201d and \u201c\u897f \u8def\u201d are similar in terms of their character composition and usage, which is not sufficiently considered in the previous work.", 
        "69": "Moreover, representations of incomplete tokens are helpful for compensating the segmentation ambiguity.", 
        "70": "Suppose that the parser makes over-segmentation errors and segments \u201c\u5357\u4eac\u4e1c\u8def\u201d to \u201c\u5357\u4eac\u201d and \u201c\u4e1c \u8def\u201d.", 
        "71": "In this case, \u201c\u4e1c\u8def\u201d becomes UNK.", 
        "72": "However, the models could infer that \u201c\u4e1c\u8def\u201d is also a location, from its character composition and neighboring words.", 
        "73": "This could give models robustness of segmentation errors.", 
        "74": "In our models, we prepare the word and character embeddings in the pretraining.", 
        "75": "We also use the embeddings of character strings for sub-words and UNK which are not in the pre-trained embeddings.", 
        "76": "The characters and words are embedded in the same vector space during pre-training.", 
        "77": "We prepare the same training corpus with the segmented word files and the segmented character files.", 
        "78": "Both files are concatenated and learned by word2vec (Mikolov et al., 2013).", 
        "79": "We use the embeddings of 1M frequent words and characters.", 
        "80": "Words and characters that are in the training set and do not have pre-trained embeddings are given randomly initialized embeddings.", 
        "81": "The development set and the test set have out-of-vocabulary (OOV) tokens for these embeddings.", 
        "82": "The embeddings of the unknown character strings are generated in the neural computation graph when they are required.", 
        "83": "Consider a character string c1c2 \u00b7 \u00b7 \u00b7 cn consisting of characters ci.", 
        "84": "When this character string is not in the pretrained embeddings, the model obtains the embeddings v(c1c2 \u00b7 \u00b7 \u00b7 cn) by the mean of each character embeddings \u2211n i=1 v(ci).", 
        "85": "Embeddings of words, characters and character strings have the same di-\nmension and are chosen in the neural computation graph.", 
        "86": "We avoid using the \u201cUNK\u201d vector as far as possible, because this degenerates the information about unknown tokens.", 
        "87": "However, models use the \u201cUNK\u201d vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon.", 
        "88": "2.3 Feed-forward Neural Network    2.3.1 Neural Network  We present a feed-forward neural network model in Figure 2.", 
        "89": "The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al.", 
        "90": "(2015).", 
        "91": "We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2.", 
        "92": "This neural network has two hidden layers with 8,000 dimensions.", 
        "93": "This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al.", 
        "94": "(2015) (1,024 or 2,048 dimensions).", 
        "95": "We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy\nneural network.", 
        "96": "There are three randomly initialized weight matrices between the embedding layers and the softmax function.", 
        "97": "The loss function L(\u03b8) for the greedy training is\nL(\u03b8) = \u2212 \u2211\ns,t\nlog pgreedys,t + \u03bb\n2 ||\u03b8||2,\npgreedys,t (\u03b2) \u221d exp\n \u2211\nj\nwtj\u03b2j + bt\n  ,\nwhere t denotes one transition among the transition set T ( t \u2208 T ).", 
        "98": "s denotes one element of the single mini-batch.", 
        "99": "\u03b2 denotes the output of the previous layer.", 
        "100": "w and b denote the weight matrix and the bias term.", 
        "101": "\u03b8 contains all parameters.", 
        "102": "We use the L2 penalty term and the Dropout.", 
        "103": "The backprop is performed including the word and character embeddings.", 
        "104": "We use Adagrad (Duchi et al., 2010) to optimize learning rate.", 
        "105": "We also consider Adam (Kingma and Ba, 2015) and SGD, but find that Adagrad performs better in this model.", 
        "106": "The other learning parameters are summarized in Table 1.", 
        "107": "In our model implementation, we divide all sentences into training batches.", 
        "108": "Sentences in the same training batches are simultaneously processed by the neural mini-batches.", 
        "109": "By doing so, the model can parse all sentences of the training batch in the number of transitions required to parse the longest sentence in the batch.", 
        "110": "This allows the model to parse more sentences at once, as long as the neural mini-batch can be allocated to the GPU memory.", 
        "111": "This can be applied to beam decoding.", 
        "112": "2.3.2 Features  The features of this neural network are listed in Table 2.", 
        "113": "We use three kinds of features: (1) features obtained from Hatori et al.", 
        "114": "(2012) by removing combinations of features, (2) features obtained from Chen and Manning (2014), (3) original features related to character strings.", 
        "115": "In particular,\nType Features\nStack word and tags s0w, s1w, s2w s0p, s1p, s2p Stack 1 children and tags s0l0w, s0r0w, s0l1w, s0r1w s0l0p, s0r0p, s0l1p, s0r1p Stack 2 children s1l0w, s1r0w, s1l1w, s1r1w Children of children s0l0lw, s0r0rw, s1l0lw, s1r0rw Buffer characters b0c, b1c, b2c, b3c Previously shifted words q0w, q1w Previously shifted tags q0p, q1p Character of q0 q0e Parts of q0 word q0f1, q0f2, q0f3 Strings across q0 and buf.", 
        "116": "q0b1, q0b2, q0b3 Strings of buffer characters b0-2, b0-3, b0-4\nthe original features include sub-words, character strings across the buffer and the stack, and character strings in the buffer.", 
        "117": "Character strings across the buffer and stack could capture the currentlysegmented word.", 
        "118": "To avoid using character strings that are too long, we restrict the length of character string to a maximum of four characters.", 
        "119": "Unlike Hatori et al.", 
        "120": "(2012), we use sequential characters of sentences for features, and avoid handengineered combinations among one-hot features, because such combinations could be automatically generated in the neural hidden layers as distributed representations (Hinton et al., 1986).", 
        "121": "In the later section, we evaluate a joint model for word segmentation and POS tagging.", 
        "122": "This model does not use the children and children-ofchildren of stack words as features.", 
        "123": "2.3.3 Beam Search  Structured learning plays an important role in previous joint parsing models for Chinese.1 In this paper, we use the structured learning model proposed by Weiss et al.", 
        "124": "(2015) and Andor et al.", 
        "125": "(2016).", 
        "126": "In Figure 2, the output layer for the beam decoding is at the top of the network.", 
        "127": "There are a perceptron layer which has inputs from the two hidden layers and the greedy output layer: [h1,h2,pgreedy(y)].", 
        "128": "This layer is learned by the following cost function (Andor et al., 2016):\nL(d\u22171:j ; \u03b8) = \u2212 j\u2211\ni=1\n\u03c1(d\u22171:i\u22121, d \u2217 i ; \u03b8)\n+ ln \u2211\nd\u20321:j\u2208B1:j exp\nj\u2211\ni=1\n\u03c1(d\u20321:i\u22121, d \u2032 i; \u03b8),\nwhere d1:j denotes the transition path and d\u22171:j denotes the gold transition path.", 
        "129": "B1:j is the set of transition paths from 1 to j step in beam.", 
        "130": "\u03c1 is the value of the top layer in Figure 2.", 
        "131": "This training can be applied throughout the network.", 
        "132": "However, we separately train the last beam layer and the previous greedy network in practice, as in Andor et al.", 
        "133": "(2016).", 
        "134": "First, we train the last perceptron layer using the beam cost function freezing the previous greedy-trained layers.", 
        "135": "After the last layer has been well trained, backprop is performed including the previous layers.", 
        "136": "We notice that training the embedding layer at this stage could make the results worse, and thus we exclude it.", 
        "137": "Note that this whole network backprop requires considerable GPU memory.", 
        "138": "Hence, we exclude particularly large batches from the training, because they cannot be on GPU memory.", 
        "139": "We use multiple beam sizes for training because models can be trained faster with small beam sizes.", 
        "140": "After the small beam size training, we use larger beam sizes.", 
        "141": "The test of this fully joint model takes place with a beam size of 16.", 
        "142": "Hatori et al.", 
        "143": "(2012) use special alignment steps in beam decoding.", 
        "144": "The AP transition has size-2 steps, whereas the other transitions have a size-1 step.", 
        "145": "Using this alignment, the total number of steps for an N -character sentence is guaranteed to be 2N \u2212 1 (excluding the root arc) for any transition path.", 
        "146": "This can be interpreted as the AP transition doing two things: appending characters and\n1Hatori et al.", 
        "147": "(2012) report that structured learning with a beam size of 64 is optimal.", 
        "148": "resolving intra-word dependencies.", 
        "149": "This alignment stepping assumes that the intra-word dependencies of characters to the right of the characters exist in each Chinese word.", 
        "150": "2.4 Bi-LSTM Model  In Section 2.3, we describe a neural network model with feature extraction.", 
        "151": "Unfortunately, although this model is fast and very accurate, it has two problems: (1) the neural network cannot see the whole sentence information.", 
        "152": "(2) it relies on feature engineering.", 
        "153": "To solve these problems, Kiperwasser and Goldberg (2016) propose a bi-LSTM neural network parsing model.", 
        "154": "Surprisingly, their model uses very few features, and bi-LSTM is applied to represent the context of the features.", 
        "155": "Their neural network consists of three parts: bi-LSTM, a feature extraction function and a multilayer perceptron (MLP).", 
        "156": "First, all tokens in the sentences are converted to embeddings.", 
        "157": "Second, the bi-LSTM reads all embeddings of the sentence.", 
        "158": "Third, the feature function extracts the feature representations of tokens from the bi-LSTM layer.", 
        "159": "Finally, an MLP with one hidden layer outputs the transition scores of the transition-based parser.", 
        "160": "In this paper, we propose a Chinese joint parsing model with simple and global features using n-gram bi-LSTM and a simple feature extraction function.", 
        "161": "The model is described in Figure 3.", 
        "162": "We consider that Chinese sentences consist of tokens, including words, UNKs and incomplete tokens, which can have some meanings and are useful for parsing.", 
        "163": "Such tokens appear in many parts of the sentence and have arbitrary lengths.", 
        "164": "To capture them, we propose the n-gram bi-LSTM.", 
        "165": "The n-gram bi-LSTM read through characters ci \u00b7 \u00b7 \u00b7 ci+n\u22121 of the sentence (ci is the i-th character).", 
        "166": "For example, the 1-gram bi-LSTM reads each character, and the 2-gram bi-LSTM reads two consecutive characters cici+1.", 
        "167": "After the n-gram forward LSTM reads character string ci \u00b7 \u00b7 \u00b7 ci+n\u22121, it next reads ci+1 \u00b7 \u00b7 \u00b7 ci+n.", 
        "168": "The backward LSTM reads from ci+1 \u00b7 \u00b7 \u00b7 ci+n toward ci \u00b7 \u00b7 \u00b7 ci+n\u22121.", 
        "169": "This allows models to capture any n-gram character strings in the input sentence.2 All n-gram inputs to bi-LSTM are given by the embeddings of words and characters or the dynamically generated embeddings of character strings, as described in\n2At the end of the sentence of length N , character strings ci \u00b7 \u00b7 \u00b7 cN (N < i+n\u22121), which are shorter than n characters, are used.", 
        "170": "Section 2.2.", 
        "171": "Although these arbitrary n-gram tokens produce UNKs, character string embeddings can capture similarities among them.", 
        "172": "Following the bi-LSTM layer, the feature function extracts the corresponding outputs of the bi-LSTM layer.", 
        "173": "We summarize the features in Table 3.", 
        "174": "Finally, MLP and the softmax function outputs the transition probability.", 
        "175": "We use an MLP with three hidden layers as for the model in Section 2.3.", 
        "176": "We train this neural network with the loss function for the greedy training.", 
        "177": "3 Experiments    3.1 Experimental Settings  We use the Penn Chinese Treebank 5.1 (CTB5) and 7 (CTB-7) datasets to evaluate our models, following the splitting of Jiang et al.", 
        "178": "(2008) for CTB-5 and Wang et al.", 
        "179": "(2011) for CTB-7.", 
        "180": "The statistics of datasets are presented in Table 4.", 
        "181": "We use the Chinese Gigaword Corpus for embedding pre-training.", 
        "182": "Our model is developed for unlabeled dependencies.", 
        "183": "The development set is used for parameter tuning.", 
        "184": "Following Hatori et al.", 
        "185": "(2012) and Zhang et al.", 
        "186": "(2014), we use the standard word-level evaluation with F1-measure.", 
        "187": "The POS tags and dependencies cannot be correct unless the corresponding words are correctly segmented.", 
        "188": "We trained three models: SegTag, SegTagDep and Dep.", 
        "189": "SegTag is the joint word segmentation and POS tagging model.", 
        "190": "SegTagDep is the full joint segmentation, tagging and dependency parsing model.", 
        "191": "Dep is the dependency parsing model which is similar to Weiss et al.", 
        "192": "(2015) and Andor et al.", 
        "193": "(2016), but uses the embeddings of character strings.", 
        "194": "Dep compensates for UNKs and segmentation errors caused by previous word segmentation using embeddings of character strings.", 
        "195": "We will examine this effect later.", 
        "196": "Most experiments are conducted on GPUs, but some of beam decoding processes are performed on CPUs because of the large mini-batch size.", 
        "197": "The neural network is implemented with Theano.", 
        "198": "3.2 Results    3.2.1 Joint Segmentation and POS Tagging  First, we evaluate the joint segmentation and POS tagging model (SegTag).", 
        "199": "Table 5 compares the performance of segmentation and POS tagging using the CTB-5 dataset.", 
        "200": "We train two modles: a greedy-trained model and a model trained with beams of size 4.", 
        "201": "We compare our model to three previous approaches: Hatori et al.", 
        "202": "(2012), Zhang et al.", 
        "203": "(2014) and Zhang et al.", 
        "204": "(2015).", 
        "205": "Our SegTag joint model is superior to these previous models, including Hatori et al.", 
        "206": "(2012)\u2019s model with rich dictionary information, in terms of both segmentation and POS tagging accuracy.", 
        "207": "3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing  Table 6 presents the results of our full joint model.", 
        "208": "We employ the greedy trained full joint model SegTagDep(g) and the beam decoding model SegTagDep.", 
        "209": "All scores for the existing models in this table are taken from Zhang et al.", 
        "210": "(2014).", 
        "211": "Though our model surpasses the previous best end-to-end joint models in terms of segmentation and POS tagging, the dependency score is slightly lower than the previous models.", 
        "212": "The greedy model SegTagDep(g) achieves slightly lower scores than beam models, although this model works considerably fast because it does not use beam decoding.", 
        "213": "3.2.3 Pipeline of Our Joint SegTag and Dep Model  We use our joint SegTag model for the pipeline input of the Dep model (SegTag+Dep).", 
        "214": "Both SegTag and Dep models are trained and tested by the beam cost function with beams of size 4.", 
        "215": "Table 7 presents the results.", 
        "216": "Our SegTag+Dep model performs best in terms of the dependency and word segmentation.", 
        "217": "The SegTag+Dep model is better than the full joint model.", 
        "218": "This is because most segmentation errors of these models occur around named entities.", 
        "219": "Hatori et al.", 
        "220": "(2012)\u2019s alignment step assumes the intra-word dependencies in words, while named entities do not always have them.", 
        "221": "For example, SegTag+Dep model treats named entity \u201c\u6d77\u8d5b\u514b\u201d, a company name, as one word, while the SegTagDep model divides this to \u201c\u6d77\u201d (sea) and \u201c\u8d5b\u514b\u201d, where \u201c\u8d5b\u514b\u201d could be used for foreigner\u2019s name.", 
        "222": "For such words, SegTagDep prefers SH because AP has size-2 step of the character appending and intra-word dependency resolution, which does not exist for named entities.", 
        "223": "This problem could be solved by adding a special transition AP_named_entity which is similar to AP but with size-1 step and used\nonly for named entities.", 
        "224": "Additionally, Zhang et al.", 
        "225": "(2014)\u2019s STD (arc-standard) model works slightly better than Hatori et al.", 
        "226": "(2012)\u2019s fully joint model in terms of the dependency score.", 
        "227": "Zhang et al.", 
        "228": "(2014)\u2019s STD model is similar to our SegTag+Dep because they combine a word segmentator and a dependency parser using \u201cdeque\u201d of words.", 
        "229": "3.2.4 Effect of Character String Embeddings  Finally, we compare the two pipeline models of SegTag+Dep to show the effectiveness of using character string representations instead of \u201cUNK\u201d embeddings.", 
        "230": "We use two dependency models with greedy training: Dep(g) for dependency model and Dep(g)-cs for dependency model without the character string embeddings .", 
        "231": "In the Dep(g)-cs model, we use the \u201cUNK\u201d embedding when the embeddings of the input features are unavailable, whereas we use the character string embeddings in model Dep(g).", 
        "232": "The results are presented in Table 8.", 
        "233": "When the models encounter unknown tokens, using the embeddings of character strings is better than using the \u201cUNK\u201d embedding.", 
        "234": "3.2.5 Effect of Features across the Buffer and Stack  We test the effect of special features: q0bX in Table 2.", 
        "235": "The q0bX features capture the tokens across the buffer and stack.", 
        "236": "Joint transition-based parsing models by Hatori et al.", 
        "237": "(2012) and Chen and Manning (2014) decide POS tags of words before corresponding word segmentations are determined.", 
        "238": "In our model, the q0bX features capture words even if their segmentations are not determined.", 
        "239": "We examine the effectiveness of these features by training greedy full joint models with and without them.", 
        "240": "The results are shown in Table 9.", 
        "241": "The q0bX features boost not only POS tagging scores but also word segmentation scores.", 
        "242": "3.2.6 CTB-7 Experiments  We also test the SegTagDep and SegTag+Dep models on CTB-7.", 
        "243": "In these experiments, we no-\ntice that the MLP with four hidden layers performs better than the MLP with three hidden layers, but we could not find definite differences in the experiments in CTB-5.", 
        "244": "We speculate that this is caused by the difference in the training set size.", 
        "245": "We present the final results with four hidden layers in Table 10.", 
        "246": "3.2.7 Bi-LSTM Model  We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3.", 
        "247": "We summarize the result in Table 11.", 
        "248": "The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering.", 
        "249": "4 Related Work  Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron.", 
        "250": "Zhang and Clark (2010) improve this model by using both character and word-based decoding.", 
        "251": "Hatori et al.", 
        "252": "(2011) propose a transition-based joint POS tagging and dependency parsing model.", 
        "253": "Zhang et al.", 
        "254": "(2013) propose a joint model using character structures of words for constituency parsing.", 
        "255": "Wang et al.", 
        "256": "(2013) also propose a lattice-based joint model for constituency parsing.", 
        "257": "Zhang et al.", 
        "258": "(2015) propose joint segmentation, POS tagging and dependency re-ranking system.", 
        "259": "This system requires\nbase parsers.", 
        "260": "In neural joint models, Zheng et al.", 
        "261": "(2013) propose a neural network-based Chinese word segmentation model based on tag inferences.", 
        "262": "They extend their models for joint segmentation and POS tagging.", 
        "263": "Zhu et al.", 
        "264": "(2015) propose the re-ranking system of parsing results with recursive convolutional neural network.", 
        "265": "5 Conclusion  We propose the joint parsing models by the feedforward and bi-LSTM neural networks.", 
        "266": "Both of them use the character string embeddings.", 
        "267": "The character string embeddings help to capture the similarities of incomplete tokens.", 
        "268": "We also explore the neural network with few features using n-gram bi-LSTMs.", 
        "269": "Our SegTagDep joint model achieves better scores of Chinese word segmentation and POS tagging than previous joint models, and our SegTag and Dep pipeline model achieves state-of-the-art score of dependency parsing.", 
        "270": "The bi-LSTM models reduce the cost of feature engineering."
    }, 
    "document_id": "P17-1111.pdf.json"
}
