{
    "abstract_sentences": {
        "1": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams.", 
        "2": "While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies.", 
        "3": "On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled.", 
        "4": "Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others.", 
        "5": "But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses, even when learned from an order of magnitude more data."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2016\u20132027 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1184  1 Introduction  Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015).", 
        "2": "However, directly mapping a finite set of word types to a continuous representation has well-known limitations.", 
        "3": "First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling.", 
        "4": "Second, it cannot exploit systematic functional relationships in learning.", 
        "5": "For example, cat and cats stand in the\nsame relationship as dog and dogs.", 
        "6": "While this relationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words sloth and sloths.", 
        "7": "These functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes.", 
        "8": "For instance, cats consists of two morphemes, cat and -s, with the latter shared by the words dogs and tarsiers.", 
        "9": "Modeling this effect is crucial for languages with rich morphology, where vocabulary sizes are larger, many more words are rare, and many more such functional relationships exist.", 
        "10": "Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schu\u0308tze, 2015).", 
        "11": "A downside of these models is that they depend on morphological segmenters or analyzers.", 
        "12": "Morphemes typically have similar orthographic representations across words.", 
        "13": "For example, the morpheme -s is realized as -es in finches.", 
        "14": "Since this variation is limited, the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; Wieting et al., 2016; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (Santos and Zadrozny, 2014; Qiu et al., 2014).", 
        "15": "These models are compact, can represent rare and unknown words, and do not require morphological analyzers.", 
        "16": "They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters?", 
        "17": "The relative merits of word, subword.", 
        "18": "and character-level models are not fully understood because each new model has been compared on\n2016\ndifferent tasks and datasets, and often compared against word-level models.", 
        "19": "A number of questions remain open:\n1.", 
        "20": "How do representations based on morphemes compare with those based on characters?", 
        "21": "2.", 
        "22": "What is the best way to compose subword representations?", 
        "23": "3.", 
        "24": "Do character-level models capture morphology in terms of predictive utility?", 
        "25": "4.", 
        "26": "How do different representations interact with languages of different morphological typologies?", 
        "27": "The last question is raised by Bender (2013): languages are typologically diverse, and the behavior of a model on one language may not generalize to others.", 
        "28": "Character-level models implicitly assume concatenative morphology, but many widely-spoken languages feature nonconcatenative morphology, and it is unclear how such models will behave on these languages.", 
        "29": "To answer these questions, we performed a systematic comparison across different models for the simple and ubiquitous task of language modeling.", 
        "30": "We present experiments that vary (1) the type of subword unit; (2) the composition function; and (3) morphological typology.", 
        "31": "To understand the extent to which character-level models capture true morphological regularities, we present oracle experiments using human morphological annotations instead of automatic morphological segments.", 
        "32": "Our results show that:\n1.", 
        "33": "For most languages, character-level representations outperform the standard word representations.", 
        "34": "Most interestingly, a previously unstudied combination of character trigrams composed with bi-LSTMs performs best on the majority of languages.", 
        "35": "2.", 
        "36": "Bi-LSTMs and CNNs are more effective composition functions than addition.", 
        "37": "3.", 
        "38": "Character-level models learn functional relationships between orthographically similar words, but don\u2019t (yet) match the predictive accuracy of models with access to true morphological analyses.", 
        "39": "4.", 
        "40": "Character-level models are effective across a range of morphological typologies, but orthography influences their effectiveness.", 
        "41": "2 Morphological Typology  A morpheme is the smallest unit of meaning in a word.", 
        "42": "Some morphemes express core meaning (roots), while others express one or more dependent features of the core meaning, such as person, gender, or aspect.", 
        "43": "A morphological analysis identifies the lemma and features of a word.", 
        "44": "A morph is the surface realization of a morpheme (Morley, 2000), which may vary from word to word.", 
        "45": "These distinctions are shown in Table 1.", 
        "46": "Morphological typology classifies languages based on the processes by which morphemes are composed to form words.", 
        "47": "While most languages will exhibit a variety of such processes, for any given language, some processes are much more frequent than others, and we will broadly identify our experimental languages with these processes.", 
        "48": "When morphemes are combined sequentially, the morphology is concatenative.", 
        "49": "However, morphemes can also be composed by nonconcatenative processes.", 
        "50": "We consider four broad categories of both concatenative and nonconcatenative processes in our experiments.", 
        "51": "Fusional languages realize multiple features in a single concatenated morpheme.", 
        "52": "For example, English verbs can express number, person, and tense in a single morpheme:\nwanted (English) want + ed\nwant + VB+1st+SG+Past Agglutinative languages assign one feature per morpheme.", 
        "53": "Morphemes are concatenated to form a word and the morpheme boundaries are clear.", 
        "54": "For example (Haspelmath, 2010):\nokursam (Turkish) oku+r+sa+m\n\u201cread\u201d+AOR+COND+1SG Root and Pattern Morphology forms words by inserting consonants and vowels of dependent morphemes into a consonantal root based on a given pattern.", 
        "55": "For example, the Arabic root ktb (\u201cwrite\u201d) produces (Roark and Sproat, 2007):\nkatab \u201cwrote\u201d (Arabic)\ntakaatab \u201cwrote to each other\u201d (Arabic) Reduplication is a process where a word form is produced by repeating part or all of the root to express new features.", 
        "56": "For example:\nanak \u201cchild\u201d (Indonesian) anak-anak \u201cchildren\u201d (Indonesian)\nbuah \u201cfruit\u201d (Indonesian) buah-buahan \u201cvarious fruits\u201d (Indonesian)  3 Representation Models  We compare ten different models, varying subword units and composition functions that have commonly been used in recent work, but evaluated on various different tasks (Table 2).", 
        "57": "Given word w, we compute its representation w as:\nw = f(Ws, \u03c3(w)) (1)\nwhere \u03c3 is a deterministic function that returns a sequence of subword units; Ws is a parameter matrix of representations for the vocabulary of subword units; and f is a composition function which takes \u03c3(w) and Ws as input and returns w. All of the representations that we consider take this form, varying only in f and \u03c3.", 
        "58": "3.1 Subword Units  We consider four variants of \u03c3 in Equation 1, each returning a different type of subword unit: character, character trigram, or one of two types of morph.", 
        "59": "Morphs are obtained from Morfessor (Smit et al., 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016).", 
        "60": "BPE works by iteratively replacing frequent pairs of characters with a single unused character.", 
        "61": "For Morfessor, we use default parameters while for BPE we set the number of merge operations to 10,000.1 When we segment into character trigrams, we consider all trigrams in the word, including those covering notional beginning and end of word characters, as in Sperr et al.", 
        "62": "(2013).", 
        "63": "Example output of \u03c3 is shown in Table 3.", 
        "64": "3.2 Composition Functions  We use three variants of f in Eq.", 
        "65": "1.", 
        "66": "The first constructs the representation w of word w by adding\n1BPE takes a single parameter: the number of merge operations.", 
        "67": "We tried different parameter values (1k, 10k, 100k) and manually examined the resulting segmentation on the English dataset.", 
        "68": "Qualitatively, 10k gave the most plausible segmentation and we used this setting across all languages.", 
        "69": "the representations of its subwords s1, .", 
        "70": ".", 
        "71": ".", 
        "72": ", sn = \u03c3(w), where the representation of si is vector si.", 
        "73": "w = n\u2211\ni=1\nsi (2)\nThe only subword unit that we don\u2019t compose by addition is characters, since this will produce the same representation for many different words.", 
        "74": "Our second composition function is a bidirectional long-short-term memory (bi-LSTM), which we adapt based on its use in the characterlevel model of Ling et al.", 
        "75": "(2015) and its widespread use in NLP generally.", 
        "76": "Given si and the previous LSTM hidden state hi\u22121, an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i:\nhi = LSTM(si,hi\u22121) (3)\ns\u0302i+1 = g(VT \u00b7 hi) (4)\nwhere s\u0302i+1 is the predicted target subword, g is the softmax function and V is a weight matrix.", 
        "77": "A bi-LSTM (Graves et al., 2005) combines the final state of an LSTM over the input sequence with one over the reversed input sequence.", 
        "78": "Given the hidden state produced from the final input of the forward LSTM, hfwn and the hidden state produced from the final input of the backward LSTM hbw0 , we compute the word representation as:\nwt = Wf \u00b7 hfwn + Wb \u00b7 hbw0 + b (5)\nwhere Wf , Wb, and b are parameter matrices and hfwn and hbw0 are forward and backward LSTM states, respectively.", 
        "79": "The third composition function is a convolutional neural network (CNN) with highway layers, as in Kim et al.", 
        "80": "(2016).", 
        "81": "Let c1, .", 
        "82": ".", 
        "83": ".", 
        "84": ", ck be the sequence of characters of word w. The character embedding matrix is C \u2208 Rd\u00d7k, where the i-th column corresponds to the embeddings of ci.", 
        "85": "We first apply a narrow convolution between C and a filter F \u2208 Rd\u00d7n of width n to obtain a feature map f \u2208 Rk\u2212n+1.", 
        "86": "In particular, the computation of the j-th element of f is defined as\nf [j] = tanh(\u3008C[\u2217, j : j + n\u2212 1],F\u3009+ b) (6)\nwhere \u3008A,B\u3009 = Tr(ABT ) is the Frobenius inner product and b is a bias.", 
        "87": "The CNN model applies filters of varying width, representing features\nof character n-grams.", 
        "88": "We then calculate the maxover-time of each feature map.", 
        "89": "yj = max j f [j] (7)\nand concatenate them to derive the word representation wt = [y1, .", 
        "90": ".", 
        "91": ".", 
        "92": ", ym], where m is the number of filters applied.", 
        "93": "Highway layers allow some dimensions of wt to be carried or transformed.", 
        "94": "Since it can learn character n-grams directly, we only use the CNN with character input.", 
        "95": "3.3 Language Model  We use language models (LM) because they are simple and fundamental to many NLP applications.", 
        "96": "Given a sequence of text s = w1, .", 
        "97": ".", 
        "98": ".", 
        "99": ", wT , our LM computes the probability of s as:\nP (w1, .", 
        "100": ".", 
        "101": ".", 
        "102": ", wT ) = T\u220f\nt=1\nP (yt|w1, .", 
        "103": ".", 
        "104": ".", 
        "105": ", wt\u22121) (8)\nwhere yt = wt if wt is in the output vocabulary and yt = UNK otherwise.", 
        "106": "Our language model is an LSTM variant of recurrent neural network language (RNN) LM (Mikolov et al., 2010).", 
        "107": "At time step t, it receives input wt and predicts yt+1.", 
        "108": "Using Eq.", 
        "109": "1, it first computes representation wt of wt.", 
        "110": "Given this representation and previous state ht\u22121, it produces a new state ht and predicts yt+1:\nht = LSTM(wt,ht\u22121) (9)\ny\u0302t+1 = g(VT \u00b7 ht) (10)\nwhere g is a softmax function over the vocabulary yielding the probability in Equation 8.", 
        "111": "Note that this design means that we can predict only words\nfrom a finite output vocabulary, so our models differ only in their representation of context words.", 
        "112": "This design makes it possible to compare language models using perplexity, since they have the same event space, though open vocabulary word prediction is an interesting direction for future work.", 
        "113": "The complete architecture of our system is shown in Figure 1, showing segmentation function \u03c3 and composition function f from Equation 1.", 
        "114": "4 Experiments  We perform experiments on ten languages (Table 4).", 
        "115": "We use datasets from Ling et al.", 
        "116": "(2015) for English and Turkish.", 
        "117": "For Czech and Russian we use Universal Dependencies (UD) v1.3 (Nivre et al., 2015).", 
        "118": "For other languages, we use preprocessed Wikipedia data (Al-Rfou et al., 2013).2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing.", 
        "119": "Preprocessing involves lowercasing (except for character models) and removing hyperlinks.", 
        "120": "To ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (Abadi et al., 2015).3 We use a common setup for all experiments based on that of Ling et al.", 
        "121": "(2015), Kim et al.", 
        "122": "(2016), and Miyamoto and Cho (2016).", 
        "123": "In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM\n2The Arabic and Hebrew dataset are unvocalized.", 
        "124": "Japanese mixes Kanji, Katakana, Hiragana, and Latin characters (for foreign words).", 
        "125": "Hence, a Japanese character can correspond to a character, syllable, or word.", 
        "126": "The preprocessed dataset is already word-segmented.", 
        "127": "3Our implementation of these models can be found at https://github.com/claravania/subword-lstm-lm\nmodels of Ling et al.", 
        "128": "(2015).", 
        "129": "Even following detailed discussion with Ling (p.c.", 
        "130": "), we were unable to reproduce their perplexities exactly\u2014our English reimplementation gives lower perplexities; our Turkish higher\u2014but we do reproduce their general result that character bi-LSTMs outperform word models.", 
        "131": "We suspect that different preprocessing and the stochastic learning explains differences in perplexities.", 
        "132": "Our final model with biLSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Treebank dataset (Marcus et al., 1993), preprocessed by Mikolov et al.", 
        "133": "(2010).", 
        "134": "4.1 Training and Evaluation  Our LSTM-LM uses two hidden layers with 200 hidden units and representation vectors for words, characters, and morphs all have dimension 200.", 
        "135": "All parameters are initialized uniformly at random from -0.1 to 0.1, trained by stochastic gradient descent with mini-batch size of 32, time steps of 20, for 50 epochs.", 
        "136": "To avoid overfitting, we apply dropout with probability 0.5 on the input-tohidden layer and all of the LSTM cells (including those in the bi-LSTM, if used).", 
        "137": "For all models which do not use bi-LSTM composition, we start with a learning rate of 1.0 and decrease it by half if the validation perplexity does not decrease by 0.1 after 3 epochs.", 
        "138": "For models with bi-LSTMs composition, we use a constant learning rate of 0.2 and stop training when validation perplexity does not improve after 3 epochs.", 
        "139": "For the character CNN model, we use the same settings as the small model of Kim et al.", 
        "140": "(2016).", 
        "141": "To make our results comparable to Ling et al.", 
        "142": "(2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token.", 
        "143": "To learn to predict unknown words, we follow Ling et al.", 
        "144": "(2015): in training, words that occur only once are stochastically replaced with the unknown token with probability 0.5.", 
        "145": "To evaluate the models, we compute perplexity on the test data.", 
        "146": "5 Results and Analysis  Table 5 presents our main results.", 
        "147": "In six of ten languages, character-trigram representations composed with bi-LSTMs achieve the lowest perplexities.", 
        "148": "As far as we know, this particular model has not been tested before, though it is similar\nto (but more general than) the model of Sperr et al.", 
        "149": "(2013).", 
        "150": "We can see that the performance of character, character trigrams, and BPE are very competitive.", 
        "151": "Composition by bi-LSTMs or CNN is more effective than addition, except for Turkish.", 
        "152": "We also observe that BPE always outperforms Morfessor, even for the agglutinative languages.", 
        "153": "We now turn to a more detailed analysis by morphological typology.", 
        "154": "Fusional languages.", 
        "155": "For these languages, character trigrams composed with bi-LSTMs outperformed all other models, particularly for Czech and Russian (up to 20%), which is unsurprising since both are morphologically richer than English.", 
        "156": "Agglutinative languages.", 
        "157": "We observe different results for each language.", 
        "158": "For Finnish, character trigrams composed with bi-LSTMs achieves the best perplexity.", 
        "159": "Surprisingly, for Turkish character trigrams composed via addition is best and addition also performs quite well for other representations, potentially useful since the addition function is simpler and faster than bi-LSTMs.", 
        "160": "We suspect that this is due to the fact that Turkish morphemes are reasonably short, hence wellapproximated by character trigrams.", 
        "161": "For Japanese, we improvements from character models are more modest than in other languages.", 
        "162": "Root and Pattern.", 
        "163": "For these languages, character trigrams composed with bi-LSTMs also achieve the best perplexity.", 
        "164": "We had wondered whether CNNs would be more effective for root-and-pattern morphology, but since these data are unvocalized, it is more likely that nonconcatenative effects are minimized, though we do\nstill find morphological variants with consonantal inflections that behave more like concatenation.", 
        "165": "For example, maktab (root:ktb) is written as mktb.", 
        "166": "We suspect this makes character trigrams quite effective since they match the tri-consonantal root patterns among words which share the same root.", 
        "167": "Reduplication.", 
        "168": "For Indonesian, BPE morphs composed with bi-LSTMs model obtain the best perplexity.", 
        "169": "For Malay, the character CNN outperforms other models.", 
        "170": "However, these improvements are small compared to other languages.", 
        "171": "This likely reflects that Indonesian and Malay are only moderately inflected, where inflection involves both concatenative and non-concatenative processes.", 
        "172": "5.1 Effects of Morphological Analysis  In the experiments above, we used unsupervised morphological segmentation as a proxy for morphological analysis (Table 3).", 
        "173": "However, as discussed in Section 2, this is quite approximate, so it is natural to wonder what would happen if we had the true morphological analysis.", 
        "174": "If characterlevel models are powerful enough to capture the effects of morphology, then they should have the predictive accuracy of a model with access to this analysis.", 
        "175": "To find out, we conducted an oracle experiment using the human-annotated morphological analyses provided in the UD datasets for Czech and Russian, the only languages in our set for which these analyses were available.", 
        "176": "In these experiments we treat the lemma and each morphological feature as a subword unit.", 
        "177": "The results (Table 6) show that bi-LSTM composition of these representations outperforms all\nother models for both languages.", 
        "178": "These results demonstrate that neither character representations nor unsupervised segmentation is a perfect replacement for manual morphological analysis, at least in terms of predictive accuracy.", 
        "179": "In light of character-level results, they imply that current unsupervised morphological analyzers are poor substitutes for real morphological analysis.", 
        "180": "However, we can obtain much more unannotated than annotated data, and we might guess that the character-level models would outperform those based on morphological analyses if trained on larger data.", 
        "181": "To test this, we ran experiments that varied the training data size on three representation models: word, character-trigram bi-LSTM, and character CNN.", 
        "182": "Since we want to see how much training data is needed to reach perplexity obtained using annotated data, we use the same output vocabulary derived from the original training.", 
        "183": "While this makes it possible to compare perplexities across models, it is unfavorable to the models trained on larger data, which may focus on other words.", 
        "184": "This is a limitation of our experimental setup, but does allow us to draw some tentative conclusions.", 
        "185": "As shown in Table 7, a characterlevel model trained on an order of magnitude more data still does not match the predictive accuracy of a model with access to morphological analysis.", 
        "186": "5.2 Automatic Morphological Analysis  The oracle experiments show promising results if we have annotated data.", 
        "187": "But these annotations are expensive, so we also investigated the use of automatic morphological analysis.", 
        "188": "We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014).4 As in the experiment using annotations, we treated each morphological feature as a subword unit.", 
        "189": "The resulting perplexities of 71.94 and 42.85 for addition and bi-LSTMs, respectively, are worse than those obtained with character trigrams (39.87), though it approaches the best perplexities.", 
        "190": "4We only experimented with Arabic since MADAMIRA disambiguates words in contexts; most other analyzers we found did not do this, and would require additional work to add disambiguation.", 
        "191": "5.3 Targeted Perplexity Results  A difficulty in interpreting the results of Table 5 with respect to specific morphological processes is that perplexity is measured for all words.", 
        "192": "But these processes do not apply to all words, so it may be that the effects of specific morphological processes are washed out.", 
        "193": "To get a clearer picture, we measured perplexity for only specific subsets of words in our test data: specifically, given target word wi, we measure perplexity of word wi+1.", 
        "194": "In other words, we analyze the perplexities when the inflected words of interest are in the most recent history, exploiting the recency bias of our LSTM-LM.", 
        "195": "This is the perplexity most likely to be strongly affected by different representations, since we do not vary representations of the predicted word itself.", 
        "196": "We look at several cases: nouns and verbs in Czech and Russian, where word classes can be identified from annotations, and reduplication in Indonesian, which we can identify mostly automatically.", 
        "197": "For each analysis, we also distinguish between frequent cases, where the inflected word occurs more than ten times in the training data, and rare cases, where it occurs fewer than ten times.", 
        "198": "We compare only bi-LSTM models.", 
        "199": "For Czech and Russian, we again use the UD annotation to identify words of interest.", 
        "200": "The results (Table 8), show that manual morphological analysis uniformly outperforms other subword models, with an especially strong effect for Czech nouns, suggesting that other models do not capture useful predictive properties of a morphological analysis.", 
        "201": "We do however note that character trigrams achieve low perplexities in most cases, similar to overall results (Table 5).", 
        "202": "We also observe that the subword models are more effective for rare words.", 
        "203": "For Indonesian, we exploit the fact that the hyphen symbol \u2018-\u2019 typically separates the first and second occurrence of a reduplicated morpheme, as in the examples of Section 2.", 
        "204": "We use the presence of word tokens containing hyphens to estimate the percentage of those exhibiting reduplication.", 
        "205": "As shown in Table 9, the numbers are quite low.", 
        "206": "Table 10 shows results for reduplication.", 
        "207": "In contrast with the overall results, the BPE bi-LSTM model has the worst perplexities, while character bi-LSTM has the best, suggesting that these models are more effective for reduplication.", 
        "208": "Looking more closely at BPE segmentation of reduplicated words, we found that only 6 of 252 reduplicated words have a correct word segmentation, with the reduplicated morpheme often combining differently with the notional start-of-word or hyphen character.", 
        "209": "One the other hand BPE correctly learns 8 out of 9 Indonesian prefixes and 4 out of 7 Indonesian suffixes.5 This analysis supports our intuition that the improvement from BPE might come from its modeling of concatenative morphology.", 
        "210": "5.4 Qualitative Analysis  Table 11 presents nearest neighbors under cosine similarity for in-vocabulary, rare, and out-of-\n5We use Indonesian affixes listed in Larasati et al.", 
        "211": "(2011)\nvocabulary (OOV) words.6 For frequent words, standard word embeddings are clearly superior for lexical meaning.", 
        "212": "Character and morph representations tend to find words that are orthographically similar, suggesting that they are better at modeling dependent than root morphemes.", 
        "213": "The same pattern holds for rare and OOV words.", 
        "214": "We suspect that the subword models outperform words on language modeling because they exploit affixes to signal word class.", 
        "215": "We also noticed similar patterns in Japanese.", 
        "216": "We analyze reduplication by querying reduplicated words to find their nearest neighbors using the BPE bi-LSTM model.", 
        "217": "If the model were sensitive to reduplication, we would expect to see morphological variants of the query word among its nearest neighbors.", 
        "218": "However, from Table 12, this is not so.", 
        "219": "With the partially reduplicated query berlembah-lembah, we do not find the lemma lembah.", 
        "220": "6 Conclusion  We presented a systematic comparison of word representation models with different levels of morphological awareness, across languages with different morphological typologies.", 
        "221": "Our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of model with explicit knowledge of morphology, even after we increase the training data size by ten times.", 
        "222": "Moreover, our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes.", 
        "223": "Although morphological analyses are available\n6https://radimrehurek.com/gensim/\nin limited quantities, our results suggest that there might be utility in semi-supervised learning from partially annotated data.", 
        "224": "Across languages with different typologies, our experiments show that the subword unit models are most effective on agglutinative languages.", 
        "225": "However, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations.", 
        "226": "We plan to explore these effects in future work.", 
        "227": "Acknowledgments  Clara Vania is supported by the Indonesian Endowment Fund for Education (LPDP), the Centre for Doctoral Training in Data Science, funded by the UK EPSRC (grant EP/L016427/1), and the University of Edinburgh.", 
        "228": "We thank Sameer Bansal, Toms Bergmanis, Marco Damonte, Federico Fancellu, Sorcha Gilroy, Sharon Goldwater, Frank Keller, Mirella Lapata, Felicia Liu, Jonathan Mallinson, Joana Ribeiro, Naomi Saphra, Ida Szubert, and the anonymous reviewers for helpful discussion of this work and comments on previous drafts of the paper."
    }, 
    "document_id": "P17-1184.pdf.json"
}
