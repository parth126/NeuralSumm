{
    "abstract_sentences": {
        "1": "There has been relatively little attention to incorporating linguistic prior to neural machine translation.", 
        "2": "Much of the previous work was further constrained to considering linguistic prior on the source side.", 
        "3": "In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation.", 
        "4": "Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward.", 
        "5": "Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72\u201378 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2012  1 Introduction  Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge.", 
        "2": "Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016).", 
        "3": "Shi et al.", 
        "4": "(2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary.", 
        "5": "On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT.", 
        "6": "Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized\nform and dependency label.", 
        "7": "Eriguchi et al.", 
        "8": "(2016) instead replaced the sequential encoder with a tree-based encoder which computes the representation of the source sentence following its parse tree.", 
        "9": "Stahlberg et al.", 
        "10": "(2016) let the lattice from a hierarchical phrase-based system guide the decoding process of neural machine translation, which results in two separate models rather than a single end-to-end one.", 
        "11": "Despite the promising improvements, these explicit approaches are limited in that the trained translation model strictly requires the availability of external tools during inference time.", 
        "12": "More recently, researchers have proposed methods to incorporate target-side syntax into NMT models.", 
        "13": "Alvarez-Melis and Jaakkola (2017) have proposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown.", 
        "14": "Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences.", 
        "15": "We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011).", 
        "16": "More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1, that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016).", 
        "17": "This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG.", 
        "18": "We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence.", 
        "19": "We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time.", 
        "20": "1Our code is available at https://github.com/ tempra28/nmtrnng.", 
        "21": "72\nWe evaluate the proposed NMT+RNNG on four language pairs ({JP, Cs, De, Ru}-En).", 
        "22": "We observe significant improvements in terms of BLEU scores on three out of four language pairs and RIBES scores on all the language pairs.", 
        "23": "2 Neural Machine Translation  Neural machine translation is a recently proposed framework for building a machine translation system based purely on neural networks.", 
        "24": "It is often built as an attention-based encoder-decoder network (Cho et al., 2015) with two recurrent networks\u2014encoder and decoder\u2014and an attention model.", 
        "25": "The encoder, which is often implemented as a bidirectional recurrent network with long short-term memory units (LSTM, Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU, Cho et al., 2014), first reads a source sentence represented as a sequence of words x = (x1, x2, .", 
        "26": ".", 
        "27": ".", 
        "28": ", xN ).", 
        "29": "The encoder returns a sequence of hidden states h = (h1, h2, .", 
        "30": ".", 
        "31": ".", 
        "32": ", hN ).", 
        "33": "Each hidden state hi is a concatenation of those from the forward and backward recurrent network: hi =[\u2212\u2192 h i; \u2190\u2212 h i ] , where\n\u2212\u2192 h i = \u2212\u2192 f enc( \u2212\u2192 h i\u22121, Vx(xi)), \u2190\u2212 h i = \u2190\u2212 f enc( \u2190\u2212 h i+1, Vx(xi)).", 
        "34": "Vx(xi) refers to the word vector of the i-th source word.", 
        "35": "The decoder is implemented as a conditional recurrent language model which models the target sentence, or translation, as\nlog p(y|x) = \u2211\nj\nlog p(yj |y<j ,x),\nwhere y = (y1, .", 
        "36": ".", 
        "37": ".", 
        "38": ", yM ).", 
        "39": "Each of the conditional probabilities in the r.h.s is computed by\np(yj = y|y<j ,x) = softmax(W>y s\u0303j), (1) s\u0303j = tanh(Wc[sj ; cj ]), (2)\nsj = fdec(sj\u22121, [Vy(yj\u22121); s\u0303j\u22121]), (3)\nwhere fdec is a recurrent activation function, such as LSTM or GRU, and Wy is the output word vector of the word y. cj is a time-dependent context vector that is computed by the attention model using the sequence h of hidden states from the encoder.", 
        "40": "The attention model first compares the current hidden\nstate sj against each of the hidden states and assigns a scalar score: \u03b2i,j = exp(h>i Wdsj) (Luong et al., 2015).", 
        "41": "These scores are then normalized across the hidden states to sum to 1, that is \u03b1i,j =\n\u03b2i,j\u2211 i \u03b2i,j .", 
        "42": "The time-dependent context vector is then a weighted-sum of the hidden states with these attention weights: cj = \u2211 i \u03b1i,jhi.", 
        "43": "3 Recurrent Neural Network Grammars  A recurrent neural network grammar (RNNG, Dyer et al., 2016) is a probabilistic syntax-based language model.", 
        "44": "Unlike a usual recurrent language model (see, e.g., Mikolov et al., 2010), an RNNG simultaneously models both tokens and their tree-based composition.", 
        "45": "This is done by having a (output) buffer, stack and action history, each of which is implemented as a stack LSTM (sLSTM, Dyer et al., 2015).", 
        "46": "At each time step, the action sLSTM predicts the next action based on the (current) hidden states of the buffer, stack and action sLSTM.", 
        "47": "That is,\np(at = a|a<t) \u221d eW > a faction(h buffer t ,h stack t ,h action t ), (4)\nwhere Wa is the vector of the action a.", 
        "48": "If the selected action is shift, the word at the beginning of the buffer is moved to the stack.", 
        "49": "When the reduce action is selected, the top-two words in the stack are reduced to build a partial tree.", 
        "50": "Additionally, the action may be one of many possible non-terminal symbols, in which case the predicted non-terminal symbol is pushed to the stack.", 
        "51": "The hidden states of the buffer, stack and action sLSTM are correspondingly updated by\nhbuffert = StackLSTM(h buffer top , Vy(yt\u22121)), (5) hstackt = StackLSTM(h stack top , rt), hactiont = StackLSTM(h action top , Va(at\u22121)),\nwhere Vy and Va are functions returning the target word and action vectors.", 
        "52": "The input vector rt of the stack sLSTM is computed recursively by\nrt = tanh(Wr[r d; rp;Va(at)]),\nwhere rd and rp are the corresponding vectors of the parent and dependent phrases, respectively (Dyer et al., 2015).", 
        "53": "This process is iterated until a complete parse tree is built.", 
        "54": "Note that the original paper of RNNG (Dyer et al., 2016) uses constituency trees, but we employ dependency trees in this paper.", 
        "55": "Both types of trees are\nrepresented as a sequence of the three types of actions in a transition-based parsing model.", 
        "56": "When the complete sentence is provided, the buffer simply summarizes the shifted words.", 
        "57": "When the RNNG is used as a generator, the buffer further generates the next word when the selected action is shift.", 
        "58": "The latter can be done by replacing the buffer with a recurrent language model, which is the idea on which our proposal is based.", 
        "59": "4 Learning to Parse and Translate    4.1 NMT+RNNG  Our main proposal in this paper is to hybridize the decoder of the neural machine translation and the RNNG.", 
        "60": "We continue from the earlier observation that we can replace the buffer of RNNG to a recurrent language model that simultaneously summarizes the shifted words as well as generates future words.", 
        "61": "We replace the RNNG\u2019s buffer with the neural translation model\u2019s decoder in two steps.", 
        "62": "Construction First, we replace the hidden state of the buffer hbuffer (in Eq.", 
        "63": "(5)) with the hidden state of the decoder of the attention-based neural machine translation from Eq.", 
        "64": "(3).", 
        "65": "As is clear from those two equations, both the buffer sLSTM and the translation decoder take as input the previous hidden state (hbuffertop and sj\u22121, respectively) and the previously decoded word (or the previously shifted word in the case of the RNNG\u2019s buffer), and returns its summary state.", 
        "66": "The only difference is that the translation decoder additionally considers the state s\u0303j\u22121.", 
        "67": "Once the buffer of the RNNG is replaced with the NMT decoder in our proposed model, the NMT decoder is also under control of the actions provided by the RNNG.2 Second, we let the next word prediction of the translation decoder as a generator of RNNG.", 
        "68": "In other words, the generator of RNNG will output a word, when asked by the shift action, according to the conditional distribution defined by the translation decoder in Eq.", 
        "69": "(1).", 
        "70": "Once the buffer sLSTM is replaced with the neural translation decoder, the action sLSTM naturally takes as input the translation decoder\u2019s hidden state when computing the action conditional distribution in Eq.", 
        "71": "(4).", 
        "72": "We call this hybrid model NMT+RNNG.", 
        "73": "2The j-th hidden state in Eq.", 
        "74": "(3) is calculated only when the action (shift) is predicted by the RNNG.", 
        "75": "This is why our proposed model can handle the sequences of words and actions which have different lengths.", 
        "76": "Learning and Inference After this integration, our hybrid NMT+RNNG models the conditional distribution over all possible pairs of translation and its parse given a source sentence, i.e., p(y,a|x).", 
        "77": "Assuming the availability of parse annotation in the target-side of a parallel corpus, we train the whole model jointly to maximize E(x,y,a)\u223cdata [log p(y,a|x)].", 
        "78": "In doing so, we notice that there are two separate paths through which the neural translation decoder receives error signal.", 
        "79": "First, the decoder is updated in order to maximize the conditional probability of the correct next word, which has already existed in the original neural machine translation.", 
        "80": "Second, the decoder is updated also to maximize the conditional probability of the correct parsing action, which is a novel learning signal introduced by the proposed hybridization.", 
        "81": "Furthermore, the second learning signal affects the encoder as well, encouraging the whole neural translation model to be aware of the syntactic structure of the target language.", 
        "82": "Later in the experiments, we show that this additional learning signal is useful for translation, even though we discard the RNNG (the stack and action sLSTMs) in the inference time.", 
        "83": "4.2 Knowledge Distillation for Parsing  A major challenge in training the proposed hybrid model is that there is not a parallel corpus augmented with gold-standard target-side parse, and vice versa.", 
        "84": "In other words, we must either parse the target-side sentences of an existing parallel corpus or translate sentences with existing goldstandard parses.", 
        "85": "As the target task of the proposed model is translation, we start with a parallel corpus and annotate the target-side sentences.", 
        "86": "It is however costly to manually annotate any corpus of reasonable size (Table 6 in Alonso et al., 2016).", 
        "87": "We instead resort to noisy, but automated annotation using an existing parser.", 
        "88": "This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al., 2009).", 
        "89": "In knowledge distillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our approach where the external parser could be thought of as a teacher and the proposed hybrid network\u2019s RNNG as a student.", 
        "90": "On the other hand, what we\npropose here is a special case of distant supervision in that the external parser provides noisy annotations to otherwise an unlabeled training set.", 
        "91": "Specifically, we use SyntaxNet, released by Andor et al.", 
        "92": "(2016), on a target sentence.3 We convert a parse tree into a sequence of one of three transition actions (SHIFT, REDUCE-L, REDUCE-R).", 
        "93": "We label each REDUCE action with a corresponding dependency label and treat it as a more finegrained action.", 
        "94": "5 Experiments    5.1 Language Pairs and Corpora  We compare the proposed NMT+RNNG against the baseline model on four different language pairs\u2013Jp-En, Cs-En, De-En and Ru-En.", 
        "95": "The basic statistics of the training data are presented in Table 1.", 
        "96": "We mapped all the low-frequency words to the unique symbol \u201cUNK\u201d and inserted a special symbol \u201cEOS\u201d at the end of both source and target sentences.", 
        "97": "Ja We use the ASPEC corpus (\u201ctrain1.txt\u201d) from the WAT\u201916 Jp-En translation task.", 
        "98": "We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT\u201916 (WAT, 2016).", 
        "99": "We use the first 100K sentence pairs of length shorter than 50 for training.", 
        "100": "The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus.", 
        "101": "We use \u201cdev.txt\u201d and \u201ctest.txt\u201d provided by WAT\u201916 respectively as development and test sets.", 
        "102": "Cs, De and Ru We use News Commentary v8.", 
        "103": "We removed noisy metacharacters and used the tokenizer from Moses (Koehn et al., 2007) to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, Ru and De.", 
        "104": "The target-side (English) vocabulary was constructed with all the unique tokens\n3When the target sentence is parsed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words.", 
        "105": "We use the plain SyntaxNet and do not train it furthermore.", 
        "106": "appearing more than three times in each corpus.", 
        "107": "We also excluded the sentence pairs which include empty lines in either a source sentence or a target sentence.", 
        "108": "We only use sentence pairs of length 50 or less for training.", 
        "109": "We use \u201cnewstest2015\u201d and \u201cnewstest2016\u201d as development and test sets respectively.", 
        "110": "5.2 Models, Learning and Inference  In all our experiments, each recurrent network has a single layer of LSTM units of 256 dimensions, and the word vectors and the action vectors are of 256 and 128 dimensions, respectively.", 
        "111": "To reduce computational overhead, we use BlackOut (Ji et al., 2015) with 2000 negative samples and \u03b1 = 0.4.", 
        "112": "When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016).", 
        "113": "For the proposed NMT+RNNG, we share the target word vectors between the decoder (buffer) and the stack sLSTM.", 
        "114": "Each weight is initialized from the uniform distribution [\u22120.1, 0.1].", 
        "115": "The bias vectors and the weights of the softmax and BlackOut are initialized to be zero.", 
        "116": "The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recommended in Jo\u0301zefowicz et al.", 
        "117": "(2015).", 
        "118": "We use stochastic gradient descent with minibatches of 128 examples.", 
        "119": "The learning rate starts from 1.0, and is halved each time the perplexity on the development set increases.", 
        "120": "We clip the norm of the gradient (Pascanu et al., 2012) with the threshold set to 3.0 (2.0 for the baseline models on RuEn and Cs-En to avoid NaN and Inf).", 
        "121": "When the perplexity of development data increased in training time, we halved the learning rate of stochastic gradient descent and reloaded the previous model.", 
        "122": "The RNNG\u2019s stack computes the vector of a dependency parse tree which consists of the generated target words by the buffer.", 
        "123": "Since the complete parse tree has a \u201cROOT\u201d node, the special token of the end of a sentence (\u201cEOS\u201d) is considered as the ROOT.", 
        "124": "We use beam search in the inference time, with the beam width selected based on the development set performance.", 
        "125": "It took about 15 minutes per epoch and about 20 minutes respectively for the baseline and the proposed model to train a full JP-EN parallel corpus in our implementation.4\n4We run all the experiments on multi-core CPUs (10  5.3 Results and Analysis  In Table 2, we report the translation qualities of the tested models on all the four language pairs.", 
        "126": "We report both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).", 
        "127": "Except for DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT+RNNG over the baseline model.", 
        "128": "It is worthwhile to note that these significant improvements have been achieved without any additional parameters nor computational overhead in the inference time.", 
        "129": "Ablation Since each component in RNNG may be omitted, we ablate each component in the proposed NMT+RNNG to verify their necessity.5 As shown in Table 3, we see that the best performance could only be achieved when all the three components were present.", 
        "130": "Removing the stack had the most adverse effect, which was found to be the case for parsing as well by Kuncoro et al.", 
        "131": "(2017).", 
        "132": "Generated Sentences with Parsed Actions The decoder part of our proposed model consists of two components: the NMT decoder to gener-\nthreads on Intel(R) Xeon(R) CPU E5-2680 v2 @2.80GHz) 5 Since the buffer is the decoder, it is not possible to completely remove it.", 
        "133": "Instead we simply remove the dependency of the action distribution on it.", 
        "134": "ate a translated sentence and the RNNG decoder to predict its parsing actions.", 
        "135": "The proposed model can therefore output a dependency structure along with a translated sentence.", 
        "136": "Figure 1 shows an example of JP-EN translation in the development dataset and its dependency parse tree obtained by the proposed model.", 
        "137": "The special symbol (\u201cEOS\u201d) is treated as the root node (\u201cROOT\u201d) of the parsed tree.", 
        "138": "The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3.", 
        "139": "The parsing actions were obtained by greedy search.", 
        "140": "The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between \u201cThe\u201d and \u201c transition\u201d should not be \u201cpobj\u201d.", 
        "141": "6 Conclusion  We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG.", 
        "142": "This model learns to parse and translate simultaneously, and training it encourages both the encoder and decoder to better incorporate linguistic priors.", 
        "143": "Our experiments confirmed its effectiveness on four language pairs ({JP, Cs, De, Ru}En).", 
        "144": "The RNNG can in principle be trained without ground-truth parses, and this would eliminate the need of external parsers completely.", 
        "145": "We leave the investigation into this possibility for future research.", 
        "146": "Acknowledgments  We thank Yuchen Qiao and Kenjiro Taura for their help to speed up the implementations of training and also Kazuma Hashimoto for his valuable comments and discussions.", 
        "147": "This work was supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number 15J12597 and\n16H01715.", 
        "148": "KC thanks support by eBay, Facebook, Google and NVIDIA."
    }, 
    "document_id": "P17-2012.pdf.json"
}
