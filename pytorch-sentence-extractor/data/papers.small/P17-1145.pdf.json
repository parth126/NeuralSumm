{
    "abstract_sentences": {
        "1": "This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings.", 
        "2": "First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary.", 
        "3": "Second, we apply word sense induction to deal with ambiguous words.", 
        "4": "Finally, we cluster the disambiguated version of the ambiguous input graph into synsets.", 
        "5": "Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph.", 
        "6": "Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1579\u20131590 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1145  1 Introduction  A synset is a set of mutual synonyms, which can be represented as a clique graph where nodes are words and edges are synonymy relations.", 
        "2": "Synsets represent word senses and are building blocks of WordNet (Miller, 1995) and similar resources such as thesauri and lexical ontologies.", 
        "3": "These resources are crucial for many natural language processing applications that require common sense reasoning, such as information retrieval (Gong et al., 2005) and question answering (Kwok et al., 2001; Zhou et al., 2013).", 
        "4": "However, for most languages, no manually-constructed resource is available that is comparable to the English WordNet in terms of coverage and quality.", 
        "5": "For instance, Kiselev et al.", 
        "6": "(2015) present a comparative analysis of lexical resources available for the Russian\nlanguage concluding that there is no resource compared to WordNet in terms of coverage and quality for Russian.", 
        "7": "This lack of linguistic resources for many languages urges the development of new methods for automatic construction of WordNetlike resources.", 
        "8": "The automatic methods foster construction and use of the new lexical resources.", 
        "9": "Wikipedia1, Wiktionary2, OmegaWiki3 and other collaboratively-created resources contain a large amount of lexical semantic information\u2014 yet designed to be human-readable and not formally structured.", 
        "10": "While semantic relations can be automatically extracted using tools such as DKPro JWKTL4 and Wikokit5, words in these relations are not disambiguated.", 
        "11": "For instance, the synonymy pairs (bank, streambank) and (bank, banking company) will be connected via the word \u201cbank\u201d, while they refer to the different senses.", 
        "12": "This problem stems from the fact that articles in Wiktionary and similar resources list undisambiguated synonyms.", 
        "13": "They are easy to disambiguate for humans while reading a dictionary article, but can be a source of errors for language processing systems.", 
        "14": "The contribution of this paper is a novel approach that resolves ambiguities in the input graph to perform fuzzy clustering.", 
        "15": "The method takes as an input synonymy relations between potentially ambiguous terms available in human-readable dictionaries and transforms them into a machine readable representation in the form of disambiguated synsets.", 
        "16": "Our method, called WATSET, is based on a new local-global meta-algorithm for fuzzy graph clustering.", 
        "17": "The underlying principle is to discover the word senses based on a local graph cluster-\n1http://www.wikipedia.org 2http://www.wiktionary.org 3http://www.omegawiki.org 4https://dkpro.github.io/dkpro-jwktl 5https://github.com/componavt/wikokit\n1579\ning, and then to induce synsets using global sense clustering.", 
        "18": "We show that our method outperforms other methods for synset induction.", 
        "19": "The induced resource eliminates the need in manual synset construction and can be used to build WordNet-like semantic networks for under-resourced languages.", 
        "20": "An implementation of our method along with induced lexical resources is available online.6  2 Related Work  Methods based on resource linking surveyed by Gurevych et al.", 
        "21": "(2016) gather various existing lexical resources and perform their linking to obtain a machine-readable repository of lexical semantic knowledge.", 
        "22": "For instance, BabelNet (Navigli and Ponzetto, 2012) relies in its core on a linking of WordNet and Wikipedia.", 
        "23": "UBY (Gurevych et al., 2012) is a general-purpose specification for the representation of lexical-semantic resources and links between them.", 
        "24": "The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required.", 
        "25": "Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text.", 
        "26": "In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Ve\u0301ronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense.", 
        "27": "An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005).", 
        "28": "In the case of WSI, such a network is a local neighborhood of one word.", 
        "29": "Nodes of the ego network are the words which are semantically similar to the target word.", 
        "30": "Such approaches are able to discover homonymous senses of words, e.g., \u201cbank\u201d as slope versus \u201cbank\u201d as organisation (Di Marco and Navigli, 2012).", 
        "31": "However, as the graphs are usually composed of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets.", 
        "32": "Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy,\n6https://github.com/dustalov/watset\nco-hyponymy, antonymy, etc.", 
        "33": "(Heylen et al., 2008; Panchenko, 2011); (2) clusters are not unique, i.e., one word can occur in clusters of different ego networks referring to the same sense, while in WordNet a word sense occurs only in a single synset.", 
        "34": "In our synset induction method, we use word ego network clustering similarly as in word sense induction approaches, but apply them to a graph of semantically clean synonyms.", 
        "35": "Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges a extracted from manually-created resources.", 
        "36": "According to the best of our knowledge, most experiments either employed graph-based word sense induction applied to text-derived graphs or relied on a linking-based method that already assumes availability of a WordNet-like resource.", 
        "37": "A notable exception is the ECO approach by Gonc\u0327alo Oliveira and Gomes (2014), which was applied to induce a WordNet of the Portuguese language called Onto.PT.7 We compare to this approach and to five other state-of-the-art graph clustering algorithms as the baselines.", 
        "38": "ECO (Gonc\u0327alo Oliveira and Gomes, 2014) is a fuzzy clustering algorithm that was used to induce synsets for a Portuguese WordNet from several available synonymy dictionaries.", 
        "39": "The algorithm starts by adding random noise to edge weights.", 
        "40": "Then, the approach applies Markov Clustering (see below) of this graph several times to estimate the probability of each word pair being in the same synset.", 
        "41": "Finally, candidate pairs over a certain threshold are added to output synsets.", 
        "42": "MaxMax (Hope and Keller, 2013) is a fuzzy clustering algorithm particularly designed for the word sense induction task.", 
        "43": "In a nutshell, pairs of nodes are grouped if they have a maximal mutual affinity.", 
        "44": "The algorithm starts by converting the undirected input graph into a directed graph by keeping the maximal affinity nodes of each node.", 
        "45": "Next, all nodes are marked as root nodes.", 
        "46": "Finally, for each root node, the following procedure is repeated: all transitive children of this root form a cluster and the root are marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster.", 
        "47": "Markov Clustering (MCL) (van Dongen, 2000) is a hard clustering algorithm for graphs based on simulation of stochastic flow in graphs.", 
        "48": "7http://ontopt.dei.uc.pt\nMCL simulates random walks within a graph by alternation of two operators called expansion and inflation, which recompute the class labels.", 
        "49": "Notably, it has been successfully used for the word sense induction task (Dorow and Widdows, 2003).", 
        "50": "Chinese Whispers (CW) (Biemann, 2006) is a hard clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step.", 
        "51": "At each iteration, the labels of all the nodes are updated according to the majority labels among the neighboring nodes.", 
        "52": "The algorithm has a meta-parameter that controls graph weights that can be set to three values: (1) top sums over the neighborhood\u2019s classes; (2) nolog downgrades the influence of a neighboring node by its degree or by (3) log of its degree.", 
        "53": "Clique Percolation Method (CPM) (Palla et al., 2005) is a fuzzy clustering algorithm for unweighted graphs that builds up clusters from k-cliques corresponding to fully connected subgraphs of k nodes.", 
        "54": "While this method is only commonly used in social network analysis, we decided to add it to the comparison as synsets are essentially cliques of synonyms, which makes it natural to apply an algorithm based on clique detection.", 
        "55": "3 The WATSET Method  The goal of our method is to induce a set of unambiguous synsets by grouping individual ambiguous synonyms.", 
        "56": "An outline of the proposed approach is depicted in Figure 1.", 
        "57": "The method takes a dictionary of ambiguous synonymy relations and a text corpus as an input and outputs synsets.", 
        "58": "Note that the method can be used without a background corpus, yet as our experiments will show, corpusbased information improves the results when utilizing it for weighting the word graph\u2019s edges.", 
        "59": "A synonymy dictionary can be perceived as a graph, where the nodes correspond to lexical entries (words) and the edges connect pairs of the nodes when the synonymy relation between them holds.", 
        "60": "The cliques in such a graph naturally form\ndensely connected sets of synonyms corresponding to concepts (Gfeller et al., 2005).", 
        "61": "Given the fact that solving the clique problem exactly in a graph is NP-complete (Bomze et al., 1999) and that these graphs typically contain tens of thousands of nodes, it is reasonable to use efficient hard graph clustering algorithms, like MCL and CW, for finding a global segmentation of the graph.", 
        "62": "However, the hard clustering property of these algorithm does not handle polysemy: while one word could have several senses, it will be assigned to only one cluster.", 
        "63": "To deal with this limitation, a word sense induction procedure is used to induce senses for all words, one at the time, to produce a disambiguated version of the graph where a word is now represented with one or many word senses.", 
        "64": "The concept of a disambiguated graph is described in (Biemann, 2012).", 
        "65": "Finally, the disambiguated word sense graph is clustered globally to induce synsets, which are hard clusters of word senses.", 
        "66": "More specifically, the method consists of five steps presented in Figure 1: (1) learning word embeddings; (2) constructing the ambiguous weighted graph of synonyms G; (3) inducing the word senses; (4) constructing the disambiguated weighted graph G\u2032 by disambiguating of neighbors with respect to the induced word senses; (5) global clustering of the graph G\u2032.", 
        "67": "3.1 Learning Word Embeddings  Since different graph clustering algorithms are sensitive to edge weighting, we consider distributional semantic similarity based on word embeddings as a possible edge weighting approach for our synonymy graph.", 
        "68": "As we show further, this approach improves over unweighted versions and yields the best overall results.", 
        "69": "3.2 Construction of a Synonymy Graph  We construct the synonymy graph G = (V,E) as follows.", 
        "70": "The set of nodes V includes every lexeme appearing in the input synonymy dictionaries.", 
        "71": "The set of undirected edges E is composed of all edges\n(u, v) \u2208 V \u00d7 V retrieved from one of the input synonymy dictionaries.", 
        "72": "We consider three edge weight representations:\n\u2022 ones that assigns every edge the constant weight of 1;\n\u2022 count that weights the edge (u, v) as the number of times the synonymy pair appeared in the input dictionaries;\n\u2022 sim that assigns every edge (u, v) a weight equal to the cosine similarity of skip-gram word vectors (Mikolov et al., 2013).", 
        "73": "As the graph G is likely to have polysemous words, the goal is to separate individual word senses using graph-based word sense induction.", 
        "74": "3.3 Local Clustering: Word Sense Induction  In order to facilitate global fuzzy clustering of the graph, we perform disambiguation of its ambiguous nodes as illustrated in Figure 2.", 
        "75": "First, we use a graph-based word sense induction method that is similar to the curvature-based approach of Dorow and Widdows (2003).", 
        "76": "In particular, removal of the nodes participating in many triangles tends to\nseparate the original graph into several connected components.", 
        "77": "Thus, given a word u, we extract a network of its nearest neighbors from the synonymy graph G. Then, we remove the original word u from this network and run a hard graph clustering algorithm that assigns one node to one and only one cluster.", 
        "78": "In our experiments, we test Chinese Whispers and Markov Clustering.", 
        "79": "The expected result of this is that each cluster represents a different sense of the word u, e.g.", 
        "80": ":\nbank1 {streambank, riverbank, .", 
        "81": ".", 
        "82": ".}", 
        "83": "bank2 {bank company, .", 
        "84": ".", 
        "85": ".}", 
        "86": "bank3 {bank building, building, .", 
        "87": ".", 
        "88": ".}", 
        "89": "bank4 {coin bank, penny bank, .", 
        "90": ".", 
        "91": ".}", 
        "92": "We denote, e.g., bank1, bank2 and other items as word senses referred to as senses(bank).", 
        "93": "We denote as ctx(s) a cluster corresponding to the word sense s. Note that the context words have no sense labels.", 
        "94": "They are recovered by the disambiguation approach described next.", 
        "95": "3.4 Disambiguation of Neighbors  Next, we disambiguate the neighbors of each induced sense.", 
        "96": "The previous step results in splitting word nodes into (one or more) sense nodes.", 
        "97": "However, nearest neighbors of each sense node are still ambiguous, e.g., (bank3, building?).", 
        "98": "To recover these sense labels of the neighboring words, we employ the following sense disambiguation approach proposed by Faralli et al.", 
        "99": "(2016).", 
        "100": "For each word u in the context ctx(s) of the sense s, we find the most similar sense of that word u\u0302 to the context.", 
        "101": "We use the cosine similarity measure between the context of the sense s and the context of each candidate sense u\u2032 of the word u:\nu\u0302 = argmax u\u2032\u2208 senses(u)\ncos(ctx(s), ctx(u\u2032)).", 
        "102": "A context ctx(\u00b7) is represented by a sparse vector in a vector space of all ambiguous words of all contexts.", 
        "103": "The result is a disambiguated context c\u0302tx(s) in a space of disambiguated words derived from its ambiguous version ctx(s):\nc\u0302tx(s) = {u\u0302 : u \u2208 ctx(s)}.", 
        "104": "3.5 Global Clustering: Synset Induction  Finally, we construct the word sense graph G\u2032 = (V \u2032, E\u2032) using the disambiguated senses instead of the original words and establishing the edges between these disambiguated senses:\nV \u2032 = \u22c3\nu\u2208V senses(u), E\u2032 =\n\u22c3\ns\u2208V \u2032 {s} \u00d7 c\u0302tx(s).", 
        "105": "Running a hard clustering algorithm on G\u2032 produces the desired set of synsets as our final result.", 
        "106": "Figure 2 illustrates the process of disambiguation of an input ambiguous graph on the example of the word \u201cbank\u201d.", 
        "107": "As one may observe, disambiguation of the nearest neighbors is a necessity to be able to construct a global version of the senseaware graph.", 
        "108": "Note that current approaches to WSI, e.g., (Ve\u0301ronis, 2004; Biemann, 2006; Hope and Keller, 2013), do not perform this step, but perform only local clustering of the graph since they do not aim at a global representation of synsets.", 
        "109": "3.6 Local-Global Fuzzy Graph Clustering  While we use our approach to synset induction in this work, the core of our method is the \u201clocalglobal\u201d fuzzy graph clustering algorithm, which can be applied to arbitrary graphs (see Figure 1).", 
        "110": "This method, summarized in Algorithm 1, takes an undirected graph G = (V,E) as the input and outputs a set of fuzzy clusters of its nodes V .", 
        "111": "This is a meta-algorithm as it operates on top of two hard clustering algorithms denoted as Clusterlocal and Clusterglobal, such as CW or MCL.", 
        "112": "At the first phase of the algorithm, for each node its senses are induced via ego network clustering (lines 1\u2013 7).", 
        "113": "Next, the disambiguation of each ego network is performed (lines 8\u201315).", 
        "114": "Finally, the fuzzy clusters are obtained by applying the hard clustering algorithm to the disambiguated graph (line 16).", 
        "115": "As a post-processing step, the sense labels can be removed to make the cluster elements subsets of V .", 
        "116": "4 Evaluation  We conduct our experiments on resources from two different languages.", 
        "117": "We evaluate our approach on two datasets for English to demonstrate its performance on a resource-rich language.", 
        "118": "Additionally, we evaluate it on two Russian datasets since Russian is a good example of an under-resourced language with a clear need for synset induction.", 
        "119": "4.1 Gold Standard Datasets  For each language, we used two differently constructed lexical semantic resources listed in Table 1 to obtain gold standard synsets.", 
        "120": "English.", 
        "121": "We use WordNet8, a popular English lexical database constructed by expert lexicographers.", 
        "122": "WordNet contains general vocabulary and\n8https://wordnet.princeton.edu\nAlgorithm 1 WATSET fuzzy graph clustering Input: a set of nodes V and a set of edges E. Output: a set of fuzzy clusters of V .", 
        "123": "1: for all u \u2208 V do 2: C \u2190 Clusterlocal(Ego(u)) // C = {C1, ...} 3: for i\u2190 1 .", 
        "124": ".", 
        "125": ".", 
        "126": "|C| do 4: ctx(ui)\u2190 Ci 5: senses(u)\u2190 senses(u) \u222a {ui} 6: end for 7: end for 8: V \u2032 \u2190 \u22c3u\u2208V senses(u) 9: for all s \u2208 V \u2032 do\n10: for all u \u2208 ctx(s) do 11: u\u0302\u2190 argmax\nu\u2032\u2208 senses(u) cos(ctx(s), ctx(u\u2032))\n12: end for 13: c\u0302tx(s)\u2190 {u\u0302 : u \u2208 ctx(s)} 14: end for 15: E\u2032 \u2190 \u22c3s\u2208V \u2032{s} \u00d7 c\u0302tx(s) 16: return Clusterglobal(V \u2032, E\u2032)\nappears to be de facto gold standard in similar tasks (Hope and Keller, 2013).", 
        "127": "We used WordNet 3.1 to derive the synonymy pairs from synsets.", 
        "128": "Additionally, we use BabelNet9, a large-scale multilingual semantic network constructed automatically using WordNet, Wikipedia and other resources.", 
        "129": "We retrieved all the synonymy pairs from the BabelNet 3.7 synsets marked as English.", 
        "130": "Russian.", 
        "131": "As a lexical ontology for Russian, we use RuWordNet10 (Loukachevitch et al., 2016), containing both general vocabulary and domainspecific synsets related to sport, finance, economics, etc.", 
        "132": "Up to a half of the words in this resource are multi-word expressions (Kiselev et al., 2015), which is due to the coverage of domainspecific vocabulary.", 
        "133": "RuWordNet is a WordNetlike version of the RuThes thesaurus that is constructed in the traditional way, namely by a small group of expert lexicographers (Loukachevitch, 2011).", 
        "134": "In addition, we use Yet Another RussNet11 (YARN) by Braslavski et al.", 
        "135": "(2016) as another gold standard for Russian.", 
        "136": "The resource is constructed using crowdsourcing and mostly covers general vocabulary.", 
        "137": "Particularly, non-expert users are allowed to edit synsets in a collaborative way loosely supervised by a team of project curators.", 
        "138": "Due to the ongoing development of the re-\n9http://www.babelnet.org 10http://ruwordnet.ru/en 11https://russianword.net/en\nsource, we selected as the gold standard only those synsets that were edited at least eight times in order to filter out noisy incomplete synsets.", 
        "139": "4.2 Evaluation Metrics  To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets.", 
        "140": "Given a synset containing n words, we generate a set of n(n\u22121)2 pairs of synonyms.", 
        "141": "The F-score calculated this way is known as Paired F-score (Manandhar et al., 2010; Hope and Keller, 2013).", 
        "142": "The advantage of this measure compared to other cluster evaluation measures, such as Fuzzy B-Cubed (Jurgens and Klapaftis, 2013), is its straightforward interpretability.", 
        "143": "4.3 Word Embeddings  English.", 
        "144": "We use the standard 300-dimensional word embeddings trained on the 100 billion tokens Google News corpus (Mikolov et al., 2013).12\nRussian.", 
        "145": "We use the 500-dimensional word embeddings trained using the skip-gram model with negative sampling (Mikolov et al., 2013) using a context window size of 10 with the minimal word frequency of 5 on a 12.9 billion tokens corpus of books.", 
        "146": "These embeddings were shown to produce state-of-the-art results in the RUSSE shared task13 and are part of the Russian Distributional Thesaurus (RDT) (Panchenko et al., 2017b).14  4.4 Input Dictionary of Synonyms  For each language, we constructed a synonymy graph using openly available language resources.", 
        "147": "The statistics of the graphs used as the input in the further experiments are shown in Table 2.", 
        "148": "12https://code.google.com/p/word2vec 13http://www.dialog-21.ru/en/\nevaluation/2015/semantic_similarity 14http://russe.nlpub.ru/downloads\nEnglish.", 
        "149": "Synonyms were extracted from the English Wiktionary15, which is the largest Wiktionary at the present moment in terms of the lexical coverage, using the DKPro JWKTL tool by Zesch et al.", 
        "150": "(2008).", 
        "151": "English words have been extracted from the dump.", 
        "152": "Russian.", 
        "153": "Synonyms from three sources were combined to improve lexical coverage of the input dictionary and to enforce confidence in jointly observed synonyms: (1) synonyms listed in the Russian Wiktionary extracted using the Wikokit tool by Krizhanovsky and Smirnov (2013); (2) the dictionary of Abramov (1999); and (3) the Universal Dictionary of Concepts (Dikonov, 2013).", 
        "154": "While the two latter resources are specific to Russian, Wiktionary is available for most languages.", 
        "155": "Note that the same input synonymy dictionaries were used by authors of YARN to construct synsets using crowdsourcing.", 
        "156": "The results on the YARN dataset show how close an automatic synset induction method can approximate manually created synsets provided the same starting material.16  5 Results  We compare WATSET with five state-of-the art graph clustering methods presented in Section 2: Chinese Whispers (CW), Markov Clustering (MCL), MaxMax, ECO clustering, and the clique percolation method (CPM).", 
        "157": "The first two algorithms perform hard clustering, while the last three are fuzzy clustering methods just like our method.", 
        "158": "While the hard clustering algorithms are able to discover clusters which correspond to synsets composed of unambigous words, they can produce wrong results in the presence of lexical ambiguity (one node belongs to several synsets).", 
        "159": "In our experiments, we rely on our own implementation of MaxMax and ECO as reference implementations are not available.", 
        "160": "For CW17, MCL18\n15We used the Wiktionary dumps of February 1, 2017.", 
        "161": "16We used the YARN dumps of February 7, 2017.", 
        "162": "17https://www.github.com/uhh-lt/\nchinese-whispers 18http://java-ml.sourceforge.net\nones, count, sim.", 
        "163": "Each bar corresponds to the top performance of a method in Tables 3 and 4.\nand CPM19, available implementations have been used.", 
        "164": "During the evaluation, we delete clusters equal or larger than the threshold of 150 words as they hardly can represent any meaningful synset.", 
        "165": "The notation WATSET[MCL, CWtop] means using MCL for local clustering and Chinese Whispers in the top mode for global clustering.", 
        "166": "5.1 Impact of Graph Weighting Schema  Figure 3 presents an overview of the evaluation results on both datasets.", 
        "167": "The first step, common for all of the tested synset induction methods, is graph construction.", 
        "168": "Thus, we started with an analysis of three ways to weight edges of the graph introduced in Section 3.2: binary scores (ones), frequencies (count), and semantic similarity scores (sim) based on word vector similarity.", 
        "169": "Results across various configurations and methods indicate that using the weights based on the similarity scores provided by word embeddings is the best strategy for all methods except MaxMax on the English datasets.", 
        "170": "However, its performance using the ones weighting does not exceed the other methods using the sim weighting.", 
        "171": "Therefore, we report all further results on the basis of the sim weights.", 
        "172": "The edge weighting scheme impacts Russian more for most algorithms.", 
        "173": "The CW algorithm however remains sensitive to the weighting also for the English dataset due to its randomized nature.", 
        "174": "19https://networkx.github.io  5.2 Comparative Analysis  Table 3 and 4 present evaluation results for both languages.", 
        "175": "For each method, we show the best configurations in terms of F-score.", 
        "176": "One may note that the granularity of the resulting synsets, especially for Russian, is very different, ranging from 4 000 synsets for the CPMk=3 method to 67 645 induced by the ECO method.", 
        "177": "Both tables report the number of words, synsets and synonyms after pruning huge clusters larger than 150 words.", 
        "178": "Without this pruning, the MaxMax and CPM methods tend to discover giant components obtaining almost zero precision as we generate all possible pairs of nodes in such clusters.", 
        "179": "The other methods did not show such behavior.", 
        "180": "WATSET robustly outperforms all other methods according to F-score on both English datasets (Table 3) and on the YARN dataset for Russian (Table 4).", 
        "181": "Also, it outperforms all other methods according to recall on both Russian datasets.", 
        "182": "The disambiguation of the input graph performed by the WATSET method splits nodes belonging to several local communities to several nodes, significantly facilitating the clustering task otherwise complicated by the presence of the hubs that wrongly link semantically unrelated nodes.", 
        "183": "Interestingly, in all the cases, the toughest competitor was a hard clustering algorithm\u2014MCL (van Dongen, 2000).", 
        "184": "We observed that the \u201cplain\u201d MCL successfully groups monosemous words, but\nisolates the neighborhood of polysemous words, which results in the recall drop in comparison to WATSET.", 
        "185": "CW operates faster due to a simplified update step.", 
        "186": "On the same graph, CW tends to produce larger clusters than MCL.", 
        "187": "This leads to a higher recall of \u201cplain\u201d CW as compared to the \u201cplain\u201d MCL, at the cost of lower precision.", 
        "188": "Using MCL instead of CW for sense induction in WATSET expectedly produces more finegrained senses.", 
        "189": "However, at the global clustering step, these senses erroneously tend to form coarsegrained synsets connecting unrelated senses of the ambiguous words.", 
        "190": "This explains the generally higher recall of WATSET[MCL, \u00b7].", 
        "191": "Despite the randomized nature of CW, variance across runs do not affect the overall ranking: The rank of different versions of CW (log, nolog, top) can change, while the rank of the best CW configuration compared to other methods remains the same.", 
        "192": "The MaxMax algorithm shows mixed results.", 
        "193": "On the one hand, it outputs large clusters uniting more than hundred nodes.", 
        "194": "This inevitably leads to a high recall, as it is clearly seen in the results for Russian because such synsets still pass\nunder our cluster size threshold of 150 words.", 
        "195": "Its synsets on English datasets are even larger and get pruned, which results in low recall.", 
        "196": "On the other hand, smaller synsets having at most 10\u201315 words were identified correctly.", 
        "197": "MaxMax appears to be extremely sensible to edge weighting, which also complicates its practical use.", 
        "198": "The CPM algorithm showed unsatisfactory results, emitting giant components encompassing thousands of words.", 
        "199": "Such clusters were automatically pruned, but the remaining clusters are relatively correctly built synsets, which is confirmed by the high values of precision.", 
        "200": "When increasing the minimal number of elements in the clique k, recall improves, but at the cost of a dramatic precision drop.", 
        "201": "We suppose that the network structure assumptions exploited by CPM do not accurately model the structure of our synonymy graphs.", 
        "202": "Finally, the ECO method yielded the worst results because the most cluster candidates failed to pass through the constant threshold used for estimating whether a pair of words should be included in the same cluster.", 
        "203": "Most synsets produced by this method were trivial, i.e., containing only a single\nword.", 
        "204": "The remaining synsets for both languages have at most three words that have been connected by a chance due to the edge noising procedure used in this method resulting in low recall.", 
        "205": "6 Discussion  On the absolute scores.", 
        "206": "The results obtained on all gold standards (Figure 3) show similar trends in terms of relative ranking of the methods.", 
        "207": "Yet absolute scores of YARN and RuWordNet are substantially different due to the inherent difference of these datasets.", 
        "208": "RuWordNet is more domainspecific in terms of vocabulary, so our input set of generic synonymy dictionaries has a limited coverage on this dataset.", 
        "209": "On the other hand, recall calculated on YARN is substantially higher as this resource was manually built on the basis of synonymy dictionaries used in our experiments.", 
        "210": "The reason for low absolute numbers in evaluations is due to an inherent vocabulary mismatch between the input dictionaries of synonyms and the gold datasets.", 
        "211": "To validate this hypothesis, we performed a cross-resource evaluation presented in Table 5.", 
        "212": "The low performance of the crossevaluation of the two resources supports the hypothesis: no single resource for Russian can obtain high recall scores on another one.", 
        "213": "Surprisingly, even BabelNet, which integrates most of available lexical resources, still does not reach a recall substantially larger than 0.5.20 Note that the results of this cross-dataset evaluation are not directly comparable to results in Table 4 since in our experiments we use much smaller input dictionaries than those used by BabelNet.", 
        "214": "On sparseness of the input dictionary.", 
        "215": "Table 6 presents some examples of the obtained synsets of various sizes for the top WATSET configuration on both languages.", 
        "216": "As one might observe, the qual-\n20We used BabelNet 3.7 extracting all 3 497 327 synsets that were marked as Russian.", 
        "217": "ity of the results is highly plausible.", 
        "218": "However, one limitation of all approaches considered in this paper is the dependence on the completeness of the input dictionary of synonyms.", 
        "219": "In some parts of the input synonymy graph, important bridges between words can be missing, leading to smallerthan-desired synsets.", 
        "220": "A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relations.", 
        "221": "7 Conclusion  We presented a new robust approach to fuzzy graph clustering that relies on hard graph clustering.", 
        "222": "Using ego network clustering, the nodes belonging to several local communities are split into several nodes each belonging to one community.", 
        "223": "The transformed \u201cdisambiguated\u201d graph is then clustered using an efficient hard graph clustering algorithm, obtaining a fuzzy clustering as the result.", 
        "224": "The disambiguated graph facilitates clustering as it contains fewer hubs connecting unrelated nodes from different communities.", 
        "225": "We apply this meta clustering algorithm to the task of synset induction on two languages, obtaining the best results on three datasets and competitive results on one dataset in terms of F-score as compared to five state-of-the-art graph clustering methods.", 
        "226": "Acknowledgments  We acknowledge the support of the Deutsche Forschungsgemeinschaft (DFG) foundation under the \u201cJOIN-T\u201d project, the DAAD, the RFBR under the project no.", 
        "227": "16-37-00354 mol a, and the RFH under the project no.", 
        "228": "16-04-12019.", 
        "229": "We also thank three anonymous reviewers for their helpful comments, Andrew Krizhanovsky for providing a parsed Wiktionary, Natalia Loukachevitch for the provided RuWordNet dataset, and Denis Shirgin who suggested the WATSET name."
    }, 
    "document_id": "P17-1145.pdf.json"
}
