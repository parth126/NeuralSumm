{
    "abstract_sentences": {
        "1": "Pew research polls report 62 percent of U.S. adults get news on social media (Gottfried and Shearer, 2016).", 
        "2": "In a December poll, 64 percent of U.S. adults said that \u201cmade-up news\u201d has caused a \u201cgreat deal of confusion\u201d about the facts of current events (Barthel et al., 2016).", 
        "3": "Fabricated stories in social media, ranging from deliberate propaganda to hoaxes and satire, contributes to this confusion in addition to having serious effects on global stability.", 
        "4": "In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four subtypes of suspicious news \u2013 satire, hoaxes, clickbait and propaganda.", 
        "5": "We show that neural network models trained on tweet content and social network interactions outperform lexical models.", 
        "6": "Unlike previous work on deception detection, we find that adding syntax and grammar features to our models does not improve performance.", 
        "7": "Incorporating linguistic features improves classification results, however, social interaction features are most informative for finer-grained separation between four types of suspicious news posts."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 647\u2013653 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2102\nPew research polls report 62 percent of U.S. adults get news on social media (Gottfried and Shearer, 2016).", 
        "2": "In a December poll, 64 percent of U.S. adults said that \u201cmade-up news\u201d has caused a \u201cgreat deal of confusion\u201d about the facts of current events (Barthel et al., 2016).", 
        "3": "Fabricated stories in social media, ranging from deliberate propaganda to hoaxes and satire, contributes to this confusion in addition to having serious effects on global stability.", 
        "4": "In this work we build predictive models to classify 130 thousand news posts as suspicious or verified, and predict four subtypes of suspicious news \u2013 satire, hoaxes, clickbait and propaganda.", 
        "5": "We show that neural network models trained on tweet content and social network interactions outperform lexical models.", 
        "6": "Unlike previous work on deception detection, we find that adding syntax and grammar features to our models does not improve performance.", 
        "7": "Incorporating linguistic features improves classification results, however, social interaction features are most informative for finer-grained separation between four types of suspicious news posts.", 
        "8": "1 Introduction  Popular social media platforms such as Twitter and Facebook have proven to be effective channels for disseminating falsified information, unverified claims, and fabricated attention-grabbing stories due to their wide reach and the speed at which this information can be shared.", 
        "9": "Recently, there has been an increased number of disturbing incidents of fabricated stories proliferated through\nsocial media having a serious impact on real-world events (Perrott, 2016; Connolly et al., 2016)\nFalse news stories distributed in social media vary depending on the intent behind falsification.", 
        "10": "Unlike verified news, suspicious news tends to build narratives rather than report facts.", 
        "11": "On one extreme is disinformation which communicates false facts to deliberately deceive readers or promote a biased agenda.", 
        "12": "These include posts generated and retweeted from propaganda and so-called clickbait (\u201ceye-catching\u201d headlines) accounts.", 
        "13": "The intent behind propaganda and clickbait varies from opinion manipulation and attention redirection to monetization and traffic attraction.", 
        "14": "Hoaxes are another type of disinformation that aims to deliberately deceive the reader (Tambuscio et al., 2015; Kumar et al., 2016).", 
        "15": "On the other extreme is satire, e.g., @TheOnion, where the writer\u2019s primary purpose is not to mislead the reader, but rather entertain or criticize (Conroy et al., 2015).", 
        "16": "However, satirical news and hoaxes may also be harmful, especially when they are shared out of context (Rubin et al., 2015).", 
        "17": "Our novel contributions in this paper are twofold.", 
        "18": "We first investigate several features and neural network architectures for automatically classifying verified and suspicious news posts, and four sub-types of suspicious news.", 
        "19": "We find that incorporating linguistic and network features via a \u201clate fusion\u201d technique boosts performance.", 
        "20": "We then investigate differences between verified and suspicious news tweets by conducting a statistical analysis of linguistic features in both types of account.", 
        "21": "We show significant differences in use of biased, subjective language and moral foundations behind suspicious and trustworthy news posts.", 
        "22": "Our analysis and experiments rely on a large Twitter corpus1 collected during a two-week pe-\n1Data available at: http://www.cs.jhu.edu/\u223csvitlana/\n647\nriod around terrorist attacks in Brussels in 2016.", 
        "23": "Our method of collection ensures that our models learn from verified and suspicious news within a predefined timeframe, and further ensures homogeneity of deceptive texts in length and writing manner (Rubin et al., 2015).", 
        "24": "Several tools have been recently developed to verify and reestablish trusted sources of information online e.g., Google fact checking (Gindras, 2016) and Facebook repost verification (Mosseri, 2016).", 
        "25": "These projects, among others, teach news literacy2 and contribute to fact-checking online.3 We believe our models and novel findings on linguistic differences between suspicious and verified news will contribute to these fact-checking systems, as well as help readers to judge the accuracy of information they consume in social media.", 
        "26": "2 Data  Suspicious News We relied on several public resources that annotate suspicious Twitter accounts or their corresponding websites as propaganda, hoax, clickbait and satire.", 
        "27": "They include propaganda accounts identified by PropOrNot,4 satire, clickbait and hoax accounts.5 In total we collected 174 suspicious news accounts.6 In addition, we manually confirmed that accounts and their corresponding webpages labeled by PropOrNot have one or more signs of propaganda listed below: (a) tries to persuade; (b) influences the specific emotions, attitudes, opinions, and actions; (c) target audiences for political, ideological, and religious purposes; and (d) contains selectively-omitting and one-sided messages.", 
        "28": "Figure 1 presents a communication network between verified and suspicious news accounts.", 
        "29": "We observe that verified accounts are connected to\n2News Literacy: http://www.thenewsliteracyproject.org/ 3Fact checking: http://reporterslab.org/fact-checking/\nHoaxy: http://hoaxy.iuni.iu.edu/ 4Propaganda: http://www.propornot.com/p/the-list.html 5http://www.fakenewswatch.com/ 6To ensure the quality of suspicious account labels we manually verified them.", 
        "30": "(via RTs and mentions) some suspicious news accounts \u2013 clickbaits and propaganda.", 
        "31": "Verified News We manually constructed a list of 252 \u201ctrusted\u201d news accounts that tweet in English and checked whether they are verified on Twitter.", 
        "32": "We release the final verified list of trusted and suspicious news accounts used in our analysis.7\nTweet Corpus We query the Twitter firehose from Mar 15 to Mar 29 2016 \u2013 one week before and after Brussels bombing on Mar 22 2016 for 174 suspicious and 252 verified news accounts.", 
        "33": "We collected retweets generated by any user that mentions one of these accounts and assign the corresponding label propagated from suspicious or trusted news.8 We de-duplicated, lowercased, and tokenized these posts and applied standard NLP preprocessing.", 
        "34": "We extracted part-of-speech tags and dependency parses for 130 thousand tweets using SyntaxNet (Petrov, 2016).", 
        "35": "3 Models  We propose linguistically-infused neural network models to classify social media posts retweeted from news accounts into verified and suspicious categories \u2013 propaganda, hoax, satire and clickbait.", 
        "36": "Our models incorporate tweet text, social graph, linguistic markers of bias and subjectivity, and moral foundation features.", 
        "37": "We experiment with several baseline models, and develop neural network architectures presented in Figure 2 in the\n7The lists of verified and suspicious news: http://www.cs.jhu.edu/\u223csvitlana/TwitterList\n8Suspicious news annotations should be done on a tweet rather than an account level.", 
        "38": "However, these annotations are extremely costly and time consuming.", 
        "39": "Keras framework.9 We rely on state-of-the-art layers effectively used in text classification \u2013 Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) (Johnson and Zhang, 2014; Zhang and Wallace, 2015).", 
        "40": "The content subnetwork consists of an embedding layer and either (a) one LSTM layer or (b) two 1-dimensional convolution layers followed by a max-pooling layer.", 
        "41": "We initialize our embedding layer with pretrained GloVe embeddings (Pennington et al., 2014).", 
        "42": "The social graph sub-network is a simple feed-forward network that takes one-hot vectors of user interactions, e.g.", 
        "43": "@mentions, as input.", 
        "44": "We are careful to exclude source @mentions from these vectors, as these were used to derive labels for our networks and would likely lead to overfitting.", 
        "45": "In addition to content and network signals, we incorporate other linguistic cues into our networks.", 
        "46": "For this we rely on the \u201clate fusion\u201d approach that has been shown to be effective in vision tasks (Karpathy et al., 2014; Park et al., 2016).", 
        "47": "\u201cFusion\u201d allows for a network to learn a combined representation of multiple input streams.", 
        "48": "This fusion can be done early (in the feature extraction layers) or later (in the later extraction layers, or in classification layers).", 
        "49": "In our case, we use fusion as a technique for training networks to learn how to combine data representations from different modalities (network and text features) to boost performance.", 
        "50": "We train our models for 10 epochs using the ADAM optimization algorithm, and evaluate them using 10 fold crossvalidation (Kingma and Ba, 2014).", 
        "51": "Baselines We compare our neural network architectures to several baselines.", 
        "52": "Word- and document-level embeddings have been shown to be effective as input to simpler classifiers.", 
        "53": "We experiment with several feature inputs for testing baseline classifiers: (a) TFIDF features, (b) Doc2Vec vectors and (c) Doc2Vec or TFIDF features concatenated with linguistic or network features.", 
        "54": "In the case of Doc2Vec features, we induce 200-dimensional vectors for each tweet using the gensim library,10 training for 15 epochs.", 
        "55": "Bias cues Inspired by earlier work on identifying biased language on Wikipedia (Recasens et al., 2013) we extract hedges (expressions of tentativeness and possibility) (Hyland, 2005), assertive verbs (the level of certainty in the complement\n9Keras: https://keras.io/ 10https://radimrehurek.com/gensim/models/doc2vec.html\nclause) (Hooper, 1975), factive verbs (presuppose the truth of their complement clause) (Kiparsky and Kiparsky, 1968), implicative verbs (imply the truth or untruth of their complement) (Karttunen, 1971) and report verbs (Recasens et al., 2013) from preprocessed tweets.", 
        "56": "Subjectivity cues We rely on external publicly available subjectivity, and positive and negative opinion lexicons to extract strongly and weakly subjective words (Riloff and Wiebe, 2003), positive and negative opinion words (Liu et al., 2005).", 
        "57": "Psycholinguistic cues In addition to biased and subjective language cues, we extract Linguistic Inquiry Word Count (LIWC) features (Pennebaker et al., 2001) to capture additional signals of persuasive and biased language in tweets.", 
        "58": "LIWC features have been successfully used for deception detection before (Hancock et al., 2007; Vrij et al., 2007; Mihalcea and Strapparava, 2009).", 
        "59": "For example, persuasive language cues in LIWC include statistics and factual data, rhetorical questions, imperative commands, personal pronouns, and emotional language.", 
        "60": "Additional biased language cues captured by LIWC are quotations, markers of certainty, inclusions and conjunctions.", 
        "61": "Extra subjective language cues in LIWC cover positive and negative emotion and anxiety words.", 
        "62": "Moral foundation cues According to Haidt and Graham (2007); Graham et al.", 
        "63": "(2009), there is a small number of basic widely supported moral values, and people differ in the way they endorse these values.", 
        "64": "Moral foundations include care and harm, fairness and cheating, loyalty and betrayal, authority and subversion, and purity and degradation.", 
        "65": "We hypothesize that suspicious news could appeal to specific moral foundations of their read-\ners in a way that is distinct from verified news accounts.", 
        "66": "Thus, they could help in predicting verified vs. suspicious news, as well as different suspicious news types.", 
        "67": "4 Results    4.1 Classification  Table 2 presents classification results for Task 1 (binary) \u2013 suspicious vs. verified news posts and Task 2 (multi-class) \u2013 four types of suspicious tweets e.g., propaganda, hoaxes, satire and clickbait.", 
        "68": "We report performance for different model and feature combinations.", 
        "69": "We find that our neural network models (both CNNs and RNNs) significantly outperform logistic regression baselines learned from all feature combinations.11 The accuracy improvement for the binary task is 0.2 and F1-macro boost for the multi-class task is 0.07.", 
        "70": "We also observe that all models learned from network and tweet text signals outperform models trained exclusively on tweets.", 
        "71": "We report 0.05 accuracy improvement for Task 1, and 0.02 F1 boost for Task 2.", 
        "72": "Adding linguistic cues to basic tweet representations significantly improves results across all models.", 
        "73": "Finally, by combining basic content with network and linguistic features via late fusion, our neural network models achieve best results in binary experiments.", 
        "74": "Interestingly, models perform best in the multiclass case when trained on tweet embeddings and fused network features alone.", 
        "75": "We report 0.95 accuracy when inferring suspicious vs. verified news posts, and 0.7 F1-macro when classifying types of suspicious news.", 
        "76": "Syntax and grammar features have been predictive of deception in the product review domain (Feng et al., 2012; Pe\u0301rez-Rosas and Mihalcea, 2015).", 
        "77": "However, unlike earlier work we find that fusing these features into our models significantly decreases performance \u2013 by 0.02 accuracy for the binary task and 0.02 F1 for multi-class.", 
        "78": "This may be explained by the domain differences between reviews and tweets which are shorter, more noisy and difficult to parse.", 
        "79": "4.2 Linguistic Analysis  We measure statistically significant differences in linguistic markers of bias, subjectivity and moral\n11We experimented with other baseline models, such as Random Forest, but found negligible difference between these results and those obtained via logistic regression.", 
        "80": "foundations across different types of suspicious news, and contrast them with verified news using resources described in Section 3.", 
        "81": "These novel findings presented in Table 3 provide deeper understanding of model performance in Table 2.", 
        "82": "Verified news tweets contain significantly less bias markers, hedges and subjective terms and less harm/care, loyalty/betrayal and authority moral cues compared to suspicious news tweets.", 
        "83": "Satirical news are the most different from propaganda and hoaxes; and propaganda, hoax and clickbait news are the most similar based on moral, bias and subjectivity cues.", 
        "84": "Propaganda news target morals more than satire and hoaxes, but less than clickbait.", 
        "85": "Satirical news contains more loyalty and less betrayal morals compared to propaganda, hoaxes and clickbait news.", 
        "86": "Propaganda news target authority more than satire and hoaxes, and fairness more than satire.", 
        "87": "Hoaxes and propaganda news contain significantly less bias markers (e.g, hedging, implicative and factive verbs) compared to satire.", 
        "88": "However, propaganda and clickbait news contain significantly more factive verbs and bias language markers compared to hoaxes.", 
        "89": "Satirical news use significantly more subjective terms compared to other news, while clickbait news use more subjective cues than propaganda and hoaxes.", 
        "90": "CUES V\u2194 F P\u2194 S P\u2194 H P\u2194 C S\u2194 H S\u2194 C H\u2194 C  4.3 Suspicious News Retweet Patterns  In addition to contrasting linguistic realizations behind different types of suspicious news on Twitter, we are interested in qualitatively evaluating differences in retweet patterns across suspicious news types (Lumezanu and Klein, 2012; Mendoza et al., 2010; Kumar et al., 2016).", 
        "91": "Figure 3 presents top retweeted tweets over time across three types of suspicious news \u2013 hoaxes, propaganda, and clickbaits and contrasts them to wellstudied retweeting behaviors of rumors.", 
        "92": "We observe that users retweeting propaganda, clickbaits and hoaxes send high volumes of tweets over short periods of time.", 
        "93": "Rumors12 are less spiky but active over significantly longer periods of time compared to other suspicious news.", 
        "94": "We also notice that rumors and propaganda contain the majority of topics related to Brussels bombing, but clickbaits and hoaxes promote very divergent set of topics.", 
        "95": "12We identified rumors relevant to Brussels bombing: http://www.cs.jhu.edu/\u223csvitlana/BrusselsRumorList  5 Summary  We built linguistically-infused neural network models that jointly learn from tweet content and social network interactions to classify suspicious and verified news tweets and infer specific types of suspicious news.", 
        "96": "Future work may focus on utilizing more sophisticated discourse and pragmatics features, and inferring degrees of credibility.", 
        "97": "We hope our findings on bias and subjectivity in suspicious news will help readers to better judge about credibility of news in social media.", 
        "98": "6 Acknowledgments  The research described in this paper was conducted under the High-Performance Analytics Program and the Laboratory Directed Research and Development Program at Pacific Northwest National Laboratory, a multiprogram national laboratory operated by Battelle for the U.S. Department of Energy."
    }, 
    "document_id": "P17-2102.pdf.json"
}
