{
    "abstract_sentences": {
        "1": "Despite sequences being core to NLP, scant work has considered how to handle noisy sequence labels from multiple annotators for the same text.", 
        "2": "Given such annotations, we consider two complementary tasks: (1) aggregating sequential crowd labels to infer a best single set of consensus annotations; and (2) using crowd annotations as training data for a model that can predict sequences in unannotated text.", 
        "3": "For aggregation, we propose a novel Hidden Markov Model variant.", 
        "4": "To predict sequences in unannotated text, we propose a neural approach using Long Short Term Memory.", 
        "5": "We evaluate a suite of methods across two different applications and text genres: Named-Entity Recognition in news articles and Information Extraction from biomedical abstracts.", 
        "6": "Results show improvement over strong baselines.", 
        "7": "Our source code and data are available online1."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 299\u2013309 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1028  1 Introduction  Many important problems in Natural Language Processing (NLP) may be viewed as sequence labeling tasks, such as part-of-speech (PoS) tagging, named-entity recognition (NER), and Information Extraction (IE).", 
        "2": "As with other machine learning tasks, automatic sequence labeling typically requires annotated corpora on which to train predictive models.", 
        "3": "While such annotation was traditionally performed by domain experts, crowdsourcing has become a popular means to acquire large labeled datasets at lower cost, though annotations from laypeople may be lower quality than those from domain experts (Snow et al., 2008).", 
        "4": "It\n1 Soure code and biomedical abstract data: www.github.com/thanhan/seqcrowd-acl17, www.byronwallace.com/EBM_abstracts_data\nis therefore essential to model crowdsourced label quality, both to estimate individual annotator reliability and to aggregate individual annotations to induce a single set of \u201creference standard\u201d consensus labels.", 
        "5": "While many models have been proposed for aggregating crowd labels for binary or multiclass classification problems (Sheshadri and Lease, 2013), far less work has explored crowdbased annotation of sequences (Finin et al., 2010; Hovy et al., 2014; Rodrigues et al., 2014).", 
        "6": "In this paper, we investigate two complementary challenges in using sequential crowd labels: how to best aggregate them (Task 1); and how to accurately predict sequences in unannotated text given training data from the crowd (Task 2).", 
        "7": "For aggregation, one might want to induce a single set of high-quality consensus annotations for various purposes: (i) for direct use at run-time (when a given application requires human-level accuracy in identifying sequences); (ii) for sharing with others; or (iii) for training a predictive model.", 
        "8": "When human-level accuracy in tagging of sequences is not crucial, automatic labeling of unannotated text is typically preferable, as it is more efficient, scalable, and cost-effective.", 
        "9": "Given a training set of crowd labels, how can we best predict sequences in unannotated text?", 
        "10": "Should we: (i) consider Task 1 as a pre-processing step and train the model using consensus labels; or (ii) instead directly train the model on all of the individual annotations, as done by Yang et al.", 
        "11": "(2010)?", 
        "12": "We investigate both directions in this work.", 
        "13": "Our approach is to augment existing sequence labeling models such as HMMs (Rabiner and Juang, 1986) and LSTMs (Hochreiter and Schmidhuber, 1997; Lample et al., 2016) by introducing an explicit \u201dcrowd component\u201d.", 
        "14": "For HMMs, we model this crowd component by including additional parameters for worker label quality and crowd label variables.", 
        "15": "For the LSTM, we introduce a vector representation for each annotator.", 
        "16": "In\n299\nboth cases, the crowd component models both the noise from labels and the label quality from each annotator.", 
        "17": "We find that principled combination of the \u201ccrowd component\u201d with the \u201csequence component\u201d yields strong improvement.", 
        "18": "For evaluation, we consider two practical applications in two text genres: NER in news and IE from medical abstracts.", 
        "19": "Recognizing namedentities such as people, organizations or locations can be viewed as a sequence labeling task in which each label specifies whether each word is Inside, Outside or Beginning (IOB) a namedentity.", 
        "20": "For this task, we consider the English portion of the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), using crowd labels collected by Rodrigues et al.", 
        "21": "(2014).", 
        "22": "For the IE application, we use a set of biomedical abstracts that describe Randomized Controlled Trials (RCTs).", 
        "23": "The crowdsourced annotations comprise labeled text spans that describe the patient populations enrolled in the corresponding RCTs.", 
        "24": "For example, an abstract may contain the text: we recruited and enrolled diabetic patients.", 
        "25": "Identifying these sequences is useful for downstream systems that process biomedical literature, e.g., clinical search engines (Huang et al., 2006; Schardt et al., 2007; Wallace et al., 2016).", 
        "26": "Contributions.", 
        "27": "We present a systematic investigation and evaluation of alternative methods for handling and utilizing crowd labels for sequential annotation tasks.", 
        "28": "We consider both how to best aggregate sequential crowd labels (Task 1) and how to best predict sequences in unannotated text given a training set of crowd annotations (Task 2).", 
        "29": "As part of this work, we propose novel models for working with noisy sequence labels from the crowd.", 
        "30": "Reported experiments both benchmark existing state-of-the-art approaches (sequential and non-sequential) and show that our proposed models achieve best-in-class performance.", 
        "31": "As noted in the Abstract, we have also shared our sourcecode and data online for use by the community.", 
        "32": "2 Related Work  We briefly review two separate threads of relevant prior work: (1) sequence labeling models; and (2) aggregation of crowdsourcing annotations.", 
        "33": "Sequence labeling.", 
        "34": "Early work on learning for sequential tasks used HMMs (Bikel et al., 1997).", 
        "35": "HMMs are a class of generative probabilistic models comprising two components: an emission\nmodel from a hidden state to an observation and a transition model from a hidden state to the next hidden state.", 
        "36": "Later work focused on discriminative models such as Maximum Entropy Models (Chieu and Ng, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001).", 
        "37": "These were able to achieve strong predictive performance by exploiting arbitrary features, but they may not be the best choice for label aggregation.", 
        "38": "Also, compared to the simple HMM model, discriminative sequentially structured models require more complex optimization and are generally more difficult to extend.", 
        "39": "Here we argue for the generative HMMs for our first task of aggregating crowd labels.", 
        "40": "The generative nature of HMMs is a good fit for existing crowd modeling techniques and also enables very efficient parameter estimation.", 
        "41": "In addition to the supervised setting, previous work has studied unsupervised HMMs, e.g., for PoS induction (Goldwater and Griffiths, 2007; Johnson, 2007).", 
        "42": "These works are similar to our work in trying to infer the hidden states without labeled data.", 
        "43": "Our graphical model is different in incorporating signal from the crowd labels.", 
        "44": "For Task 2 (training predictive models), we consider CRFs and LSTMs.", 
        "45": "CRFs are undirected, conditional models that can exploit arbitrary features.", 
        "46": "They have achieved strong performance on many sequence labeling tasks (McCallum and Li, 2003), but they depend on hand-crafted features.", 
        "47": "Recent work has considered end-to-end neural architectures that learn features, e.g., Convolutional Neural Networks (CNNs) (Collobert et al., 2011; Kim, 2014; Zhang and Wallace, 2015) and LSTMs (Lample et al., 2016).", 
        "48": "Here we modify the LSTM model proposed by Lample et al.", 
        "49": "(2016) by augmenting the network with \u2018crowd worker vectors\u2019.", 
        "50": "Crowdsourcing.", 
        "51": "Acquiring labeled data is critical for training supervised models.", 
        "52": "Snow et al.", 
        "53": "(2008) proposed using Amazon Mechanical Turk to collect labels in NLP quickly and at low cost, albeit with some degradation in quality.", 
        "54": "Subsequent work has developed models for improving aggregate label quality (Raykar et al., 2010; Felt et al., 2015; Kajino et al., 2012; Bi et al., 2014; Liu et al., 2012; Hovy et al., 2013).", 
        "55": "Sheshadri and Lease (2013) survey and benchmark methods.", 
        "56": "However, these models are almost all in the binary or multiclass classification setting; only a few have considered sequence labeling.", 
        "57": "Dredze et al.", 
        "58": "(2009) proposed a method for learning a CRF\nmodel from multiple labels (although the identities of the annotators or workers were not used).", 
        "59": "Rodrigues et al.", 
        "60": "(2014) extended this approach to account for worker identities, providing a joint \u201dcrowd-CRF\u201d model.", 
        "61": "They collected a dataset of crowdsourced labels for a portion of the CoNLL 2003 dataset.", 
        "62": "Using this, they showed that their model outperformed Dredze et al.", 
        "63": "(2009)\u2019s model and other baselines.", 
        "64": "However, due to the technical difficulty of the joint approach with CRFs, they resorted to strong modeling assumptions.", 
        "65": "For example, their model assumes that for each word, only one worker provides the correct answer while all others label the word completely randomly.", 
        "66": "While this assumption captures some aspects of label quality, it is potentially problematic, such as for \u2018easy words\u2019 labeled correctly by all workers.", 
        "67": "More recently, ?", 
        "68": "proposed HMM models for aggregating crowdsourced discourse segmentation labels.", 
        "69": "However, they did not consider the general sequence labeling setting.", 
        "70": "Their method includes task-specific assumptions, e.g., that discourse segment lengths follow some empirical distribution estimated from data.", 
        "71": "In the absence of a gold standard, they evaluated by checking that workers accuracies are consistent and by comparing their two models to each other.", 
        "72": "We include their approach along with Rodrigues et al.", 
        "73": "(2014) as a baseline in our evaluation.", 
        "74": "3 Methods  We present our Task 1 HMM approach in Section 3.1 and our Task 2 LSTM approach in Section 3.2.", 
        "75": "3.1 HMMs with Crowd Workers  Model: We first define a standard HMM with hidden states hi, observations vi, transition parameter vectors \u03c4 hi and emission parameter vectors \u2126hi :\nhi+1|hi \u223c Discrete(\u03c4 hi) (1) vi|hi \u223c Discrete(\u2126hi) (2)\nThe discrete distributions here are governed by Multinomials.", 
        "76": "In the context of our task, vi is the word at position i and hi is the true, latent class of vi (e.g., entity or non-entity).", 
        "77": "For the crowd component, assume there are n classes, and let lij be the label for word i provided by worker j.", 
        "78": "Further, let C(j) be the confusion matrix for worker j, i.e., C(j)k is a vector of size n in which element k\u2032 is the probability of worker j\nproviding the label k\u2032 for a word of true class k:\nlij |hi \u223c Discrete(C(j)hi ) (3) Figure 1 shows the factor graph of this model, which we call HMM-Crowd.", 
        "79": "Note that we assume that individual crowdworker labels are conditionally independent given the (hidden) true label.", 
        "80": "A common problem with crowdsourcing models is data sparsity.", 
        "81": "For workers who provide only a few labels, it is hard to derive a good estimate of their confusion matrices.", 
        "82": "This is exacerbated when the label distribution is imbalanced, e.g., most words are not part of a named entity, concentrating the counts in a few confusion matrix entries.", 
        "83": "Solutions for this problem include hierarchical models of \u2018worker communities\u2019 (Venanzi et al., 2014) or correlations between confusion matrix entries (Nguyen et al., 2016).", 
        "84": "Although effective, these methods are also quite computationally expensive.", 
        "85": "For our models, to keep parameter estimation efficient, we use a simpler solution of \u2018collapsing\u2019 the confusion matrix into a \u2018confusion vector\u2019.", 
        "86": "For worker j, instead of having the n \u00d7 n matrix C(j), we use the n \u00d7 1 vector C\u2032(j) where C\u2032(j)k is the probability of worker j labeling a word with true class k correctly.", 
        "87": "We also smooth the estimate of C\u2032 with prior counts as in (Liu and Wang, 2012; Kim and Ghahramani, 2012).", 
        "88": "Learning: We use the Expectation Maximization (EM) algorithm (Dempster et al., 1977) to learn the parameters (\u03c4 ,\u2126,C\u2032), given the observations (all the words V and all the worker labels L).", 
        "89": "In the E-step, given the current estimates of the parameters, we take a forward and a backward\npass in the HMM to infer the hidden states, i.e.", 
        "90": "to calculate p(hi|V,L) and p(hi, hi+1|V,L) for all appropriate i.", 
        "91": "Let \u03b1(hi) = p(hi, v1:i, l1:i) where v1:i are the words from position 1 to i and l1:i are the crowd labels for these words from all workers.", 
        "92": "Similarly, let \u03b2(hi) = p(vi+1:n, li+1:n|hi).", 
        "93": "We have the recursions:\n\u03b1(hi) = \u2211\nhi\u22121\np(vi|hi)p(hi|hi\u22121) \u220f\nj\np(lij |hi)\u03b1(hi\u22121)\n(4)\n\u03b2(hi) = \u2211\nhi+1\np(hi+1|hi)p(vi+1|hi+1)\n\u220f\nj\np(li+1,j |hi+1)\u03b2(hi+1) (5)\nThese are the standard \u03b1 and \u03b2 recursions for HMMs augmented with the crowd model: the product \u220f j over the workers j who have provided labels for word i (or i + 1).", 
        "94": "The posteriors can then be easily evaluated: p(hi|V,L) \u221d \u03b1(hi)\u03b2(hi) and p(hi, hi+1|V,L) \u221d \u03b1(hi)p(hi+1|hi)p(vi+1|hi+1)\u03b2(hi+1)\nIn the standard M-step, the parameters are estimated using maximum likelihood.", 
        "95": "However, we found a Variational Bayesian (VB) update procedure for the HMM parameters similar to (Johnson, 2007; Beal, 2003) provides some improvement and stability.", 
        "96": "We first define the Dirichlet priors over the transition and emission parameters:\np(\u03c4 hi) = Dir(at) (6)\np(\u2126hi) = Dir(ae) (7)\nWith these priors, the variational M-step updates the parameters as follows2:\n\u03c4 h\u2032|h = exp{\u03a8(Eh\u2032|h + at)} exp{\u03a8(Eh + nat)}\n(8)\n\u2126v|h = exp{\u03a8(Ev|h + ae)} exp{\u03a8(Eh +mae)}\n(9)\nwhere \u03a8 is the Digamma function, n is the number of states and m is the number of observations.", 
        "97": "E denotes the expected counts, calculated from the posteriors inferred in the E-step.", 
        "98": "Eh\u2032|h is the expected number of times the HMM transitioned from state h to state h\u2032, where the expectation is with respect to the posterior distribution p(hi, hi+1|V,L) that we infer in the E step:\nEh\u2032|h = \u2211\ni\np(hi = h, hi+1 = h \u2032|V,L) (10)\n2See Beal (2003) for the derivation and Johnson (2007) for further discussion for the Variational Bayesian approach.", 
        "99": "Similarly, Eh is the expected number of times the HMM is at state h: Eh = \u2211 i p(hi = h|V,L) and Ev|h is the expected number of times the HMM emits the observation v from the state h: Ev|h =\u2211\ni,vi=v p(hi = h|V,L).", 
        "100": "For the crowd parameters C\u2032(j), we use the (smoothed) maximum likelihood estimate:\nC \u2032(j) k =\nE (j) k|k + ac\nE (j) k + nac\n(11)\nwhere ac is the smoothing parameter and E (j) k|k is the expected number of times that worker j correctly labeled a word of true class k as k while E(j)k is the expected total number of words belonging to class k worker j has labeled.", 
        "101": "Again, the expectation in E is taken under the posterior distributions that we infer in the E step.", 
        "102": "3.2 Long Short Term Memory with Crowds  For Task 2, we extend the LSTM architecture (Hochreiter and Schmidhuber, 1997) for NER (Lample et al., 2016) to account for noisy crowdsourced labels (this can be easily adapted to other sequence labeling tasks).", 
        "103": "In this model, the sentence input is first fed into an LSTM block (which includes character- and word-level bi-directional LSTM units).", 
        "104": "The LSTM block\u2019s output then becomes input to a (fully connected) hidden layer, which produces a vector of tags scores for each word.", 
        "105": "This tag score vector is the word-level prediction, representing the likelihood of the word being from each tag.", 
        "106": "All the tags scores are then fed into a \u2018CRF layer\u2019 that \u2018connects\u2019 the word-level predictions in the sentence and produces the final output: the most likely sequence of tags.", 
        "107": "We introduce a crowd representation in which a worker vector represents the noise associated with her labels.", 
        "108": "In other words, the parameters in the original architecture learns the correct sequence labeling model while the crowd vectors add noise to its predictions to \u2018explain\u2019 the lower quality of the labels.", 
        "109": "We assume a perfect worker has a zero vector as her representation while an unreliable worker is represented by a large magnitude vector.", 
        "110": "At test time, we ignore the crowd component and make predictions by feeding the unlabeled sentence into the original LSTM architecture.", 
        "111": "At train time, an example consists of the labeled sentence and the ID of the worker who provided the labels.", 
        "112": "Worker IDs are mapped to vector representations and incorporated into the LSTM architecture.", 
        "113": "We propose two strategies for incorporating the crowd vector into the LSTM: (1) adding the crowd vector to the tags scores and (2) concatenating the crowd vector to the output of the LSTM block.", 
        "114": "LSTM-Crowd.", 
        "115": "The first strategy is illustrated in Figure 2.", 
        "116": "We set the dimension of the crowd vectors to be equal to the number of tags and the addition is element-wise.", 
        "117": "In this strategy, the crowd vectors have a nice interpretation: the tagconditional noise for the worker.", 
        "118": "This is useful for worker evaluation and intelligent task routing (i.e.", 
        "119": "assigning the right work to the right worker).", 
        "120": "LSTM-Crowd-cat.", 
        "121": "The second strategy is illustrated in Figure 3.", 
        "122": "We set the crowd vectors to be additional inputs for the Hidden Layer (along with the LSTM block output).", 
        "123": "In this way, we are free to set the dimension of the crowd vectors and we have a more flexible model of worker noise.", 
        "124": "This comes with a cost of reduced interpretability and additional parameters in the hidden layer.", 
        "125": "For both strategies, the crowd vectors are randomly initialized and learned in the same LSTM architecture using Back Propagation (Rumelhart et al., 1985) and Stochastic Gradient Descent (SGD) (Bottou, 2010).", 
        "126": "4 Evaluation Setup    4.1 Datasets & Tuning  NER.", 
        "127": "We use the English portion of the CoNLL2003 dataset (Tjong Kim Sang and De Meulder, 2003), which includes over 21,000 annotated sentences from 1,393 news articles split into 3 sets: train, validation and test.", 
        "128": "We also use crowd labels collected by Rodrigues et al.", 
        "129": "(2014) for 400 articles in the train set3.", 
        "130": "For Task 1 (aggregating crowd labels), to avoid overfitting, we split these 400 articles into 50% validation and 50% test4.", 
        "131": "For Task 2 (predicting sequences on unannotated text), we follow Rodrigues et al.", 
        "132": "(2014) in using the CoNLL validation and test sets.", 
        "133": "Biomedical IE.", 
        "134": "We use 5,000 medical paper abstracts describing randomized control trials (RCTs) involving people.", 
        "135": "Each abstract is annotated by roughly 5 Amazon Mechanical Turk workers.", 
        "136": "Annotators were asked to mark all text spans in a given abstract which identify the population enrolled in the clinical trial.", 
        "137": "The annotations are therefore binary: inside or outside a span.", 
        "138": "In addition to annotations collected from laypeople via Mechanical Turk, we also use gold annotations by medical students for a small set of 200 abstracts, which we split into 50% validation and 50% test.", 
        "139": "For Task 1, we run methods being compared on all 5,000 abstracts, but we evaluate them only using the validation/test set.", 
        "140": "For Task 2, the validation and test sets are held out.", 
        "141": "Table 1 presents key statistics of datasets used.", 
        "142": "Tuning: In all experiments, validation set results are used to tune the models hyper-parameters.", 
        "143": "For HMM-Crowd, we have a smoothing parameter and two Dirichlet priors.", 
        "144": "For our two LSTMs, we have a L2 regularization parameter.", 
        "145": "For LSTMCrowd-cat, we also have the crowd vector dimen-\n3http://www.fprodrigues.com/software/ crf-ma-sequence-labeling-with-multiple-annotators/\n4Rodrigues et al.", 
        "146": "(2014)\u2019s results on the \u2018training set\u2019 are not directly comparable to ours since they do not partition the crowd labels into validation and test sets.", 
        "147": "sion.", 
        "148": "For each hyper-parameter, we consider a few (less then 5) different parameter settings for light tuning.", 
        "149": "We report results achieved on the test set.", 
        "150": "4.2 Baselines  Task 1.", 
        "151": "For aggregating crowd labels, we consider the following baselines:\n\u2022 Majority Voting (MV) at the token level.", 
        "152": "Rodrigues et al.", 
        "153": "(2014) show that this generally performs better than MV at the entity level.", 
        "154": "\u2022 Dawid and Skene (1979) weighted voting at the token level.", 
        "155": "We tested both a popular public implementation5 of Dawid-Skene and our own and found that ours performed better (likely due to smoothing), so we report it.", 
        "156": "\u2022 MACE (Hovy et al., 2013), using the authors\u2019 public implementation6.", 
        "157": "\u2022 Dawid-Skene then HMM.", 
        "158": "We propose a simple heuristic to aggregate sequential crowd labels: (1) use Dawid and Skene (1979) to induce consensus labels from individual crowd labels; (2) train a HMM using the input text and consensus labels; and then (3) use the trained HMM to predict and output labels for the input text.", 
        "159": "We also tried using a CRF or LSTM as the sequence labeler but found the HMM performed best.", 
        "160": "This is not surprising: CRFs and LSTM are good at predicting unseen sequences, whereas the predictions here are on the seen training sequences.", 
        "161": "\u2022 Rodrigues et al.", 
        "162": "(2014)\u2019s CRF with Multiple Annotators (CRF-MA).", 
        "163": "We use the source code provided by the authors.", 
        "164": "\u2022 ?\u2019s Interval-dependent (ID) HMM using the authors\u2019 source code7.", 
        "165": "Since they assume binary labels, we can only apply this to the biomedical IE task.", 
        "166": "For non-sequential aggregation baselines, we evaluate majority voting (MV) and Dawid and Skene (1979) as perhaps the most widely known and used in practice.", 
        "167": "A recent benchmark evaluation of aggregation methods for (non-sequential) crowd labels found that classic Dawid-Skene was the most consistently strong performing method\n5https://github.com/ipeirotis/Get-Another-Label 6http://www.isi.edu/publications/licensed-sw/mace/ 7https://academiccommons.columbia.edu/catalog/ac:199939\namong those considered, despite its age, while majority voting was often outperformed by other methods (Sheshadri and Lease, 2013).", 
        "168": "Dawid and Skene (1979) models a confusion matrix for each annotator, using EM estimation of these matrices as parameters and the true token labels as hidden variables.", 
        "169": "This is roughly equivalent to our proposed HMM-Crowd model (Section 3), but without the HMM component.", 
        "170": "Task 2.", 
        "171": "To predict sequences on unannotated text when trained on crowd labels, we consider two broad approaches: (1) directly train the model on all individual crowd annotations; and (2) induce consensus labels via Task 1 and train on them.", 
        "172": "For approach (1), we report as baselines:\n\u2022 Rodrigues et al.", 
        "173": "(2014)\u2019s CRF-MA\n\u2022 Lample et al.", 
        "174": "(2016)\u2019s LSTM trained on all crowd labels (ignoring worker IDs)\nFor approach (2), we report as baselines:\n\u2022 Majority Voting (MV) then Conditional Random Field (CRF).", 
        "175": "We train the CRF using the CRF Suite package (Okazaki, 2007) with the same features as in Rodrigues et al.", 
        "176": "(2014), who also report this baseline.", 
        "177": "\u2022 Lample et al.", 
        "178": "(2016)\u2019s LSTM trained on Dawid-Skene (DS) consensus labels.", 
        "179": "4.3 Metrics  NER.", 
        "180": "We use the CoNLL 2003 metrics of entitylevel precision, recall and F1.", 
        "181": "The predicted entity must match the gold entity exactly (i.e.", 
        "182": "no partial credit is given for partial matches).", 
        "183": "Biomedical IE.", 
        "184": "The above metrics are overly strict for the biomedical IR task, in which annotated sequences are typically far longer than named-entities.", 
        "185": "We therefore \u2018relax\u2019 the metric to credit partial matches as follows.", 
        "186": "For each predicted positive contiguous text span, we calculate:\nPrecision = # true positive words\n# words in this predicted span\nFor example, for a predicted span of 10 words, if 6 words are truly positive, the Precision is 60%.", 
        "187": "We evaluate this \u2018local\u2019 precision for each predicted span and then take the average as the \u2018global\u2019 precision.", 
        "188": "Similarly, for each gold span, we calculate:\nRecall = # words in a predicted span # words in this gold span\nThe recall scores for all the gold spans are again averaged to get a global recall score.", 
        "189": "For the biomedical IE task, because we have gold labels for only a small set of 200 abstracts, we create 100 bootstrap re-samples of the (predicted and gold) spans and perform the evaluation for each re-sample.", 
        "190": "We then report the mean and standard deviation over these 100 re-samples.", 
        "191": "5 Evaluation Results    5.1 Named-Entity Recognition (NER)  Table 2 presents Task 1 results for aggregating crowd labels.", 
        "192": "For the non-sequential aggregation baselines, we see that Dawid and Skene (1979) outperforms both majority voting and MACE (Hovy et al., 2013).", 
        "193": "For sequential methods, our heuristic \u2018Dawid-Skene then HMM\u2019 method performs surprisingly well, nearly as well as HMM-Crowd.", 
        "194": "However, we will see that this heuristic does not work as well for biomedical IR.", 
        "195": "Rodrigues et al.", 
        "196": "(2014)\u2019s CRF-MA achieves the highest Precision of all methods, but surprisingly the lowest F1.", 
        "197": "We use their public implementation but observe different results from what they report (we observed similar results when using all the crowd data without validation/test split as they do).", 
        "198": "We suspect their released source code may be optimized for Task 2, though we could not reach the authors to verify this.", 
        "199": "Table 3 reports NER results for Task 2: predicting sequences on unannotated text when trained on crowd labels.", 
        "200": "Results for Rodrigues et al.", 
        "201": "(2014)\u2019s CRF-MA are reproduced using their public implementation and match their reported results.", 
        "202": "While CRF-MA outperforms \u2018Majority Vote then CRF\u2019 as the authors reported, and achieves the highest Recall of all methods, its F1 results are generally not competitive with other methods.", 
        "203": "Methods based on Lample et al.", 
        "204": "(2016)\u2019s LSTM generally outperform the CRF methods.", 
        "205": "Adding a crowd component to the LSTM yields marked improvement of 2.5-3 points F1 vs. the LSTM trained on individual crowd annotations or consensus MV annotations.", 
        "206": "LSTM-Crowd (trained on individual labels) and \u2018HMM-Crowd then LSTM\u2019 (LSTM trained on HMM consensus labels) offer different paths to achieving comparable, best results.", 
        "207": "5.2 Biomedical Information Extraction (IE)  Tables 4 and 5 present Biomedical IE results for Tasks 1 and 2, respectively.", 
        "208": "We were unable to run Rodrigues et al.", 
        "209": "(2014)\u2019s CRF-MA public implementation on the Biomedical IE dataset (due to an \u2018Out of Memory Error\u2019 with 8gb max heapsize).", 
        "210": "For Task 1, Majority Vote achieves nearly 92% Precision but suffers from very low Recall.", 
        "211": "As with NER, HMM-Crowd achieves the highest Recall and F1, showing 2 points F1 improvement here over non-sequential Dawid and Skene (1979).", 
        "212": "In contrast with the NER results, our heuristic \u2018Dawid-Skene then HMM\u2019 performs much worse for Biomedical IE.", 
        "213": "In general, we expect heuristics to be less robust than principled methods.", 
        "214": "For Task 2, as with NER, we again see that LSTM-Crowd (trained on individual labels) and \u2018HMM-Crowd then LSTM\u2019 (LSTM trained on HMM consensus labels) offer different paths to achieving fairly comparable results.", 
        "215": "While LSTM-Crowd-cat again achieves slightly lower F1, simply training Lample et al.", 
        "216": "(2016)\u2019s LSTM directly on all crowd labels performs much better than seen earlier with NER, likely due to the relatively larger size of this dataset (see Table 1).", 
        "217": "To further investigate, we study the performances of these LSTM models as a function of training data available.", 
        "218": "In Figure 4, we see that as the amount of training data decreases, our crowd-augmented LSTM models produce greater relative improvement compared to the original LSTM architecture.", 
        "219": "Table 6 presents an example from Task 1 of a sentence with its gold span, annotations and the outputs from Dawid-Skene and HMM-Crowd.", 
        "220": "Dawid-Skene aggregates labels based only on the crowd labels while our HMM-Crowd combines that with a sequence model.", 
        "221": "HMM-Crowd is able to return the longer part of the correct span.", 
        "222": "6 Conclusions and Future Work  Given a dataset of crowdsourced sequence labels, we presented novel methods to: (1) aggregate sequential crowd labels to infer a best single set of consensus annotations; and (2) use crowd annotations as training data for a model that can predict sequences in unannotated text.", 
        "223": "We evaluated our approaches on two datasets representing different domains and tasks: general NER and biomedical IE.", 
        "224": "Results showed that our methods show improvement over strong baselines.", 
        "225": "We expect our methods to be applicable to and similarly benefit other sequence labeling tasks, such as POS tagging and chunking (Hovy et al., 2014).", 
        "226": "Our methods also provide an estimate of each worker\u2019s label quality, which can be transfered between tasks and is useful for error analysis and intelligent task routing (Bragg et al., 2014).", 
        "227": "We also plan to investigate extension of the crowd component in our HMM method with hierarchical models, as well as a fully-Bayesian approach.", 
        "228": "Acknowledgements  We thank the reviewers for their valuable comments.", 
        "229": "This work is supported in part by by National Science Foundation grant No.", 
        "230": "1253413 and the National Cancer Institute (NCI) of the National Institutes of Health (NIH), award number UH2CA203711.", 
        "231": "Any opinions, findings, and conclusions or recommendations expressed by the authors are entirely their own and do not represent those of the sponsoring agencies."
    }, 
    "document_id": "P17-1028.pdf.json"
}
