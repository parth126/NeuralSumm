{
    "abstract_sentences": {
        "1": "In this paper, we propose an extractive multi-document summarization (MDS) system using joint optimization and active learning for content selection grounded in user feedback.", 
        "2": "Our method interactively obtains user feedback to gradually improve the results of a state-of-the-art integer linear programming (ILP) framework for MDS.", 
        "3": "Our methods complement fully automatic methods in producing highquality summaries with a minimum number of iterations and feedbacks.", 
        "4": "We conduct multiple simulation-based experiments and analyze the effect of feedbackbased concept selection in the ILP setup in order to maximize the user-desired content in the summary."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1353\u20131363 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1124  1 Introduction  The task of producing summaries from a cluster of multiple topic-related documents has gained much attention during the Document Understanding Conference1 (DUC) and the Text Analysis Conference2 (TAC) series.", 
        "2": "Despite a lot of research in this area, it is still a major challenge to automatically produce summaries that are on par with human-written ones.", 
        "3": "To a large extent, this is due to the complexity of the task: a good summary must include the most relevant information, omit redundancy and irrelevant information, satisfy a length constraint, and be cohesive and grammatical.", 
        "4": "But an even bigger challenge is the high degree of subjectivity in content selection, as it can be seen in the small overlap of what is considered\n1http://duc.nist.gov/ 2http://www.nist.gov/tac/\nimportant by different users.", 
        "5": "Optimizing a system towards one single best summary that fits all users, as it is assumed by current state-of-the-art systems, is highly impractical and diminishes the usefulness of a system for real-world use cases.", 
        "6": "In this paper, we propose an interactive conceptbased model to assist users in creating a personalized summary based on their feedback.", 
        "7": "Our model employs integer linear programming (ILP) to maximize user-desired content selection while using a minimum amount of user feedback and iterations.", 
        "8": "In addition to the joint optimization framework using ILP, we explore pool-based active learning to further reduce the required feedback.", 
        "9": "Although there have been previous attempts to assist users in single-document summarization, no existing work tackles the problem of multi-document summaries using optimization techniques for user feedback.", 
        "10": "Additionally, most existing systems produce only a single, globally optimal solution.", 
        "11": "Instead, we put the human in the loop and create a personalized summary that better captures the users\u2019 needs and their different notions of importance.", 
        "12": "Need for personalization.", 
        "13": "Table 1 shows the ROUGE scores (Lin, 2004) of multiple existing summarization systems, namely TF*IDF (Luhn, 1958), LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004), LSA (Gong and Liu, 2001), KL-Greedy (Haghighi and Vanderwende, 2009), provided by the sumy package3 and ICSI4 (Gillick and Favre, 2009; Boudin et al., 2015), a strong state-of-the-art approach (Hong et al., 2014) in comparison to the extractive upper bound on DUC\u201904 and DBS.", 
        "14": "DUC\u201904 is an English dataset of abstractive summaries from ho-\n3https://github.com/miso-belica/sumy 4https://github.com/boudinfl/sume\n1353\nmogenous news texts, whereas DBS (Benikova et al., 2016) is a German dataset of cohesive extracts from heterogeneous sources from the educational domain (see details in section 4.1).", 
        "15": "For each dataset, we compute an extractive upper bound (UB) by optimizing the sentence selection which maximizes ROUGE-2, i.e., the occurrence of bigrams as in the reference summary (Cao et al., 2016).", 
        "16": "Although some systems achieve state-ofthe-art performance, their scores are still far from the extractive upper bound of individual reference summaries as shown in Figure 1.", 
        "17": "This is due to low inter-annotator agreement for concept selection: Zechner (2002) reports, for example, \u03ba = .13 and Benikova et al.", 
        "18": "(2016) \u03ba = .23.", 
        "19": "Most systems try to optimize for all reference summaries instead of personalizing, which we consider essential to capture user-desired content.", 
        "20": "Need for user feedback.", 
        "21": "The goal of concept selection is finding the important information\nwithin a given set of source documents.", 
        "22": "Although existing summarization algorithms come up with a generic notion of importance, it is still far from the user-specific importance as shown in Figure 1.", 
        "23": "In contrast, humans can easily assess importance given a topic or a query.", 
        "24": "One way to achieve personalized summarization is thus by combining the advantages of both human feedback and the generic notion of importance built in a system.", 
        "25": "This allows users to interactively steer the summarization process and integrate their user-specific notion of importance.", 
        "26": "Contributions.", 
        "27": "In this work, (1) we propose a novel ILP-based model using an interactive loop to create multi-document user-desired summaries, and (2) we develop models using pool-based active learning and joint optimization techniques to collect user feedback on identifying important concepts of a topic.", 
        "28": "In order to encourage the community to advance research and replicate our results, we provide our interactive summarizer implementation as open-source software.5.", 
        "29": "Our proposed method and our new interactive summarization framework can be used in multiple application scenarios: as an interactive annotation tool, which highlights important sentences for the annotators, as a journalistic writing aid that suggests important, user-adapted content from multiple source feeds (e.g., live blogs), and as a medical data analysis tool that suggests key information assisting a patient\u2019s personalized medical diagnosis.", 
        "30": "The rest of the paper is structured as follows: In section 2, we discuss related work.", 
        "31": "Section 3\n5https://github.com/UKPLab/ acl2017-interactive_summarizer\nintroduces our computer-assisted summarization framework using the concept-based optimization.", 
        "32": "Section 4 describes our experiment data and setup.", 
        "33": "In section 5, we then discuss our results and analyze the performance of our models across different datasets.", 
        "34": "Finally, we conclude the paper in section 6 and discuss future work.", 
        "35": "2 Related Work  Previous works related to our research address extractive summarization as a budgeted subset selection problem, computer-assisted approaches, and personalized summarization models.", 
        "36": "Bugeted subset selection.", 
        "37": "Extractive summarization systems that compose a summary from a number of important sentences from the source documents are by far the most popular solution for MDS.", 
        "38": "This task can be modeled as a budgeted maximum coverage problem.", 
        "39": "Given a set of sentences in the document collection, the task is to maximize the coverage of the subset of sentences under a length constraint.", 
        "40": "The scoring function estimates the importance of the content units for a summary.", 
        "41": "Most previous works consider sentences as content units and try different scoring functions to optimize the summary.", 
        "42": "One of the earliest systems by McDonald (2007) models a scoring function by simultaneously maximizing the relevance scores of the selected content units and minimizing their pairwise redundancy scores.", 
        "43": "They solve the global optimization problem using an ILP framework.", 
        "44": "Later, several state-of-the-art results employed an ILP to maximize the number of relevant concepts in the created summary: Gillick and Favre (2009) use an ILP with bigrams as concepts and hand-coded deletion rules for compression.", 
        "45": "Berg-Kirkpatrick et al.", 
        "46": "(2011) combine grammatical features relating to the parse tree and use a maximum-margin SVM trained on annotated gold-standard compressions.", 
        "47": "Woodsend and Lapata (2012) jointly optimize content selection and surface realization, Li et al.", 
        "48": "(2013) estimate the weights of the concepts using supervised methods, and Boudin et al.", 
        "49": "(2015) propose an approximation algorithm to achieve the optimal solution.", 
        "50": "Although these approaches achieve state-of-the-art performance, they produce only one globally optimal summary which is impractical for various users due to the subjectivity of the task.", 
        "51": "Therefore, we research interactive computer-assisted approaches in order to\nproduce personalized summaries.", 
        "52": "Computer-assisted summarization.", 
        "53": "The majority of the existing computer-assisted summarization tools (Craven, 2000; Narita et al., 2002; Ora\u030csan et al., 2003; Ora\u030csan and Hasler, 2006) present important elements of a document to the user.", 
        "54": "Creating a summary then requires the human to cut, paste, and reorganize the important elements in order to formulate a final text.", 
        "55": "The work by Ora\u030csan and Hasler (2006) is closely related to ours, since they assist users in creating summaries for a source document based on the output of a given automatic summarization system.", 
        "56": "However, their system is neither interactive nor does it consider the user\u2019s feedback in any way.", 
        "57": "Instead, they suggest the output of the state-of-the-art (singledocument) summarization method as a summary draft and ask the user to construct the summary without further interaction.", 
        "58": "Personalized summarization.", 
        "59": "While most previous work focuses on generic summaries, there have been a few attempts to take a user\u2019s preferences into account.", 
        "60": "The study by Berkovsky et al.", 
        "61": "(2008) shows that users prefer personalized summaries that precisely reflect their interests.", 
        "62": "These interests are typically modeled with the help of a query (Park and An, 2010) or keyword annotations reflecting the user\u2019s opinions (Zhang et al., 2003).", 
        "63": "In another strand of research, D\u0131\u0301az and Gerva\u0301s (2007) create user models based on social tagging and Hu et al.", 
        "64": "(2012) rank sentences by combining informativeness scores with a user\u2019s interests based on fuzzy clustering of social tags.", 
        "65": "Extending the use of social content, another recent work showed how personalized review summaries (Poussevin et al., 2015) can be useful in recommender systems beyond rating predictions.", 
        "66": "Although these approaches show that personalized summaries are more useful than generic summaries, they do not attempt to iteratively refine a summary in an interactive user\u2013system dialog.", 
        "67": "3 Approach  The goal of our work is maximizing the userdesired content in a summary within a minimum number of iterations.", 
        "68": "To this end, we propose an interactive loop that alternates the automatic creation of a summary and the acquisition of user feedback to refine the next iteration\u2019s summary.", 
        "69": "3.1 Summary Creation  Our starting point is the concept-based ILP summarization framework by Boudin et al.", 
        "70": "(2015).", 
        "71": "Let C be the set of concepts in a given set of source documents D, ci the presence of the concept i in the resulting summary, wi a concept\u2019s weight, `j the length of sentence j, sj the presence of sentence j in the summary, and Occij the occurrence of concept i in sentence j.", 
        "72": "Based on these definitions, we formulate the following ILP:\nmax \u2211\niwici (1)\n\u2200j.", 
        "73": "\u2211j`jsj \u2264 L (2) \u2200i, j.", 
        "74": "\u2211jsjOccij \u2265 ci (3) \u2200i, j. sjOccij \u2264 ci (4) \u2200i.", 
        "75": "ci \u2208 {0, 1} (5) \u2200j.", 
        "76": "sj \u2208 {0, 1} (6)\nThe objective function (1) maximizes the occurrence of concepts ci in the summary based on their weights wi.", 
        "77": "The constraint formalized in (2) ensures that the summary length is restricted to a maximum length L, (3) ensures the selection of all concepts in a sentence sj if sj has been selected for the summary.", 
        "78": "Constraint (4) ensures that a concept is only selected if it is present in at least one of the selected sentences.", 
        "79": "The two key factors for the performance of this ILP are defining the concept set C and a method to estimate the weights wi \u2208 W .", 
        "80": "Previous works have used word bigrams as concepts (Gillick and Favre, 2009; Li et al., 2013; Boudin et al., 2015) and either use document frequency (i.e.", 
        "81": "the number of source documents containing the concept) as weights (Woodsend and Lapata, 2012; Gillick and Favre, 2009) or estimate them using a supervised regression model (Li et al., 2013).", 
        "82": "For our implementation, we likewise use bigrams as concepts and document frequency as weights, as Boudin et al.", 
        "83": "(2015) report good results with this simple strategy.", 
        "84": "Our approach is, however, not limited to this setup, as our interactive approach allows for any definition of C and W , including potentially more sophisticated weight estimation methods, e.g., based on deep neural networks.", 
        "85": "In section 5.2, we additionally analyze how other notions of concepts can be integrated into our approach.", 
        "86": "3.2 Interactive Summarization Loop  Algorithm 1 provides an overview of our interactive summarization approach.", 
        "87": "The system takes the set of source documents D as input, derives the set of concepts C, and initializes their weights W .", 
        "88": "In line 5, we start the interactive feedback loop iterating over t = 0, .", 
        "89": ".", 
        "90": ".", 
        "91": ", T .", 
        "92": "We first create a summary St (line 6) by solving the ILP and then extract a set of concepts Qt (line 7), for which we query the user in line 11 As the user feedback in the current time step, we use the concepts It \u2286 Qt that have been considered important by the user.", 
        "93": "For updating the weights W in line 12, we may use all feedback collected until the current time step t, i.e., It0 = \u22c3t j=0 Ij and the set of\nconcepts Qt0 = \u22c3t j=0Qj seen by the user (with Q\u221210 = \u2205).", 
        "94": "If there are no more concepts to query (i.e., Qt = \u2205), we stop the iteration and return the personalized summary St.\nAlgorithm 1 Interactive summarizer 1: procedure INTERACTIVESUMMARIZER() 2: input: Documents D 3: C \u2190 extractConcepts(D) 4: W \u2190 conceptWeights(C) 5: for t = 0...T do 6: St \u2190 getSummary(C,W ) 7: Qt \u2190 extractConcepts(St)\u2212Qt\u221210 8: if Qt = \u2205 then 9: return St 10: else 11: It \u2190 obtainFeedback(St, Qt) 12: W \u2190 updateWeights(W, It0, Qt0) 13: end if 14: end for 15: end procedure  3.3 User Feedback Optimization  To optimize the summary creation based on user feedback, we iteratively change the concept weights in the objective function of the ILP setup.", 
        "95": "We define the following models:\nAccept model (ACCEPT).", 
        "96": "This model presents the current summary St with highlighted concepts Qt to a user and asks him/her to select all important concepts It.", 
        "97": "We assign the maximum weight MAX to all concepts in It and consider the remaining Qt \u2212 It as unimportant by setting their weight to 0 (see equation 7 and 8).", 
        "98": "The intuition\nbehind this baseline is that the modified scores cause the ILP to prefer the user-desired concepts while avoiding unimportant ones.", 
        "99": "\u2200i \u2208 It0.", 
        "100": "wi = MAX (7) \u2200i \u2208 Qt0 \u2212 It0.", 
        "101": "wi = 0 (8)\nJoint ILP with User Feedback (JOINT).", 
        "102": "The ACCEPT model fails in cases where the user could not accept concepts that never appear in one of the St summaries.", 
        "103": "To tackle this, in our JOINT model, we change the objective function of the ILP in order to create St by jointly optimizing importance and user feedback.", 
        "104": "We thus replace the equation (1) with:\nmax\n{\u2211 i 6\u2208Qt0 wici \u2212 \u2211 i\u2208Qt0 wici if t \u2264 \u03c4\u2211\niwici if t > \u03c4 (9)\nEquation (9) maximizes the use of concepts for which we yet lack feedback (i 6\u2208 Qt0) and minimizes the use of concepts for which we already have feedback (i \u2208 Qt0).", 
        "105": "In this JOINT model, we use an exploration phase t = 0 .", 
        "106": ".", 
        "107": ".", 
        "108": "\u03c4 to collect the feedback, which terminates when the user does not return any important concepts (i.e., It = \u2205).", 
        "109": "In the exploratory phase, the minus term in the equation 9 helps to reduce the score of the sentences whose concepts have received feedback already.", 
        "110": "In other words, it causes higher scores for sentences consisting of concepts which yet lack feedback.", 
        "111": "After the exploration step, we fall back to the original importance-based optimization function from equation (1).", 
        "112": "Active learning with uncertainty sampling (AL).", 
        "113": "Our JOINT model explores well in terms of prioritizing the concepts which yet lack user feedback.", 
        "114": "However, it gives equal probabilities to all the unseen concepts.", 
        "115": "The AL model employs pool-based active learning (Kremer et al., 2014) during the exploration phase in order to prioritize concepts for which the model is most uncertain.", 
        "116": "We distinguish the unlabeled concept pool Cu = {\u03a6(x\u03031),\u03a6(x\u03032), ...,\u03a6(x\u0303N )} and the labeled concept poolC` = {(\u03a6(x1), y1), (\u03a6(x2), y2), .", 
        "117": ".", 
        "118": ".", 
        "119": ", (\u03a6(xN ), yN )}, where each concept xi is represented as a d-dimensional feature vector \u03a6(xi) \u2208 Rd.", 
        "120": "The labels yi \u2208 {\u22121, 1} are 1 for all important concepts in It0 and\u22121 for all unimportant concepts in Qt0 \u2212 It0.", 
        "121": "Initially, the labeled concept pool C`\nis small or empty, whereas the unlabeled concept pool Cu is relatively large.", 
        "122": "The learning algorithm is presented with a C = C` \u222a Cu and is first called to learn a decision function f (0) : Rd \u2192 R, where the function f (0)(\u03a6(x\u0303)) is taken to predict the label of the input vector \u03a6(x\u0303).", 
        "123": "Then, in each tth iteration, where t = 1, 2, .", 
        "124": ".", 
        "125": ".", 
        "126": ", \u03c4 , the querying algorithm selects an instance of x\u0303t \u2208 Cu for which the learning algorithm is least certain.", 
        "127": "Thus, our learning goal of active learning is to minimize the expected loss L (i.e., hinge loss) with limited querying opportunities to obtain a decision function f (1), f (2), .", 
        "128": ".", 
        "129": ".", 
        "130": ", f (\u03c4) that can achieve low error rates:\nminE(\u03a6(x),y)\u2208C` [ L(f (t)(\u03a6(x)), y) ] (10)\nAs the learning algorithm, we use a support vector machine (SVM) with a linear kernel.", 
        "131": "To obtain the probability distribution over classes we use Platt\u2019s calibration (Platt, 1999), an effective approach for transforming classification models into a probability distribution.", 
        "132": "Equation (11) shows the probability estimates for f (t), where f (t) is the uncalibrated output of the SVM in the tth iteration and A, B are scalar parameters that are learned by the calibration algorithm.", 
        "133": "The uncertainty scores are calculated as described in the equation (12) for all the concepts which lack feedback (Cu).", 
        "134": "p(y | f (t)) = 1 1 + exp(Af (t) +B)\n(11)\nui = 1\u2212 max y\u2208{\u22121,1} p(y | f (t)) (12)\nFor our AL model, we now change the objective function in order to create St by multiplying uncertainty scores ui to the weights wi.", 
        "135": "We thus replace the objective function from (9) with\nmax {\u2211 i 6\u2208Qt0 uiwici if t \u2264 \u03c4\u2211 i wici if t > \u03c4\n(13)\nActive learning with positive sampling (AL+).", 
        "136": "One way to sample the unseen concepts is using uncertainty as in AL, but another way is to actively choose samples for which the learning algorithm predicts as a possible important concept.", 
        "137": "In AL+, we introduce the notion of certainty (1\u2212ui) for the positively predicted samples (f (t)(\u03a6(x\u0303i)) = 1) in\nthe objective function (1) for producing St\nmax {\u2211 i 6\u2208Qt0 (1\u2212 ui)`iwici if t \u2264 \u03c4\u2211 i wici if t > \u03c4\n(14)\nwhere `i = { 0 if f (t)(\u03a6(x\u0303i)) = \u22121 1 if f (t)(\u03a6(x\u0303i)) = 1\n(15)  4 Experimental Setup    4.1 Data  For our experiments, we mainly focus on the DBS corpus, which is an MDS dataset of coherent extracts created from heterogeneous sources about multiple educational topics (Benikova et al., 2016).", 
        "138": "This corpus is well-suited for our evaluation setup, since we are able to easily simulate a user\u2019s feedback based on the overlap between generated and reference summary.", 
        "139": "Additionally, we carry out experiments on the most commonly used evaluation corpora published by DUC/NIST from the generic multidocument summarization task carried out in DUC\u201901, DUC\u201902 and DUC\u201904.", 
        "140": "The documents are all from the news domain and are grouped into various topic clusters.", 
        "141": "Table 2 shows the properties of these corpora.", 
        "142": "For evaluating the summaries against the reference summary we use ROUGE (Lin, 2004) with the parameters suggested by (Owczarzak et al., 2012) yielding high correlation with human judgments (i.e., with stemming and without stopword removal).6 Since DBS summaries do not have a fixed length, we use a variable length parameter L for evaluation, where L denotes the length of the reference summary.", 
        "143": "All results are averaged across all topics and reference summaries.", 
        "144": "4.2 Data Pre-processing and Features  To pre-process the datasets, we perform tokenization and stemming with NLTK (Loper and Bird, 2002) and constituency parsing with the Stanford parser (Klein and Manning, 2003) for English and\n6-n 4 -m -a -x -c 95 -r 1000 -f A -p 0.5 -t 0 -2 -4 -u\nGerman.", 
        "145": "The parse trees will be used in section 5.2 below to experiment with a syntactically motivated concept notion.", 
        "146": "As a concept\u2019s feature representation \u03a6 for our active learning setups AL and AL+, we use pre-trained word embeddings.", 
        "147": "We use the Google News embeddings with 300 dimensions by Mikolov et al.", 
        "148": "(2013) for English and the 100- dimensional news- and Wikipedia-based embeddings by Reimers et al.", 
        "149": "(2014) for German.", 
        "150": "Additionally, we add TF*IDF, number of stop words, presence of named entities, and word capitalization as features.", 
        "151": "Discrete features, such as part-ofspeech tags, are mapped into the word representation via lookup tables.", 
        "152": "4.3 Oracle-Based Simulation of User Feedback  The presence of a human in the loop typically demands for a user study based evaluation, but to collect sufficient data for various settings of our models would be too expensive.", 
        "153": "Therefore, we resort to an oracle-based approach, where the oracle is a system simulating the user by generating the feedback based on reference outputs.", 
        "154": "This idea has been widely used in the development of interactive systems (Gonza\u0301lez-Rubio et al., 2012; Knowles and Koehn, 2016) for studying the problem and exhibiting solutions in a theoretical and controlled environment.", 
        "155": "To simulate user feedback in our setting, we consider all concepts It \u2286 Qt from the systemsuggested summary St as important if they are present in the reference summary.", 
        "156": "Let Ref be the set of concepts in the reference summary.", 
        "157": "In the tth iteration, we return It = Qt \u2229Ref as the simulated user feedback.", 
        "158": "Thus, the goal of our system is to reach the upper bound for a user\u2019s reference summary within a minimal number of iterations.", 
        "159": "We limit our experiments to ten iterations, since it appears unrealistic that users are willing to participate in more feedback cycles.", 
        "160": "Petrie and Bevan (2009) even report only three to five iterations.", 
        "161": "5 Results and Analysis    5.1 Methods  Table 3 shows the evaluation results of our four models.", 
        "162": "When evaluating a summarization system, it is common to report the mean ROUGE scores across clusters using all the reference summaries.", 
        "163": "However, since we aim at personalizing\nthe summary for an individual user, we evaluate our models based on the mean ROUGE scores across clusters per reference summary.", 
        "164": "In Table 4, we additionally evaluate the models based on the amount of feedback (#F = |IT0 |) taken by the oracles to converge to the upper bound within ten iterations.", 
        "165": "To examine the system performance based on user feedback, we analyze our models\u2019 performance on multiple datasets.", 
        "166": "The results in Table 3 show that our idea of interactive multi-document summarization allows users to steer a general summary towards a personalized summary consistently across all datasets.", 
        "167": "From the results, we can see that the AL model starts from the conceptbased ILP summarization and nearly reaches the upper bound for all the datasets within ten iterations.", 
        "168": "AL+ performs similar to AL in terms of ROUGE, but requires less feedback (compare Table 4).", 
        "169": "Furthermore, the ACCEPT and JOINT models get stuck in a local optimum due to the less exploratory nature of the models.", 
        "170": "5.2 Concept Notion  Our interactive summarization approach is based on the scalable global concept-based model which uses bigrams as concepts.", 
        "171": "Thus, it is intuitive to use bigrams for collecting user feedback as well.7 Although our models reach the upper bound when using bigram-based feedback, they require a significantly large number of iterations and much feedback to converge, as shown in Table 4.", 
        "172": "To reduce the amount of feedback, we also consider content phrases to collect feedback.", 
        "173": "That is, syntactic chunks from the constituency parse trees consisting of non-function words (i.e., nouns, verbs, adjectives, and adverbs).", 
        "174": "For DBS being extractive dataset, we use bigrams and content phrases as concepts, both for the objective function in equation (1) and as feedback items, whereas for the DUC datasets, the concepts are always bigrams for both the feedback types (bigrams/content phrases).", 
        "175": "For DUC being abstractive, in the case of feedback given on content phrases, they are projected back to the bigrams to change the concept weights in order to have more overlap of simulated feedback.", 
        "176": "Table 4 shows feedbacks based on the content phrases reduces the number of feedbacks by a factor of 2.", 
        "177": "Furthermore, when content phrases are used as concepts for DBS, the performance of the models is lower compared to bigrams, as seen in Table 3.", 
        "178": "5.3 Datasets  Figure 2 compares the ROUGE-2 scores and the amount of feedback used over time when applied to the DBS and the DUC\u201904 corpus.", 
        "179": "We can see from the figure that all models show an improvement of +.45 ROUGE-2 after merely 4 iterations\n7We prune bigrams consisting of only functional words.", 
        "180": "on DBS.", 
        "181": "For DUC\u201904, the improvements are +.1 ROUGE-2 after ten iterations, which is relatively notable considering the lower upper bound of .21 ROUGE-2.", 
        "182": "This is primarily because DBS is a corpus of cohesive extracts, whereas DUC\u201904 consists of abstractive summaries.", 
        "183": "As a result, the oracles created using abstractive reference summaries have lower overlap of concepts as compared to that of the oracles created using extractive summaries.", 
        "184": "For DBS, it becomes clear that the JOINT model converges faster with an optimum amount of feedback as compared to other models.", 
        "185": "ACCEPT takes relatively more feedbacks than JOINT, but performs low in terms of ROUGE scores.", 
        "186": "The best performing models are AL and AL+, which reach closest to the upper bound.", 
        "187": "This is clearly due to the exploratory nature of the models which use semantic representations of the concepts to predict uncertainty and importance of possible concepts for user feedback.", 
        "188": "For DUC\u201904, the JOINT model reaches the closest to the upper bound, closely followed by AL.", 
        "189": "The JOINT model consistently stays above all\nother models and it gathers more important concepts due to optimizing feedbacks for concepts which lack feedback.", 
        "190": "Interestingly, AL+ performs rather worse in terms of both ROUGE scores and gathering important concepts.", 
        "191": "The primary reason for this is the fewer feedback collected from the simulation due to the abstractive property of reference summaries, which makes the AL+ model\u2019s prediction inconsistent.", 
        "192": "5.4 Personalization  Figure 3 shows the performance of different models in comparison to two different oracles for the same document cluster.", 
        "193": "For DBS, the JOINT, AL, and AL+ models consistently converge to the upper bound in 4 iterations for different oracles, whereas ACCEPT takes longer for one oracle and does not reach the upper bound for the other.", 
        "194": "For DUC\u201904, JOINT and AL show consistent performance across the oracles, whereas AL+ performs worse than the state-of-the-art system (iteration 0) for oracle created using abstractive summaries as shown in Figure 3 (right) for User:1.", 
        "195": "However, for User:2, we observe a ROUGE-2 improvement of +.1 indicating that the predictions of the active learning system are better if there is more feedback.", 
        "196": "Nevertheless, we expect that in practical use, the human summarizers may give more feedback similar to DBS in comparison to DUC\u201904 simulation setting.", 
        "197": "6 Conclusion and Future Work  We propose a novel ILP-based approach using interactive user feedback to create multi-document user-desired summaries.", 
        "198": "In this paper, we investigate pool-based active learning and joint optimization techniques to collect user feedback for identifying important concepts for a summary.", 
        "199": "Our models show that interactively collecting feedback consistently steers a general summary towards a user-desired personalized summary.", 
        "200": "We empirically checked the validity of our approach on standard datasets using simulated user feedback and observed that our framework shows promising results in terms of producing personalized multi-\ndocument summaries.", 
        "201": "As future work, we plan to investigate more sophisticated sampling strategies based on active learning and concept graphs to incorporate lexicalsemantic information for concept selection.", 
        "202": "We also plan to look into ways to propagate feedback to similar and related concepts with partial feedback, to reduce the total amount of feedback.", 
        "203": "This is a promising direction as we have shown that interactive methods help to create user-desired personalized summaries, and with minimum amount of feedbacks, it has propitious use in scenarios where user-adapted content is a requirement.", 
        "204": "Acknowledgments  This work has been supported by the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No.", 
        "205": "GRK 1994/1.", 
        "206": "We also acknowledge the useful comments and suggestions of the anonymous reviewers."
    }, 
    "document_id": "P17-1124.pdf.json"
}
