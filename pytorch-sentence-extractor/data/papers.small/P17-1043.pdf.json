{
    "abstract_sentences": {
        "1": "We present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%.", 
        "2": "AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more.", 
        "3": "The AMR parser does not rely on a syntactic preparse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 463\u2013472 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1043\nAbstract Meaning Representation Parsing using LSTM Recurrent Neural Networks\nWilliam R. Foland Jr. Department of Computer Science\nUniversity of Colorado Boulder, CO 80309\nWilliam.Foland@colorado.edu\nJames H. Martin Department of Computer Science and\nInstitute of Cognitive Science University of Colorado\nBoulder, CO 80309 James.Martin@colorado.edu\nAbstract\nWe present a system which parses sentences into Abstract Meaning Representations, improving state-of-the-art results for this task by more than 5%.", 
        "2": "AMR graphs represent semantic content using linguistic properties such as semantic roles, coreference, negation, and more.", 
        "3": "The AMR parser does not rely on a syntactic preparse, or heavily engineered features, and uses five recurrent neural networks as the key architectural components for inferring AMR graphs.", 
        "4": "1 Introduction  Semantic analysis is the process of extracting meaning from text, revealing key ideas such as \u201dwho did what to whom, when, how, and where?\u201d, and is considered to be one of the most complex tasks in natural language processing.", 
        "5": "Historically, an important consideration has been the definition of the output of the task - how can the concepts in a sentence be captured in a general, consistent and expressive manner that facilitates downstream semantic processing?", 
        "6": "Over the years many formalisms have been proposed as suitable target representations including variants of first order logic, semantic networks, and frame-based slot-filler notations.", 
        "7": "Such representations have found a place in many semantic applications but there is no clear consensus as to the best representation.", 
        "8": "However, with the rise of supervised machine learning techniques, a new requirement has come to the fore: the ability of human annotators to quickly and reliably generate semantic representations as training data.", 
        "9": "Abstract Meaning Representation (AMR) (Banarescu et al., 2012)1 was developed to provide\n1http://amr.isi.edu/language.html\na computationally useful and expressive representation that could be reliably generated by human annotators.", 
        "10": "Sentence meanings in AMR are represented in the form of graphs consisting of concepts (nodes) connected by labeled relations (edges).", 
        "11": "AMR graphs include a number of traditional NLP representations including named entities (Nadeau and Sekine, 2007), word senses (Banerjee and Pedersen, 2002), coreference relations, and predicate-argument structures (Kingsbury and Palmer, 2002; Palmer et al., 2005).", 
        "12": "More recent innovations include wikification of named entities and normalization of temporal expressions (Verhagen et al., 2010; Stro\u0308tgen and Gertz, 2010).", 
        "13": "(2016) provides an insightful discussion of the relationship between AMR and other formal representations including first order logic.", 
        "14": "The process of creating AMR\u2019s for sentences is called AMR Parsing and was first introduced in (Flanigan et al., 2014).", 
        "15": "A key factor driving the development of AMR systems has been the increasing availability of training resources in the form of corpora where each sentence is paired with a corresponding AMR representation 2.", 
        "16": "A consistent framework for evaluating AMR parsers was defined by the Semeval-2016 Meaning Representation Parsing Task3.", 
        "17": "Standard training, development and test splits for the AMR Annotation Release 1 corpus are provided, as well as an additional out-of-domain test dataset, for system comparisons.", 
        "18": "4\nViewed as a structured prediction task, AMR parsing poses some difficult challenges not faced by other related language processing tasks including part of speech tagging, syntactic parsing or se-\n2See amr.isi.edu for information on currently available resources\n3http://alt.qcri.org/semeval2016/task8/# 4Available from LDC as LDC2015E86 DEFT Phase 2 AMR Annotation R1 dataset.", 
        "19": "463\nmantic role labeling.", 
        "20": "The prediction task in these settings can be cast as per-token labeling tasks (i.e.", 
        "21": "IOB tags) or as a sequence of discrete parser actions, as in transition-based (shift-reduce) approaches to dependency parsing.", 
        "22": "The first challenge is that AMR representations are by design abstracted away from their associated surface forms.", 
        "23": "AMR corpora pair sentences with their corresponding representations, without providing an explicit annotation, or alignment, that links the parts of the representation to their corresponding elements of the sentence.", 
        "24": "Not surprisingly, this complicates training, decoding and evaluation.", 
        "25": "The second challenge is the fact that, as noted earlier, the AMR parsing task is an amalgam of predicate identification and classification, entity recognition, co-reference, word sense disambigua-\ntion and semantic role labeling \u2014 each of which relies on the others for successful analysis.", 
        "26": "The architecture and system presented in the following sections is largely motivated by these two challenges.", 
        "27": "2 Related Work    2.1 AMR Parsers  Most current AMR parsers are constructed using some form of supervised machine learning that exploits existing AMR corpora.", 
        "28": "In general, these systems make use of features derived from various forms of syntactic analysis, ranging from partof-speech tagging to more complex dependency or phrase-structure analysis.", 
        "29": "Currently, most systems fall into two classes: (1) systems that incrementally transform a dependency parse into an AMR\ngraph using transition-based systems (Wang et al., 2015, 2016), and (2) graph-oriented approaches that use syntactic features to score edges between all concept pairs, and then use a maximum spanning connected subgraph (MSCG) algorithm to select edges that will constitute the graph (Flanigan et al., 2014; Werling et al., 2015).", 
        "30": "As expected, there are exceptions to these general approaches.", 
        "31": "The largely rule-based approach of (2015) converts logical forms from an existing semantic analyzer into AMR graphs.", 
        "32": "They demonstrate the ability to use their existing system to generate AMRs in German, French, Spanish and Japanese without the need for a native AMR corpus.", 
        "33": "(2015) proposes a synchronous hyperedge replacement grammar solution, (2015) uses syntaxbased machine translation techniques to create tree structures similar to AMR, while (2015) creates logical form representations of sentences and then converts these to AMR.", 
        "34": "An exception to the use of heavily engineered features is the deep learning approach of (2016), which, following (Collobert et al., 2011), relies on word embeddings and recurrent neural networks to generate AMR graphs.", 
        "35": "2.2 Bidirectional LSTM Neural Networks  Unlike relatively simple sequence processing tasks like part-of-speech tagging and NER, semantic analysis requires the ability to keep track of relevant information that may be arbitrarily far away from the words currently under consideration.", 
        "36": "Recurrent neural networks (RNNs) are a class of neural architecture that use a form of short-term memory in order to solve this semantic distance problem.", 
        "37": "Basic RNN systems have been enhanced with the use of special memory cell units, referred to as Long Short-Term Memory neural networks, or LSTM\u2019s (Hochreiter and Schmidhuber, 1997).", 
        "38": "Such systems can effectively process information dispersed over hundreds of words (Schmidhuber et al., 2002; Gers et al., 2001).", 
        "39": "Bidirectional LSTMs (B-LSTM) networks are LSTMs that are connected so that both future and past sequence context can be examined.", 
        "40": "(2015), successfully used a bidirectional LSTM network for semantic role labelling.", 
        "41": "We use the LSTM cell as described in (Graves et al., 2013), configured in a B-LSTM shown in Figure 2, as the core network architecture in the system.", 
        "42": "Five B-LSTM Neural\nNetworks comprise the parser.", 
        "43": "3 Parser Overview  Our parser5 will be explained using this example sentence: France plans further nuclear cooperation with numerous countries .", 
        "44": "A graphical depiction of an AMR for this sentence is shown in Figure 1a.", 
        "45": "Given an input sentence, the approach taken in our AMR parser is similar to (Flanigan et al., 2014) in that it consists of two subtasks: (1) discover the concepts (nodes and sub-graphs) present in the sentence, and (2) determine the relations (arcs) that connect the concepts (relations capture both traditional predicate-argument structures (ARGs), as well as additional modifier relations that capture notions including quantification, polarity, and cardinality.)", 
        "46": "Neither of these tasks is straightforward in the AMR context.", 
        "47": "Among the complications are the fact that individual words may contribute to more than one node (as in the case of France), parts of the graph may be \u201creentrant\u201d, participating in relations with multiple concepts, and predicate-argument and modifier relations can be introduced by arbitrary parts of the input.", 
        "48": "At a high level, our system takes an input sentence in form of a vector of word embeddings\n5source at https://github.com/BillFoland/daisyluAMR\nand uses a series of recurrent neural networks to (1) discover the basic set of nodes and subgraphs that comprise the AMR, (2) discover the set of predicate-argument relations among those concepts, and (3) identifying any relevant modifier relations that are present.", 
        "49": "A high level block diagram of the parser is shown in Figure 1b.", 
        "50": "The parser extracts features from the sentence which are processed by a bidirectional LSTM network (B-LSTM) to create a set of AMR subgraphs, which contain one or two concepts as well as their internal relations to each other.", 
        "51": "Features based on the sentence and these subgraphs are then processed by a pair of B-LSTM networks to compute the probabilities of relations between all subgraphs.", 
        "52": "All subgraphs are then connected using an iterative, greedy algorithm to compute a single component graph, with all subgraphs connected by relations.", 
        "53": "Separately, another two B-LSTM networks compute attribute and name categories, which are then appended to the graph.", 
        "54": "Finally, the subgraphs are expanded into the most probable AMR concept and relation primitives to create the final AMR.", 
        "55": "4 Detailed Parser Architecture    4.1 AMR Spans, Subgraphs, and Subgraph Decoding  Mapping the words in a sentence to AMR concepts is a critical first step in the parsing process, and can influence the performance of all subsequent processing.", 
        "56": "Although the most common mapping is one word to one concept, a series of consecutive words, or span, can also be associated with an AMR concept.", 
        "57": "Likewise, a span of words can be mapped to a small connected subgraph, such as the single word span France which is mapped to a subgraph composed of two concepts connected by a name relation.", 
        "58": "(see the shaded section of Figure 1a).", 
        "59": "Training corpora provide sentences which are annotated by humans with AMR graphs, not necessarily including a reference span to subgraph mapping.", 
        "60": "An automatic AMR aligner can be used to predict relationships between words and gold AMR\u2019s.", 
        "61": "We use the alignments produced by the aligner of (2014), along with the words and reference AMR graphs, to identify a subgraph type to associate with each span.", 
        "62": "Each word in the sentence is then associated with an IOBES subgraph type tag.", 
        "63": "We call the algorithm which defines span\nto subgraph mapping the Expert Span Identifier, and use it to train the SG Network.", 
        "64": "A convenient development detail stems from the fact that during the AMR creation process, the identified subgraphs must be expanded into individual concepts and relations.", 
        "65": "For example, the subgraph type \u201dNamed\u201d, along with the span France, must be expanded to create the concepts, relations, and attributes shown in Figure 1a.", 
        "66": "A Subgraph Expander algorithm implements this task, which is essentially the inverse of the Expert Span Identifier.", 
        "67": "The Expert Span Identifier and Subgraph Expander were developed by cascading the two in a test configuration as shown in Figure 3a.", 
        "68": "4.2 Features  All input features for the five networks correspond to the sequence of words in the input sentence, and are presented to the networks as indices into lookup tables.", 
        "69": "With the exception of pre-trained word embeddings, these lookup tables are randomly initialized prior to training and representations are created during the training process.", 
        "70": "4.2.1 Word Embeddings  The use of distributed word representations generated from large text corpora is pervasive in modern NLP.", 
        "71": "We start with 300 dimension GloVe representations (Pennington et al., 2014) trained on the 840 billion word common crawl (Smith et al., 2013).", 
        "72": "We added two binary dimensions: one for out of vocabulary words, and one for padding, resulting in vectors with a width of 302.", 
        "73": "These embeddings are mapped from the words in the sentence, and are then trained using back propagation just like other parameters in the network.", 
        "74": "4.2.2 Wikifier  The AMR standard was expanded to include the annotation of named entities with a canonical form, using Wikipedia as the standard (see France in Figure 1a).", 
        "75": "The wiki link associated with this \u201dwikification\u201d is expressed using the :wiki attribute, which requires some kind of global external knowledge of the Wikipedia ontology.", 
        "76": "We use the University of Illinois Wikifier (Ratinov et al., 2011; Cheng and Roth, 2013) to identify the :link directly, and use the possible categories output from the wikifier as feature inputs to the NCat Network.", 
        "77": "Named Entity Recognition can be valuable input to a parser, and state-of-the-art NER systems can be created using convolutional neural networks (Collobert et al., 2011) or LSTM (Chiu and Nichols, 2015) aided by information from gazetteers.", 
        "78": "These gazetteers are large dictionaries containing well known named entities (e.g., (Florian et al., 2003)).", 
        "79": "Rather than add gazetteer features to our system, we make use of the NER information already calculated and provided by the Univ.", 
        "80": "of Illinois Wikifier.", 
        "81": "We then encode the classified named entities output from the wikifier as feature embeddings, which are used by the SG Network.", 
        "82": "4.2.3 AMR Subgraph (SG) Network  The features used as input to the SG network are:\n\u2022 word: 45Kx302, the word embeddings \u2022 suffix: 430x5, embeddings based on the final\ntwo letters of each word.", 
        "83": "\u2022 caps: 5x5, embeddings based on the capital-\nization pattern of the word.", 
        "84": "\u2022 NER: 5x5, embeddings indexed by NER\nfrom the Wikifier, \u2019O\u2019, \u2019LOC\u2019, \u2019ORG\u2019, \u2019PER\u2019 or \u2019MISC\u2019.", 
        "85": "The SG Network produces probabilities for 46 BIOES tagged subgraph types, and the highest probability tag is chosen for each word, as shown for the example sentence in Table 1.", 
        "86": "4.2.4 Predicate Argument Relations (Args) Network  The AMR concepts (nodes) are connected by relations (arcs).", 
        "87": "We found it convenient to distinguish predicate argument relations, or \u201dArgs\u201d from other relations, which we call \u201dNargs\u201d.", 
        "88": "For example, see ARG0 and ARG1 relations in Figure 1a are \u201dArgs\u201d, compared with the name, degree, mod, or quant relations which are \u201dNargs\u201d.", 
        "89": "The Args Network is run once for each predicate subgraph, and produces a matrix Pargs which defines the probability (prior to the identification of any relations6) of a type of predicate argument relation from a predicate subgraph to any other SG identified subgraph.", 
        "90": "(For example, see ARG0 and ARG1 relations in Figure 1a.)", 
        "91": "The matrix has dimensions 5 by s, where 5 is the number of predicate arg relations identified by the network, and s is the total number of subgraphs identified by the SG Network for the sentence.", 
        "92": "The Args features, calculated for each source predicate subgraph, are:\n\u2022 Word, Suffix and Caps as in the SG network.", 
        "93": "\u2022 SG: 46x5, indexed by the SG network identi-\nfied subgraph.", 
        "94": "\u2022 PredWords[5], 45Kx302: The word embed-\ndings of the word and surrounding 2 words associated with the source predicate subgraph.", 
        "95": "6relation probabilities change as hard decisions are made, see section 4.3\nDistance[4] 5\nTable 2: Args Network Features for the word France while evaluating outgoing args for the word cooperation, associated with predicate cooperate-01\n\u2022 PredSG[5], 46x10: The SG embedding of the word and surrounding 2 words associated with the source predicate subgraph.", 
        "96": "\u2022 regionMark: 21x5, indexed by the distance in words between the word and the word associated with the source predicate subgraph.", 
        "97": "Table 2 shows an example feature set for one subgraph while evaluating a predicate subgraph.", 
        "98": "4.2.5 Non-Predicate Relations (Nargs) Network  The Nargs Network uses features similar to the Args network.", 
        "99": "It is run once for each subgraph, and produces a matrix Pnargs which defines the probability of a type of relation from a subgraph to any other subgraph, prior to the identification of any relations.7 The matrix has dimensions 43 by s, where 43 is the number of non-arg relations identified by the network, and s is the total number of subgraphs identified by the SG Network for the sentence.", 
        "100": "4.2.6 Attributes (Attr) Network  The Attr Network determines a primary attribute for each subgraph, if any.8 This network is simplified to detect only one attribute (there could be\n7Degree, mod, or quant are examples of Narg relations in Figure 1a.", 
        "101": "8(TOP: plan-01) and (op1: france) are attribute examples shown in Figure 1a.", 
        "102": "many) per subgraph, and only computes probabilities for the two most common attributes: TOP and polarity.", 
        "103": "Note that subgraph expansion also identifies many attributes, for example the words associated with named entities, or the normalized quantity and date representations.", 
        "104": "A known shortcoming of this network is that the TOP and polarity attributes are not mutually exclusive, but noting that the cooccurrence of the two does not occur in the training data, we chose to avoid adding a separate network to allow the prediction of both attributes for a single subgraph.", 
        "105": "4.2.7 Named Category (NCat) Network  The NCat Network uses features similar to the SG Network, along with the suggested categories (up to eight) from the Wikifier, and produces probabilities for each of 68 :instance roles, or categories, for named entities identified in the training set AMR\u2019s.", 
        "106": "\u2022 Word, Suffix and Caps as in the SG network.", 
        "107": "\u2022 WikiCat[8]: 108 x 5, indexed by suggested\ncategories from the Wikifier.", 
        "108": "4.3 Relation Resolution  The generated Pargs and Pnargs for each SG identified subgraph are processed to determine the most likely relation connections, using the constraints:\n1.", 
        "109": "AMR\u2019s are single component graphs without cycles.", 
        "110": "2.", 
        "111": "AMR\u2019s are simple directed graphs, a max of one relation between any two subgraphs is allowed.", 
        "112": "3.", 
        "113": "Outgoing predicate relations are limited to one of each kind (i.e.", 
        "114": "can\u2019t have two ARG0\u2019s)\nWe initialize a graph description with all the subgraphs identified by the SG network.", 
        "115": "Probabilities for all possible edges are represented in the Pargs and Pnargs matrices.", 
        "116": "The Subgraphs are connected to one another by applying a greedy algorithm, which repeatedly selects the most probable edge from the Pargs and Pnargs matrices and adds the edge to the graph description.", 
        "117": "After an edge is selected to be added to the graph, we adjust Pargs and Pnargs based on the constraints (hard decisions change the probabilities), and repeat adding edges until all remaining edge probabilities are below a threshold.", 
        "118": "(The optimum value of this threshold, 0.55, was found by experimenting with the development data set).", 
        "119": "From then on, only the most probable edges which span graph components are chosen, until the graph contains a single component.", 
        "120": "Expressed as a step by step procedure, we first define pconnect as the probability threshold at which to require graph component spanning, and we repeat the following, until any two subgraphs in the graph are connected by at least one path.", 
        "121": "1.", 
        "122": "Select the most probable outgoing relation from any of the identified subgraph probability matrices.", 
        "123": "Denote this probability as pr.", 
        "124": "2.", 
        "125": "If pr < pconnect, keep selecting most probable relations until a component spanning connection is found.", 
        "126": "3.", 
        "127": "Add the selected relation to the graph.", 
        "128": "If a cycle is created, reverse the relation direction and label.", 
        "129": "4.", 
        "130": "Eliminate impossible relations based on the constraints and re-normalize the affected Pargs and Pnargs matrices.", 
        "131": "4.4 AMR Construction  AMR Construction converts the connected subgraph AMR into the final AMR graph form, with proper concepts, relations, and root, as follows:\n1.", 
        "132": "The TOP attribute occurs exactly once in each AMR, so the subgraph with highest TOP probability produced by the Attr network is\nidentified.", 
        "133": "The AMR graph is adjusted so that it is rooted with the most probable TOP subgraph.", 
        "134": "After graph adjustment, new cycles are sometimes created, which are removed by using -of relation reversal.", 
        "135": "2.", 
        "136": "The subgraphs identified by the SG network, which were considered to be single nodes during relation resolution, are expanded to basic AMR concepts and relations to form a concept/relation AMR graph representation, using the Subgraph Expander component developed as shown in Figure 3b.", 
        "137": "When a subgraph contains two concepts, the choice of connecting to parent or child within the subgraph is made based on training data statistics of each relation type (Arg or Narg) for each subgraph type.", 
        "138": "3.", 
        "139": "Nationalities are normalized (e.g.", 
        "140": "French to France).", 
        "141": "4.", 
        "142": "A very basic coreference resolution is performed by merging all concepts representing \u201dI\u201d into a single concept.", 
        "143": "Coreference resolution was otherwise ignored due to development time constraints.", 
        "144": "5 Experimental Setup  Semantic graph comparison can be tricky because direct graph alignment fails in the presence of just a few miscompares.", 
        "145": "A practical graph comparison program called Smatch (Cai and Knight, 2013) is used to consistently evaluate AMR parsers.", 
        "146": "The smatch python script provides an F1 evaluation metric for whole-sentence semantic graph analysis by comparing sets of triples which describe portions of the graphs, and uses a hill climbing algorithm for efficiency.", 
        "147": "All networks, including SG, were trained using stochastic gradient descent (SGD) with a fixed learning rate.", 
        "148": "We tried sentence level loglikelihood, which trains a viterbi decoder, as a training objective, but found no improvement over word-level likelihood (cross entropy).", 
        "149": "After all LSTM and linear layers, we added dropout to minimize overfitting (Hinton et al., 2012) and batch normalization to reduce sensitivity to learning rates and initialization (Ioffe and Szegedy, 2015).", 
        "150": "For each of the five networks, we used the LDC2015E86 training split to train parameters, and periodically interrupted training to run the dev split (forward) in order to monitor performance.", 
        "151": "The model parameters which resulted in best dev performance were saved as the final model.", 
        "152": "The test split was used as the \u201din domain\u201d data set to assess the fully assembled parser.", 
        "153": "The inferred AMR\u2019s were then evaluated using the smatch program to produce an F1 score.", 
        "154": "An evaluation dataset was provided for Semeval 2016 task 8, which is significantly different from the LDC2015E86 split dataset.", 
        "155": "((2016) describes the eval dataset as \u201dquite difficult to parse, particularly due to creative approaches to word representation in the web forum portion\u201d).", 
        "156": "6 Results  We report the statistics for smatch results of the \u201dtest\u201d and \u201deval\u201d datasets for 12 trained systems in Table 3.", 
        "157": "The top five scores for Semeval 2016 task 8, representing the previous state-of-the-art, are shown for context.", 
        "158": "With a smatch score of between 0.651 and 0.654, and a mean of 0.652, our system improves the state-of-the-art AMR parser performance by between 5.07% and 5.55%, and by a mean of 5.22%.", 
        "159": "The best performing systems for in-domain (dev and test) data correlated well with the best ones for the out-of-domain (eval) data, although the scores for the eval dataset were lower overall.", 
        "160": "6.1 Individual Network Results  The word spans tagged by the SG network are used to determine the features for the other networks.", 
        "161": "In particular, every span identified as a predicate will trigger the system to evaluate the Args network in order to determine the probabilities of outgoing predicate ARG relations.", 
        "162": "Likewise, all spans identified as subgraphs (other than named subgraphs) will lead to a Nargs network evaluation to determine outgoing non-Arg relations.", 
        "163": "The SG network identifies predicates with 0.93 F1, named subgraphs with 0.91 F1, and all other subgraphs with 0.94 F1.", 
        "164": "The Args network identifies ARG0 and ARG1 relations with 0.73 F1, but identification of ARG2, ARG3, and ARG4 drops down to (0.53, 0.20, and 0.43).", 
        "165": "It is difficult for the system to generalize among these relation tags because they differ significantly between predicates.", 
        "166": "7 Conclusion and Future Work  We have shown that B-LSTM neural networks can be used as the basis for a graph based semantic\nparser.", 
        "167": "Our AMR parser effectively exploits the ability of B-LSTM networks to learn to selectively extract information from words separated by long distances in a sentence, and to build up higher level representations by rejecting or remembering important information during sequence processing.", 
        "168": "There are changes which could be made to eliminate all pre-processing and to further improve parser performance.", 
        "169": "Eliminating the need for syntactic pre-parsing is valuable since a syntactic parser takes up significant time and computational resources, and errors in the generated syntax will propagate into an AMR parser.", 
        "170": "Our approach avoids both of these problems, while generating high quality results.", 
        "171": "Wikification tasks are generally independent from parsing, but wiki links are a requirement for the latest AMR specification.", 
        "172": "Since our preferred wikifier application generates NER information, we used the generated NER tags as input to the SG network.", 
        "173": "But it would also be fairly easy to add gazetteer information to the network features in order to remove the need for NER preprocessing.", 
        "174": "Therefore, the wikification subtask is the only portion of the parser which requires any pre-processing at all.", 
        "175": "Incorporating wikification gazetteers as B-LSTM features might allow a performant, fully self contained parser to be created.", 
        "176": "Sense disambiguation is not a very generalizable task, senses other than 01 and 02 for different predicates may differ from each other in ways which are very difficult to discern.", 
        "177": "A better approach to disambiguation is to consider predicates separately, solving for a set of coefficients for each verb found in the training set.", 
        "178": "A general set of model parameters could then be used to handle unseen examples.", 
        "179": "Likewise, high level ARGs like ARG2 and ARG3 don\u2019t generalize very well among different predicates, and ARG inference accuracy could be improved with predicatespecific network parameters for the most common cases.", 
        "180": "The alignment between concepts and words is not a reliable, direct mapping: some concepts cannot be grounded to words, some are ambiguous, and automatic aligners tend to have high error rates relative to human aligning judgements.", 
        "181": "Improvements in the quality of the alignment in training data would improve parsing results."
    }, 
    "document_id": "P17-1043.pdf.json"
}
