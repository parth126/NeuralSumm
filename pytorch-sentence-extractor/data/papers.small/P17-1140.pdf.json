{
    "abstract_sentences": {
        "1": "This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance.", 
        "2": "Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty.", 
        "3": "Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins.", 
        "4": "Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1524\u20131534 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1140  1 Introduction  Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words.", 
        "2": "It is separately trained and then incorporated into the SMT framework in a pipeline style.", 
        "3": "In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016).", 
        "4": "An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015)\nis widely used, in which an encoder compresses the source sentence, an attention mechanism evaluates related source words and a decoder generates target words.", 
        "5": "The attention mechanism evaluates the distribution of to-be-translated source words in a content-based addressing fashion (Graves et al., 2014) which tends to attend to the source words regarding the content relation with current translation status.", 
        "6": "Lack of explicit models to exploit the word reordering knowledge may lead to attention faults and generate fluent but inaccurate or inadequate translations.", 
        "7": "Table 1 shows a translation instance and Figure 1 depicts the corresponding word alignment matrix that produced by the attention mechanism.", 
        "8": "In this example, even though the word \u201czuixin (latest)\u201d is a common adjective in Chinese and its following word should be translated soon in Chinese to English translation direction, the word \u201cyiju (evidence)\u201d does not obtain appropriate attention which leads to the incorrect translation.", 
        "9": "1524\nTo enhance the attention mechanism, implicit word reordering knowledge needs to be incorporated into attention-based NMT.", 
        "10": "In this paper, we introduce three distortion models that originated from SMT (Brown et al., 1993; Koehn et al., 2003; Och et al., 2004; Tillmann, 2004; Al-Onaizan and Papineni, 2006), so as to model the word reordering knowledge as the probability distribution of the relative jump distances between the newly translated source word and the to-be-translated source word.", 
        "11": "Our focus is to extend the attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty.", 
        "12": "Our models have three merits:\n1.", 
        "13": "Extended word reordering knowledge.", 
        "14": "Our models capture explicit word reordering knowledge to guide the attending process for attention mechanism.", 
        "15": "2.", 
        "16": "Convenient to be incorporated into attention-based NMT.", 
        "17": "Our distortion models are differentiable and can be trained in the end-to-end style.", 
        "18": "The interpolation approach ensures that the proposed models can coordinately work with the original attention mechanism.", 
        "19": "3.", 
        "20": "Flexible to utilize variant context for computing the word reordering penalty.", 
        "21": "In this paper, we exploit three categories of information as distortion context conditions\nto compute the word reordering penalty, but variant context information can be utilized due to our model\u2019s flexibility.", 
        "22": "We validate our models on the ChineseEnglish translation task and achieve notable improvements:\n\u2022 On 16K vocabularies, NMT models are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points.", 
        "23": "\u2022 On 30K vocabularies, the improvements over the phrase-based Moses and the attention-based NMT baseline system are average 6.06 and 1.57 BLEU points respectively.", 
        "24": "\u2022 Compared with previous work on identical corpora, we achieve the state-of-theart translation performance on average.", 
        "25": "The word alignment quality evaluation shows that our model can effectively improve the word alignment quality that is crucial for improving translation quality.", 
        "26": "2 Background  We aim to capture word reordering knowledge for the attention-based NMT by incorporating distortion models.", 
        "27": "This section briefly introduces attention-based NMT and distortion models in SMT.", 
        "28": "2.1 Attention-based Neural Machine Translation  Formally, given a source sentence x = x1, ...,xm and a target sentence y = y1, ...,yn, NMT models the translation probability as\nP (y|x) = n\u220f\nt=1\nP (yt|y<t,x), (1)\nwhere y<t = y1, ...,yt\u22121.", 
        "29": "The generation probability of yt is\nP (yt|y<t,x) = g(yt\u22121, ct, st), (2)\nwhere g(\u00b7) is a softmax regression function, yt\u22121 is the newly translated target word and\ny1 y2 yn...\nst is the hidden states of decoder which represents the translation status.", 
        "30": "The attention ct denotes the related source words for generating yt and is computed as the weighted-sum of source representation h upon an alignment vector \u03b1t shown in Eq.", 
        "31": "(3) where the align(\u00b7) function is a feedforward network with softmax normalization.", 
        "32": "ct = m\u2211\nj=1\n\u03b1t,jhj\n\u03b1t,j = align(st,hj) (3)\nThe hidden states st is updated as\nst = f(st\u22121,yt\u22121, ct), (4)\nwhere f(\u00b7) is a recurrent function.", 
        "33": "We adopt a varietal attention mechanism1 in our in-house RNNsearch model which is implemented as\ns\u0303t = f1(st\u22121,yt\u22121), \u03b1t,j = align(s\u0303t,hj), st = f2(s\u0303t, ct),\n(5)\nwhere f1(\u00b7) and f2(\u00b7) are recurrent functions.", 
        "34": "As shown in Eq.", 
        "35": "(3), the attention mechanism attends to source words in a contentbased addressing way without considering any explicit word reordering knowledge.", 
        "36": "We introduce distortion models to capture explicit word reordering knowledge for enhancing the attention mechanism and improving translation quality.", 
        "37": "1https://github.com/nyu-dl/dl4mttutorial/tree/master/session2  2.2 Distortion Models in SMT  In SMT, distortion models are linearly combined with other features, as follows,\ny\u2217 = argmax y exp[\u03bbdd(x,y, b)+\nR\u22121\u2211\nr=1\n\u03bbrhr(x,y, b)], (6)\nwhere d(\u00b7) is the distortion feature, hr(\u00b7) represents other features, \u03bbd and \u03bbr are the weights, b is the latent variable that represents translation knowledge and R is the number of features.", 
        "38": "IBM Models (Brown et al., 1993) depicted the word reordering knowledge as positional relations between source and target words.", 
        "39": "Koehn et al.", 
        "40": "(2003) proposed a distortion model for phrase-based SMT based on jump distances between the newly translated phrases and to-be-translated phrases which does not consider specific lexical information.", 
        "41": "Och et al.", 
        "42": "(2004) and Tillmann (2004) proposed orientation-based distortion models that consider translation orientations.", 
        "43": "Yaser and Papineni (2006) proposed a distortion model to estimate probability distribution on possible relative jumps conditioned on source words.", 
        "44": "These models are proposed for SMT and separately trained as sub-components.", 
        "45": "Inspired by these previous work, we introduce the distortion models into NMT model for modeling the word reordering knowledge.", 
        "46": "Our proposed models are designed for NMT which can be trained in the end-to-end style.", 
        "47": "3 Distortion Models for attention-based NMT  The basic idea of our proposed distortion models is to estimate the probability distribution of the possible relative jump distances between the newly translated source word and the tobe-translated source word upon the context condition.", 
        "48": "Figure 2 shows the general architecture of our proposed model.", 
        "49": "3.1 General Architecture  We employ an interpolation approach to incorporate distortion models into attention-based NMT as\n\u03b1t = \u03bb \u00b7 dt + (1\u2212 \u03bb)\u03b1\u0302t, (7)\nwhere \u03b1t is the ultimate alignment vector for computing the related source context ct, dt is the alignment vector calculated by the distortion model, \u03b1\u0302t is the alignment vector computed by the basic attention mechanism and \u03bb is a hyper-parameter to control the weight of the distortion model.", 
        "50": "In the proposed distortion model, relative jumps on source words are depicted as the \u201cshift\u201d actions of the alignment vector \u03b1t\u22121 which is shown in the Figure 3.", 
        "51": "The right shift of \u03b1t\u22121 indicates that the translation orientation of source words is forward and the left shift represents that the translation orientation is backward.", 
        "52": "The extent of a shift action measures the word reordering distance.", 
        "53": "Alignment vector dt, which is produced by the distortion model, is the expectation of all possible shifts of \u03b1t\u22121 conditioned on certain context.", 
        "54": "Formally, the proposed distortion model is\ndt = E[\u0393(\u03b1t\u22121)]\n=\nl\u2211\nk=\u2212l P (k|\u03a8) \u00b7 \u0393(\u03b1t\u22121, k),\n(8)\nwhere k \u2208 [\u2212l, l] is the possible relative jump distance, l is the window size parameter and P (k|\u03a8) stands for the probability of jump distance k that conditioned on the context \u03a8.", 
        "55": "Function \u0393(\u00b7) for shifting the alignment vector is defined as\n\u0393(\u03b1t\u22121, k) =   {\u03b1t\u22121,\u2212k, ..., \u03b1t\u22121,m, 0, ..., 0}, k<0 \u03b1t\u22121, k= 0 {0, ..., 0, \u03b1t\u22121,1, ..., \u03b1t\u22121,m\u2212k}, k>0 (9)\nwhich can be implemented as matrix multiplication computations.", 
        "56": "We respectively exploit source context, target context and translation status context (hidden states of decoder) as \u03a8 and derive three distortion models: Source-based Distortion (S-Distortion) model , Targetbased Distortion (T-Distortion) model and Translation-status-based Distortion (H-Distortion) model.", 
        "57": "Our framework is capable of utilizing arbitrary context as the condition \u03a8 to predict the relative jump distances.", 
        "58": "3.2 S-Distortion model  S-Distortion model adopts previous source context ct\u22121 as the context \u03a8 with the intuition that certain source word indicate certain jump distance.", 
        "59": "The to-be-translated source word have intense positional relations with the newly translated one.", 
        "60": "The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs.", 
        "61": "Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry specific word reordering knowledge.", 
        "62": "To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows,\nNP \u2212\u2192 JJ NN | JJ NN JJ \u2212\u2192 zuixin | latest.", 
        "63": "(10)\nFrom the above grammar, we can conjecture the speculation that after the word \u201dzuixin(latest)\u201d is translated, the translation orientation is forward with shift distance 1.", 
        "64": "The probability function in S-Distortion model is defined as follows,\nP (\u00b7|\u03a8) = z(ct\u22121) = softmax(Wcct\u22121 + bc),\n(11)\nwhere Wc \u2208 R(2l+1)\u00d7dim(ct\u22121) and bc \u2208 R2l+1 are weight matrix and bias parameters.", 
        "65": "3.3 T-Distortion Model  T-Distortion model exploits the embedding of the previous generated target word yt\u22121 as the context condition to predict the probability distribution of distortion distances.", 
        "66": "It focuses on the word reordering knowledge upon target\nword context.", 
        "67": "As illustrated in Eq.", 
        "68": "(10), the target word \u201clatest\u201d possesses word reordering knowledge that is identical with source word \u201czuixin\u201d.", 
        "69": "The probability function in T-Distortion model is defined as follows,\nP (\u00b7|\u03a8) = z(yt\u22121) = softmax(Wyemb(yt\u22121) + by), (12)\nwhere emb(yt\u22121) is the embedding of yt\u22121, Wy \u2208 R(2l+1)\u00d7dim(emb(yt\u22121)) and by \u2208 R2l+1 are weight matrix and bias parameters.", 
        "70": "3.4 H-Distortion Model  The hidden states s\u0303t\u22121 reflect the translation status and contains both source context and target context information.", 
        "71": "Therefore, we exploit s\u0303t\u22121 as context \u03a8 in the H-Distortion model to predict shift distances.", 
        "72": "The probability function in H-Distortion model is defined as follows,\nP (\u00b7|\u03a8) = z(s\u0303t\u22121) = softmax(Wss\u0303t\u22121 + bs)\n(13)\nwhere Ws \u2208 R(2l+1)\u00d7dim(s\u0303t\u22121) and bs \u2208 R2l+1 are the weight matrix and bias parameters.", 
        "73": "4 Experiments  We carry the translation task on the ChineseEnglish direction to evaluate the effectiveness of our models.", 
        "74": "To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus.", 
        "75": "We also conduct the experiments to observe effects of hyper-parameters and the training strategies.", 
        "76": "4.1 Data and Metrics  Data: Our Chinese-English training corpus consists of 1.25M sentence pairs extracted from LDC corpora2 with 27.9M Chinese words and 34.5M English words respectively.", 
        "77": "16K vocabularies cover approximately 95.8% and 98.3% words and 30K vocabularies cover approximately 97.7% and 99.3% words in Chinese and English respectively.", 
        "78": "We choose NIST 2002 dataset as the validation set.", 
        "79": "NIST\n2The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.", 
        "80": "2003-2006 are used as test sets.", 
        "81": "To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs.", 
        "82": "Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002).", 
        "83": "Sign-test (Collins et al., 2005) is exploited for statistical significance test.", 
        "84": "Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality.", 
        "85": "4.2 Comparison Systems  We compare our approaches with three baseline systems: Moses (Koehn et al., 2007): An open source phrase-based SMT system with default settings.", 
        "86": "Words are aligned with GIZA++ (Och and Ney, 2003).", 
        "87": "The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data by SRILM (Stolcke et al., 2002).", 
        "88": "Groundhog4: An open source attentionbased NMT system with default settings.", 
        "89": "RNNsearch\u2217: Our in-house implementation of NMT system with the varietal attention mechanism and other settings that presented in section 4.3.", 
        "90": "4.3 Training  Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions.", 
        "91": "Following Bahdanau et al.", 
        "92": "(2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder.", 
        "93": "The forward representation and the backward representation are concatenated at the corresponding position as the ultimate representation of a source word.", 
        "94": "The word embedding dimension is set to 620 and the hidden layer size is 1000.", 
        "95": "The interpolation parameter \u03bb is 0.5 and the window size l is set to 3.", 
        "96": "Training details: Square matrices are initialized in a random orthogonal way.", 
        "97": "Non-square matrices are initialized by sampling each element from the\n3ftp://jaguar.ncsl.nist.gov/mt/resources/ mteval-v11b.pl\n4https://github.com/lisa-groundhog/ GroundHog\nGaussian distribution with mean 0 and variance 0.012.", 
        "98": "All bias are initialized to 0.", 
        "99": "Parameters are updated by Mini-batch Gradient Descent and the learning rate is controlled by the AdaDelta (Zeiler, 2012) algorithm with decay constant \u03c1 = 0.95 and denominator constant \u03f5 = 1e \u2212 6.", 
        "100": "The batch size is 80.", 
        "101": "Dropout strategy (Srivastava et al., 2014) is applied to the output layer with the dropout rate 0.5 to avoid over-fitting.", 
        "102": "The gradients of the cost function which have L2 norm larger than a predefined threshold 1.0 is normalized to the threshold to avoid gradients explosion (Pascanu et al., 2013).", 
        "103": "We exploit length normalization (Cho et al., 2014a) on candidate translations and the beam size for decoding is 12.", 
        "104": "For NMT with distortion models, we use trained RNNsearch\u2217 model to initialize parameters except for those related to distortions.", 
        "105": "4.4 Results  The translation quality experiment results are shown in Table 2.", 
        "106": "We carry the experiments on different vocabulary sizes for that different vocabulary sizes cause different degrees of the rare word collocations.", 
        "107": "Through this way, we can validate the effects of our proposed models in alleviating the rare word collocations problem that leads to incorrect word alignments.", 
        "108": "On 16K vocabularies: The phrase-based Moses performs better than the basic NMTs including Groundhog and RNNsearch\u2217.", 
        "109": "Be-\nsides the differences between model architectures, restricted vocabularies and sentence length also affect the performance of NMTs.", 
        "110": "However, RNNsearch\u2217 with distortion models surpass phrase-based Moses by average 3.60, 4.27 and 4.43 BLEU points.", 
        "111": "RNNsearch\u2217 outperforms Groundhog by average 1.96 BLEU points due to the varietal attention mechanism, length normalization and dropout strategies.", 
        "112": "Distortion models bring about remarkable improvements as 4.26, 4.92 and 5.09 BLEU points over the RNNsearch\u2217 model.", 
        "113": "On 30K vocabularies: RNNsearch\u2217 with distortion models yield average gains by 1.57, 1.21 and 1.45 BLEU points over RNNsearch\u2217 and outperform phrase-based Moses by average 6.06, 5.70 and 5.94 BLEU points and surpass GroundHog by average 5.56, 5.20 and 5.44 BLEU points.", 
        "114": "RNNsearch\u2217(16K) with distortion models achieve close performances with RNNsearch\u2217(30K).", 
        "115": "The improvements on 16K vocabularies are larger than that on 30K vocabularies for the intuition that more \u201dUNK\u201d words lead to more rare word collocations, which results in serious attention ambiguities.", 
        "116": "The RNNsearch\u2217 with distortion models yield tremendous improvements on BLEU scores proves the effectiveness of proposed approaches in improving translation quality.", 
        "117": "Comparison with previous work: We present the performance comparison with pre-\nvious work that employ identical training corpora in Table 3.", 
        "118": "Our work evidently outperforms previous work on average performance.", 
        "119": "Although we restrict the maximum length of sentence to 50, our model achieves the state-\nof-the-art BLEU scores on almost all test sets except NIST2006.", 
        "120": "4.5 Analysis  We investigate the effects on the alignment quality of our models and conduct the experiments to evaluate the influence of the hyperparameter settings and the training strategies.", 
        "121": "4.5.1 Alignment Quality  Distortion models concentrate on attending to to-be-translated words based on the word reordering knowledge and can intuitively enhance the word alignment quality.", 
        "122": "To investigate the effect on word alignment quality, we apply the BLEU and AER evaluations on Tsinghua manually aligned data set.", 
        "123": "Table 4 lists the BLEU and AER scores of Chinese-English translation with 30K vocabulary.", 
        "124": "RNNsearch*(30K) with distortion models achieve significant improvements on BLEU scores and obvious decrease on AER scores.", 
        "125": "The results shows that the proposed model can effectively improve the word alignment quality\nFigure 4 shows the output of distortion model and ultimate alignment matrix of the above-mentioned instance.", 
        "126": "Compared with Figure 1, the alignment matrix produced by NMT with distortion models is more concentrated and accurate.", 
        "127": "The output of distortion model shows its capacity of modeling word reordering knowledge.", 
        "128": "4.5.2 Effect of Hyper-parameters  To investigate the effect of the weight hyperparameter \u03bb and window hyper-parameter l in the proposed model, we carry experiments on H-Distortion model with variable hyperparameter settings.", 
        "129": "We fix l = 3 for exploring the effect of \u03bb and fix \u03bb = 0.5 for observing the effect of l. Figure 5 presents the translation performances with respect to hyperparameters.", 
        "130": "With the increase of weight \u03bb, the BLEU scores first rise and then drop, which shows the distortion model provides additional helpful information while can not fully cover the attention mechanism for its insufficient content searching ability.", 
        "131": "For window\nl, the experiments show that larger windows bring slight further improvements, which indicates that distortion model pays more attention to the short-distance reordering knowledge.", 
        "132": "4.5.3 Pre-training VS No Pre-training  We conduct the experiment without using pretraining strategy to observe the effect of the initialization.", 
        "133": "As is shown in Table 5, the no-pre-training model achieves consistent improvements with the pre-training one which verifies the stable effectiveness of our approach.", 
        "134": "Initialization with pre-training strategy provides a fast approach to obtain the model for it needs fewer training iterations.", 
        "135": "5 Related Work  Our work is inspired by the distortion models that widely used in SMT.", 
        "136": "The most related work in SMT is the distortion model proposed by Yaser and Papineni (2006).", 
        "137": "Their model is identical to our S-Distortion model that captures the relative jump distance knowledge on source words.", 
        "138": "However, our approach is deliberately designed for the attention-based NMT system and is capable of exploiting variant context information to predict the relative jump distances.", 
        "139": "Our work is related to the work (Luong et al., 2015a; Feng et al., 2016; Tu et al., 2016;\nCohn et al., 2016; Meng et al., 2016; Wang et al., 2016) that concentrate on the improvement of the attention mechanism.", 
        "140": "To remit the computing cost of the attention mechanism when dealing with long sentences, Luong et al.", 
        "141": "(2015a) proposed the local attention mechanism by just focusing on a subscope of source positions.", 
        "142": "Cohn et al.", 
        "143": "(2016) incorporated structural alignment biases into the attention mechanism and obtained improvements across several challenging language pairs in low-resource settings.", 
        "144": "Feng et al.", 
        "145": "(2016) passed the previous attention context to the attention mechanism by adding recurrent connections as the implicit distortion model.", 
        "146": "Tu et al.", 
        "147": "(2016) maintained a coverage vector for keeping the attention history to acquire accurate translations.", 
        "148": "Meng et al.", 
        "149": "(2016) proposed the interactive attention with the attentive read and attentive write operation to keep track of the interaction history.", 
        "150": "Wang et al.", 
        "151": "(2016) utilized an external memory to store additional information for guiding the attention computation.", 
        "152": "These works are different from ours, as our distortion models explicitly capture word reordering knowledge through estimating the probability distribution of relative jump distances on source words to incorporate word reordering knowledge into the attention-based NMT.", 
        "153": "6 Conclusions  We have presented three distortion models to enhance attention-based NMT through incorporating the word reordering knowledge.", 
        "154": "The basic idea of proposed distortion models is to enable the attention mechanism to attend to the source words regarding both semantic requirement and the word reordering penalty.", 
        "155": "Experiments show that our models can evidently improve the word alignment quality and translation performance.", 
        "156": "Compared with previous work on identical corpora, our model achieves the state-of-the-art performance on average.", 
        "157": "Our model is convenient to be applied in the attention-based NMT and can be trained in the end-to-end style.", 
        "158": "We also investigated the effect of hyper-parameters and pre-training strategy and further proved the stable effectiveness of our model.", 
        "159": "In the future, we plan to validate the effectiveness of\nour model on more language pairs.", 
        "160": "7 Acknowledgement  Qun Liu\u2019s work is partially supported by Science Foundation Ireland in the ADAPT Centre for Digital Content Technology (www.adaptcentre.ie) at Dublin City University funded under the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Regional Development Fund.", 
        "161": "We are grateful to Qiuye Zhao, Fandong Meng and Daqi Zheng for their helpful suggestions.", 
        "162": "We thank the anonymous reviewers for their insightful comments."
    }, 
    "document_id": "P17-1140.pdf.json"
}
