{
    "abstract_sentences": {
        "1": "Word embeddings have become widelyused in document analysis.", 
        "2": "While a large number of models for mapping words to vector spaces have been developed, it remains undetermined how much net gain can be achieved over traditional approaches based on bag-of-words.", 
        "3": "In this paper, we propose a new document clustering approach by combining any word embedding with a state-of-the-art algorithm for clustering empirical distributions.", 
        "4": "By using the Wasserstein distance between distributions, the word-to-word semantic relationship is taken into account in a principled way.", 
        "5": "The new clustering method is easy to use and consistently outperforms other methods on a variety of data sets.", 
        "6": "More importantly, the method provides an effective framework for determining when and how much word embeddings contribute to document analysis.", 
        "7": "Experimental results with multiple embedding models are reported."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1847\u20131856 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1169  1 Introduction  Word embeddings (a.k.a.", 
        "2": "word vectors) have been broadly adopted for document analysis (Mikolov et al., 2013a,b).", 
        "3": "The embeddings can be trained from external large-scale corpus and then easily utilized for different data.", 
        "4": "To a certain degree, the knowledge mined from the corpus, possibly in very intricate ways, is coded in the vector space,\nCorrespondence should be sent to J. Ye (jxy198@psu.edu) and J. Li (jiali@psu.edu).", 
        "5": "The work was done when Z. Wu was with Penn State.", 
        "6": "the samples of which are easy to describe and ready for mathematical modeling.", 
        "7": "Despite the appeal, researchers will be interested in knowing how much gain an embedding can bring forth over the performance achievable by existing bag-ofwords based approaches.", 
        "8": "Moreover, how can the gain be quantified?", 
        "9": "Such a preliminary evaluation will be carried out before building a sophisticated pipeline of analysis.", 
        "10": "Almost every document analysis model used in practice is constructed assuming a certain basic representation\u2014bag-of-words or word embeddings\u2014for the sake of computational tractability.", 
        "11": "For example, after word embedding is done, high-level models in the embedded space, such as entity representations, similarity measures, data manifolds, hierarchical structures, language models, and neural architectures, are designed for various tasks.", 
        "12": "In order to invent or enhance analysis tools, we want to understand precisely the pros and cons of the highlevel models and the underlying representations.", 
        "13": "Because the model and the representation are tightly coupled in an analytical system, it is not easy to pinpoint where the gain or loss found in practice comes from.", 
        "14": "Should the gain be credited to the mechanism of the model or to the use of word embeddings?", 
        "15": "As our experiments demonstrate, introducing certain assumptions will make individual methods effective only if certain constraints are met.", 
        "16": "We will address this issue under an unsupervised learning framework.", 
        "17": "Our proposed clustering paradigm has several advantages.", 
        "18": "Instead of packing the information of a document into a fixed-length vector for subsequent analysis, we treat a document more thoroughly as a distributional entity.", 
        "19": "In our approach, the distance between two empirical\n1847\nnonparametric measures (or discrete distributions) over the word embedding space is defined as the Wasserstein metric (a.k.a.", 
        "20": "the Earth Mover\u2019s Distance or EMD) (Wan, 2007; Kusner et al., 2015).", 
        "21": "Comparing with a vector representation, an empirical distribution can represent with higher fidelity a cloud of points such as words in a document mapped to a certain space.", 
        "22": "In the extreme case, the empirical distribution can be set directly as the cloud of points.", 
        "23": "In contrast, a vector representation reduces data significantly, and its effectiveness relies on the assumption that the discarded information is irrelevant or nonessential to later analysis.", 
        "24": "This simplification itself can cause degradation in performance, obscuring the inherent power of the word embedding space.", 
        "25": "Our approach is intuitive and robust.", 
        "26": "In addition to a high fidelity representation of the data, the Wasserstein distance takes into account the crossterm relationship between different words in a principled fashion.", 
        "27": "According to the definition, the distance between two documents A and B are the minimum cumulative cost that words from document A need to \u201ctravel\u201d to match exactly the set of words for document B.", 
        "28": "Here, the travel cost of a path between two words is their (squared) Euclidean distance in the word embedding space.", 
        "29": "Therefore, how much benefit the Wasserstein distance brings also depends on how well the word embedding space captures the semantic difference between words.", 
        "30": "While Wasserstein distance is well suited for document analysis, a major obstacle of approaches based on this distance is the computational intensity, especially for the original D2-clustering method (Li and Wang, 2008).", 
        "31": "The main technical hurdle is to compute efficiently the Wasserstein barycenter, which is itself a discrete distribution, for a given set of discrete distributions.", 
        "32": "Thanks to the recent advances in the algorithms for solving Wasserstein barycenters (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017), one can now perform document clustering by directly treating them as empirical measures over a word embedding space.", 
        "33": "Although the computational cost is still higher than the usual vector-based clustering methods, we believe that the new clustering approach has reached a level of efficiency to justify its usage given how important it is to obtain high-quality clustering of unstructured text data.", 
        "34": "For instance, clustering is\na crucial step performed ahead of cross-document co-reference resolution (Singh et al., 2011), document summarization, retrospective events detection, and opinion mining (Zhai et al., 2011).", 
        "35": "1.1 Contributions  Our work has two main contributions.", 
        "36": "First, we create a basic tool of document clustering, which is easy to use and scalable.", 
        "37": "The new method leverages the latest numerical toolbox developed for optimal transport.", 
        "38": "It achieves state-of-theart clustering performance across heterogeneous text data\u2014an advantage over other methods in the literature.", 
        "39": "Second, the method enables us to quantitatively inspect how well a word-embedding model can fit the data and how much gain it can produce over the bag-of-words models.", 
        "40": "2 Related Work  In the original D2-clustering framework proposed by Li and Wang (2008), calculating Wasserstein barycenter involves solving a large-scale LP problem at each inner iteration, severely limiting the scalability and robustness of the framework.", 
        "41": "Such high magnitude of computations had prohibited it from deploying in many real-world applications until recently.", 
        "42": "To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2-clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).", 
        "43": "Although the effectiveness of Wasserstein distance has been well recognized in the computer vision and multimedia literature, the property of Wasserstein barycenter has not been well understood.", 
        "44": "To our knowledge, there still lacks systematic study of applying Wasserstein barycenter and D2-clustering in document analysis with word embeddings.", 
        "45": "A closely related work by Kusner et al.", 
        "46": "(2015) connects the Wasserstein distance to the word embeddings for comparing documents.", 
        "47": "Our work differs from theirs in the methodology.", 
        "48": "We directly pursue a scalable clustering setting rather than construct a nearest neighbor graph based on calculated distances, because the calculation of the Wasserstein distances of all pairs is too expensive to be practical.", 
        "49": "Kusner et al.", 
        "50": "(2015) used a lower bound that was less costly to compute in order to prune unnecessary full distance calculation, but\nthe scalability of this modified approach is still limited, an issue to be discussed in Section 4.3.", 
        "51": "On the other hand, our approach adopts the framework similar to the K-means which is of complexity O(n) per iteration and usually converges within just tens of iterations.", 
        "52": "The computation of D2clustering, though in its original form was magnitudes heavier than typical document clustering methods, can now be efficiently carried out with parallelization and proper implementations (Ye et al., 2017).", 
        "53": "3 The Method  This section introduces the distance, the D2clustering technique, the fast computation framework, and how they are used in the proposed document clustering method.", 
        "54": "3.1 Wasserstein Distance  Suppose we represent each document dk consisting mk unique words by a discrete measure or a discrete distribution, where k = 1, .", 
        "55": ".", 
        "56": ".", 
        "57": ", N with N being the sample size:\ndk = Xmk\ni=1 w (k) i x(k)i .", 
        "58": "(1)\nHere x denotes the Dirac measure with support x, and w(k)i 0 is the \u201cimportance weight\u201d for the i-th word in the k-th document, withPmk\ni=1 w (k) i = 1.", 
        "59": "And x (k) i 2 Rd, called a support point, is the semantic embedding vector of the i-th word.", 
        "60": "The 2nd-order Wasserstein distance between two documents d1 and d2 (and likewise for any document pairs) is defined by the following LP problem: W 2(d1, d2) :=\nmin \u21e7\nP i,j \u21e1i,jkx (1) i x (2) j k22\ns.t.", 
        "61": "Pm2 j=1 \u21e1i,j = wi, 8i, Pm1\ni=1 \u21e1i,j = wj , 8j \u21e1i,j 0, 8i, j ,\n(2) where \u21e7 = {\u21e1i,j} is a m1 \u21e5m2 coupling matrix, and let {Ci,j := kx(1)i x (2) j k22} be transportation costs between words.", 
        "62": "Wasserstein distance is a true metric (Villani, 2003) for measures, and its best exact algorithm has a complexity of O(m3 log m) (Orlin, 1993), if m1 = m2 = m.  3.2 Discrete Distribution (D2-) Clustering  D2-clustering (Li and Wang, 2008) iterates between the assignment step and centroids updating step in a similar way as the Lloyd\u2019s K-means.", 
        "63": "Suppose we are to find K clusters.", 
        "64": "The assignment step finds each member distribution its nearest mean from K candidates.", 
        "65": "The mean of each cluster is again a discrete distribution with m support points, denoted by ci, i = 1, .", 
        "66": ".", 
        "67": ".", 
        "68": ", K. Each mean is iteratively updated to minimize its total within cluster variation.", 
        "69": "We can write the D2clustering problem as follows: given sample data {dk}Nk=1, support size of means m, and desired number of clusters K, D2-clustering solves\nmin c1,...,cK XN k=1 min 1iK W 2(dk, ci) , (3)\nwhere c1, .", 
        "70": ".", 
        "71": ".", 
        "72": ", cK are Wasserstein barycenters.", 
        "73": "At the core of solving the above formulation is an optimization method that searches the Wasserstein barycenters of varying partitions.", 
        "74": "Therefore, we concentrate on the following problem.", 
        "75": "For each cluster, we reorganize the index of member distributions from 1, .", 
        "76": ".", 
        "77": ".", 
        "78": ", n. The Wasserstein barycenter (Agueh and Carlier, 2011; Cuturi and Doucet, 2014) is by definition the solution of\nmin c Xn k=1 W 2(dk, c) , (4)\nwhere c = Pm\ni=1 wi xi .", 
        "79": "The above Wasserstein barycenter formulation involves two levels of optimization: the outer level finding the minimizer of total variations, and the inner level solving Wasserstein distances.", 
        "80": "We remark that in D2clustering, we need to solve multiple Wasserstein barycenters rather than a single one.", 
        "81": "This constitutes the third level of optimization.", 
        "82": "3.3 Modified Bregman ADMM for Computing Wasserstein Barycenter  The recent modified Bregman alternating direction method of multiplier (B-ADMM) algorithm (Ye et al., 2017), motivated by the work by Wang and Banerjee (2014), is a practical choice for computing Wasserstein barycenters.", 
        "83": "We briefly sketch their algorithmic procedure of this optimization method here for the sake of completeness.", 
        "84": "To solve for Wasserstein barycenter defined in Eq.", 
        "85": "(4), the key procedure of the modified Bregman ADMM involves iterative updates of four block of primal variables: the support points of c \u2014 {xi}mi=1 (with transportation costs {Ci,j}(k) for k = 1, .", 
        "86": ".", 
        "87": ".", 
        "88": ", n), the importance weights of c \u2014 {wi}mi=1, and two sets of split matching variables \u2014 {\u21e1(k,1)i,j } and {\u21e1 (k,2) i,j }, for k = 1, .", 
        "89": ".", 
        "90": ".", 
        "91": ", n, as well as Lagrangian variables { (k)i,j } for k = 1, .", 
        "92": ".", 
        "93": ".", 
        "94": ", n.\nIn the end, both {\u21e1(k,1)i,j } and {\u21e1 (k,2) i,j } converge to the matching weight in Eq.", 
        "95": "(2) with respect to d(c, dk).", 
        "96": "The iterative algorithm proceeds as follows until c converges or a maximum number of iterations are reached: given constant \u2327 10, \u21e2 /P\ni,j,k C (k) i,jPn\nk=1 mkm and round-off tolerance \u270f = 10 10, those variables are updated in the following order.", 
        "97": "Update {xi}mi=1 and {C (k) i,j } in every \u2327 iterations:\nxi := 1\nnwi\nXn k=1 Xmk j=1 \u21e1 (k,1) i,j x (k) j , 8i, (5)\nC (k) i,j := kxi x (k) j k22, 8i, j and k. (6)\nUpdate {\u21e1(k,1)i,j } and {\u21e1 (k,2) i,j }.", 
        "98": "For each i, j and k,\n\u21e1 (k,2) i,j := \u21e1 (k,2) i,j exp\nC(k)i,j\n(k) i,j\n\u21e2\n!", 
        "99": "+ \u270f , (7)\n\u21e1 (k,1) i,j := w (k) j \u21e1 (k,2) i,j .\u21e3Xm l=1 \u21e1 (k,2) l,j \u2318 , (8) \u21e1 (k,1) i,j := \u21e1 (k,1) i,j exp \u21e3 (k) i,j /\u21e2 \u2318 + \u270f .", 
        "100": "(9)\nUpdate {wi}mi=1.", 
        "101": "For i = 1, .", 
        "102": ".", 
        "103": ".", 
        "104": ", m ,\nwi :=\nnX\nk=1\nPmk j=1 \u21e1\n(k,1) i,jP\ni,j \u21e1 (k,1) i,j\n, (10)\nwi := wi .\u21e3Xm i=1 wi \u2318 .", 
        "105": "(11)\nUpdate {\u21e1(k,2)i,j } and { (k) i,j }.", 
        "106": "For each i, j and k,\n\u21e1 (k,2) i,j := wi\u21e1 (k,1) i,j .\u21e3Xmk l=1 \u21e1 (k,1) i,l \u2318 , (12)\n(k) i,j := (k) i,j + \u21e2\n\u21e3 \u21e1\n(k,1) i,l \u21e1 (k,2) i,l\n\u2318 .", 
        "107": "(13)\nEq.", 
        "108": "(5)-(13) can all be vectorized as very efficient numerical routines.", 
        "109": "In a data parallel implementation, only Eq.", 
        "110": "(5) and Eq.", 
        "111": "(10) (involving Pn k=1) needs to be synchronized.", 
        "112": "The software package detailed in (Ye et al., 2017) was used to generate relevant experiments.", 
        "113": "We make available our codes and pre-processed datasets for reproducing all experiments of our approach.", 
        "114": "4 Experimental Results    4.1 Datasets and Evaluation Metrics  We prepare six datasets to conduct a set of experiments.", 
        "115": "Two short-text datasets are created as follows.", 
        "116": "(D1) BBCNews abstract: We concatenate\nthe title and the first sentence of news posts from BBCNews dataset1 to create an abstract version.", 
        "117": "(D2) Wiki events: Each cluster/class contains a set of news abstracts on the same story such as \u201c2014 Crimean Crisis\u201d crawled from Wikipedia current events following (Wu et al., 2015); this dataset offers more challenges because it has more finegrained classes and fewer documents (with shorter length) per class than the others.", 
        "118": "It also shows more realistic nature of applications such as news event clustering.", 
        "119": "We also experiment with two long-text datasets and two domain-specific text datasets.", 
        "120": "(D3) Reuters-21578: We obtain the original Reuters-21578 text dataset and process as follows: remove documents with multiple categories, remove documents with empty body, remove duplicates, and select documents from the largest ten categories.", 
        "121": "Reuters dataset is a highly unbalanced dataset (the top category has more than 3,000 documents while the 10-th category has fewer than 100).", 
        "122": "This imbalance induces some extra randomness in comparing the results.", 
        "123": "(D4) 20Newsgroups \u201cbydate\u201d version: We obtain the raw \u201cbydate\u201d version and process them as follows: remove headers and footers, remove URLs and Email addresses, delete documents with less than ten words.", 
        "124": "20Newsgroups have roughly comparable sizes of categories.", 
        "125": "(D5) BBCSports.", 
        "126": "(D6) Ohsumed and Ohsumed-full: Documents are medical abstracts from the MeSH categories of the year 1991.", 
        "127": "Specifically, there are 23 cardiovascular diseases categories.", 
        "128": "Evaluating clustering results is known to be nontrivial.", 
        "129": "We use the following three sets of quantitative metrics to assess the quality of clusters by knowing the ground truth categorical labels of documents: (i) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (ii) Adjusted Mutual Information (AMI) (Vinh et al., 2010); and (iii) Adjusted Rand Index (ARI) (Rand, 1971).", 
        "130": "For sensitivity analysis, we use the homogeneity score (Rosenberg and Hirschberg, 2007) as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels.", 
        "131": "Generally speaking, more clusters leads to higher homogeneity by chance.", 
        "132": "1BBCNews and BBCSport are downloaded from http://mlg.ucd.ie/datasets/bbc.html  4.2 Methods in Comparison  We examine four categories of methods that assume a vector-space model for documents, and compare them to our D2-clustering framework.", 
        "133": "When needed, we use K-means++ to obtain clusters from dimension reduced vectors.", 
        "134": "To diminish the randomness brought by K-mean initialization, we ensemble the clustering results of 50 repeated runs (Strehl and Ghosh, 2003), and report the metrics for the ensembled one.", 
        "135": "The largest possible vocabulary used, excluding word embedding based approaches, is composed of words appearing in at least two documents.", 
        "136": "On each dataset, we select the same set of Ks, the number of clusters, for all methods.", 
        "137": "Typically, Ks are chosen around the number of ground truth categories in logarithmic scale.", 
        "138": "We prepare two versions of the TF-IDF vectors as the unigram model.", 
        "139": "The ensembled K-means methods are used to obtain clusters.", 
        "140": "(1) TF-IDF vector (Sparck Jones, 1972).", 
        "141": "(2) TF-IDF-N vector is found by choosing the most frequent N words in a corpus, where N 2 {500, 1000, 1500, 2000}.", 
        "142": "The difference between the two methods highlights the sensitivity issue brought by the size of chosen vocabulary.", 
        "143": "We also compare our approach with the following seven additional baselines.", 
        "144": "They are (3) Spectral Clustering (Laplacian), (4) Latent Semantic Indexing (LSI) (Deerwester et al., 1990), (5) Locality Preserving Projection (LPP) (He and Niyogi, 2004; Cai et al., 2005), (6) Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999; Xu et al., 2003), (7) Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Hoffman et al., 2010), (8) Average of word vectors (AvgDoc), and (9) Paragraph Vectors (PV) (Le and Mikolov, 2014).", 
        "145": "Details on their experimental setups and hyper-parameter search strategies can be found in the Appendix.", 
        "146": "4.3 Runtime  We report the runtime for our approach on two largest datasets.", 
        "147": "The experiments regarding other smaller datasets all terminate within minutes in a single machine, which we omit due to space limitation.", 
        "148": "Like K-means, the runtime by our approach depends on the number of actual iterations before a termination criterion is met.", 
        "149": "In the Newsgroups dataset, with m = 100 and K = 45, the time per iteration is 121 seconds on 48\nprocessors.", 
        "150": "In Reuters dataset, with m = 100 and K = 20, the time per iteration is 190 seconds on 24 processors.", 
        "151": "Each run terminates in around tens of iterations typically, upon which the percentage of label changes is less than 0.1%.", 
        "152": "Our approach adopts the Elkan\u2019s algorithm (2003) pruning unnecessary computations of Wasserstein distance in assignment steps of K-means.", 
        "153": "For the Newsgroups data (with m = 100 and K = 45), our approach terminates in 36 iterations, and totally computes 12, 162, 717 (\u21e1 3.5% \u21e5 186122) distance pairs in assignment steps, saving 60% (\u21e1 1 12,162,71736\u21e545\u21e518612 ) distance pairs to calculate in the standard D2clustering.", 
        "154": "In comparison, the clustering approaches based on K-nearest neighbor (KNN) graph with the prefetch-and-prune method of (Kusner et al., 2015) needs substantially more pairs to compute Wasserstein distance, meanwhile the speed-ups also suffer from the curse of dimensionality.", 
        "155": "Their detailed statistics are reported in Table 1.", 
        "156": "Based on the results, our approach is much more practical as a basic document clustering tool.", 
        "157": "4.4 Results  We now summarize our numerical results.", 
        "158": "Regular text datasets.", 
        "159": "The first four datasets in Table 2 cover quite general and broad topics.", 
        "160": "We consider them to be regular and representative datasets encountered more frequently in applications.", 
        "161": "We report the clustering performances of the ten methods in Fig.", 
        "162": "1, where three different metrics are plotted against the clustering homogeneity.", 
        "163": "The higher result at the same level of homogeneity is better, and the ability to achieve higher homogeneity is also welcomed.", 
        "164": "Clearly, D2-clustering is the only method that shows ro-\nbustly superior performances among all ten methods.", 
        "165": "Specifically, it ranks first in three datasets, and second in the other one.", 
        "166": "In comparison, LDA performs competitively on the \u201cReuters\u201d dataset, but is substantially unsuccessful on others.", 
        "167": "Meanwhile, LPP performs competitively on the \u201cWiki events\u201d and \u201cNewsgroups\u201d datasets, but it underperforms on the other two.", 
        "168": "Laplacian, LSI, and Tfidf-N can achieve comparably performance if their reduced dimensions are fine tuned, which\nunfortunately is unrealistic in practice.", 
        "169": "NMF is a simple and effective method which always gives stable, though subpar, performance.", 
        "170": "Short texts vs. long texts.", 
        "171": "D2-clustering performs much more impressively on short texts (\u201cBBC abstract\u201d and \u201cWiki events\u201d) than it does on long texts (\u201cReuters\u201d and \u201cNewsgroups\u201d).", 
        "172": "This outcome is somewhat expected, because the bagof-words method suffers from high sparsity for short texts, and word-embedding based methods in theory should have an edge here.", 
        "173": "As shown in Fig.", 
        "174": "1, D2-clustering has indeed outperformed other non-embedding approaches by a large margin on short texts (improved by about 40% and 20% respectively).", 
        "175": "Nevertheless, we find lifting from word embedding to document clustering is not without a cost.", 
        "176": "Neither AvgDoc nor PV can perform as competitively as D2-clustering performs on both.", 
        "177": "Domain-specific text datasets.", 
        "178": "We are also interested in how word embedding can help group domain-specific texts into clusters.", 
        "179": "In particular, does the semantic knowledge \u201cembedded\u201d in words provides enough clues to discriminate fine-grained concepts?", 
        "180": "We report the best AMI achieved by each method in Table 3.", 
        "181": "Our preliminary result indicates state-of-the-art word embeddings do not provide enough gain here to exceed the performance of existing methodologies.", 
        "182": "On the unchallenging one, the \u201cBBCSport\u201d dataset, basic bag-of-words approaches (Tfidf and Tfidf-N) already suffice to discriminate different sport categories; and on the difficult one, the \u201cOhsumed\u201d dataset, D2-clustering only slightly improves over Tfidf and others, ranking behind\nLPP.", 
        "183": "Meanwhile, we feel the overall quality of clustering \u201cOhsumed\u201d texts is quite far from useful in practice, no matter which method to use.", 
        "184": "More discussions will be provided next.", 
        "185": "4.5 Sensitivity to Word Embeddings.", 
        "186": "We validate the robustness of D2-clustering with different word embedding models, and we also show all their results in Fig.", 
        "187": "2.", 
        "188": "As we mentioned, the effectiveness of Wasserstein document clustering depends on how relevant the utilized word embeddings are with the tasks.", 
        "189": "In those general document clustering tasks, however, word embedding models trained on general corpus perform robustly well with acceptably small variations.", 
        "190": "This outcome reveals our framework as generally effective and not dependent on a specific word embedding model.", 
        "191": "In addition, we also conduct experiments with word embeddings with smaller dimensions, at 50 and 100.", 
        "192": "Their results are not as good as those we have reported (therefore detailed numbers are not included due to space limitation).", 
        "193": "Inadequate embeddings may not be disastrous.", 
        "194": "In addition to our standard running set, we also used D2-clustering with purely random word embeddings, meaning each word vector is independently sampled from spherical Gaussian at 300 dimension, to see how deficient it can be.", 
        "195": "Experimental results show that random word embeddings degrade the performance of D2-clustering, but it still performs much better than purely random clustering, and is even consistently better than LDA.", 
        "196": "Its performances across different datasets is highly correlated with the bag-of-words (Tfidf and Tfidf-N).", 
        "197": "By comparing a pre-trained word embedding model to a randomly generated one, we find that the extra gain is significant (> 10%) in clustering four of the six datasets.", 
        "198": "Their detailed statistics are in Table 4 and Fig.", 
        "199": "3.", 
        "200": "5 Discussions  Performance advantage.", 
        "201": "There has been one immediate observation from these studies, D2clustering always outperforms two of its degenerated cases, namely Tf-idf and AvgDoc, and three other popular methods: LDA, NMF, and PV, on all tasks.", 
        "202": "Therefore, for document clustering, users can expect to gain performance improvements by using our approach.", 
        "203": "Clustering sensitivity.", 
        "204": "From the four 2D plots in Fig.", 
        "205": "1, we notice that the results of Laplacian,\nLSI and Tfidf-N are rather sensitive to their extra hyper-parameters.", 
        "206": "Once the vocabulary\nset, weight scheme and embeddings of words are fixed, our framework involves only two additional hyper-parameters: the number of intended clusters, K, and the selected support size of centroid distributions, m. We have chosen more than one m in all related experiments (m = {64, 100} for long documents, and m = {10, 20} for short documents).", 
        "207": "Our empirical experiments show that the effect of m on different metrics is less\nsensitive than the change of K. Results at different K are plotted for each method (Fig.", 
        "208": "1).", 
        "209": "The gray dots denote results of multiple runs of D2clustering.", 
        "210": "They are always contracted around the top-right region of the whole population, revealing the predictive and robustly supreme performance.", 
        "211": "When bag-of-words suffices.", 
        "212": "Among the results of \u201cBBCSport\u201d dataset, Tfidf-N shows that by restricting the vocabulary set into a smaller one (which may be more relevant to the interest of tasks), it already can achieve highest clustering AMI without any other techniques.", 
        "213": "Other unsupervised regularization over data is likely unnecessary, or even degrades the performance slightly.", 
        "214": "Toward better word embeddings.", 
        "215": "Our experiments on the Ohsumed dataset have been limited.", 
        "216": "The result shows that it could be highly desirable to incorporate certain domain knowledge to derive more effective vector embeddings of words and phrases to encode their domain-specific knowledge, such as jargons that have knowledge dependencies and hierarchies in educational data mining, and signal words that capture multidimensional aspects of emotions in sentiment analysis.", 
        "217": "Finally, we report the best AMIs of all methods on all datasets in Table 3.", 
        "218": "By looking at each method and the average of best AMIs over six datasets, we find our proposed clustering framework often performs competitively and robustly, which is the only method reaching more than 90% of the best AMI on each dataset.", 
        "219": "Furthermore, this observation holds for varying lengths of documents and varying difficulty levels of clustering tasks.", 
        "220": "6 Conclusions and Future Work  This paper introduces a nonparametric clustering framework for document analysis.", 
        "221": "Its computational tractability, robustness and supreme performance, as a fundamental tool, are empirically validated.", 
        "222": "Its ease of use enables data scientists to apply it for the pre-screening purpose of examining word embeddings in a specific task.", 
        "223": "Finally, the gains acquired from word embeddings are quantitatively measured from a nonparametric unsupervised perspective.", 
        "224": "It would also be interesting to investigate several possible extensions to the current clustering work.", 
        "225": "One direction is to learn a proper\nground distance for word embeddings such that the final document clustering performance can be improved with labeled data.", 
        "226": "The work by (Huang et al., 2016; Cuturi and Avis, 2014) have partly touched this goal with an emphasis on document proximities.", 
        "227": "A more appealing direction is to develop problem-driven methods to represent a document as a distributional entity, taking into consideration of phrases, sentence structures, and syntactical characteristics.", 
        "228": "We believe the framework of Wasserstein distance and D2-clustering creates room for further investigation on complex structures and knowledge carried by documents.", 
        "229": "Acknowledgments  This material is based upon work supported by the National Science Foundation under Grant Nos.", 
        "230": "ECCS-1462230, DMS-1521092, and Research Grants Council of Hong Kong under Grant No.", 
        "231": "PolyU 152094/14E.", 
        "232": "The primary computational infrastructures used were supported by the Foundation under Grant Nos.", 
        "233": "ACI-0821527 (CyberStar) and ACI-1053575 (XSEDE)."
    }, 
    "document_id": "P17-1169.pdf.json"
}
