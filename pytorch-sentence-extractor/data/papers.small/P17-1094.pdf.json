{
    "abstract_sentences": {
        "1": "An assential aspect of structured prediction is the evaluation of an output structure against the gold standard.", 
        "2": "Especially in the loss-augmented setting, the need of finding the max-violating constraint has severely limited the expressivity of effective loss functions.", 
        "3": "In this paper, we trade off exact computation for enabling the use of more complex loss functions for coreference resolution (CR).", 
        "4": "Most noteworthily, we show that such functions can be (i) automatically learned also from controversial but commonly accepted CR measures, e.g., MELA, and (ii) successfully used in learning algorithms.", 
        "5": "The accurate model comparison on the standard CoNLL\u20132012 setting shows the benefit of more expressive loss for Arabic and English data."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1018\u20131028 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1094  1 Introduction  In recent years, interesting structured prediction methods have been developed for coreference resolution (CR), e.g., (Fernandes et al., 2014; Bjo\u0308rkelund and Kuhn, 2014; Martschat and Strube, 2015).", 
        "2": "These models are supposed to output clusters but, to better control the exponential nature of the problem, the clusters are converted into tree structures.", 
        "3": "Although this simplifies the problem, optimal solutions are associated with an exponential set of trees, requiring to maximize over such a set.", 
        "4": "This originated latent models (Yu and Joachims, 2009) optimizing the so-called lossaugmented objective functions.", 
        "5": "In this setting, loss functions need to be factorizable together with the feature representations for finding the max-violating constraints.", 
        "6": "The consequence is that only simple loss functions, basically\njust counting incorrect edges, were applied in previous work, giving up expressivity for simplicity.", 
        "7": "This is a critical limitation as domain experts consider more information than just counting edges.", 
        "8": "In this paper, we study the use of more expressive loss functions in the structured prediction framework for CR, although some findings are clearly applicable to more general settings.", 
        "9": "We attempted to optimize the complicated official MELA measure1 (Pradhan et al., 2012) of CR within the learning algorithm.", 
        "10": "Unfortunately, MELA is the average of measures, among which CEAFe has an excessive computational complexity preventing its direct use.", 
        "11": "To solve this problem, we defined a model for learning MELA from data using a fast linear regressor, which can be then effectively used in structured prediction algorithms.", 
        "12": "We defined features to learn such a loss function, e.g., different link counts or aggregations such as Precision and Recall.", 
        "13": "Moreover, we designed methods for generating training data from which our regression loss algorithm (RL) can generalize well and accurately predict MELA values on unseen data.", 
        "14": "Since RL is not factorizable2 over a mention graph, we designed a latent structured perceptron (LSP) that can optimize non-factorizable loss functions on CR graphs.", 
        "15": "We tested LSP using RL and other traditional loss functions using the same setting of the CoNLL\u20132012 Shared Task, thus enabling an exact comparison with previous work.", 
        "16": "The results confirmed that RL can be effectively learned and used in LSP, although the improvement was smaller than expected, considering that our RL provides the algorithm with a more accurate feedback.", 
        "17": "Thus, we analyzed the theory behind this pro-\n1Received most consensus in the NLP community.", 
        "18": "2We have not found yet a possible factorization.", 
        "19": "1018\ncess by also contributing to the definition of the properties of loss optimality.", 
        "20": "These show that the available loss functions, e.g., by Fernandes et al.", 
        "21": "; Yu and Joachims, are enough for optimizing MELA on the training set, at least when the data is separable.", 
        "22": "Thus, in such conditions, we cannot expect a very large improvement from RL.", 
        "23": "To confirm such a conjecture, we tested the models in a more difficult setting, in terms of separability.", 
        "24": "We used different feature sets of a smaller size and found out that in such conditions, RL requires less epochs for converging and produces better results than the other simpler loss functions.", 
        "25": "The accuracy of RL-based model, using 16 times less features, decreases by just 0.3 points, still improving the state of the art in structured prediction.", 
        "26": "Accordingly, in the Arabic setting, where the available features are less discriminative, our approach highly improves the standard LSP.", 
        "27": "2 Related Work  There is a number of works attempting to directly optimize coreference metrics.", 
        "28": "The solution proposed by Zhao and Ng (2010) consists in finding an optimal weighting (by beam search) of training instances, which would maximize the target coreference metric.", 
        "29": "Their models, optimizing MUC and B3, deliver a significant improvement on the MUC and ACE corpora.", 
        "30": "Uryupina et al.", 
        "31": "(2011) benefited from applying genetic algorithms for the selection of features and architecture configuration by multi-objective optimization of MUC and the two CEAF variants.", 
        "32": "Our approach is different in that the evaluation measure (its approximation) is injected directly into the learning algorithm.", 
        "33": "Clark and Manning (2016) optimize B3 directly as well within a mention-ranking model.", 
        "34": "For the efficiency reasons, they omit optimization of CEAF, which we enable in this work.", 
        "35": "SVMcluster \u2013 a structured output approach by Finley and Joachims (2005) \u2013 enables optimization to any clustering loss function (including nondecomposable ones).", 
        "36": "The authors experimentally show that optimizing particular loss functions results into a better classification accuracy in terms of the same functions.", 
        "37": "However, these are in general fast to compute, which is not the MELA case.", 
        "38": "While Finley and Joachims are compelled to perform approximate inference to overcome the intractability of finding an optimal clustering, the latent variable structural approaches \u2013 SVM of Yu and Joachims (2009) and perceptron of Fernan-\ndes et al.", 
        "39": "(2014) \u2013 render exact inference possible by introducing auxiliary graph structures.", 
        "40": "The modeling of Fernandes et al.", 
        "41": "(also referred to as the antecedent tree approach) is exploited in the works of Bjo\u0308rkelund and Kuhn (2014), Martschat and Strube (2015), and Lassalle and Denis (2015).", 
        "42": "Like us, the first couples such approach with approximate inference but for enabling the use of non-local features.", 
        "43": "The current state-of-the-art model of Wiseman et al.", 
        "44": "(2016) also employs a greedy inference procedure as it has global features from an RNN as a non-decomposable term in the inference objective.", 
        "45": "3 Structure Output Learning for CR  We consider online learning algorithms for linking structured input and output patterns.", 
        "46": "More formally, such algorithms find a linear mapping f(x,y) = \u3008w,\u03a6(x,y)\u3009, where f : X \u00d7 Y \u2192 R, w is a linear model, \u03a6(x,y) is a combined feature vector of input variables X and output variables Y .", 
        "47": "The predicted structure is derived with the argmax\ny\u2208Y f(x,y).", 
        "48": "In the next sections, we show\nhow to learn w for CR using structured perceptron.", 
        "49": "Additionally, we provide a characterization of effective loss functions for separable cases.", 
        "50": "3.1 Modeling CR  In this framework, CR is essentially modeled as a clustering problem, where an input-output example is described by a tuple (x,y,h), x is a set of entity mentions contained in a text document, y is set of the corresponding mention clusters, and h is a latent variable, i.e., an auxiliary structure that can represent the clusters of y.", 
        "51": "For example, given the following text:\nAlthough (she)m1 was supported by (President Obama)m2 , (Mrs. Clinton)m3 missed (her)m4 (chance)m5 , (which)m6 looked very good before counting votes.", 
        "52": "the clusters of the entity mentions are represented by the latent tree in Figure 1, where its nodes are\nAlgorithm 1 Latent Structured Perceptron 1: Input: X = {(xi,yi)}ni=1, w0, C, T 2: w\u2190 w0; t\u2190 0 3: repeat 4: for i = 1, ..., n do 5: h\u2217i \u2190 argmax\nh\u2208H(xi,yi) \u3008wt,\u03a6(xi,h)\u3009\n6: h\u0302i \u2190 argmax h\u2208H(xi) \u3008wt,\u03a6(xi,h)\u3009+C\u00d7\u2206(yi,h\u2217i ,h) 7: if \u2206(yi,h\u2217i, h\u0302i) > 0 then 8: wt+1 \u2190 wt + \u03a6(xi,h\u2217i )\u2212 \u03a6(xi, h\u0302i) 9: end if\n10: end for 11: t\u2190 t + 1 12: until t < nT\n13: w\u2190 1 t t\u2211 i=1 wi\nreturn w\nmentions and the subtrees connected to the additional root node form distinct clusters.", 
        "53": "The tree h is called a latent variable as it is consistent with y, i.e., it contains only links between mention nodes that corefer or fall into the same cluster according to y.", 
        "54": "Clearly, an exponential set of trees, H , can be associated with one and the same clustering y.", 
        "55": "Using only one tree to represent a clustering makes the search for optimal mention clusters tractable.", 
        "56": "In particular, structured prediction algorithms select h that maximizes the model learned at time t as shown in the next section.", 
        "57": "3.2 Latent Structured Perceptron (LSP)  The LSP model proposed by Sun et al.", 
        "58": "(2009) and specialized for solving CR tasks by Fernandes et al.", 
        "59": "(2012) is described by Alg.", 
        "60": "1.", 
        "61": "Given a training set {(xi,yi)}ni=1, initial w03, a trade off parameter C, and the maximum number of epochs T , LSP iterates the following operations: Line 5 finds a latent tree h\u2217i that maximizes \u3008wt,\u03a6(xi,h)\u3009 for the current example (xi,yi).", 
        "62": "It basically finds the max ground truth tree with respect to the current wt.", 
        "63": "Finding such max requires an exploration over the tree set H(xi,yi), which only contains arcs between mentions that corefer according to the gold standard clustering yi.", 
        "64": "Line 6 seeks for the max-violating tree h\u0302i in H(xi), which is the set of all candidate trees using any possible combination of arcs.", 
        "65": "Line 7 tests if the produced tree h\u0302i has some mistakes with respect to the gold clustering yi, using loss function \u2206(yi,h \u2217 i , h\u0302i).", 
        "66": "Note that some models define a loss exploiting also the current best latent tree h\u2217i .", 
        "67": "If the test is verified, the model is updated with the vector \u03a6(xi,h\u2217i )\u2212 \u03a6(xi, h\u0302i).", 
        "68": "3Either 0 or a random vector.", 
        "69": "Fernandes et al.", 
        "70": "(2012) used exactly the directed trees we showed as latent structures and applied Edmonds\u2019 spanning tree algorithm (Edmonds, 1967) for finding the max.", 
        "71": "Their model achieved the best results in the CoNLL\u20132012 Shared Task, a challenge for CR systems (Pradhan et al., 2012).", 
        "72": "Their selected loss function also plays an important role as shown in the following.", 
        "73": "3.3 Loss functions  When defining a loss, it is very important to preserve the factorization of the model components along the latent tree edges since this leads to efficient maximization algorithms (see Section 5).", 
        "74": "Fernandes et al.", 
        "75": "uses a loss function that (i) compares a predicted tree h\u0302 against the gold tree h\u2217 and (ii) factorizes over the edges in the way the model does.", 
        "76": "Its equation is:\n\u2206F (h \u2217, h\u0302) =\nM\u2211\ni=1\n1h\u0302(i) 6=h\u2217(i)(1+r \u00b71h\u2217(i)=0), (1)\nwhere h\u2217(i) and h\u0302(i) output the parent of the mention node i in the gold and predicted tree, respectively, whereas 1h\u2217(i) 6=h\u0302(i) just checks if the parents are different, and if yes, penalty of 1 (or 1 + r if the gold parent is the root) is added.", 
        "77": "Yu and Joachims\u2019s loss is based on undirected tree without a root and on the gold clustering y.", 
        "78": "It is computed as:\n\u2206Y J(y, h\u0302) = n(y)\u2212 k(y) + \u2211\ne\u2208h\u0302 l(y, e), (2)\nwhere n(y) is the number of graph nodes, k(y) is the number of clusters in y, and l(y, e) assigns\u22121 to any edge e that connects nodes from the same cluster in y, and r otherwise.", 
        "79": "In our experiments, we adopt both loss functions, however, in contrast to Fernandes et al., we always measure \u2206F against the gold label y and not against the current h\u2217, i.e., in the way it is done by Martschat and Strube (2015), who employ an equivalent LSP model in their work.", 
        "80": "3.4 On optimality of simple loss functions  The above loss functions are rather simple and mainly based on counting the number of mistaken edges.", 
        "81": "Below, we show that such simple loss functions achieve training data separation (if it exists) of a general task measure reaching its max on their 0 mistakes.", 
        "82": "The latter is a desirable characteristic of many measures used in CR and NLP research.", 
        "83": "Proposition 1 (Sufficient condition for optimality of loss functions for learning graphs).", 
        "84": "Let \u2206(y,h\u2217, h\u0302) \u2265 0 be a simple, edge-factorizable loss function, which is also monotone in the number of edge errors, and let \u00b5(y, h\u0302) be any graphbased measure maximized by no edge errors.", 
        "85": "Then, if the training set is linearly separable LSP optimizing \u2206 converges to the \u00b5 optimum.", 
        "86": "Proof.", 
        "87": "If the data is linearly separable the perceptron converges \u21d2 \u2206(yi,h\u2217i, h\u0302i) = 0, \u2200xi.", 
        "88": "The loss is factorizable, i.e.,\n\u2206(yi,h \u2217 i, h\u0302i) =\n\u2211\ne\u2208h\u0302i\nl(yi,h \u2217 i, e), (3)\nwhere l(\u00b7) is an edge loss function.", 
        "89": "Thus,\u2211 e\u2208h\u0302i l(yi,h \u2217 i, e) = 0.", 
        "90": "The latter equation and monotonicity imply l(yi,h\u2217i, e) = 0,\u2200e \u2208 h\u0302i, i.e., there are no edge mistakes, otherwise by fixing such edges, we would have a smaller \u2206, i.e., negative, contradicting the initial positiveness hypothesis.", 
        "91": "Thus, no edge mistake in any xi implies that \u00b5(y, h\u0302) is maximized on the training set.", 
        "92": "Corollary 1.", 
        "93": "\u2206F (h\u2217, h\u0302) and \u2206Y J(y, h\u0302) are both optimal loss functions for graphs.", 
        "94": "Proof.", 
        "95": "Equations 1 and 2 show that both are 0 when applied to a clustering with no mistake on the edges.", 
        "96": "Additionally, for each edge mistake more, both loss functions increase, implying monotonicity.", 
        "97": "Thus, they satisfy all the assumptions of Proposition 1.", 
        "98": "The above characteristic suggests that \u2206F and \u2206Y J can optimize any measure that reasonably targets no mistakes as its best outcome.", 
        "99": "Clearly, this property does not guarantee loss functions to be suitable for a given task measure, e.g., the latter may have different max points and behave rather discontinuously.", 
        "100": "However, a common practice in NLP is to optimize the maximum of a measure, e.g., in case of Precision and Recall, or Accuracy, therefore, loss functions able to at least achieve such an optimum are preferable.", 
        "101": "4 Automatically learning a loss function  How to measure a complex task such as CR has generated a long and controversial discussion in the research community.", 
        "102": "While such a debate is progressing, the most accepted and used measure is the so-called Mention, Entity, and Link Average (MELA) score.", 
        "103": "As it will be clear from the description below, MELA is not easily interpretable\nand not robust to the mention identification effect (Moosavi and Strube, 2016).", 
        "104": "Thus, loss functions showing the optimality property may not be enough to optimize it.", 
        "105": "Our proposal is to use a version of MELA transformed in a loss function optimized by an LSP algorithm with inexact inference.", 
        "106": "However, the computational complexity of the measure prevents to carry out an effective learning.", 
        "107": "Our solution is thus to learn MELA with a fast linear regressor, which also produces a continuos version of the measure.", 
        "108": "4.1 Measures for CR  MELA is the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (CEAF variant with entity-based similarity) (Luo, 2005; Cai and Strube, 2010) scores, having heterogeneous nature.", 
        "109": "MUC is based on the number of correctly predicted links between mentions.", 
        "110": "The number of links required for obtaining the key entity set K is \u2211\nki\u2208K(|ki|\u22121), where ki are key entities in K (cardinality of each entity minus one).", 
        "111": "MUC recall computes what fraction of these were predicted, and the predicted were as many as\n\u2211 ki\u2208K(|ki| \u2212\n|p(ki)|) = \u2211\nki\u2208K(|ki|\u22121\u2212 (|p(ki)|\u22121)), where p(ki) is a partition of the key entity ki formed by intersecting it with the corresponding response entities rj \u2208 R, s.t., ki\u2229 rj 6= \u2205.", 
        "112": "This number equals to the number of the key links minus the number of missing links, required to unite the parts of the partition p(ki) to obtain ki.", 
        "113": "B3 computes Precision and Recall individually for each mention.", 
        "114": "For mention m: Recallm = |kmi \u2229rmj | |kmi | , where kmi and r m j , subscripted with m, denote, correspondingly, the key and response entities into which m falls.", 
        "115": "The over-document Recall is then an average of these taken with respect to the number of the key mentions.", 
        "116": "The MUC and B3 Precision is computed by interchanging the roles of the key and response entities.", 
        "117": "CEAFe computes similarity between key and system entities after finding an optimal alignment between them.", 
        "118": "Using \u03c8(ki, rj) =\n2|ki\u2229rj | |ki|+|rj | as the\nentity similarity measure, it finds an optimal oneto-one map g\u2217 : K \u2192 R, which maps every key entity to a response entity, maximazing an overall similarity \u03a8(g) =\n\u2211 ki\u2208K \u03c8(ki, g(ki)) of the ex-\nample.", 
        "119": "This is solved as a bipartite matching problem by the Kuhn-Munkres algorithm.", 
        "120": "Then Preci-\nAlgorithm 2 Finding a Max-violating Spanning Tree 1: Input: training example (x,y); graph G(x) with ver-\ntices V denoting mentions; set of the incoming candidate edges, E(v), v \u2208 V ; weight vector w\n2: h\u2217 \u2190 \u2205 3: for v \u2208 V do 4: e\u2217 = argmax\ne\u2208E(v) \u3008w, e\u3009+ C \u00d7 l(y, e)\n5: h\u2217 = h\u2217 \u222a e\u2217 6: end for 7: return max-violating tree h\u2217 8: (clustering y\u2217 is induced by the tree h\u2217)\nsion and Recall are \u03a8(g \u2217)\u2211\nrj\u2208R \u03c8(rj ,rj)\nand \u03a8(g \u2217)\u2211\nki\u2208K \u03c8(ki,ki)\n,\nrespectively.", 
        "121": "MELA computation is rather expensive mostly because of CEAFe.", 
        "122": "Its complexity is bounded by O(Ml2 log l) (Luo, 2005), where M and l are, correspondingly, the maximum and minimum number of entities in y and y\u0302.", 
        "123": "Computing CEAFe is especially slow for the candidate outputs y\u0302 with a low quality of prediction, i.e, when l is big, and the coherence with the gold y is scarse.", 
        "124": "Finally, B3 and CEAFe are strongly influenced by the mention identification effect (Moosavi and Strube, 2016).", 
        "125": "Thus, \u2206F and \u2206Y J may output identical values for different clusterings that can have a big gap in terms of MELA.", 
        "126": "4.2 Features for learning measures  As computational reasons prevent to use MELA in LSP (see our inexact search algorithm in Section 5), we study methods for approximating it with a linear regressor.", 
        "127": "For this purpose, we define nine features, which count either exact or simplified versions of Precision, Recall and F1 of each of the three metric-components of MELA.", 
        "128": "Clearly, neither \u2206F nor \u2206Y J provide the same values.", 
        "129": "Apart from the computational complexity, the difficulty of evaluating the quality of the predicted clustering y\u0302 during training is also due to the fact that CR is carried out on automatically detected mentions, while it needs to be compared against a gold standard clustering of a gold mention set.", 
        "130": "However, we can use simple information about automatic mentions and how they relate to gold mentions and gold clusters.", 
        "131": "In particular, we use four numbers: (i) correctly detected automatic mentions, (ii) links they have in the gold standard, (iii) gold mentions, and (iv) gold links.", 
        "132": "The last one enables the precise computation of Precision, Recall and F1-measure values of MUC; the required partitions p(ki) of key entities are also available at\ntraining time as they contain only automatic mentions.", 
        "133": "These are the first three features that we design.", 
        "134": "Likewise for B3, the feature values can be derived using (ii) and (iii).", 
        "135": "For computing CEAFe heuristics, we do not perform cluster alignment to find an optimal \u03a8(g\u2217).", 
        "136": "Instead of \u03a8(g\u2217), which can be rewritten as \u2211 m\u2208K\u2229R\n2 |kmi |+|g\u2217(kmi )| if summing up over\nthe mentions not the entities, we simply use \u03a8\u0303 =\u2211 m\u2208K\u2229R\n2 |kmi |+|rmj | , pretending that for each m its key kmi and response r m j entities are aligned.\u2211\nrj\u2208R \u03c8(rj , rj) and \u2211\nki\u2208K \u03c8(ki, ki) in the denominators of the Precision and Recall are the number of predicted and gold clusters, correspondingly.", 
        "137": "The imprecision of the CEAFe related features is expected to be leveraged when put together with the exact B3 and MUC values into the regression learning using the exact MELA values (implicitly exact CEAFe values as well).", 
        "138": "4.3 Generating training and test data  The features described above can be used to characterize the clustering variables y\u0302.", 
        "139": "For generating training data, we collected all the maxviolating y\u0302 produced during LSPF (using \u2206F ) learning and associate them with their correct MELA scores from the scorer.", 
        "140": "This way, we can have both training and test data for our regressor.", 
        "141": "In our experiments, for the generation purpose, we decided to run LSPF on each document separately to obtain more variability in y\u0302\u2019s.", 
        "142": "We use a simple linear SVM to learn a model w\u03c1.", 
        "143": "Considering that MELA(y, y\u0302) score lies in the interval [100, 0], a simple approximation of the loss could be:\n\u2206\u03c1(y, y\u0302) = 100\u2212w\u03c1 \u00b7 \u03c6(y, y\u0302).", 
        "144": "(4) Below, we show its improved version and an LSP for learning with it based on inexact search.", 
        "145": "5 Learning with learned loss functions  Our experiments will demonstrate that \u2206\u03c1 can be accurately learned from data.", 
        "146": "However, the features we used for this are not factorizable over the edges of the latent trees.", 
        "147": "Thus, we design a new LSP algorithm that can use our learned loss in an approximated max search.", 
        "148": "5.1 A general inexact algorithm for CR  If the loss function can be factorized over tree edges (see Equation 3) the max-violating constraint in Line 6 of Alg.", 
        "149": "1 can be efficiently found by exact decoding, e.g., using Edmonds\u2019 algorithm as in Fernandes et al.", 
        "150": "(2014) or Kruskal\u2019s as\nAlgorithm 3 Inexact Inference of a Max-violating Spanning Tree with a Global Loss 1: Input: training example (x,y); graph G(x) with ver-\ntices V denoting mentions; set of the incoming candidate edges, E(v), v \u2208 V ; w, ground truth tree h\u2217\n2: h\u0302\u2190 \u2205 3: score\u2190 0 4: repeat 5: prev score = score 6: score = 0 7: for v \u2208 V do 8: h = h\u0302 \\ e(v) 9: e\u0302 = argmax\ne\u2208E(v) \u3008w, e\u3009+ C \u00d7\u2206(y,h\u2217,h \u222a e)\n10: h\u0302 = h \u222a e\u0302 11: score = score + \u3008w, e\u0302\u3009 12: end for 13: score = score + \u2206(y,h\u2217, h\u0302) 14: until score = prev score 15: return max-violating tree h\u0302\nin Yu and Joachims (2009).", 
        "151": "The candidate graph, by construction, does not contain cycles, and the inference by Edmonds\u2019 algorithm does technically the same as the \u201dbest-left-link\u201d inference algorithm by Chang et al.", 
        "152": "(2012).", 
        "153": "This can be schematically represented in Alg.", 
        "154": "2.", 
        "155": "When we deal with \u2206\u03c1, Alg.", 
        "156": "2 cannot be longer applied as our new loss function is nonfactorizable.", 
        "157": "Thus, we designed a greedy solution, Alg.", 
        "158": "3, which still uses the spanning tree algorithm, though, it is not guaranteed to deliver the max-violating constraint.", 
        "159": "However, finding even a suboptimal solution optimizing a more accurate loss function may achieve better performance both in terms of speed and accuracy.", 
        "160": "We reformulate Step 4 of Alg.", 
        "161": "2, where a maxviolating incoming edge e\u0302 is identified for a vertex v. The new max-violating inference objective contains now a global loss measured on the partial structure h\u0302 built up to now plus a candidate edge e for a vertex v in consideration (Line 10 of Alg.", 
        "162": "3).", 
        "163": "On a high level, this resembles the inference procedure of Wiseman et al.", 
        "164": "(2016), who use it for optimizing global features coming from an RNN.", 
        "165": "Differently though, after processing all the vertices, we repeat the procedure until the score of h\u0302 no longer improves.", 
        "166": "Note that Bjo\u0308rkelund and Kuhn (2014) perform inexact search on the same latent tree structures to extend the model to non-local features.", 
        "167": "In contrast to our approach, they use beam search and accumulate the early updates.", 
        "168": "In addition to the design of an algorithm enabling the use of our \u2206\u03c1, there are other intricacies\ncaused by the lack of factorization that need to be taken into account (see the next section).", 
        "169": "5.2 Approaching factorization properties  The \u2206\u03c1 defined by Equation 4 approximately falls into the interval [0, 100].", 
        "170": "However, the simple optimal loss functions, \u2206F and \u2206Y J , output a value dependent on the size of the input training document in terms of edges (as they factorize in terms of edges).", 
        "171": "Since this property cannot be learned from MELA by our regression algorithm, we calibrate our loss with respect to the number of correctly predicted mentions, c, in that document, obtaining \u2206\u2032\u03c1 = c 100\u2206\u03c1.", 
        "172": "Finally, another important issue is connected to the fact that on the way as we incrementally construct a max-violating tree according to Alg.", 
        "173": "3, \u2206\u03c1 decreases (and MELA grows), as we add more mentions to the output, traversing the tree nodes v. Thus, to equalize the contribution of the loss among the candidate edges of different nodes, we also scale the loss of the candidate edges of the node v having order i in the document, according to the formula \u2206\u2032\u2032\u03c1 = i |V |\u2206 \u2032 \u03c1.", 
        "174": "This can be interpreted as giving more weight to the hard-toclassify instances \u2013 an important issue alleviated by Zhao and Ng (2010).", 
        "175": "Towards the end of the document, the probability of correctly predicting an incoming edge for a node generally decreases, as increases the number of hypotheses.", 
        "176": "6 Experiments  In our experiments, we first show that our regressor for learning MELA approximates it rather accurately.", 
        "177": "Then, we examine the impact of our \u2206\u03c1 on state-of-the-art systems in comparison with other loss functions.", 
        "178": "Finally, we show that the impact of our model is amplified when learning in smaller feature spaces.", 
        "179": "6.1 Setup  Data We conducted our experiments on English and Arabic parts of the corpus from CoNLL 2012-Shared Task4.", 
        "180": "The English data contains 2,802, 343, and 348 documents in the training,\n4conll.cemantix.org/2012/data.html\ndev.", 
        "181": "and test parts, respectively.", 
        "182": "The Arabic data includes 359, 44, and 44 documents for training, dev.", 
        "183": "and test sets, respectively.", 
        "184": "Models We implement our version of LSP, where LSPF , LSPY J , and LSP\u03c1 use the loss functions, \u2206F , \u2206Y J , and \u2206\u03c1, defined in Section 3.3 and 5.2, respectively.", 
        "185": "We used cort5 \u2013 coreference toolkit by Martschat and Strube (2015) both to preprocess the English data and to extract candidate mentions and features (the basic set).", 
        "186": "For Arabic, we used mentions and features from BART6 (Uryupina et al., 2012).", 
        "187": "We extended the initial feature set for Arabic with the feature combinations proposed by Durrett and Klein (2013), those permitted by the available initial features.", 
        "188": "Parametrization All the perceptron models require tuning of a regularization parameter C. LSPF and LSPY J \u2013 also tuning of a specific loss parameter r. We select the parameters on the entire dev.", 
        "189": "set by training on 100 random documents from the training set.", 
        "190": "We pick up C \u2208 {1.0, 100.0, 1000.0, 2000.0}, the r values for LSPF from the interval [0.5, 2.5] with step 0.5, and the r values for LSPY J \u2013 from {0.05, 0.1, 0.5}.", 
        "191": "Ultimately, for English, we used C = 1000.0 in all the models; r = 1.0 in LSPF and r = 0.1 in LSPY J .", 
        "192": "And wider ranges of parameter values were considered for Arabic, due to the lower mention detection rate: C = 1000.0, r = 6.0 for LSPF , C = 1000.0, r = 0.01 for LSPY J , and C = 5000.0 \u2013 for LSP\u03c1.", 
        "193": "A standard previous work setting for the number of epochs T of LSP is 5 (Martschat and Strube, 2015).", 
        "194": "Fernandes et al.", 
        "195": "(2014) noted that T = 50 was sufficient for convergence.", 
        "196": "We selected the best T from 1 to 50 on the dev.", 
        "197": "set.", 
        "198": "Evaluation measure We used MUC, B3, CEAFe and their average MELA for evaluation, computed by the version 8 of the official CoNLL scorer.", 
        "199": "5http://smartschat.de/software 6http://www.bart-coref.org/  6.2 Learning loss functions  For learning MELA, we generated training and test examples from LSPF according to the procedure described in Section 4.3.", 
        "200": "In the first experiment, we trained the w\u03c1 model on a set of examples S1, generated from a sample of 100 English documents and tested on a set of examples S2, generated from another sample of the same size, and vice versa.", 
        "201": "The results in Table 1 show that with just 5, 000/6, 000, the Mean Squared Error (MSE) is roughly between \u223c 2.4 \u2212 2.7: these are rather small numbers considering that the regression output values in the interval [0, 100].", 
        "202": "Squared Correlation Coefficient (SCC) reaches a correlation of about 99.7%, demonstrating that our regression approach is effective in estimating MELA.", 
        "203": "Additionally, Figure 2 shows the regression learning curves evaluated with MSE and SCC.", 
        "204": "The former rapidly decreases and, with about 1, 000 examples, reaches a plateau of around 2.3.", 
        "205": "The latter shows a similar behaviour, approaching a correlation of about 99.8% with real MELA.", 
        "206": "6.3 State of the art and model comparison  We first experimented with the standard CoNLL setting to compare the LSP accuracy in terms of MELA using the three different loss functions, i.e., LSPF , LSPY J and LSP\u03c1.", 
        "207": "In particular, we used all the documents of the training set and all N \u223c 16.8M features from cort, and tested on the both dev.", 
        "208": "and test sets.", 
        "209": "The results are reported in Columns All of Table 2.", 
        "210": "We note first that our \u2206\u03c1 is effective as it stays on a par with \u2206F and \u2206Y J on the dev.", 
        "211": "set.", 
        "212": "This is interesting as Corollary 1 shows that such functions can optimize MELA, the reported values refer to the optimal epoch numbers.", 
        "213": "Also, LSP\u03c1 improves the other models on the test set by 0.3 percent points (statistical significant at the 93% level of confidence).", 
        "214": "Secondly, all the three models improve the state of the art on CR using LSP, i.e., by Martschat and Strube (2015) using antecedent trees (M&S AT) or mention ranking (M&S MR), Bjo\u0308rkelund and Kuhn (2014) using a global feature model (B&K) and Fernandes et al.", 
        "215": "(2014) (Fer).", 
        "216": "Noted that all the LSP models were trained on the training set only, without retraining on the training and dev.", 
        "217": "sets together, thus our scores can be improved.", 
        "218": "Thirdly, Table 3 shows the breakdown of the MELA results in terms of its components on the test set.", 
        "219": "Interestingly, LSP\u03c1 is noticeably better in terms of B3 and CEAFe, while LSP with simple losses, as expected, deliver higher MUC score.", 
        "220": "Finally, the overall improvement of \u2206\u03c1 is not impressive.", 
        "221": "This mainly depends on the optimality of the competing loss functions, which in a setting of \u223c 16.8M features, satisfy the separability condition of Proposition 1.", 
        "222": "6.4 Learning in more challenging conditions  In these experiments, we verify the hypothesis that when the optimality property is partially or\ntotally missing \u2206\u03c1 is more visibly superior to \u2206F and \u2206Y J .", 
        "223": "As we do not want to degrade their effectiveness, the only condition dependent on the setting is the data inseparability or at least harder to be separated.", 
        "224": "These conditions can be obtained by reducing the size of the feature space.", 
        "225": "However, since we aim at testing conditions, where \u2206\u03c1 is practically useful, we filter out less important features, preserving the model accuracy (at least when the selection is not extremely harsh).", 
        "226": "For this purpose, we use a feature selection approach using a basic binary classifier trained to discriminate between correct and incorrect mention pairs.", 
        "227": "It is typically used in non structured CR methods and has a nice property of using the same features of LSP (we do not use global features in our study).", 
        "228": "We carried out a selection using the absolute values of the model weights of the classifier for ranking features and then selecting those having higher rank (Haponchyk and Moschitti, 2017).", 
        "229": "The MELA produced by our models using all the training data is presented in Figure 3.", 
        "230": "The first 7 plots show learning curves in terms of LSP epochs for different feature sets with increasing size N , evaluated on the dev.", 
        "231": "set.", 
        "232": "We note that: firstly, the fewer features are available, the better LSP\u03c1 curves are than those of LSPF and LSPY J in terms of accuracy and convergence speed.", 
        "233": "The intuition is that finding a separation of the training set (generalizing well) becomes more challenging (e.g., with 10k features, the data is not linearly sep-\narable) thus a loss function which is closer to the real measure provides some advantages.", 
        "234": "Secondly, when using all features, LSP\u03c1 is still overall better than the other models but clearly the latter can achieve the same MELA on the dev.", 
        "235": "set.", 
        "236": "Thirdly, the last plot shows the MELA produced by LSP models on the test set, when trained with the best epoch derived from the dev.", 
        "237": "set (previous plots).", 
        "238": "We observe that LSP\u03c1 is constantly better than the other models, though decreasing its effect as the feature number increases.", 
        "239": "Next, in Column 1 (Selected) of Table 2, we report the model MELA using 1 million features.", 
        "240": "We note that LSP\u03c1 improves the other models by at least 0.6 percent points, achieving the same accuracy as the best of its competitors, i.e., LSPF , using all the features.", 
        "241": "Finally, \u2206\u03c1 does not satisfy Proposition 1, therefore, generally, we do not know if it can optimize any \u00b5-type measure over graphs.", 
        "242": "However, being learned to optimize MELA, it clearly separates data maximizing such a measure.", 
        "243": "We empirically verified this by checking the MELA score obtained on the training set: we found that LSP\u03c1 always optimizes MELA, iterating for fewer epochs than the other loss functions.", 
        "244": "6.5 Generalization to other languages  Here, we test the effectiveness of the proposed method on Arabic using all available data and features.", 
        "245": "The results in Table 4 reveal an indisputable superiority of LSP\u03c1 over the counterparts optimizing simple loss functions.", 
        "246": "They support the results of the previous section as we had to deal with the insufficiency of the expert-based features for Arabic.", 
        "247": "In such an uneasy case, LSP\u03c1 was able to improve over LSPF by more than 4.7 points.", 
        "248": "We also tested the loss model w\u03c1 trained for the experiments on the English data (resp.", 
        "249": "setting All of Section 6.3) in LSP\u03c1 on Arabic.", 
        "250": "This corresponds to LSPEN\u03c1 model.", 
        "251": "Notably, it performs even better, 1.5 points more, than LSP\u03c1 using a loss learned from Arabic examples.", 
        "252": "This suggests a nice property of data invariance of \u2206\u03c1.", 
        "253": "The improvement delivered by the \u201dEnglish\u201d w\u03c1 is due to the fact that it was trained on the data which is richer: (i) quantitatively, since coming from almost 8 times more training documents in comparison to Arabic and (ii) qualitatively, in a sense of diversity with respect to the RL target value.", 
        "254": "Indeed, the Arabic data is much less separable than\nthe English data and this prevents to have examples where MELA values are higher.", 
        "255": "7 Conclusions  In this paper, we studied the use of complex loss functions in structured prediction for CR.", 
        "256": "Given the scale of our investigation, we limited our study to LSP, which is anyway considered state of the art.", 
        "257": "We derived several findings: (i) for the first time, up to our knowledge, we showed that a complex measure, such as MELA, can be learned by a linear regressor (RL) with high accuracy and effective generalization.", 
        "258": "(ii) The latter was essential for designing a new LSP based on inexact search and RL.", 
        "259": "(iii) We showed that an automatically learned loss can be optimized and provides stateof-the-art performance in a real setting, including thousands of documents and millions of features, such as CoNLL\u20132012 Shared Task.", 
        "260": "(iv) We defined a property of optimal loss functions for CR, which shows that in separable cases, such losses are enough to get the state of the art.", 
        "261": "However, as soon as separability becomes more complex simple loss functions lose optimality and RL becomes more accurate and faster.", 
        "262": "(v) Our MELA approximation provides a loss that is data invariant which, once learned, can be optimized in LSP on different datasets and in different languages.", 
        "263": "Our study opens several future directions, ranging from defining algorithms based on automatically learned loss functions to learning more effective measures from expert examples.", 
        "264": "Acknowledgements  We would like to thank Olga Uryupina for providing us with the preprocessed data from BART for Arabic.", 
        "265": "This work has been supported by the EC project CogNet, 671625 (H2020-ICT-2014-2, Research and Innovation action).", 
        "266": "Many thanks to the anonymous reviewers for their valuable suggestions."
    }, 
    "document_id": "P17-1094.pdf.json"
}
