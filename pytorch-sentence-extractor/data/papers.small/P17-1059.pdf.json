{
    "abstract_sentences": {
        "1": "Human verbal communication includes affective messages which are conveyed through use of emotionally colored words.", 
        "2": "There has been a lot of research in this direction but the problem of integrating state-of-the-art neural language models with affective information remains an area ripe for exploration.", 
        "3": "In this paper, we propose an extension to an LSTM (Long Short-Term Memory) language model for generating conversational text, conditioned on affect categories.", 
        "4": "Our proposed model, Affect-LM enables us to customize the degree of emotional content in generated sentences through an additional design parameter.", 
        "5": "Perception studies conducted using Amazon Mechanical Turk show that AffectLM generates naturally looking emotional sentences without sacrificing grammatical correctness.", 
        "6": "Affect-LM also learns affectdiscriminative word representations, and perplexity experiments show that additional affective information in conversational text can improve language model prediction."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 634\u2013642 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1059  1 Introduction  Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emotion (Scherer et al., 2010).", 
        "2": "Picard (1997) provides a detailed discussion of the importance of affect analysis in human communication and interaction.", 
        "3": "Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include sentiment analysis from Twitter (Nakov et al., 2016), affect analysis from poetry (Kao and Jurafsky,\n2012) and studies of correlation between function words and social/psychological processes (Pennebaker, 2011).", 
        "4": "People exchange verbal messages which not only contain syntactic information, but also information conveying their mental and emotional states.", 
        "5": "Examples include the use of emotionally colored words (such as furious and joy) and swear words.", 
        "6": "The automated processing of affect in human verbal communication is of great importance to understanding spoken language systems, particularly for emerging applications such as dialogue systems and conversational agents.", 
        "7": "Statistical language modeling is an integral component of speech recognition systems, with other applications such as machine translation and information retrieval.", 
        "8": "There has been a resurgence of research effort in recurrent neural networks for language modeling (Mikolov et al., 2010), which have yielded performances far superior to baseline language models based on n-gram approaches.", 
        "9": "However, there has not been much effort in building neural language models of text that leverage affective information.", 
        "10": "Current literature on deep learning for language understanding focuses mainly on representations based on\n634\nword semantics (Mikolov et al., 2013), encoderdecoder models for sentence representations (Cho et al., 2015), language modeling integrated with symbolic knowledge (Ahn et al., 2016) and neural caption generation (Vinyals et al., 2015), but to the best of our knowledge there has been no work on augmenting neural language modeling with affective information, or on data-driven approaches to generate emotional text.", 
        "11": "Motivated by these advances in neural language modeling and affective analysis of text, in this paper we propose a model for representation and generation of emotional text, which we call the Affect-LM.", 
        "12": "Our model is trained on conversational speech corpora, common in language modeling for speech recognition applications (Bulyko et al., 2007).", 
        "13": "Figure 1 provides an overview of our Affect-LM and its ability to generate emotionally colored conversational text in a number of affect categories with varying affect strengths.", 
        "14": "While these parameters can be manually tuned to generate conversational text, the affect category can also be automatically inferred from preceding context words.", 
        "15": "Specifically for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool (Pennebaker et al., 2001).", 
        "16": "Our primary research questions in this paper are: Q1:Can Affect-LM be used to generate affective sentences for a target emotion with varying degrees of affect strength through a customizable model parameter?", 
        "17": "Q2:Are these generated sentences rated as emotionally expressive as well as grammatically correct in an extensive crowd-sourced perception experiment?", 
        "18": "Q3:Does the automatic inference of affect category from the context words improve language modeling performance of the proposed Affect-LM over the baseline as measured by perplexity?", 
        "19": "The remainder of this paper is organized as follows.", 
        "20": "In Section 2, we discuss prior work in the fields of neural language modeling, and generation of affective conversational text.", 
        "21": "In Section 3 we describe the baseline LSTM model and our proposed Affect-LM model.", 
        "22": "Section 4 details the experimental setup, and in Section 5, we discuss results for customizable emotional text generation, perception studies for each affect category, and perplexity improvements over the baseline model\nbefore concluding the paper in Section 6.", 
        "23": "2 Related Work  Language modeling is an integral component of spoken language systems, and traditionally ngram approaches have been used (Stolcke et al., 2002) with the shortcoming that they are unable to generalize to word sequences which are not in the training set, but are encountered in unseen data.", 
        "24": "Bengio et al.", 
        "25": "(2003) proposed neural language models, which address this shortcoming by generalizing through word representations.", 
        "26": "Mikolov et al.", 
        "27": "(2010) and Sundermeyer et al.", 
        "28": "(2012) extend neural language models to a recurrent architecture, where a target word wt is predicted from a context of all preceding words w1, w2, ..., wt\u22121 with an LSTM (Long Short-Term Memory) neural network.", 
        "29": "There also has been recent effort on building language models conditioned on other modalities or attributes of the data.", 
        "30": "For example, Vinyals et al.", 
        "31": "(2015) introduced the neural image caption generator, where representations learnt from an input image by a CNN (Convolutional Neural Network) are fed to an LSTM language model to generate image captions.", 
        "32": "Kiros et al.", 
        "33": "(2014) used an LBL model (Log-Bilinear language model) for two applications - image retrieval given sentence queries, and image captioning.", 
        "34": "Lower perplexity was achieved on text conditioned on images rather than language models trained only on text.", 
        "35": "In contrast, previous literature on affective language generation has not focused sufficiently on customizable state-of-the-art neural network techniques to generate emotional text, nor have they quantitatively evaluated their models on multiple emotionally colored corpora.", 
        "36": "Mahamood and Reiter (2011) use several NLG (natural language generation) strategies for producing affective medical reports for parents of neonatal infants undergoing healthcare.", 
        "37": "While they study the difference between affective and non-affective reports, their work is limited only to heuristic based systems and do not include conversational text.", 
        "38": "Mairesse and Walker (2007) developed PERSONAGE, a system for dialogue generation conditioned on extraversion dimensions.", 
        "39": "They trained regression models on ground truth judge\u2019s selections to automatically determine which of the sentences selected by their model exhibit appropriate extroversion attributes.", 
        "40": "In Keshtkar and Inkpen (2011), the authors use heuristics and rule-based approaches\nfor emotional sentence generation.", 
        "41": "Their generation system is not training on large corpora and they use additional syntactic knowledge of parts of speech to create simple affective sentences.", 
        "42": "In contrast, our proposed approach builds on state-ofthe-art approaches for neural language modeling, utilizes no syntactic prior knowledge, and generates expressive emotional text.", 
        "43": "3 Model    3.1 LSTM Language Model  Prior to providing a formulation for our proposed model, we briefly describe a LSTM language model.", 
        "44": "We have chosen this model as a baseline since it has been reported to achieve state-of-the-art perplexities compared to other approaches, such as n-gram models with Kneser-Ney smoothing (Jozefowicz et al., 2016).", 
        "45": "Unlike an ordinary recurrent neural network, an LSTM network does not suffer from the vanishing gradient problem which is more pronounced for very long sequences (Hochreiter and Schmidhuber, 1997).", 
        "46": "Formally, by the chain rule of probability, for a sequence of M words w1, w2, ..., wM , the joint probability of all words is given by:\nP (w1, w2, ..., wM ) = t=M\u220f\nt=1\nP (wt|w1, w2, ...., wt\u22121)\n(1) If the vocabulary consists of V words, the conditional probability of word wt as a function of its context ct\u22121 = (w1, w2, ...., wt\u22121) is given by:\nP (wt = i|ct\u22121) = exp(Ui T f(ct\u22121) + bi)\u2211V j=1 exp(Uj\nT f(ct\u22121) + bj) (2)\nf(.)", 
        "47": "is the output of an LSTM network which takes in the context words w1, w2, ..., wt\u22121 as inputs through one-hot representations, U is a matrix of word representations which on visualization we have found to correspond to POS (Part of Speech) information, while bi is a bias term capturing the unigram occurrence of word i.", 
        "48": "Equation 2 expresses the word wt as a function of its context for a LSTM language model which does not utilize any additional affective information.", 
        "49": "3.2 Proposed Model: Affect-LM\nThe proposed model Affect-LM has an additional energy term in the word prediction, and can be de-\nscribed by the following equation:\nP (wt = i|ct\u22121, et\u22121) = exp (Ui\nT f(ct\u22121) + \u03b2ViTg(et\u22121) + bi)\u2211V j=1 exp(Uj\nT f(ct\u22121) + \u03b2VjTg(et\u22121) + bj) (3)\net\u22121 is an input vector which consists of affect category information obtained from the words in the context during training, and g(.)", 
        "50": "is the output of a network operating on et\u22121.Vi is an embedding learnt by the model for the i-th word in the vocabulary and is expected to be discriminative of the affective information conveyed by each word.", 
        "51": "In Figure 4 we present a visualization of these affective representations.", 
        "52": "The parameter \u03b2 defined in Equation 3, which we call the affect strength defines the influence of the affect category information (frequency of emotionally colored words) on the overall prediction of the target word wt given its context.", 
        "53": "We can consider the formulation as an energy based model (EBM), where the additional energy term captures the degree of correlation between the predicted word and the affective input (Bengio et al., 2003).", 
        "54": "3.3 Descriptors for Affect Category Information  Our proposed model learns a generative model of the next word wt conditioned not only on the previous words w1, w2, ..., wt\u22121 but also on the affect category et\u22121 which is additional information about emotional content.", 
        "55": "During model training, the affect category is inferred from the context data itself.", 
        "56": "Thus we define a suitable feature extractor which can utilize an affective lexicon to infer emotion in the context.", 
        "57": "For our experiments, we have utilized the Linguistic Inquiry and Word Count (LIWC) text analysis program for feature extraction through keyword spotting.", 
        "58": "Introduced by Pennebaker et al.", 
        "59": "(2001), LIWC is based on a dictionary, where each word is assigned to a predefined LIWC category.", 
        "60": "The categories are chosen based on their association with social, affective, and cognitive processes.", 
        "61": "For example, the dictionary word worry is assigned to LIWC category anxiety.", 
        "62": "In our work, we have utilized all word categories of LIWC corresponding to affective processes: positive emotion, angry, sad, anxious, and negative emotion.", 
        "63": "Thus the descriptor et\u22121 has five features with each feature denoting\npresence or absence of a specific emotion, which is obtained by binary thresholding of the features extracted from LIWC.", 
        "64": "For example, the affective representation of the sentence i will fight in the war is et\u22121 ={\u201csad\u201d:0, \u201cangry\u201d:1, \u201canxiety\u201d:0, \u201cnegative emotion\u201d:1, \u201cpositive emotion\u201d:0}.", 
        "65": "3.4 Affect-LM for Emotional Text Generation\nAffect-LM can be used to generate sentences conditioned on the input affect category, the affect strength \u03b2, and the context words.", 
        "66": "For our experiments, we have chosen the following affect categories - positive emotion, anger, sad, anxiety, and negative emotion (which is a superclass of anger, sad and anxiety).", 
        "67": "As described in Section 3.2, the affect strength \u03b2 defines the degree of dominance of the affect-dependent energy term on the word prediction in the language model, consequently after model training we can change \u03b2 to control the degree of how \u201cemotionally colored\u201d a generated utterance is, varying from \u03b2 = 0 (neutral; baseline model) to \u03b2 = \u221e (the generated sentences only consist of emotionally colored words, with no grammatical structure).", 
        "68": "When Affect-LM is used for generation, the affect categories could be either (1) inferred from the context using LIWC (this occurs when we provide sentence beginnings which are emotionally colored themselves), or (2) set to an input emotion descriptor e (this is obtained by setting e to a binary vector encoding the desired emotion and works even for neutral sentence beginnings).", 
        "69": "Given an initial starting set of M words w1, w2, ..., wM to complete, affect strength \u03b2, and the number of words N to generate each ith generated word is obtained by sampling from P (wi|w1, w2, ..., wi\u22121, e;\u03b2) for i \u2208 {M+1,M+ 2, ...,M +N}.", 
        "70": "4 Experimental Setup  In Section 1, we have introduced three primary research questions related to the ability of the proposed Affect-LM model to generate emotionally colored conversational text without sacrific-\ning grammatical correctness, and to obtain lower perplexity than a baseline LSTM language model when evaluated on emotionally colored corpora.", 
        "71": "In this section, we discuss our experimental setup to address these questions, with a description of Affect-LM\u2019s architecture and the corpora used for training and evaluating the language models.", 
        "72": "4.1 Speech Corpora  The Fisher English Training Speech Corpus is the main corpus used for training the proposed model, in addition to which we have chosen three emotionally colored conversational corpora.", 
        "73": "A brief description of each corpus is given below, and in Table 1, we report relevant statistics, such as the total number of words, along with the fraction of emotionally colored words (those belonging to the LIWC affective word categories) in each corpus.", 
        "74": "Fisher English Training Speech Parts 1 & 2: The Fisher dataset (Cieri et al., 2004) consists of speech from telephonic conversations of 10 minutes each, along with their associated transcripts.", 
        "75": "Each conversation is between two strangers who are requested to speak on a randomly selected topic from a set.", 
        "76": "Examples of conversation topics are Minimum Wage, Time Travel and Comedy.", 
        "77": "Distress Assessment Interview Corpus (DAIC): The DAIC corpus introduced by Gratch (2014) consists of 70+ hours of dyadic interviews between a human subject and a virtual human, where the virtual human asks questions designed to diagnose symptoms of psychological distress in the subject such as depression or PTSD (Post Traumatic Stress Disorder).", 
        "78": "SEMAINE dataset: SEMAINE (McKeown et al., 2012) is a large audiovisual corpus consisting of interactions between subjects and an operator simulating a SAL (Sensitive Artificial Listener).", 
        "79": "There are a total of 959 conversations which are approximately 5 minutes each, and are transcribed and annotated with affective dimensions.", 
        "80": "Multimodal Opinion-level Sentiment Intensity Dataset (CMU-MOSI): (Zadeh et al., 2016) This is a multimodal annotated corpus of opinion\nvideos where in each video a speaker expresses his opinion on a commercial product.", 
        "81": "The corpus consist of speech from 93 videos from 89 distinct speakers (41 male and 48 female speakers).", 
        "82": "This corpus differs from the others since it contains monologues rather than conversations.", 
        "83": "While we find that all corpora contain spoken language, they have the following characteristics different from the Fisher corpus: (1) More emotional content as observed in Table 1, since they have been generated through a human subject\u2019s spontaneous replies to questions designed to generate an emotional response, or from conversations on emotion-inducing topics (2) Domain mismatch due to recording environment (for example, the DAIC corpus was created in a mental health setting, while the CMU-MOSI corpus consisted of opinion videos uploaded online).", 
        "84": "(3) Significantly smaller than the Fisher corpus, which is 25 times the size of the other corpora combined.", 
        "85": "Thus, we perform training in two separate stages - training of the baseline and Affect-LM models on the Fisher corpus, and subsequent adaptation and fine-tuning on each of the emotionally colored corpora.", 
        "86": "4.2 Affect-LM Neural Architecture For our experiments, we have implemented a baseline LSTM language model in Tensorflow (Abadi et al., 2016), which follows the non-regularized implementation as described in Zaremba et al.", 
        "87": "(2014) and to which we have added a separate energy term for the affect category in implementing Affect-LM.", 
        "88": "We have used a vocabulary of 10000 words and an LSTM network with 2 hidden layers and 200 neurons per hidden layer.", 
        "89": "The network is unrolled for 20 time steps, and the size of each minibatch is 20.", 
        "90": "The affect category et\u22121 is processed by a multi-layer perceptron with a single hidden layer of 100 neurons and sigmoid activation function to yield g(et\u22121).", 
        "91": "We have set the output layer size to 200 for both f(ct\u22121) and g(et\u22121).", 
        "92": "We have kept the network architecture constant throughout for ease of comparison between the baseline and Affect-LM.", 
        "93": "4.3 Language Modeling Experiments  Affect-LM can also be used as a language model where the next predicted word is estimated from the words in the context, along with an affect category extracted from the context words themselves (instead of being encoded externally as in generation).", 
        "94": "To evaluate whether additional emotional\ninformation could improve the prediction performance, we train the corpora detailed in Section 4.1 in two stages as described below: (1) Training and validation of the language models on Fisher dataset- The Fisher corpus is split in a 75:15:10 ratio corresponding to the training, validation and evaluation subsets respectively, and following the implementation in Zaremba et al.", 
        "95": "(2014), we train the language models (both the baseline and Affect-LM) on the training split for 13 epochs, with a learning rate of 1.0 for the first four epochs, and the rate decreasing by a factor of 2 after every subsequent epoch.", 
        "96": "The learning rate and neural architecture are the same for all models.", 
        "97": "We validate the model over the affect strength \u03b2 \u2208 [1.0, 1.5, 1.75, 2.0, 2.25, 2.5, 3.0].", 
        "98": "The best performing model on the Fisher validation set is chosen and used as a seed for subsequent adaptation on the emotionally colored corpora.", 
        "99": "(2) Fine-tuning the seed model on other corpora- Each of the three corpora - CMU-MOSI, DAIC and SEMAINE are split in a 75:15:10 ratio to create individual training, validation and evaluation subsets.", 
        "100": "For both the baseline and AffectLM, the best performing model from Stage 1 (the seed model) is fine-tuned on each of the training corpora, with a learning rate of 0.25 which is constant throughout, and a validation grid of \u03b2 \u2208 [1.0, 1.5, 1.75, 2.0].", 
        "101": "For each model adapted on a corpus, we compare the perplexities obtained by Affect-LM and the baseline model when evaluated on that corpus.", 
        "102": "4.4 Sentence Generation Perception Study  We assess Affect-LM\u2019s ability to generate emotionally colored text of varying degrees without severely deteriorating grammatical correctness, by conducting an extensive perception study on Amazon\u2019s Mechanical Turk (MTurk) platform.", 
        "103": "The MTurk platform has been successfully used in the past for a wide range of perception experiments and has been shown to be an excellent resource to collect human ratings for large studies (Buhrmester et al., 2011).", 
        "104": "Specifically, we generated more than 200 sentences for four sentence beginnings (namely the three sentence beginnings listed in Table 2 as well as an end of sentence token indicating that the model should generate a new sentence) in five affect categories happy(positive emotion), angry, sad, anxiety, and negative emotion.", 
        "105": "The Affect-LM model trained\non the Fisher corpus was used for sentence generation.", 
        "106": "Each sentence was evaluated by two human raters that have a minimum approval rating of 98% and are located in the United States.", 
        "107": "The human raters were instructed that the sentences should be considered to be taken from a conversational rather than a written context: repetitions and pause fillers (e.g., um, uh) are common and no punctuation is provided.", 
        "108": "The human raters evaluated each sentence on a seven-point Likert scale for the five affect categories, overall affective valence as well as the sentence\u2019s grammatical correctness and were paid 0.05USD per sentence.", 
        "109": "We measured inter-rater agreement using Krippendorffs \u03b1 and observed considerable agreement between raters across all categories (e.g., for valence \u03b1 = 0.510 and grammatical correctness \u03b1 = 0.505).", 
        "110": "For each target emotion (i.e., intended emotion of generated sentences) we conducted an initial MANOVA, with human ratings of affect categories the DVs (dependent variables) and the affect strength parameter \u03b2 the IV (independent variable).", 
        "111": "We then conducted follow-up univariate ANOVAs to identify which DV changes significantly with \u03b2.", 
        "112": "In total we conducted 5 MANOVAs and 30 follow-up ANOVAs, which required us to update the significance level to p<0.001 following a Bonferroni correction.", 
        "113": "5 Results    5.1 Generation of Emotional Text  In Section 3.4 we have described the process of sampling text from the model conditioned on input affective information (research question Q1).", 
        "114": "Table 2 shows three sentences generated by the model for input sentence beginnings I feel so ..., Why did you ... and I told him to ... for each of five\naffect categories - happy(positive emotion), angry, sad anxiety, and neutral(no emotion).", 
        "115": "They have been selected from a pool of 20 generated sentences for each category and sentence beginning.", 
        "116": "5.2 MTurk Perception Experiments  In the following we address research question Q2 by reporting the main statistical findings of our MTurk study, which are visualized in Figures 2 and 3.", 
        "117": "Positive Emotion Sentences.", 
        "118": "The multivariate result was significant for positive emotion generated sentences (Pillai\u2019s Trace=.327, F(4,437)=6.44, p<.0001).", 
        "119": "Follow up ANOVAs revealed significant results for all DVs except angry with p<.0001, indicating that both affective valence and happy DVs were successfully manipulated with \u03b2, as seen in Figure 2(a).", 
        "120": "Grammatical correctness was also significantly influenced by the affect strength parameter \u03b2 and results show that the correctness deteriorates with increasing \u03b2 (see Figure 3).", 
        "121": "However, a post-hoc Tukey test revealed that only the highest \u03b2 value shows a significant drop in grammatical correctness at p<.05.", 
        "122": "Negative Emotion Sentences.", 
        "123": "The multivariate result was significant for negative emotion generated sentences (Pillai\u2019s Trace=.130, F(4,413)=2.30, p<.0005).", 
        "124": "Follow up ANOVAs revealed significant results for affective valence and happy DVs with p<.0005, indicating that the affective valence DV was successfully manipulated with \u03b2, as seen in Figure 2(b).", 
        "125": "Further, as intended there were no significant differences for DVs angry, sad and anxious, indicating that the negative emotion DV refers to a more general affect related concept rather than a specific negative emotion.", 
        "126": "This finding is in concordance with the intended LIWC category of negative affect that forms a parent category above the more\nspecific emotions, such as angry, sad, and anxious (Pennebaker et al., 2001).", 
        "127": "Grammatical correctness was also significantly influenced by the affect strength \u03b2 and results show that the correctness deteriorates with increasing \u03b2 (see Figure 3).", 
        "128": "As for positive emotion, a post-hoc Tukey test revealed that only the highest \u03b2 value shows a significant drop in grammatical correctness at p<.05.", 
        "129": "Angry Sentences.", 
        "130": "The multivariate result was significant for angry generated sentences (Pillai\u2019s Trace=.199, F(4,433)=3.76, p<.0001).", 
        "131": "Follow up ANOVAs revealed significant results for affective valence, happy, and angry DVs with p<.0001, indicating that both affective valence and angry DVs were successfully manipulated with \u03b2, as seen in Figure 2(c).", 
        "132": "Grammatical correctness was not significantly influenced by the affect strength parameter \u03b2, which indicates that angry sentences are highly stable across a wide range of \u03b2 (see Figure 3).", 
        "133": "However, it seems that human raters could not successfully distinguish between angry, sad, and anxious affect categories, indicating that the generated sentences likely follow a general negative affect dimension.", 
        "134": "Sad Sentences.", 
        "135": "The multivariate result was significant for sad generated sentences (Pillai\u2019s Trace=.377, F(4,425)=7.33, p<.0001).", 
        "136": "Follow up ANOVAs revealed significant results only for the sad DV with p<.0001, indicating that while the sad DV can be successfully manipulated with \u03b2, as seen in Figure 2(d).", 
        "137": "The grammatical correctness deteriorates significantly with \u03b2.", 
        "138": "Specifically, a post-hoc Tukey test revealed that only the two highest \u03b2 values show a significant drop in grammatical correctness at p<.05 (see Figure 3).", 
        "139": "A post-hoc Tukey test for sad reveals that \u03b2 = 3 is optimal for this DV, since it leads to a significant jump in the perceived sadness scores at p<.005 for \u03b2 \u2208 {0, 1, 2}.", 
        "140": "Anxious Sentences.", 
        "141": "The multivariate result was significant for anxious generated sentences (Pillai\u2019s Trace=.289, F(4,421)=6.44, p<.0001).", 
        "142": "Follow up ANOVAs revealed significant results for affective valence, happy and anxious DVs with p<.0001, indicating that both affective valence and anxiety DVs were successfully manipulated with \u03b2, as seen in Figure 2(e).", 
        "143": "Grammatical correctness was also significantly influenced by the affect strength parameter \u03b2 and results show that the correctness deteriorates with increasing \u03b2.", 
        "144": "Similarly for sad, a post-hoc Tukey test revealed that only the two highest \u03b2 values show a significant drop in grammatical correctness at p<.05 (see Figure 3).", 
        "145": "Again, a post-hoc Tukey test for anxious reveals that \u03b2 = 3 is optimal for this DV, since it leads to a significant jump in the perceived\nanxiety scores at p<.005 for \u03b2 \u2208 {0, 1, 2}.", 
        "146": "5.3 Language Modeling Results  In Table 3, we address research question Q3 by presenting the perplexity scores obtained by the baseline model and Affect-LM, when trained on the Fisher corpus and subsequently adapted on three emotional corpora (each adapted model is individually trained on CMU-MOSI, DAIC and SEMAINE).", 
        "147": "The models trained on Fisher are evaluated on all corpora while each adapted model is evaluated only on it\u2019s respective corpus.", 
        "148": "For all corpora, we find that Affect-LM achieves lower perplexity on average than the baseline model, implying that affect category information obtained from the context words improves language model prediction.", 
        "149": "The average perplexity improvement is 1.44 (relative improvement 1.94%) for the model trained on Fisher, while it is 0.79 (1.31%) for the adapted models.", 
        "150": "We note that larger improvements in perplexity are observed for corpora with higher content of emotional words.", 
        "151": "This is supported by the results in Table 3, where AffectLM obtains a larger reduction in perplexity for the CMU-MOSI and SEMAINE corpora, which respectively consist of 2.76% and 2.75% more emotional words than the Fisher corpus.", 
        "152": "5.4 Word Representations  In Equation 3, Affect-LM learns a weight matrix V which captures the correlation between the predicted word wt, and the affect category et\u22121.", 
        "153": "Thus, each row of the matrix Vi is an emotionally meaningful embedding of the i-th word in the vocabulary.", 
        "154": "In Figure 4, we present a t-SNE visualization of these embeddings, where each data point is a separate word, and words which appear in the LIWC dictionary are colored based on which affect category they belong to (we have labeled only words in categories positive emotion, negative emotion, anger, sad and anxiety since\nthese categories contain the most frequent words).", 
        "155": "Words colored grey are those not in the LIWC dictionary.", 
        "156": "In Figure 4, we observe that the embeddings contain affective information, where the positive emotion is highly separated from the negative emotions (sad, angry, anxiety) which are clustered together.", 
        "157": "6 Conclusions and Future Work  In this paper, we have introduced a novel language model Affect-LM for generating affective conversational text conditioned on context words, an affective category and an affective strength parameter.", 
        "158": "MTurk perception studies show that the model can generate expressive text at varying degrees of emotional strength without affecting grammatical correctness.", 
        "159": "We also evaluate Affect-LM as a language model and show that it achieves lower perplexity than a baseline LSTM model when the affect category is obtained from the words in the context.", 
        "160": "For future work, we wish to extend this model by investigating language generation conditioned on other modalities such as facial images and speech, and to applications such as dialogue generation for virtual agents.", 
        "161": "Acknowledgments  This material is based upon work supported by the U.S. Army Research Laboratory under contract number W911NF-14-D-0005.", 
        "162": "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Government, and no official endorsement should be inferred.", 
        "163": "Sayan Ghosh also acknowledges the Viterbi Graduate School Fellowship for funding his graduate studies."
    }, 
    "document_id": "P17-1059.pdf.json"
}
