{
    "abstract_sentences": {
        "1": "Topical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents.", 
        "2": "The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model.", 
        "3": "In this paper, we propose a modification of TPR, called Salience Rank.", 
        "4": "Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets.", 
        "5": "In addition to quality and efficiency benefits, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 530\u2013535 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2084\nTopical PageRank (TPR) uses latent topic distribution inferred by Latent Dirichlet Allocation (LDA) to perform ranking of noun phrases extracted from documents.", 
        "2": "The ranking procedure consists of running PageRank K times, where K is the number of topics used in the LDA model.", 
        "3": "In this paper, we propose a modification of TPR, called Salience Rank.", 
        "4": "Salience Rank only needs to run PageRank once and extracts comparable or better keyphrases on benchmark datasets.", 
        "5": "In addition to quality and efficiency benefits, our method has the flexibility to extract keyphrases with varying tradeoffs between topic specificity and corpus specificity.", 
        "6": "1 Introduction  Automatic keyphrase extraction consists of finding a set of terms in a document that provides a concise summary of the text content (Hasan and Ng, 2014).", 
        "7": "In this paper we consider unsupervised keyphrase extraction, where no human labeled corpus of documents is used for training a classifier (Grineva et al., 2009; Pasquier, 2010; Liu et al., 2009b; Zhao et al., 2011; Liu et al., 2009a).", 
        "8": "This is a scenario often arising in practical applications as human annotation and tagging is both time and resource consuming.", 
        "9": "Unsupervised keyphrase extraction is typically casted as a ranking problem \u2013 first, candidate phrases are extracted from documents, typically noun phrases identified by part-of-speech tagging; then these candidates are ranked.", 
        "10": "The performance of unsupervised keyphrase extraction algorithms is evaluated by comparing the most highly ranked keyphrases with keyphrases assigned by annotators.", 
        "11": "\u2217 Work done as an intern at Amazon.", 
        "12": "This paper proposes Salience Rank, a modification of Topical PageRank algorithm by Liu et al.", 
        "13": "(2010).", 
        "14": "Our method is close in spirit to Single Topical PageRank by Sterckx et al.", 
        "15": "(2015) and includes it as a special case.", 
        "16": "The advantages of Salience Rank are twofold: Performance: The algorithm extracts highquality keyphrases that are comparable to, and sometimes better than, the ones extracted by Topical PageRank.", 
        "17": "Salience Rank is more efficient than Topical PageRank as it runs PageRank once, rather than multiple times.", 
        "18": "Configurability: The algorithm is based on the concept of \u201cword salience\u201d (hence its name), which is described in Section 3 and can be used to balance topic specificity and corpus specificity of the extracted keyphrases.", 
        "19": "Depending on the use case, the output of the Salience Rank algorithm can be tuned accordingly.", 
        "20": "2 Review of Related Models  Below we introduce some notation and discuss approaches that are most related to ours.", 
        "21": "Let W = {w1, w2, .", 
        "22": ".", 
        "23": ".", 
        "24": ", wN} be the set of all the words present in a corpus of documents.", 
        "25": "Let G = (W,E) denote a word graph, whose vertices represent words and an edge e(wi, wj) \u2208 E indicates the relatedness between words wi and wj in a document (measured, e.g., by co-occurrence or number of co-occurrences between the two words).", 
        "26": "The outdegree of vertex wi is given by Out(wi) = \u2211 i:wi\u2192wj e(wi, wj).", 
        "27": "2.1 Topical PageRank  The main idea behind Topical PageRank (TPR) (Liu et al., 2010) is to incorporate topical information by performing Latent Dirichlet Allocation (LDA) (Blei et al., 2003) on a corpus of documents.", 
        "28": "TPR constructs a word graph G = (W,E) based on the word co-occurrences within documents.", 
        "29": "It uses LDA to find the latent topics of the\n530\ndocument, reweighs the word graph according to each latent topic, and runs PageRank (Page et al., 1998) once per topic.", 
        "30": "In LDA each word w of a document d is assumed to be generated by first sampling a topic t \u2208 T (where T is a set ofK topics) from d\u2019s topic distribution \u03b8d and then sampling a word from the distribution over words \u03c6t of topic t. Both \u03b8d and \u03c6t are drawn from conjugate Dirichlet priors \u03b1 and \u03b2, respectively.", 
        "31": "Thus, the probability of word w, given document d and the priors \u03b1 and \u03b2, is\np(w | d, \u03b1, \u03b2) = \u2211\nt\u2208T p(w | t, \u03b2) p(t | d, \u03b1) .", 
        "32": "(1)\nAfter running LDA, TPR ranks each word wi \u2208 W of G by Rt(wi) = \u03bb \u2211\nj:wj\u2192wi\ne(wi, wj) Out(wj) Rt(wj) + (1\u2212\u03bb)p(t |wi) ,\n(2)\nfor t \u2208 T , where p(t |w) is estimated via LDA.", 
        "33": "TPR assigns a topic specific preference value p(t |w) to each w \u2208 W as the jump probability at each vertex depending on the underlying topic.", 
        "34": "Intuitively, p(t |w) indicates how much the word w focuses on topic t.1\nAt the next step of TPR, the word scores (2) are accumulated into keyphrase scores.", 
        "35": "In particular, for each topic t, a candidate keyphrase is ranked by the sum of the word scores\nRt(phrase) = \u2211\nwi\u2208phrase Rt(wi) .", 
        "36": "(3)\nBy combining the topic specific keyphrase scores Rt(phrase) with the probability p(t | d) derived from the LDA we can compute the final keyphrase scores across all K topics:\nR(phrase) = \u2211\nt\u2208T Rt(phrase) p(t | d) .", 
        "37": "(4)  2.2 Single Topical PageRank  Single Topical PageRank (STPR) was recently proposed by Sterckx et al.", 
        "38": "(2015).", 
        "39": "It aims to reduce the runtime complexity of TPR and at the same time maintain its predictive performance.", 
        "40": "Similar to Salience Rank, it runs PageRank once.", 
        "41": "STPR is based on the idea of \u201ctopical word importance\u201d TWI (w), which is defined as the cosine similarity between the vector of\n1Liu et al.", 
        "42": "(2010) proposed two other quantities to bias the random walk, p(w | t) and p(w | t)p(t |w), and showed that p(t |w) achieves the best empirical result.", 
        "43": "We therefore adopt the use of p(t |w) here.", 
        "44": "word-topic probabilities [p(w | t1), .", 
        "45": ".", 
        "46": ".", 
        "47": ", p(w | tK)] and the vector of document-topic probabilities [p(t1 | d), .", 
        "48": ".", 
        "49": ".", 
        "50": ", p(tK | d)], for each word w given the document d. STPR then uses PageRank to rank each word wi \u2208 W by replacing p(t |wi) in (2) with TWI (wi)\u2211\nwk\u2208W TWI (wk) .", 
        "51": "STPR can be seen as a special case of Salience Rank, where topic specificity of a word is considered when constructing the random walk, but corpus specificity is neglected.", 
        "52": "In practice, however, balancing these two concepts is important.", 
        "53": "It may explain why Salience Rank outperforms STPR in our experiments.", 
        "54": "3 Salience Rank  In order to achieve performance and configurability, the Salience Rank (SR) algorithm combines the K latent topics estimated by LDA into a word metric, called word salience, and uses it as a preference value for each wi \u2208 W .", 
        "55": "Thus, SR needs to perform only a single run of PageRank on the word graph G in order to obtain a ranking of the words in each document.", 
        "56": "3.1 Word Salience  In the following we provide quantitative measures for topic specificity and corpus specificity, and define word salience.", 
        "57": "Definition 3.1 The topic specificity of a word w is\nTS (w) = \u2211 t\u2208T p(t |w) log p(t |w) p(t)\n= KL (p(t |w) \u2016 p(t)) .", 
        "58": "(5)\nThe definition of topic specificity of a word w is equivalent to Chuang et al.", 
        "59": "(2012)\u2019s proposal of the distinctiveness of a word w, which is in turn equivalent to the Kullback-Leibler (KL) divergence from the marginal probability p(t), i.e., the likelihood that any randomly selected word is generated by topic t, to the conditional probability p(t |w), i.e., the likelihood that an observed word w is generated by a latent topic t. Intuitively, topic specificity measures how much a word is shared across topics: The less w is shared across topics, the higher its topic specificity TS (w).", 
        "60": "As TS (w) is non-negative and unbounded, we can empirically normalize it to [0, 1] by\nTS (w)\u2212minuTS (u) maxuTS (u)\u2212minuTS (u)\nwith the minimum and maximum topic specificity values in the corpus.", 
        "61": "In what follows, we always use normalized topic specificity values, unless explicitly stated otherwise.", 
        "62": "We apply a straightforward definition for corpus specificity.", 
        "63": "Definition 3.2 The corpus specificity of a word w is\nCS (w) = p(w | corpus) .", 
        "64": "(6)\nThe corpus specificity CS (w) of a word w can be estimated by counting word frequencies in the corpus of interest.", 
        "65": "Finally, a word\u2019s salience is defined as a linear combination of its topic specificity and corpus specificity.", 
        "66": "Definition 3.3 The salience of a word w is\nS(w) = (1\u2212 \u03b1)CS (w) + \u03b1TS (w) , (7)\nwhere \u03b1 \u2208 [0, 1] is a parameter controlling the tradeoff between the corpus specificity and the topic specificity of w.\nOn one hand, we aim to extract keyphrases that are relevant to one or more topics while, on the other hand, the extracted keyphrases as a whole should have a good coverage of the topics in the document.", 
        "67": "Depending on the downstream applications, it is often useful to be able to control the balance between these two competing principles.", 
        "68": "In other words, sometimes keyphrases with high topic specificity (i.e., phrases that are representative exclusively for certain topics) are more appropriate, while other times keyphrases with high corpus specificity (i.e., phrases that are representative of the corpus as a whole) are more appropriate.", 
        "69": "Intuitively, it is advantageous for a keyphrase extraction algorithm to have an internal \u201cswitch\u201d tuning the extent to which extracted keyphrases are skewed towards particular topics and, conversely, the extent to which keyphrases generalize across different topics.", 
        "70": "It needs to be emphasized that the choice of quantitative measures for topic specificity and corpus specificity used above is just one among many possibilities.", 
        "71": "For example, for topic specificity, one can make use of the topical word importance by Sterckx et al.", 
        "72": "(2015), or the several other alternatives mentioned in Section 2.1 proposed by Liu et al.", 
        "73": "(2010).", 
        "74": "For corpus specificity, alternatives besides vanilla term frequencies, such as augmented frequency (to discount\nlonger documents) and logarithmically scaled frequency, quickly come into mind.", 
        "75": "Taking word salience into account, we modify (2) as follow:\nR(wi) = \u03bb \u2211\nj:wj\u2192wi\ne(wj , wi) Out(wj) R(wj) + (1\u2212 \u03bb)S(wi) .", 
        "76": "(8)\nThe substantial efficiency boost of SR comparing to TPR lies in the fact that in (2) K PageRanks are required to calculate Rt(wi), t = 1 .", 
        "77": ".", 
        "78": ".K before obtaining R(wi), while in (8) R(wi) is obtained with a single PageRank.", 
        "79": "3.2 Algorithm Description  First, SR performs LDA to estimate the latent topics p(t) presented in the corpus and the probability p(t |w), which are used to calculate the topic specificity and the salience of each word w.\nSimilarly to TPR, SR is performed on the word co-occurrence graph G = (W,E).", 
        "80": "We use undirected graphs: When sliding a window of size s through the document, a link between two vertices is added if these two words appear within the window.", 
        "81": "It was our observation that the edge direction does not affect the keyphrase extraction performance much.", 
        "82": "The same observation was noted by Mihalcea and Tarau (2004) and Liu et al.", 
        "83": "(2010).", 
        "84": "We then run the updated version of PageRank derived in (8) and compute the scores of the candidate keyphrases similarly to the way TPR does using (4).", 
        "85": "For a fair comparison, noun phrases with the pattern (adjective)*(noun)+ are chosen as candidate keyphrases, which represents zero or more adjectives followed by one or more nouns.", 
        "86": "It is the same pattern suggested by Liu et al.", 
        "87": "(2010) in the original TPR paper.", 
        "88": "SR combines the K PageRank runs in TPR into a single one using salience as a preference value in the word graph.", 
        "89": "4 Results  Our experiments are conducted on two widely used datasets in the keyphrase extraction literature, 500N-KPCrowd (Marujo et al., 2013) and Inspec (Hulth, 2003).", 
        "90": "The 500N-KPCrowd dataset consists of 500 news articles, 50 stories for each of 10 categories, manually annotated with keyphrases by 20 Amazon Mechanical Turk workers.", 
        "91": "The Inspec dataset is a collection of 2000 paper abstracts of Computer Science & Information Technology journal with manually assigned\nkeyphrases by the authors.", 
        "92": "Following the evaluation process described in Mihalcea and Tarau (2004), we use only the uncontrolled set of annotated keyphrases for our analysis.", 
        "93": "Since our approach is completely unsupervised, we combine the training, testing, and validation datasets.", 
        "94": "Top 50 and 10 keyphrases were used for evaluation on 500N-KPCrowd and Inspec, respectively.2\nWe compare the performance of Salience Rank (SR), Topical PageRank (TPR), and Single Topical PageRank (STPR) in terms of precision, recall and F measure on 500N-KPCrowd and Inspec.", 
        "95": "The results are summarized in Table 1.", 
        "96": "Details on parametrization are given in the caption.", 
        "97": "In terms of the F measure, SR achieves the best results on both datasets.", 
        "98": "It ties TPR and outperforms STPR on 500N-KPCrowd, and outperforms both TPR and STPR on Inspec.", 
        "99": "The source code is available at https://github.com/methanet/ saliencerank.git.", 
        "100": "We further experiment with varying the num-\n2There are two common ways to set the number of output keyphrases: using a fixed value a priori as we do (Turney, 1999) or deciding a value with heuristics at runtime (Mihalcea and Tarau, 2004).", 
        "101": "ber of topics K used for fitting the LDA model in SR. Table 2 shows how the F measures change on 500N-KPCrowd as the number of topics varies.", 
        "102": "Overall, the impact of topic size is mild, with K = 50 being the optimal value.", 
        "103": "The impact ofK on TPR can be found in Liu et al.", 
        "104": "(2010).", 
        "105": "In our approach, the random walk derived in (8) depends on the word salience, which in turn depends onK; In TPR, not only the individual random walk (2) depends on K, but the final aggregation of rankings of keyphrases also depends on K.\nWe also experiment with varying the tradeoff parameter \u03b1 of SR. With 500N-KPCrowd, Table 3 illustrates that different \u03b1 can have a considerable impact on various performance measures.", 
        "106": "To complement the quantitative results in Table 3, Table 4 presents a concrete example, showing that varying \u03b1 can lead to qualitative changes in the top ranked keyphrases.", 
        "107": "In particular, when \u03b1 = 0 the corpus specificity of the keyphrases SR extracts is high.", 
        "108": "This is demonstrated by the fact that words such as \u201ctheory\u201d and \u201cfunction\u201d are among\nthe top keyphrases SR selects, which are highly common words in scientific papers.", 
        "109": "On the other hand, when \u03b1 = 1 these keyphrases are not presented among the top.", 
        "110": "This toy example illustrates the relevance of balancing topic and corpus specificity in practice: When presenting the keyphrases to a layman, high corpus specificity is suitable as it conveys more high-level information; when presenting to an expert in the area, high topic specificity is suitable as it dives deeper into topic specific details.", 
        "111": "5 Conclusions & Remarks  In this paper, we propose a new keyphrase extraction method, called Salience Rank.", 
        "112": "It improves upon the Topical PageRank algorithm by Liu et al.", 
        "113": "(2010) and the Single Topical PageRank algorithm by Sterckx et al.", 
        "114": "(2015).", 
        "115": "The key advantages of this new method are twofold: (i) While maintaining and sometimes improving the quality of extracted keyphrases, it only runs PageRank once instead of K times as in Topical PageRank, therefore leads to lower runtime; (ii) By constructing the underlying word graph with newly proposed word salience, it allows the user to balance topic and corpus specificity of the extracted keyphrases.", 
        "116": "These three methods rely only on the input cor-\npus.", 
        "117": "They can be benefited by external resources like Wikipedia and WordNet, as indicated by, e.g., Medelyan et al.", 
        "118": "(2009), Grineva et al.", 
        "119": "(2009), Martinez-Romo et al.", 
        "120": "(2016).", 
        "121": "In the keyphrase extraction literature, LDA is the most commonly used topic modeling method.", 
        "122": "Other methods, such as probabilistic latent semantic indexing (Hofmann, 1999), nonnegative matrix factorization (Sra and Inderjit, 2006), are viable alternatives.", 
        "123": "However, it is hard to tell in general if the keyphrase quality improves with these alternatives.", 
        "124": "We suspect that strongly depends on the domain of the dataset and a choice may be made depending on other practical considerations.", 
        "125": "We have fixed the tradeoff parameter \u03b1 throughout the experiments for a straightforward comparison to other methods.", 
        "126": "In practice, one should search the optimal value of \u03b1 for the task at hand.", 
        "127": "An open question is how to theoretically quantify the relationship between \u03b1 and various performance measures, such as the F measure.", 
        "128": "Acknowledgments  We would like to thank Matthias Seeger, Cedric Archambeau, Jan Gasthaus, Alex Klementiev, Ralf Herbrich, and the anonymous ACL reviewers for their valuable inputs."
    }, 
    "document_id": "P17-2084.pdf.json"
}
