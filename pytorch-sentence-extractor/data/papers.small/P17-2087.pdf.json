{
    "abstract_sentences": {
        "1": "This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model.", 
        "2": "The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis.", 
        "3": "The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis.", 
        "4": "We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 547\u2013553 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2087\nThis paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model.", 
        "2": "The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses and a Long Short-Term Memory (LSTM) language model to score the underlying fluent sentences of each analysis.", 
        "3": "The LSTM language model scores, along with other features, are used in a MaxEnt reranker to identify the most plausible analysis.", 
        "4": "We show that using an LSTM language model in the reranking process of noisy channel disfluency model improves the state-of-the-art in disfluency detection.", 
        "5": "1 Introduction  Disfluency is a characteristic of spontaneous speech which is not present in written text.", 
        "6": "Disfluencies are informally defined as interruptions in the normal flow of speech that occur in different forms, including false starts, corrections, repetitions and filled pauses.", 
        "7": "According to Shriberg\u2019s (1994) definition, the basic pattern of speech disfluencies contains three parts: reparandum1, interregnum and repair.", 
        "8": "Example 1 illustrates a disfluent structure, where the reparandum to Boston is the part of the utterance that is replaced, the interregnum uh, I mean is an optional part of a disfluent structure that consists of a filled pause uh and a discourse marker I mean and the repair to Denver replaces the reparandum.", 
        "9": "The fluent version of Example 1 is obtained by deleting\n1Reparandum is sometimes called edit.", 
        "10": "reparandum and interregnum words.", 
        "11": "I want a flight reparandum\ufe37 \ufe38\ufe38 \ufe37 to Boston,\nuh, I mean\ufe38 \ufe37\ufe37 \ufe38 interregnum to Denver\ufe38 \ufe37\ufe37 \ufe38 repair\non Friday (1)\nWhile disfluency rate varies with the context, age and gender of speaker, Bortfeld et al.", 
        "12": "(2001) reported disfluencies once in every 17 words.", 
        "13": "Such frequency is high enough to reduce the readability of speech transcripts.", 
        "14": "Moreover, disfluencies pose a major challenge to natural language processing tasks, such as dialogue systems, that rely on speech transcripts (Ostendorf et al., 2008).", 
        "15": "Since such systems are usually trained on fluent, clean corpora, it is important to apply a speech disfluency detection system as a pre-processor to find and remove disfluencies from input data.", 
        "16": "By disfluency detection, we usually mean identifying and deleting reparandum words.", 
        "17": "Filled pauses and discourse markers belong to a closed set of words, so they are trivial to detect (Johnson and Charniak, 2004).", 
        "18": "In this paper, we introduce a new model for detecting restart and repair disfluencies in spontaneous speech transcripts called LSTM Noisy Channel Model (LSTM-NCM).", 
        "19": "The model uses a Noisy Channel Model (NCM) to generate n-best candidate disfluency analyses, and a Long Short-Term Memory (LSTM) language model to rescore the NCM analyses.", 
        "20": "The language model scores are used as features in a MaxEnt reranker to select the most plausible analysis.", 
        "21": "We show that this novel approach improves the current state-of-the-art.", 
        "22": "2 Related Work  Approaches to disfluency detection task fall into three main categories: sequence tagging, parsingbased and noisy channel model.", 
        "23": "The sequence\n547\ntagging models label words as fluent or disfluent using a variety of different techniques, including conditional random fields (Ostendorf and Hahn, 2013; Zayats et al., 2014; Ferguson et al., 2015), hidden Markov models (Liu et al., 2006; Schuler et al., 2010) or recurrent neural networks (Hough and Schlangen, 2015; Zayats et al., 2016).", 
        "24": "Although sequence tagging models can be easily generalized to a wide range of domains, they require a specific state space for disfluency detection, such as begin-inside-outside (BIO) style states that label words as being inside or outside of a reparandum word sequence.", 
        "25": "The parsing-based approaches refer to parsers that detect disfluencies, as well as identifying the syntactic structure of the sentence (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Yoshikawa et al., 2016).", 
        "26": "Training a parsing-based model requires large annotated tree-banks that contain both disfluencies and syntactic structures.", 
        "27": "Noisy channel models (NCMs) use the similarity between reparandum and repair as an indicator of disfluency.", 
        "28": "However, applying an effective language model (LM) inside an NCM is computationally complex.", 
        "29": "To alleviate this problem, some researchers use more effective LMs to rescore the NCM disfluency analyses.", 
        "30": "Johnson and Charniak (2004) applied a syntactic parsing-based LM trained on the fluent version of the Switchboard corpus to rescore the disfluency analyses.", 
        "31": "Zwarts and Johnson (2011) trained external n-gram LMs on a variety of large speech and non-speech corpora to rank the analyses.", 
        "32": "Using the external LM probabilities as features into the reranker improved the baseline NCM (Johnson and Charniak, 2004).", 
        "33": "The idea of applying external language models in the reranking process of the NCM motivates our model in this work.", 
        "34": "3 LSTM Noisy Channel Model  We follow Johnson and Charniak (2004) in using an NCM to find the n-best candidate disfluency analyses for each sentence.", 
        "35": "The NCM, however, lacks an effective language model to capture more complicated language structures.", 
        "36": "To overcome this problem, our idea is to use different LSTM language models to score the underlying fluent sentences of the analyses proposed by the NCM and use the language model scores as features to a MaxEnt reranker to select the best analysis.", 
        "37": "In the following, we describe our model and its components in detail.", 
        "38": "In the NCM of speech disfluency, we assume that there is a well-formed source utterance X to which some noise is added and generates a disfluent utterance Y as follows.", 
        "39": "X = a flight to Denver Y = a flight to Boston uh I mean to Denver\nGiven Y , the goal of the NCM is to find the most likely source sentence X\u0302 such that:\nX\u0302 = argmax X\nP(Y |X)P(X) (2)\nAs shown in Equation 2, the NCM contains two components: the channel model P(Y |X) and the language model P(X).", 
        "40": "Calculating the channel model and language model probabilities, the NCM generates 25-best candidate disfluency analyses as follows.", 
        "41": "(3)\nExample 3 shows sample outputs of the NCM, where potential reparandum words are specified with strikethrough text.", 
        "42": "The MaxEnt reranker is applied on the candidate analyses of the NCM to select the most plausible one.", 
        "43": "3.1 Channel Model  We assume that X is a substring of Y , so the source sentence X is obtained by deleting words from Y .", 
        "44": "For each sentence Y , there are only a finite number of potential source sentences.", 
        "45": "However, with the increase in the length of Y , the number of possible source sentences X grows exponentially, so it is not feasible to do exhaustive search.", 
        "46": "Moreover, since disfluent utterances may contain an unbounded number of crossed dependencies, a context-free grammar is not suitable for finding the alignments.", 
        "47": "The crossed dependencies refer to the relation between repair and reparandum words which are usually the same or very similar words in roughly the same order as in Example 4.", 
        "48": "(4)\nWe apply a Tree Adjoining Grammar (TAG) based transducer (Johnson and Charniak, 2004)\nwhich is a more expressive formalism and provides a systematic way of formalising the channel model.", 
        "49": "The TAG channel model encodes the crossed dependencies of speech disfluency, rather than reflecting the syntactic structure of the sentence.", 
        "50": "The TAG transducer is effectively a simple first-order Markov model which generates each word in the reparandum conditioned on the preceding word in the reparandum and the corresponding word in the repair.", 
        "51": "Further detail about the TAG channel model can be found in (Johnson and Charniak, 2004).", 
        "52": "3.2 Language Model  The language model of the NCM evaluates the fluency of the sentence with disfluency removed.", 
        "53": "The language model is expected to assign a very high probability to a fluent sentence X (e.g.", 
        "54": "a flight to Denver) and a lower probability to a sentence Y which still contains disfluency (e.g.", 
        "55": "a flight to Boston uh I mean to Denver).", 
        "56": "However, it is computationally complex to use an effective language model within the NCM.", 
        "57": "The reason is the polynomial-time dynamic programming parsing algorithms of TAG can be used to search for likely repairs if they are used with simple language models such as a bigram LM (Johnson and Charniak, 2004).", 
        "58": "The bigram LM within the NCM is too simple to capture more complicated language structure.", 
        "59": "In order to alleviate this problem, we follow Zwarts and Johnson (2011) by training LMs on different corpora, but we apply state-ofthe-art recurrent neural network (RNN) language models.", 
        "60": "LSTM We use a long short-term memory (LSTM) neural network for training language models.", 
        "61": "LSTM is a particular type of recurrent neural networks which has achieved state-of-the-art performance in many tasks including language modelling (Mikolov et al., 2010; Jozefowicz et al., 2016).", 
        "62": "LSTM is able to learn long dependencies between words, which can be highly beneficial for the speech disfluency detection task.", 
        "63": "Moreover, it allows for adopting a distributed representation of words by constructing word embedding (Mikolov et al., 2013).", 
        "64": "We train forward and backward (i.e.", 
        "65": "input sentences are given in reverse order) LSTM language models using truncated backpropagation through time algorithm (Rumelhart et al., 1986) with mini-\nbatch size 20 and total number of epochs 13.", 
        "66": "The LSTM model has two layers and 200 hidden units.", 
        "67": "The initial learning rate for stochastic gradient optimizer is chosen to 1 which is decayed by 0.5 for each epoch after maximum epoch 4.", 
        "68": "We limit the maximum sentence length for training our model due to the high computational complexity of longer histories in the LSTM.", 
        "69": "In our experiments, considering maximum 50 words for each sentence leads to good results.", 
        "70": "The size of word embedding is 200 and it is randomly initialized for all LSTM LMs2.", 
        "71": "Using each forward and backward LSTM language model, we assign a probability to the underlying fluent parts of each candidate analysis.", 
        "72": "3.3 Reranker  In order to rank the the 25-best candidate disfluency analyses of the NCM and select the most suitable one, we apply the MaxEnt reranker proposed by Johnson et al.", 
        "73": "(2004).", 
        "74": "We use the feature set introduced by Zwarts and Johnson (2011), but instead of n-gram scores, we apply the LSTM language model probabilities.", 
        "75": "The features are so good that the reranker without any external language model is already a state-of-the-art system, providing a very strong baseline for our work.", 
        "76": "The reranker uses both model-based scores (including NCM scores and LM probabilities) and surface pattern features (which are boolean indicators) as described in Table 1.", 
        "77": "Our reranker optimizes the expected f-score approximation described in Zwarts and Johnson (2011) with L2 regularisation.", 
        "78": "4 Corpora for Language Modelling  In this work, we train forward and backward LSTM language models on Switchboard (Godfrey and Holliman, 1993) and Fisher (Cieri et al., 2004) corpora.", 
        "79": "Fisher consists of 2.2\u00d7 107 tokens of transcribed text, but disfluencies are not annotated in it.", 
        "80": "Switchboard is the largest available corpus (1.2\u00d7 106 tokens) in which disfluencies are annotated according to Shriberg\u2019s (1994) scheme.", 
        "81": "Since the bigram language model of the NCM is trained on this corpus, we cannot directly use Switchboard to build LSTM LMs.", 
        "82": "The reason is that if the training data of Switchboard is used both for predicting language fluency and optimizing the loss function, the reranker will overestimate the\n2All code is written in TensorFlow (Abadi et al., 2015)\nweight related to the LM features extracted from Switchboard.", 
        "83": "This is because the fluent sentence itself is part of the language model (Zwarts and Johnson, 2011).", 
        "84": "As a solution, we apply a k-fold cross-validation (k = 20) to train the LSTM language models when using Switchboard corpus.", 
        "85": "We follow Charniak and Johnson (2001) in splitting Switchboard corpus into training, development and test set.", 
        "86": "The training data consists of all sw[23]\u2217.dps files, development training consists of all sw4[5-9]\u2217.dps files and test data consists of all sw4[0-1]\u2217.dps files.", 
        "87": "Following Johnson and Charniak (2004), we remove all partial words and punctuation from the training data.", 
        "88": "Although partial words are very strong indicators of disfluency, standard speech recognizers never produce them in their outputs, so this makes our evaluation both harder and more realistic.", 
        "89": "5 Results and Discussion  We assess the proposed model for disfluency detection with all MaxEnt features described in Table 1 against the baseline model.", 
        "90": "The noisy channel model with exactly the same reranker features except the LSTM LMs forms the baseline model.", 
        "91": "To evaluate our system, we use two metrics f-score and error rate.", 
        "92": "Charniak and Johnson (2001) used the f-score of labelling reparanda\nor \u201cedited\u201d words, while Fiscus et al (2004) defined an \u201cerror rate\u201d measure, which is the number of words falsely labelled divided by the number of reparanda words.", 
        "93": "Since only 6% of words are disfluent in Switchboard corpus, accuracy is not a good measure of system performance.", 
        "94": "F-score, on the other hand, focuses more on detecting \u201cedited\u201d words, so it is a decent metric for highly skewed data.", 
        "95": "According to Tables 2 and 3, the LSTM noisy channel model outperforms the baseline.", 
        "96": "The experiment on Switchboard and Fisher corpora demonstrates that the LSTM LMs provide information about the global fluency of an analysis that the local features of the reranker do not capture.", 
        "97": "The LSTM language model trained on Switchboard corpus results in the greatest improvement.", 
        "98": "Switchboard is in the same domain as the test data and it is also disfluency annotated.", 
        "99": "Either or both of these might be the reason why Switchboard seems to be better in comparison with Fisher which is a larger corpus and might be expected to make a better language model.", 
        "100": "Moreover, the backward LSTMs have better performance in comparison with the forward ones.", 
        "101": "It seems when sentences are fed in reverse order, the model can more easily detect the unexpected word order associated with the reparandum to detect disfluencies.", 
        "102": "In other words, that the disfluency is observed \u201cafter\u201d the fluent repair in a backward language model is helpful for recognizing disfluencies.", 
        "103": "We compare the performance of Kneser-Ney\nsmoothed 4-gram language models with the LSTM corresponding on the reranking process of the noisy channel model.", 
        "104": "We estimate the 4- gram models and assign probabilities to the fluent parts of disflueny analyses using the SRILM toolkit (Stolcke, 2002).", 
        "105": "As Tables 4 and 5 show including scores from a conventional 4-gram language model does not improve the model\u2019s ability to find disfluencies, suggesting that the LSTM model contains all the useful information that the 4-gram model does.", 
        "106": "In order to give a more general idea on the performance of LSTM over standard LM, we evaluate our model when the language model scores are used as the only features of the reranker.", 
        "107": "The f-score for the NCM alone without applying the reranker is 78.7, while using 4-gram language model scores in the reranker increases the f-score to 81.0.", 
        "108": "Replacing the 4-gram scores with LSTM language model probabilities leads to further improvement, resulting an f-score 82.3.", 
        "109": "We also compare our best model on the development set to the state-of-the-art methods in the literature.", 
        "110": "As shown in Table 6, the LSTM noisy channel model outperforms the results of prior work, achieving a state-of-the-art performance of 86.8.", 
        "111": "It also has better performance in comparison with Ferguson et al.", 
        "112": "(2015) and Zayat et al.\u2019s (2016) models, even though they use richer input that includes prosodic features or partial words.", 
        "113": "6 Conclusion and Future Work  In this paper, we present a new model for disfluency detection from spontaneous speech tran-\nscripts.", 
        "114": "It uses a long short-term memory neural network language model to rescore the candidate disfluency analyses produced by a noisy channel model.", 
        "115": "The LSTM language model scores as features in a MaxEnt reranker improves the model\u2019s ability to detect restart and repair disfluencies.", 
        "116": "The model outperforms other models reported in the literature, including models that exploit richer information from the input.", 
        "117": "As future work, we apply more complex LSTM language models such as sequence-to-sequence on the reranking process of the noisy channel model.", 
        "118": "We also intend to investigate the effect of integrating LSTM language models into other kinds of disfluency detection models, such as sequence labelling and parsingbased models.", 
        "119": "Acknowledgements  We would like to thank the anonymous reviewers for their insightful comments and suggestions.", 
        "120": "This research was supported by a Google award through the Natural Language Understanding Focused Program, and under the Australian Research Councils Discovery Projects funding scheme (project number DP160102156)."
    }, 
    "document_id": "P17-2087.pdf.json"
}
