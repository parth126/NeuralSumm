{
    "abstract_sentences": {
        "1": "In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets.", 
        "2": "We work on Japanese sentence compression by a similar approach.", 
        "3": "To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language.", 
        "4": "The created dataset is used to train Japanese sentence compression models based on the recurrent neural network."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281\u2013286 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2044  1 Introduction  Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality.", 
        "2": "Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013).", 
        "3": "In this paper, we work on Japanese sentence compression by deleting words.", 
        "4": "One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space.", 
        "5": "Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence.", 
        "6": "There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006).", 
        "7": "In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015).", 
        "8": "While it is impractical to create a large\ntraining dataset by hand, one can be created automatically from news articles (Filippova and Altun, 2013).", 
        "9": "The procedure is as follows (where S, H, and C respectively denote the first sentence of an article, the headline, and the created compressed sentence of S).", 
        "10": "Firstly, to restrict the training data to grammatical and informative sentences, only news articles satisfying certain conditions are used.", 
        "11": "Then, nouns, verbs, adjectives, and adverbs (i.e., content words) shared by S and H are identified by matching word lemmas, and a rooted dependency subtree that contains all the shared content words is regarded as C. However, their method is designed for English, and cannot be applied to Japanese as it is.", 
        "12": "Thus, in this study, their method is modified based on the following three characteristics of the Japanese language: (a) Abbreviation of nouns and nominalization of verbs frequently occur in Japanese.", 
        "13": "(b) Words that are not verbs can also be the root node especially in headlines.", 
        "14": "(c) Subjects and objects that can be easily estimated from the context are often omitted.", 
        "15": "The created training dataset is used to train three models.", 
        "16": "The first model is the original Filippova et al.\u2019s model, an encoder-decoder model with a long short-term memory (LSTM), which we extend in this paper to make the other two models that can control the output length (Kikuchi et al., 2016), because controlling the output length makes a compressed sentence more informative under the desired length.", 
        "17": "2 Creating training dataset for Japanese  Filippova et al.\u2019s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min-\n281\nimum rooted subtree.", 
        "18": "We modified their method based on the characteristics of the Japanese language as follows.", 
        "19": "To explain our method, a dependency tree of S and a sequence of bunsetsu chunks of H in Japanese are shown in Figures 1 and 2.", 
        "20": "Note that nodes of dependency trees in Japanese are bunsetsu chunks each consisting of content words followed by function words.", 
        "21": "2.1 Identification of shared content words  Content words shared by S and H are identified by matching lemmas and pronominal anaphora resolution in Filippova et al.\u2019s method.", 
        "22": "Abbreviation of nouns and nominalization of verbs frequently occur in Japanese (characteristic (a) in Introduction), and it is difficult to identify these transformations simply by matching lemmas.", 
        "23": "Thus, after the identification by matching lemmas, two identification methods (described below) using character-level information are applied.", 
        "24": "Note that pronominal anaphora resolution is not used, because pronouns are often omitted in Japanese.", 
        "25": "Abbreviation of nouns: There are two types of abbreviations of nouns in Japanese.", 
        "26": "One is the abbreviation of a proper noun, which shortens the form by deleting characters (e.g., the pair with \u201c+\u201d in Figures 1 and 2).", 
        "27": "The other is the abbreviation of consecutive nouns, which deletes nouns that behave as adjectives (e.g., the pair with \u201c-\u201d).", 
        "28": "To deal with such cases, if the character sequence of the noun sequence in a chunk in H is identical to a subsequence of characters composing the noun sequence in a chunk in S, the noun sequences in H and S are regarded as shared.", 
        "29": "Nominalization of verbs: Many verbs in Japanese have corresponding nouns with similar meanings (e.g., the pair with # in Figures 1 and\n1 Words in red are regarded as shared through lemma matching, while words in blue (also underlined) are through other ways.", 
        "30": "2).", 
        "31": "Such pairs often share the same Chinese character, kanji.", 
        "32": "Kanji is an ideogram and is more informative than the other types of Japanese letters.", 
        "33": "Thus, if a noun in H and a verb in S2 share one kanji, the noun and the verb are regarded as shared.", 
        "34": "2.2 Transformation of a dependency tree  Some edges in dependency trees cannot be cut without changing the meaning or losing the grammaticality of the sentence.", 
        "35": "In Filippova et al.\u2019s method, the nodes linked by such an edge are merged into a single node before extraction of the subtree.", 
        "36": "The method is adapted to Japanese as follows.", 
        "37": "If the function word of a chunk is one of the specific particles3, which often make obligatory cases, the chunk and its modifiee are merged into a single node.", 
        "38": "In Figure 1, the chunks at the start and end of a bold arrow are merged into a single node.", 
        "39": "2.3 Extraction of the minimum rooted subtree  In Filippova et al.\u2019s method, the minimum rooted subtree that contains all the shared content words is extracted from the transformed tree.", 
        "40": "We modify their method to take into account the characteristics (a) and (b).", 
        "41": "Deleting the global root: In English, only verbs can be the root node of a subtree.", 
        "42": "However, in Japanese, words with other parts-of-speech can also be the root node in headlines (characteristics (b)).", 
        "43": "Therefore, the global root, which is the root node of S, and the chunks including a word that can be located at the end of a sentence4, are the candidates for the root node of a subtree.", 
        "44": "Then, if the root node is not the global root, words succeeding the word that can be located at the end are removed from the root node.", 
        "45": "In Figure 1, among the two words with \u201c*\u201d that can be located at the\n2Candidates are restricted to verbs consisting of one kanji and some following hiragana.", 
        "46": "3\u201c\u3092 (wo)\u201d, \u201c\u306b (ni)\u201d, \u201c\u304c (ga)\u201d and \u201c\u306f (ha)\u201d 4An auxiliary verb, a noun, or a verb of which next word\nis not a noun, an auxiliary verb, or \u201c\u306e (no)\u201d\nend, the latter is extracted as the root, and the succeeding word is removed from the chunk.", 
        "47": "Reflecting abbreviated forms: Abbreviation of nouns frequently occurs in Japanese (characteristic (a)).", 
        "48": "Thus, in C, original forms are replaced by their abbreviated forms obtained as explained in Section 2.1 (e.g., the pair with \u201c-\u201d in Figures 1 and 2).", 
        "49": "However, we do not allow the head of a chunk to be deleted to keep the grammaticality.", 
        "50": "We also restrict here ourselves to word-level deletion and do not allow character-level deletion, because our purpose is to construct a training dataset for compression by word deletion.", 
        "51": "In the example of Figure 1, chunks in bold squares are extracted as C.  2.4 Conditions imposed on news articles  Filippova et al.\u2019s method imposed eight conditions on news articles to restrict the training data to grammatical and informative sentences.", 
        "52": "In our method, these conditions are modified to adapt to Japanese.", 
        "53": "Firstly, the condition \u201cS should include content words of H in the same order\u201d is removed, because word order in Japanese is relatively free.", 
        "54": "Secondly, the condition \u201cS should include all content words of H\u201d is relaxed to \u201cthe ratio of shared content words to content words in H is larger than threshold \u03b8\u201d because in Japanese, subjects and objects that can be easily estimated from the context are often omitted (characteristics (c)).", 
        "55": "In addition, two conditions \u201cH should have a verb\u201d and \u201cH must not begin with a verb\u201d are removed, leaving four conditions5.", 
        "56": "3 Sentence compression with LSTM  Three models are used for sentence compression.", 
        "57": "Each model predicts a label sequence, where the label for each word is either \u201cretain\u201d or \u201cdelete\u201d (Filippova et al., 2015).", 
        "58": "The first is an encoder-decoder model with LSTM (lstm) (Sutskever et al., 2014).", 
        "59": "Given an input sequence x = (x1, x2, .", 
        "60": ".", 
        "61": ".", 
        "62": ", xn), yt in label sequence y = (y1, y2, .", 
        "63": ".", 
        "64": ".", 
        "65": ", yn) is computed as follows:\ns0 = encoder(x)\n(st,mt) = decoder(st\u22121,mt\u22121, e1(xt)\u2295el(yt\u22121)) yt = softmax(W \u2217 st + b),\n5\u201cH is not a question\u201d, \u201cboth H and S are not less than four words.\u201d, \u201cS is more than 1.5 times longer than H\u201d, and \u201cS is more than 1.5 times longer than C\u201d.", 
        "66": "where\u2295 indicates concatenation, st andmt are respectively a hidden state and a memory cell at time t, and e1(word) is embedding of word.", 
        "67": "If label is \u201cretain\u201d, el(label) is (1; 0); otherwise, (0; 1).", 
        "68": "m0 is a zero vector.", 
        "69": "As the second and the third models, we extend the first model to control the output length (Kikuchi et al., 2016).", 
        "70": "The second model, lstm+leninit, initializes the memory cell of the decoder as follows: m0 = tarlen\u2217blen where tarlen is the desired output length, and blen is a trainable parameter.", 
        "71": "The third model, lstm+lenemb, uses the embedding of the potential desired length e2(length) as an additional input.", 
        "72": "In this case, e1(xt) \u2295 el(yt\u22121) \u2295 e2(lt) is used as the input of the decoder where lt is the potential desired length at time t.  4 Experiments  The created training datasets were used to train three models for sentence compression.", 
        "73": "To see the effect of the modified subtree extraction method (Section 2.3), two training datasets were tested: rooted and multi-root+.", 
        "74": "rooted includes only deletion of the leafs in a dependency tree.", 
        "75": "In contrast, multi-root+ includes deleting the global root and reflecting abbreviated forms besides it.", 
        "76": "Setting: Training datasets were created from seven million, 35-years\u2019 worth of news articles from the Mainichi, Nikkei, and Yomiuri newspapers, from which duplicate sentences and sentences in test data were filtered out.", 
        "77": "Gold-standard data were composed of the first sentences of the 1,000 news articles from Mainichi, each of which has 5 compressed sentences separately created by five human annotators.", 
        "78": "100 sentences of the gold-standard data were used as development data, while the other sentences were used as test data.", 
        "79": "The three models were trained on datasets created with each value of threshold \u03b86, which is the parameter used in the condition (introduced in Sec-\n6In Table 2, the thresholds for our models are 0.7, 0.5, 0.3, 0.6, 0.4, and 0.2, respectively, from the top.", 
        "80": "tion 2.4).", 
        "81": "The properties of the dataset in relation to \u03b8 are shown in Table 1. \u03b8 is tuned for ROUGE2 score on the development data.", 
        "82": "In Table 1, we show the tendency of the ROUGE-2 scores when lstm+leninit was trained on created rooted with each \u03b8.", 
        "83": "From Table 1, we chose 0.5 as \u03b8.", 
        "84": "All models are three stacked LSTM layers7.", 
        "85": "Words with frequency lower than five are regarded as unknown words.", 
        "86": "ADAM8 was used as the optimization method.", 
        "87": "The desired length was set to the bytes of a compressed sentence randomly chosen from the five human-generated sentences.", 
        "88": "In the test step, beam-search was used (beam size: 20) and candidates exceeding the desired length were truncated.", 
        "89": "tree-base and prop-w-dpnd: Existing methods for Japanese sentence compression are not based on the training on a large dataset.", 
        "90": "Therefore, the proposed method is compared with two methods, tree-base, (Filippova and Strube, 2008) and propw-dpnd (Harashima and Kurohashi, 2012), which are not based on supervised learning.", 
        "91": "tree-base is implemented as an integer linear programming problem that finds a subtree of a dependency tree.", 
        "92": "prop-w-dpnd is also implemented as an integer linear programming problem, but modified based on characteristics of the Japanese language.", 
        "93": "prop-wdpnd allows the deletion inside the chunks.", 
        "94": "4.1 Automatic evaluation  Table 2 shows the ROUGE score of each model.", 
        "95": "We used ROUGE-1, ROUGE-2, and ROUGEL (R-1, R-2, and R-L) for evaluation.", 
        "96": "In addition, we also show the precision and F-measure of ROUGE-2 because the output length of each model is not same.", 
        "97": "Compression ratio (CR) is the ratio of the bytes of the compressed sentence to the bytes of the source sentence.", 
        "98": "Average\n7Dimension of word embedding and hidden layer:256, dropout:20%\n8\u03b1:0.001, \u03b21:0.9, \u03b22:0.999, eps:1e-8, batchsize:180\nCR of the test data is approximately 61.0%.", 
        "99": "We think we should focus on F-measure of ROUGE2 because of the different CR of the models.", 
        "100": "The models trained on a large training dataset achieved higher F-measure than the unsupervised models.", 
        "101": "Moreover, F-measure of lstm+leninit and lstm+lenemb, which were trained on either multiroot+ or rooted, is significantly better than propw-dpnd (p < 0.001).", 
        "102": "lstm achieved lower R-1 and R-2 than the other models trained on a large training dataset, probably because it tends to generate too short sentences, as indicated by low CR.", 
        "103": "It is also noteworthy that CRs of lstm+leninit and lstm+lenemb are mostly closer to the average CR of the test data than lstm.", 
        "104": "Furthermore, lstm+leninit and lstm+lenemb trained on multiroot+ instead of rooted worked better in terms of F-measure.", 
        "105": "We consider it is because various types of deletion make the compression model more flexible, as indicated by closer CR of the model trained on multi-root+ instead of rooted to the average CR of the test data.", 
        "106": "4.2 Human evaluation  The difference between lstm+leninit and lstm+lenemb trained on multi-root+ was investigated first.", 
        "107": "With lstm+leninit, 2 out of 100 sentences, chosen randomly, ended with a word that cannot be located at the end of a sentence.", 
        "108": "In contrast, with lstm+lenemb, 24 sentences ended with such words and therefore are ungrammatical, although lenemb has shown to be effective in abstractive sentence summarization (Kikuchi et al., 2016).", 
        "109": "This result suggests that lstm+lenemb is excessively affected by the desired length because lenemb receives the potential desired length at each time of decoding.", 
        "110": "In fact, 21 out of the 24 sentences are as long as the desired length.", 
        "111": "Then, lstm+leninit trained on multi-root+ was evaluated by crowdsourcing in comparison with the gold-standard and tree-base.", 
        "112": "Each crowd-\nsourcing worker reads each source sentence and a set of the compressed sentences, reordered randomly, and gives a score from 1 to 5 (where 5 is best) to each compressed sentence in terms of informativeness (info) and readability (read).", 
        "113": "As shown in Table 3, in terms of both read and info, lstm+leninit archived higher scores than prop-wdpnd, and the difference is significant (p< 0.001).", 
        "114": "Next, lstm+leninit was compared with lstm (both are trained on multi-root+), and multi-root+ was compared with rooted (lstm+leninit was used as the model), by human relative evaluation.", 
        "115": "In this evaluation, each worker votes for one of lstm+leninit, lstm, or tie, and for one of multiroot+, rooted, or tie.", 
        "116": "250 votes in total were received (50 sentences \u00d7 5 votes).", 
        "117": "The results are shown in Table 4. lstm+leninit is better than lstm in terms of read, which is not directly related to the output length, as well as of info.", 
        "118": "It is also clear that multi-root+ achieves much higher info with a negligible reduction in read than rooted.", 
        "119": "5 Conclusion  Filippova et al.\u2019s method of creating training data for English sentence compression was modified to create a training dataset for Japanese sentence compression.", 
        "120": "The effectiveness of the created Japanese training dataset was verified by automatic and human evaluation.", 
        "121": "Our method can refine the created dataset by giving more flexibility in compression and achieved better informativeness with negligible reduction in readability.", 
        "122": "Furthermore, it has been shown that controlling the output length improves the performance of the sentence compression models.", 
        "123": "Acknowledgment  This work was supported by Google Research Award, JSPS KAKENHI Grant Number JP26280080, and JPMJPR1655."
    }, 
    "document_id": "P17-2044.pdf.json"
}
