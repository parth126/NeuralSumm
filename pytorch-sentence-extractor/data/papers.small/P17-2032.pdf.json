{
    "abstract_sentences": {
        "1": "We investigate the problem of sentence-level supporting argument detection from relevant documents for user-specified claims.", 
        "2": "A dataset containing claims and associated citation articles is collected from online debate website idebate.org.", 
        "3": "We then manually label sentence-level supporting arguments from the documents along with their types as STUDY, FACTUAL, OPINION, or REASONING.", 
        "4": "We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task.", 
        "5": "Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 203\u2013208 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2032  1 Introduction  Argumentation plays a crucial role in persuasion and decision-making processes.", 
        "2": "An argument usually consists of a central claim (or conclusion) and several supporting premises.", 
        "3": "Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning (Rieke et al., 1997; Park and Cardie, 2014).", 
        "4": "For instance, as shown in Figure 1, the editor on idebate.org \u2013 a Wikipedia-style website for gathering pro and con arguments on controversial issues, utilizes arguments based on study, factual evidence, and expert opinion to support the anti-gun claim \u201clegally owned guns are frequently stolen and used by criminals\u201d.", 
        "5": "However, it would require substantial human effort to collect information from diverse resources to support argument construction.", 
        "6": "In order to facilitate this process, there is a pressing need for tools that can automatically detect supporting arguments.", 
        "7": "To date, most of the argument mining research focuses on recognizing argumentative components\nand their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016).", 
        "8": "Limited work has been done for retrieving supporting arguments from external resources.", 
        "9": "Initial effort by Rinott et al.", 
        "10": "(2015) investigates the detection of relevant factual evidence from Wikipedia articles.", 
        "11": "However, it is unclear whether their method can perform well on documents of different genres (e.g.", 
        "12": "news articles vs. blogs) for detecting distinct types of supporting information.", 
        "13": "In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim.", 
        "14": "Take Figure 2 as an example: assume we are given a claim on the topic of \u201cbanning cosmetic surgery\u201d and a relevant article (cited for argument construction), we aim to automatically pinpoint the sentence(s) (in italics) among all sentences in the cited article that can be used to back up the claim.", 
        "15": "We define such tasks as supporting argument detection.", 
        "16": "Furthermore, another goal of\n203\nthis work is to understand and characterize different types of supporting arguments.", 
        "17": "Indeed, human editors do use different types of information to promote persuasiveness as we will show in Section 3.", 
        "18": "Prediction performance also varies among different types of supporting arguments.", 
        "19": "Given that none of the existing datasets is suitable for our study, we collect and annotate a corpus from Idebate, which contains hundreds of debate topics and corresponding claims.1 As is shown in Figure 2, each claim is supported with some human constructed argument, with cited articles marked on sentence level.", 
        "20": "After careful inspection on the supporting arguments, we propose to label them as STUDY, FACTUAL, OPINION, or REASONING.", 
        "21": "Substantial inter-annotator agreement rate is achieved for both supporting argument labeling (with Cohen\u2019s \u03ba of 0.8) and argument type annotation, on 200 topics with 621 reference articles.", 
        "22": "Based on the new corpus, we first carry out a study on characterizing arguments of different types via type prediction.", 
        "23": "We find that arguments\n1The labeled dataset along with the annotation guideline will be released at xyhua.me.", 
        "24": "of STUDY and FACTUAL tend to use more concrete words, while arguments of OPINION contain more named entities of person names.", 
        "25": "We then investigate whether argument type can be leveraged to assist supporting argument detection.", 
        "26": "Experimental results based on LambdaMART (Burges, 2010) show that utilizing features composite with argument types achieves a Mean Reciprocal Rank (MRR) score of 57.65, which outperforms an unsupervised baseline and the same ranker trained without type information.", 
        "27": "Feature analysis also demonstrates that salient features have significantly different distribution over different argument types.", 
        "28": "For the rest of the paper, we summarize related work in Section 2.", 
        "29": "The data collection and annotation process is described in Section 3, which is followed by argument type study (Section 4).", 
        "30": "Experiment on supporting argument detection is presented in Section 5.", 
        "31": "We finally conclude in Section 6.", 
        "32": "2 Related Work  Our work is in line with argumentation mining, which has recently attracted significant research interest.", 
        "33": "Existing work focuses on argument extraction from news articles, legal documents, or online comments without given userspecified claim (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011; Park and Cardie, 2014).", 
        "34": "Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments.", 
        "35": "To the best of our knowledge, none of them studies the interaction between types of arguments and their usage to support a user-specified claim.", 
        "36": "This is the gap we aim to fill.", 
        "37": "3 Data and Annotation  We rely on data from idebate.org, where human editors construct paragraphs of arguments, either supporting or opposing claims under controversial topics.", 
        "38": "We also extract textual citation articles as source of information used by editors during argument construction.", 
        "39": "In total we collected 383 unique debates, out of which 200 debates are randomly selected for study.", 
        "40": "After removing invalid ones, our final dataset includes 450 claims\nand 621 citation articles with about 53,000 sentences.", 
        "41": "Annotation Process.", 
        "42": "As shown in Figure 2, we first annotate which sentence(s) from a citation articles is used by the editor as supporting arguments.", 
        "43": "Then we annotate the type for each of them as STUDY, FACTUAL, OPINION, or REASONING, based on the scheme in Table 1.2 For instance, the highlighted supporting argument in Figure 2 is labeled as REASONING.", 
        "44": "Two experienced annotators were hired to identify supporting arguments by reading through the whole cited article and locating the sentences that best match the reference human constructed argument.", 
        "45": "This task is rather complicated since human do not just repeat or directly quote the original sentences from citation articles, they also paraphrase, summarize, and generalize.", 
        "46": "For instance, the original sentence is \u201cThe global counterfeit drug trade, a billion-dollar industry, is thriving in Africa\u201d, which is paraphrased to \u201cThis is exploited by the billion dollar global counterfeit drug trade\u201d in human constructed argument.", 
        "47": "The annotators were asked to annotate independently, then discuss and resolve disagreements and give feedback about current scheme.", 
        "48": "We compute inter-annotator agreement based on Cohen\u2019s \u03ba for both supporting arguments labeling and argument type annotation.", 
        "49": "For supporting arguments we have a high degree of consensus, with Cohen\u2019s \u03ba ranges from 0.76 to 0.83 in all rounds and 0.80 overall.", 
        "50": "For argument type annotation, we achieve Cohen\u2019s \u03ba of 0.61 for STUDY, 0.75 for FACTUAL, 0.71 for OPINION, and 0.29 for REASONING3\n2We end up with the four-type scheme as a trade-off between complexity and its coverage of the arguments.", 
        "51": "3Many times annotators have different interpretation on REASONING, and frequently label it as OPINION.", 
        "52": "This results\nStatistics.", 
        "53": "In total 995 sentences are identified as supporting arguments.", 
        "54": "Among those, 95 (9.55%) are labeled as STUDY, 497 (49.95%) as FACTUAL, 363 (36.48%) as OPINION, and 40 (4.02%) as REASONING.", 
        "55": "We further analyze the source of the supporting arguments.", 
        "56": "Domain names of the citation articles are collected based on their URL, and then categorized into \u201cnews\u201d, \u201corganization\u201d, \u201cscientific\u201d, \u201cblog\u201d, \u201creference\u201d, and others, according to a taxonomy provided by Alexa4 with a few edits to fit our dataset.", 
        "57": "News articles are the major source for all types, which account for roughly 50% for each.", 
        "58": "We show the distribution of other four types in Figure 3.", 
        "59": "Arguments of STUDY and REASONING are mostly from \u201cscientific\u201d websites (14.9% and 22.9%), whereas \u201corganization\u201d websites contribute a large portion of arguments of FACTUAL (18.5%) and OPINION (16.7%).", 
        "60": "4 A Study On Argument Type Prediction  Here we characterize arguments of different types based on diverse features under the task of predicting argument types.", 
        "61": "Supporting arguments identified from previous section are utilized for experiments.", 
        "62": "We also leverage the learned classifier in this section to label the sentences that are not supporting arguments, which will be used for supporting argument detection in the next section.", 
        "63": "Four major types of features are considered.", 
        "64": "Basic Features.", 
        "65": "We calculate frequencies of unigram and bigram words, number of four major types of part-of-speech tags (verb, noun, adjective, and adverb), number of dependency relations, and\nin a low agreement for REASONING.", 
        "66": "4http://www.alexa.com/topsites/category\nnumber of seven types of named entities (Chinchor and Robinson, 1997).", 
        "67": "Sentiment Features.", 
        "68": "We also compute number of positive, negative and neutral words in MPQA lexicon (Wilson et al., 2005), and number of words from a subset of semantic categories from General Inquirer (Stone et al., 1966).5 Discourse Features.", 
        "69": "We use the number of discourse connectives from the top two levels of Penn Discourse Tree Bank (Prasad et al., 2007).", 
        "70": "Style Features.", 
        "71": "We measure word attributes for their concreteness (perceptible vs. conceptual), valence (or pleasantness), arousal (or intensity of emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al.", 
        "72": "(2014) and Warriner et al.", 
        "73": "(2013).", 
        "74": "We utilize Log-linear model for argument type prediction with one-vs-rest setup.", 
        "75": "Three baselines are considered: (1) random guess, (2) majority class, and (3) unigrams and bigrams as features for Log-linear model.", 
        "76": "Identified supporting arguments are used for experiments, and divided into training set (50%), validation set (25%) and test set (25%).", 
        "77": "From Table 2, we can see that Loglinear model trained with all features outperforms the ones trained with ngram features.", 
        "78": "To further characterize arguments of different types, we display sample features with significant different values in Figure 4.", 
        "79": "As can be seen, arguments of STUDY and FACTUAL tend to contain more concrete words and named entities.", 
        "80": "Arguments of OPINION mention more person names, which implies that expert opinions are commonly quoted.", 
        "81": "5 Supporting Argument Detection  We cast the sentence-level supporting argument detection problem as a ranking task.6 Features\n5Categories used: Strong, Weak, Virtue, Vice, Ovrst (Overstated), Undrst (Understated), Academ (Academic), Doctrin (Doctrine), Econ (Economic), Relig (Religious), Causal, Ought, and Perceiv (Perception).", 
        "82": "6Many sentences in the citation article is relevant to the topic to various degrees.", 
        "83": "We focus on detecting the most relevant ones, and thus treat it as a ranking problem instead of a\nin Section 4 are also utilized here as \u201cSentence features\u201d with additional features considering the sentence position in the article.", 
        "84": "We further employ features that measure similarity between claims and sentences, and the composite features that leverage argument type information.", 
        "85": "Similarity Features.", 
        "86": "We compute similarity between claim and candidate sentence based on TFIDF and average word embeddings.", 
        "87": "We also consider ROUGE (Lin, 2004), a recall oriented metric for summarization evaluation.", 
        "88": "In particular, ROUGE-L, a variation based on longest common subsequence, is computed by treating claim as reference and each candidate sentence as sample summary.", 
        "89": "In similar manner we use BLEU (Papineni et al., 2002), a precision oriented metric.", 
        "90": "Composite Features.", 
        "91": "We adopt composite features to study the interaction of other features with type of the sentence.", 
        "92": "Given claim c and sentence s with any feature mentioned above, a composite feature function \u03c6M(type, feature)(s, c) is set to the actual feature value if and only if the argument type matches.", 
        "93": "For instance, if the ROUGE-L score is 0.2, and s is of type STUDY, then \u03c6M(study, ROUGE)(s, c) = 0.2 \u03c6M(factual, ROUGE)(s, c), \u03c6M(opinion, ROUGE)(s, c), \u03c6M(reasoning, ROUGE)(s, c) are all set to 0.\nbinary classification task.", 
        "94": "We choose LambdaMART (Burges, 2010) for experiments, which is shown to be successful for many text ranking problems (Chapelle and Chang, 2011).", 
        "95": "Our model is evaluated by Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) using 5-fold cross validation.", 
        "96": "We compare to TFIDF and Word embedding similarity baselines, and LambdaMART trained with ngrams (unigrams and bigrams).", 
        "97": "Results in Table 3 show that using composite features with argument type information (Comp(type, Sen) + Comp(type, Simi)) can improve the ranking performance.", 
        "98": "Specifically, the best performance is achieved by adding composite features to sentence features, similarity features, and ngram features.", 
        "99": "As can be seen, supervised methods outperform unsupervised baseline methods.", 
        "100": "And similarity features have similar performance as those baselines.", 
        "101": "The best performance is achieved by combination of sentence features, Ngrams, similarity, and two composite types, which is boldfaced.", 
        "102": "Feature sets that significantly outperform all three baselines are marked with \u2217.", 
        "103": "For feature analysis, we conduct t-test for individual feature values between supporting arguments and the others.", 
        "104": "We breakdown features according to their argument types and show top salient composite features in Table 4.", 
        "105": "For all sentences of type STUDY, relevant ones tend to contain more \u201cpercentage\u201d and more concrete words.", 
        "106": "We also notice those sentences with more hedging words are more likely to be considered.", 
        "107": "For sentences of FACTUAL, position of sentence in article\nplays an important role, as well as their similarity to the claim based on ROUGE scores.", 
        "108": "For type OPINION, unlike all other types, position of sentence seems to be insignificant.", 
        "109": "As we could imagine, opinionated information might scatter around the whole documents.", 
        "110": "For sentences of REASONING, the ones that can be used as supporting arguments tend to be less concrete and less emotional, as opposed to opinion.", 
        "111": "6 Conclusion  We presented a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim.", 
        "112": "Based on our newly-collected dataset, we characterized arguments of different types with a rich feature set.", 
        "113": "We also showed that leveraging argument type information can further improve the performance of supporting argument detection.", 
        "114": "Acknowledgments  This work was supported in part by National Science Foundation Grant IIS-1566382 and a GPU gift from Nvidia.", 
        "115": "We thank Kechen Qin for his help on data collection.", 
        "116": "We also appreciate the valuable suggestions on various aspects of this work from three anonymous reviewers."
    }, 
    "document_id": "P17-2032.pdf.json"
}
