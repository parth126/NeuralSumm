{
    "abstract_sentences": {
        "1": "Recently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks.", 
        "2": "Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods.", 
        "3": "This work proposes a direct hidden Markov model (HMM) with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm.", 
        "4": "The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% BLEU scores on two different translation tasks."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 125\u2013131 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2020\nRecently, the neural machine translation systems showed their promising performance and surpassed the phrase-based systems for most translation tasks.", 
        "2": "Retreating into conventional concepts machine translation while utilizing effective neural models is vital for comprehending the leap accomplished by neural machine translation over phrase-based methods.", 
        "3": "This work proposes a direct hidden Markov model (HMM) with neural network-based lexicon and alignment models, which are trained jointly using the Baum-Welch algorithm.", 
        "4": "The direct HMM is applied to rerank the n-best list created by a state-of-the-art phrase-based translation system and it provides improvements by up to 1.0% BLEU scores on two different translation tasks.", 
        "5": "1 Introduction  The hidden Markov model (HMM) was first introduced to statistical machine translation for addressing the word alignment problem (Vogel et al., 1996).", 
        "6": "Then the HMM-based approach was widely used along with the IBM models (Brown et al., 1993) for aligning the source and target words.", 
        "7": "In the conventional approach, the Bayes\u2019 theorem is used and the HMM is applied to the inverse translation model\nPr(eI1|fJ1 ) = Pr(eI1) \u00b7 Pr(fJ1 |eI1) = \u2211\naJ1\nPr(fJ1 , a J 1 |eI1) (1)\nIn this case, as a part of a noisy channel model, the marginalisation becomes intractable for every e.\nThis work proposes a novel concept focusing on direct HMM for Pr(eI1|fJ1 ), in which the alignment direction is from target to source positions.", 
        "8": "This specific property allows us to introduce dependencies into the translation model that take the full source sentence into account.", 
        "9": "This aspect will be important for the future decoder to be developed.", 
        "10": "The lexicon and alignment probabilities in the HMM are modeled using feedforward neural networks (FFNN) and they are trained jointly.", 
        "11": "The trained HMM is then applied for reranking the n-best lists created by a state-of-the-art open source phrase-based translation system.", 
        "12": "The experiments are conducted on the IWSLT 2016 German\u2192English and BOLT Chinese\u2192English translation tasks.", 
        "13": "The FFNNbased hybrid HMM provides improvements by up to 1.0% BLEU scores.", 
        "14": "2 Related Work  In order to discuss related work, we will consider the following two key concepts that are essential for the work to be presented:\n\u2022 Neural lexicon and alignment models The idea of using neural networks for lexicon modeling is not new (Schwenk, 2012; Sundermeyer et al., 2014; Devlin et al., 2014).", 
        "15": "Apart from differences in the neural network architecture, the important difference to this work is that those approaches did not include the concepts of HMM models and end-to-end training.", 
        "16": "In addition to neural lexicon modeling, (Alkhouli et al., 2016) also applied a neural network for alignment modeling like this work, but their training procedure was based on the maximum approximation and on predefined GIZA++ (Och and Ney, 2003) alignments.", 
        "17": "125\nThere were other studies that focused on feature-rich alignment models (Blunsom and Cohn, 2006; Berg-Kirkpatrick et al., 2010; Dyer et al., 2011), but those studies did not use a neural network to automatically learn features (as we do in this work).", 
        "18": "(Yang et al., 2013) used neural network-based lexicon and alignment models inside the HMM alignment model, but they model alignments using a simple distortion model that has no dependence on lexical context.", 
        "19": "Their goal was to improve the alignment quality in the context of a phrase-based translation system.", 
        "20": "However, apart from (Dyer et al., 2011), no results on translation were reported.", 
        "21": "The idea of using neural networks is the basis of the state-of-the-art attention-based approach to machine translation (Bahdanau et al., 2015; Luong et al., 2015).", 
        "22": "However, that approach is not based on the principle of an explicit and separate lexicon model.", 
        "23": "\u2022 End-to-end training The HMM in combination with the neural translation model lends itself to what is usually called end-to-end training.", 
        "24": "The training criterion is the logarithm of the target sentence posterior probability.", 
        "25": "This criterion results in a specific training algorithm that can be interpreted as a combination of forwardbackward algorithm (as in EM style training of HHMs) and backpropagation.", 
        "26": "To the best of our knowledge, this end-to-end training has not been considered before for machine translation.", 
        "27": "In the context of signal processing and recognition, the connectionist temporal classification (CTC) approach (Graves et al., 2006) leads to a similar training procedure.", 
        "28": "(Tran et al., 2016) studied neural networks for unsupervised training for a part-ofspeech tagging task.", 
        "29": "In their approach, the training criterion for this problem results in a combination of EM framework and backpropagation, which has a certain similarity to the training algorithm for translation as presented in this work.", 
        "30": "3 Definition of neural network-based HMM  Similar to hidden alignments aj = j \u2192 i between the source string fJ1 = f1...fj ...fJ and the target\nstring eI1 = e1...ei...eI in the conventional HMM, we define the alignments in direct HMM as bi = i\u2192 j.", 
        "31": "Then the model can be defined as:\nPr(eI1|fJ1 ) = \u2211\nbI1\nPr(eI1, b I 1|fJ1 ) (2)\nPr(eI1, b I 1|fJ1 ) = I\u220f\ni=1\np(ei, bi|bi\u221211 , ei\u221211 , fJ1 )\n= I\u220f\ni=1 p(ei|bi1, ei\u221211 , fJ1 )\ufe38 \ufe37\ufe37 \ufe38 lexicon model \u00b7 p(bi|bi\u221211 , ei\u221211 , fJ1 )\ufe38 \ufe37\ufe37 \ufe38 alignment model\n(3)\nOur feed-forward alignment model has the same architecture (Figure 1) as the one proposed in (Alkhouli et al., 2016).", 
        "32": "Thus the alignment probability can be modeled by:\np(bi|bi\u221211 , ei\u221211 , fJ1 ) = p(\u2206i|f bi\u22121+\u03b3m bi\u22121\u2212\u03b3m , e i\u22121 i\u2212n)\n(4) where \u03b3m = m\u221212 and m indicates the window size.", 
        "33": "\u2206i = bi \u2212 bi\u22121 denotes the jump from the predecessor position to the current position.", 
        "34": "Thus, the jump over the source is estimated based on a m-words source context window and n predecessor target words.", 
        "35": "For the lexicon model, we assume a similar dependence as in the alignment model with a shift, namely on the source words within a window centred on the aligned source word and n predecessor target words.", 
        "36": "To overcome the high costs of the softmax function for large vocabularies, we adopt the class-factored output layer consisting of a class layer and a word layer (Goodman, 2001; Morin\nand Bengio, 2005).", 
        "37": "The model in this case is defined as\np(ei|bi1, ei\u221211 , fJ1 ) = p(ei|f bi+\u03b3mbi\u2212\u03b3m , e i\u22121 i\u2212n)\n= p(ei|c(ei), f bi+\u03b3mbi\u2212\u03b3m , e i\u22121 i\u2212n) \u00b7 p(c(ei)|f bi+\u03b3m bi\u2212\u03b3m , e i\u22121 i\u2212n)\n(5) where c denotes a word mapping that assigns each target word to a single class, where the number of classes is chosen to be much smaller than the vocabulary size.", 
        "38": "The lexicon model architecture is shown in Figure 2.", 
        "39": "4 Training  The training data of the direct HMM are the source and target sequences, without any alignment information.", 
        "40": "In the training of direct HMM including neural network-based models, the weights have to be updated along with the posterior probabilities calculated by the Baum-Welch algorithm.", 
        "41": "Similar to the training procedure used in (BergKirkpatrick et al., 2010), we apply the EM algorithm and define the auxiliary function as\nQ(\u03b8; \u03b8\u0302)\n= \u2211\nbI1\np(bI1|fJ1 , eI1, \u03b8) log p(eI1, bI1|fJ1 , \u03b8\u0302)\n= \u2211\nbI1\np(bI1|fJ1 , eI1, \u03b8) I\u2211\ni=1\n[log p(ei|fbi+\u03b3mbi\u2212\u03b3m , e i\u22121 i\u2212n, \u03b1\u0302)\n+ log p(\u2206i|fbi\u22121+\u03b3mbi\u22121\u2212\u03b3m , e i\u22121 i\u2212n, \u03b2\u0302)]\n= \u2211\ni\n\u2211\nj\npi(j|eI1, fJ1 , \u03b8) log p(ei|f j+\u03b3mj\u2212\u03b3m , e i\u22121 i\u2212n, \u03b1\u0302)\n+ \u2211\ni\n\u2211\nj\u2032\n\u2211\nj\npi(j \u2032, j|eI1, fJ1 , \u03b8) log p(\u2206i|f j \u2032+\u03b3m j\u2032\u2212\u03b3m , e i\u22121 i\u2212n, \u03b2\u0302)\n(6)\nwhere \u03b8\u0302 = {\u03b1\u0302, \u03b2\u0302}, j\u2032 = bi\u22121 and\npi(j|eI1, fJ1 , \u03b8) = \u2211\nbI1:bi=j\np(bI1|eI1, fJ1 , \u03b8) (7)\nThen the parameters can be separated for lexicon model and alignment model:\nQ(\u03b8; \u03b8\u0302) = Qlex(\u03b8; \u03b1\u0302) +Qalign(\u03b8; \u03b2\u0302) (8)\nwhere\n\u2202Qlex(\u03b8, \u03b1\u0302)\n\u2202\u03b1\u0302 =\n\u2211\ni\n\u2211\nj\nforward-backward algorithm\ufe37 \ufe38\ufe38 \ufe37 pi(j|eI1, fJ1 , \u03b8)\n\u00b7 \u2202 \u2202\u03b1\u0302 log p(ei|f j+\u03b3mj\u2212\u03b3m , e i\u22121 i\u2212n, \u03b1\u0302)\n\ufe38 \ufe37\ufe37 \ufe38 backpropagation\n(9)\n\u2202Qalign(\u03b8, \u03b2\u0302)\n\u2202\u03b2\u0302 =\n\u2211\ni\n\u2211\nj\u2032\n\u2211\nj\nforward-backward algorithm\ufe37 \ufe38\ufe38 \ufe37 pi(j \u2032, j|eI1, fJ1 , \u03b8)\n\u00b7 \u2202 \u2202\u03b2\u0302 log p(\u2206i|f j \u2032+\u03b3m j\u2032\u2212\u03b3m , e i\u22121 i\u2212n, \u03b2\u0302)\n\ufe38 \ufe37\ufe37 \ufe38 backpropagation\n(10) From Equations (9) and (10) we can observe that the marginalisation of hidden alignments ( \u2211\nj pi(j|eI1, fJ1 , \u03b8)) is the only difference compared to the derivative of neural network training based on word-aligned data.", 
        "42": "In this approach we iterate over all source positions and the word alignment toolkit such as GIZA++ is not required.", 
        "43": "Furthermore, the word-aligned data generated e.g.", 
        "44": "by GIZA++ might contain unaligned and multiply aligned words, which make the data difficult to use for training neural networks.", 
        "45": "Thus the heuristicbased approaches (Sundermeyer et al., 2014; Devlin et al., 2014) have to be used in order to guarantee the one-on-one alignments, which may negatively influence the quality of the alignments.", 
        "46": "By contrast, the neural network-based HMM is not constrained by these heuristics.", 
        "47": "In addition, even though the training process of the direct HMM takes more time than the neural network training on the word-aligned data, we should note that generating the word-aligned data using GIZA++ is also a time-consuming process.", 
        "48": "In general, our training procedure can be summarized as follows:\n1.", 
        "49": "One iteration IBM-1 model training to create lexicon table for initializing the forwardbackward table.", 
        "50": "2.", 
        "51": "In the first epoch, for each sentence pair calculate and save the entire table of posterior probabilities pi(b|eI1, fJ1 ) (also pi(b\n\u2032, b|eI1, fJ1 ) for alignment model) using forward-backward algorithm based on the results of IBM-1 model.", 
        "52": "3.", 
        "53": "Training neural network lexicon and alignment models based on the posterior probabilities.", 
        "54": "4.", 
        "55": "From the second epoch onwards:\n(a) For each sentence pair, calculating the posterior probabilities based on the lexicon and alignment probabilities estimated by neural network models.", 
        "56": "(b) Updating weights of neural networks based on the posterior probabilities.", 
        "57": "(c) Repeating step 4 until the perplexity converges.", 
        "58": "In this work the IBM-1 initialization is required.", 
        "59": "We tried to train neural network models from scratch, but the perplexity converges towards a bad local minimum and gets stuck in it.", 
        "60": "We also attempted other heuristics for initialization, such as assigning probability 0.9 to diagonal alignments and spreading the left 0.1 evenly among other source positions.", 
        "61": "The resulted perplexity is much higher compared to initializing using IBM-1.", 
        "62": "5 Experimental Results  The experiments are conducted on the IWSLT 2016 German\u2192English and BOLT Chinese\u2192English translation tasks, which consist of 20M and 4M parallel sentence pairs respectively.", 
        "63": "The feed-forward neural network alignment and lexicon models are jointly trained on the subset of about 200K sentence pairs.", 
        "64": "As an initial research of this topic, our new model is only applied for reranking n-best lists created by a phrase-based decoder.", 
        "65": "The maximum size of the n-best lists is 500.", 
        "66": "The translation quality is evaluated by case-insensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics using MultEval (Clark et al., 2011).", 
        "67": "The scaling factors are tuned with MERT (Och, 2003) with BLEU as optimization criterion on the development sets.", 
        "68": "For the translation experiments, the\naveraged scores are presented on the development set from three optimization runs.", 
        "69": "Our direct HMM consists of a feed-forward neural network lexicon model with following configuration:\n\u2022 Five one-hot input vectors for source words and three for target words\n\u2022 Projection layer size 100 for each word \u2022 Two non-linear hidden layers with 1000 and\n500 nodes respectively\n\u2022 A class-factored output layer with 1000 singleton classes dedicated to the most frequent words, and 1000 classes shared among the rest of the words.", 
        "70": "and a feed-forward neural network alignment model with the same configuration as the lexicon model, except a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from \u2212100 to 100 (Alkhouli et al., 2016).", 
        "71": "We conducted experiments on the source and target window size of both network models.", 
        "72": "Larger source and target windows could not provide significant improvements on BLEU scores, at least for rescoring experiments.", 
        "73": "The model is applied for reranking the n-best lists created by the Jane toolkit (Vilar et al., 2010; Wuebker et al., 2012) with a log-linear framework containing phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (Galley and Manning, 2008), a word class language mode (Wuebker et al., 2013) and an n-gram language model.", 
        "74": "The word alignments used for the training of phrase-tables are generated by GIZA++, which performs the alignment training sequentially for IBM-1, HMM and IBM-4.", 
        "75": "More details about our phrase-based baseline system can be found in (Peter et al., 2015).", 
        "76": "The experimental results are demonstrated in Table 1.", 
        "77": "The rescoring experiments are conducted by adding HMM probability as feature and tuned with MERT.", 
        "78": "The applied attention-based neural network is a neural machine translation system similar to (Bahdanau et al., 2015).", 
        "79": "The decoder and encoder word embeddings are of size 620, the encoder uses a bidirectional layer with 1000 LSTMs (Hochreiter and Schmidhuber, 1997) to encode the source side.", 
        "80": "A layer with 1000 LSTMs\nis used by the decoder.", 
        "81": "The data is converted into subword units using byte pair encoding with 20000 operations (Sennrich et al., 2016).", 
        "82": "During training a batch size of 50 is used.", 
        "83": "More details about our neural machine translation system can be found in (Peter et al., 2016).", 
        "84": "With n-best rescoring, all neural network-based systems achieve significant improvements over the phrase-based system.", 
        "85": "The neural network-based HMMs provide promising performance, even with simple feed-forward neural networks.", 
        "86": "The direct HMM trained by the EM procedure with marginalizing the hidden alignments outperformed the same model trained on the word-aligned data.", 
        "87": "For the rescoring tasks, it provides comparable performance with the attention-based network.", 
        "88": "The neural network-based HMM also helps the phrase-based system achieve comparable results with the stand-alone attention-based system on the German\u2192English task.", 
        "89": "6 Conclusion and Future Work  This work aims to close the gap between the conventional word alignment models and the novel neural machine translation.", 
        "90": "The proposed direct HMM consists of neural network-based alignment and lexicon models, both models are trained jointly and without any alignment information.", 
        "91": "With the simple feed-forward neural network models, the HMM model already provides promising results and significantly improves the strong phrase-based translation system.", 
        "92": "As future work, we are searching for alternatives to initialize the training instead of using IBM-1.", 
        "93": "We will investigate recurrent model struc-\ntures, such as the LSTM representation for source and target word embeddings (Luong et al., 2015).", 
        "94": "In addition to the network structure, we will implement a stand-alone decoder based on this novel model.", 
        "95": "The first step would be to apply maximum approximation for the search problem as elucidated in (Yu et al., 2017).", 
        "96": "Then we plan to investigate heuristics for marginalizing the hidden alignment during search.", 
        "97": "Acknowledgments  The work reported in this paper results from two projects, SEQCLAS and QT21.", 
        "98": "SEQCLAS has received funding from\nthe European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement no 694537.", 
        "99": "QT21 has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement no 645452.", 
        "100": "The work reflects only the authors\u2019 views and neither the European Commission nor the European Research Council Executive Agency are responsible for any use that may be made of the information it contains.", 
        "101": "Tamer Alkhouli was partly funded by the 2016 Google PhD Fellowship for North America, Europe and the Middle East.", 
        "102": "The authors would like to thank Jan-Thorsten Peter for providing the attention-based neural network models."
    }, 
    "document_id": "P17-2020.pdf.json"
}
