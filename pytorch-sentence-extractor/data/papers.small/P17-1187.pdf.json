{
    "abstract_sentences": {
        "1": "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes.", 
        "2": "Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases.", 
        "3": "In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks.", 
        "4": "The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately.", 
        "5": "More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts.", 
        "6": "We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines.", 
        "7": "The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2049\u20132058 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1187  1 Introduction  Sememes are defined as minimum semantic units of word meanings, and there exists a limited close set of sememes to compose the semantic meanings of an open set of concepts (i.e.", 
        "2": "word sense).", 
        "3": "However, sememes are not explicit\n\u2217 indicates equal contribution \u2020Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)\nfor each word.", 
        "4": "Hence, people manually annotate word sememes and build linguistic common-sense knowledge bases.", 
        "5": "HowNet (Dong and Dong, 2003) is one of such knowledge bases, which annotates each concept in Chinese with one or more relevant sememes.", 
        "6": "Different from WordNet (Miller, 1995), the philosophy of HowNet emphasizes the significance of part and attribute represented by sememes.", 
        "7": "HowNet has been widely utilized in word similarity computation (Liu and Li, 2002) and sentiment analysis (Xianghua et al., 2013), and in section 3.2 we will give a detailed introduction to sememes, senses and words in HowNet.", 
        "8": "In this paper, we aim to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space.", 
        "9": "WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al., 2014).", 
        "10": "There have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice balance between effectiveness and efficiency.", 
        "11": "In word2vec, each word corresponds to one single embedding, ignoring the polysemy of most words.", 
        "12": "To address this issue, (Huang et al., 2012) introduces a multiprototype model for WRL, conducting unsupervised word sense induction and embeddings according to context clusters.", 
        "13": "(Chen et al., 2014) further utilizes the synset information in WordNet to instruct word sense representation learning.", 
        "14": "From these previous studies, we conclude that word sense disambiguation are critical for WRL, and we believe that the sememe annotation of word senses in HowNet can provide necessary semantic regularization for the both tasks.", 
        "15": "To explore its feasibility, we propose a novel Sememe-Encoded Word Representation Learning\n2049\n(SE-WRL) model, which detects word senses and learns representations simultaneously.", 
        "16": "More specifically, this framework regards each word sense as a combination of its sememes, and iteratively performs word sense disambiguation according to their contexts and learn representations of sememes, senses and words by extending Skip-gram in word2vec (Mikolov et al., 2013).", 
        "17": "In this framework, an attention-based method is proposed to select appropriate word senses according to contexts automatically.", 
        "18": "To take full advantages of sememes, we propose three different learning and attention strategies for SE-WRL.", 
        "19": "In experiments, we evaluate our framework on two tasks including word similarity and word analogy, and further conduct case studies on sememe, sense and word representations.", 
        "20": "The evaluation results show that our models outperform other baselines significantly, especially on word analogy.", 
        "21": "This indicates that our models can build better knowledge representations with the help of sememe information, and also implies the potential of our models on word sense disambiguation.", 
        "22": "The key contributions of this work are concluded as follows: (1) To the best of our knowledge, this is the first work to utilize sememes in HowNet to improve word representation learning.", 
        "23": "(2) We successfully apply the attention scheme to detect word senses and learn representations according to contexts with the favor of the sememe annotation in HowNet.", 
        "24": "(3) We conduct extensive experiments and verify the effectiveness of incorporating word sememes for improved WRL.", 
        "25": "2 Related Work    2.1 Word Representation  Recent years have witnessed the great thrive in word representation learning.", 
        "26": "It is simple and straightforward to represent words using one-hot representations, but it usually struggles with the data sparsity issue and the neglect of semantic relations between words.", 
        "27": "To address these issues, (Rumelhart et al., 1988) proposes the idea of distributed representation which projects all words into a continuous low-dimensional semantic space, considering each word as a vector.", 
        "28": "Distributed word representations are powerful and have been widely utilized in many NLP tasks, including neural language models (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Sutskever et al., 2014; Bahdanau\net al., 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al., 2015).", 
        "29": "Word distributed representations are capable of encoding semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks.", 
        "30": "There are large amounts of efforts devoted to learning better word representations.", 
        "31": "As the exponential growth of text corpora, model efficiency becomes an important issue.", 
        "32": "(Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency.", 
        "33": "These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts.", 
        "34": "(Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations.", 
        "35": "However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses.", 
        "36": "(Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations and build distinct vectors for each word sense.", 
        "37": "(Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word.", 
        "38": "(Rothe and Schu\u0308tze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset representations in the same semantic space.", 
        "39": "This paper, for the first time, jointly learns representations of sememes, senses and words.", 
        "40": "The sememe annotation in HowNet provides useful semantic regularization for WRL.", 
        "41": "Moreover, the unified representations incorporated with sememes also provide us more explicit explanations of both word and sense embeddings.", 
        "42": "2.2 Word Sense Disambiguation and Representation Learning  Word sense disambiguation (WSD) aims to identify word senses or meanings in a certain context computationally.", 
        "43": "There are mainly two approaches for WSD, namely the supervised methods and the knowledge-based methods.", 
        "44": "Supervised methods usually take the surrounding words or senses as features and use classifiers like SVM for word sense disambiguation (Lee et al., 2004), which are intensively limited to the time-consuming human annotation of training data.", 
        "45": "On contrary, knowledge-based methods utilize\nlarge external knowledge resources such as knowledge bases or dictionaries to suggest possible senses for a word.", 
        "46": "(Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm.", 
        "47": "(Bordes et al., 2011) introduces synset information in WordNet to WRL.", 
        "48": "(Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning.", 
        "49": "(Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations.", 
        "50": "Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies.", 
        "51": "(Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words.", 
        "52": "In this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet.", 
        "53": "To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning.", 
        "54": "3 Methodology  In this section, we present our framework Sememe-Encoded WRL (SE-WRL) that considers sememe information for word sense disambiguation and representation learning.", 
        "55": "Specifically, we learn our models on a large-scale text corpus with the semantic regularization of the sememe annotation in HowNet and obtain sememe, sense and word embeddings for evaluation tasks.", 
        "56": "In the following sections, we first introduce HowNet and the structures of sememes, senses and words.", 
        "57": "Then we discuss the conventional WRL model Skip-gram that we utilize for the sememeencoded framework.", 
        "58": "Finally, we propose three sememe-encoded models in details.", 
        "59": "3.1 Sememes, Senses and Words in HowNet  In this section, we first introduce the arrangement of sememes, senses and words in HowNet.", 
        "60": "HowNet annotates precise senses to each word, and for each sense, HowNet annotates the significance of parts and attributes represented by sememes.", 
        "61": "Fig.", 
        "62": "1 gives an example of sememes, senses and words in HowNet.", 
        "63": "The first layer represents the\nword \u201capple\u201d.", 
        "64": "The word \u201capple\u201d actually has two main senses shown on the second layer: one is a sort of juicy fruit (apple), and another is a famous computer brand (Apple brand).", 
        "65": "The third and following layers are those sememes explaining each sense.", 
        "66": "For instance, the first sense Apple brand indicates a computer brand, and thus has sememes computer, bring and SpeBrand.", 
        "67": "From Fig.", 
        "68": "1 we can find that, sememes of many senses in HowNet are annotated with various relations, such as define and modifier, and form complicated hierarchical structures.", 
        "69": "In this paper, for simplicity, we only consider all annotated sememes of each sense as a sememe set without considering their internal structure.", 
        "70": "HowNet assumes the limited annotated sememes can well represent senses and words in the real-world scenario, and thus sememes are expected to be useful for both WSD and WRL.", 
        "71": "We introduce the notions utilized in the following sections as follows.", 
        "72": "We define the overall sememe, sense and word sets used in training as X , S and W respectively.", 
        "73": "For each w \u2208 W , there are possible multiple senses s(w)i \u2208 S(w) where S(w) represents the sense set ofw.", 
        "74": "Each sense s(w)i consists of several sememes x(si)j \u2208 X (w) i .", 
        "75": "For each target word w in a sequential plain text, C(w) represents its context word set.", 
        "76": "3.2 Conventional Skip-gram Model  We directly utilize the widely-used model Skipgram to implement our SE-WRL model, because Skip-gram has well balanced effectiveness as well as efficiency (Mikolov et al., 2013).", 
        "77": "The standard skip-gram model assumes that word embeddings should relate to their context words.", 
        "78": "It aims at\nmaximizing the predictive probability of context words conditioned on the target word w. Formally, we utilize a sliding window to select the context word set C(w).", 
        "79": "For a word sequence H = {w1, \u00b7 \u00b7 \u00b7 , wn}, Skip-gram model intends to maximize:\nL(H) = n\u2212K\u2211\ni=K\nlog Pr(wi\u2212K , \u00b7 \u00b7 \u00b7 , wi+K |wi), (1)\nwhere K is the size of sliding window.", 
        "80": "Pr(wi\u2212K , \u00b7 \u00b7 \u00b7 , wi+K |wi) represents the predictive probability of context words conditioned on the target word wi, formalized by the following softmax function:\nPr(wi\u2212K , \u00b7 \u00b7 \u00b7 , wi+K |wi) = \u220f\nwc\u2208C(wi) Pr(wc|wi)\n= \u220f\nwc\u2208C(wi)\nexp(w>c \u00b7wi)\u2211 w\u2032i\u2208W exp(w > c \u00b7w\u2032i) ,\n(2)\nin which wc and wi stand for embeddings of context word wc \u2208 C(wi) and target word wi respectively.", 
        "81": "We can also follow the strategies of hierarchical softmax and negative sampling proposed in (Mikolov et al., 2013) to accelerate the calculation of softmax.", 
        "82": "3.3 SE-WRL Model  In this section, we introduce the SE-WRL models with three different strategies to utilize sememe information, including Simple Sememe Aggregation Model (SSA), Sememe Attention over Context Model (SAC) and Sememe Attention over Target Model (SAT).", 
        "83": "3.3.1 Simple Sememe Aggregation Model  The Simple Sememe Aggregation Model (SSA) is a straightforward idea based on Skip-gram model.", 
        "84": "For each word, SSA considers all sememes in all senses of the word together, and represents the target word using the average of all its sememe embeddings.", 
        "85": "Formally, we have:\nw = 1\nm\n\u2211\ns (w) i \u2208S(w)\n\u2211\nx (si) j \u2208X (w) i\nx (si) j , (3)\nwhich means the word embedding of w is composed by the average of all its sememe embeddings.", 
        "86": "Here, m stands for the overall number of sememes belonging to w.\nThis model simply follows the assumption that, the semantic meaning of a word is composed of the semantic units, i.e., sememes.", 
        "87": "As compared to the conventional Skip-gram model, since sememes are shared by multiple words, this model can utilize sememe information to encode latent semantic correlations between words.", 
        "88": "In this case, similar words that share the same sememes may finally obtain similar representations.", 
        "89": "3.3.2 Sememe Attention over Context Model  The SSA Model replaces the target word embedding with the aggregated sememe embeddings to encode sememe information into word representation learning.", 
        "90": "However, each word in SSA model still has only one single representation in different contexts, which cannot deal with polysemy of most words.", 
        "91": "It is intuitive that we should construct distinct embeddings for a target word according to specific contexts, with the favor of word sense annotation in HowNet.", 
        "92": "To address this issue, we come up with the Sememe Attention over Context Model (SAC).", 
        "93": "SAC utilizes the attention scheme to automatically select appropriate senses for context words according to the target word.", 
        "94": "That is, SAC conducts word sense disambiguation for context words to learn better representations of target words.", 
        "95": "The structure of the SAC model is shown in Fig.", 
        "96": "2.", 
        "97": "More specifically, we utilize the original word embedding for target word w, but use sememe embeddings to represent context word wc instead of original context word embeddings.", 
        "98": "Suppose a word typically demonstrates some specific senses in one sentence.", 
        "99": "Here we employ the target word embedding as an attention to select the most appropriate senses to make up context word embeddings.", 
        "100": "We formalize the context word embedding\nwc as follows:\nwc =\n|S(wc)|\u2211\nj=1\natt(s (wc) j ) \u00b7 s (wc) j , (4)\nwhere s(wc)j stands for the j-th sense embedding ofwc, and att(s (wc) j ) represents the attention score of the j-th sense with respect to the target word w, defined as follows:\natt(s (wc) j ) = exp(w \u00b7 s\u0302(wc)j ) \u2211|S(wc)|\nk=1 exp(w \u00b7 s\u0302 (wc) k )\n.", 
        "101": "(5)\nNote that, when calculating attention, we use the average of sememe embeddings to represent each sense s(wc)j :\ns\u0302 (wc) j =\n1\n|X(wc)j |\n|X(wc)j |\u2211\nk=1\nx (sj) k .", 
        "102": "(6)\nThe attention strategy assumes that the more relevant a context word sense embedding is to the target word w, the more this sense should be considered when building context word embeddings.", 
        "103": "With the favor of attention scheme, we can represent each context word as a particular distribution over its sense.", 
        "104": "This can be regarded as soft WSD.", 
        "105": "As shown in experiments, it will help learn better word representations.", 
        "106": "3.3.3 Sememe Attention over Target Model  The Sememe Attention over Context Model can flexibly select appropriate senses and sememes for context words according to the target word.", 
        "107": "The process can also be applied to select appropriate senses for the target word, by taking context words as attention.", 
        "108": "Hence, we propose the Sememe Attention over Target Model (SAT) as shown in Fig.", 
        "109": "3.", 
        "110": "Wt-2 Wt-1 Wt+1 Wt+2\nDifferent from SAC model, SAT learns the original word embeddings for context words, but sememe embeddings for target words.", 
        "111": "We apply context words as attention over multiple senses of the target word w to build the embedding of w, formalized as follows:\nw =\n|S(w)|\u2211\nj=1\natt(s (w) j ) \u00b7 s (w) j , (7)\nwhere s(w)j stands for the j-th sense embedding of w, and the context-based attention is defined as follows:\natt(s (w) j ) = exp(w\u2032c \u00b7 s\u0302(w)j ) \u2211|S(w)|\nk=1 exp(w \u2032 c \u00b7 s\u0302(w)k )\n, (8)\nwhere, similar to Eq.", 
        "112": "(6), we also use the average of sememe embeddings to represent each sense s (w) j .", 
        "113": "Here, w \u2032 c is the context embedding, consisting of a constrained window of word embeddings in C(wi).", 
        "114": "We have:\nw\u2032c = 1\n2K \u2032\nk=i+K\u2032\u2211 k=i\u2212K\u2032 wk, k 6= i.", 
        "115": "(9)\nNote that, since in experiment we find the sense selection of the target word only relies on more limited context words for calculating attention, hence we select a smaller K \u2032 as compared to K.\nRecall that, SAC only uses one target word as attention to select senses of context words, but SAT uses several context words together as attention to select appropriate senses of target words.", 
        "116": "Hence SAT is expected to conduct more reliable WSD and result in more accurate word representations, which will be explored in experiments.", 
        "117": "4 Experiments  In this section, we evaluate the effectiveness of our SE-WRL1 models on two tasks including word similarity and word analogy, which are two classical evaluation tasks mainly focusing on evaluating the quality of learned word representations.", 
        "118": "We also explore the potential of our models in word sense disambiguation with case study, showing the power of our attention-based models.", 
        "119": "1https://github.com/thunlp/SE-WRL  4.1 Dataset  We use the web pages in Sogou-T2 as the text corpus to learn WRL models.", 
        "120": "Sogou-T is provided by a Chinese commercial search engine, which contains 2.7 billion words in total.", 
        "121": "We also utilize the sememe annotation in HowNet.", 
        "122": "The number of distinct sememes used in this paper is 1, 889.", 
        "123": "The average senses for each word are about 2.4, while the average sememes for each sense are about 1.6.", 
        "124": "Throughout the Sogou-T corpus, we find that 42.2% of words have multiple senses.", 
        "125": "This indicates the significance of WSD.", 
        "126": "For evaluation, we choose wordsim-240 and wordsim-2973 to evaluate the performance of word similarity computation.", 
        "127": "The two datasets both contain frequently-used Chinese word pairs with similarity scores annotated manually.", 
        "128": "We choose the Chinese Word Analogy dataset proposed by (Chen et al., 2015) to evaluate the performance of word analogy inference, that is, w(\u201cking\u201d) \u2212 w(\u201cman\u201d) ' w(\u201cqueen\u201d) \u2212 w(\u201cwoman\u201d).", 
        "129": "4.2 Experimental Settings  We evaluate three SE-WRL models including SSA, SAC and SAT on all tasks.", 
        "130": "As for baselines, we consider three conventional WRL models including Skip-gram, CBOW and GloVe.", 
        "131": "For Skipgram and CBOW, we directly use the code released by Google (Mikolov et al., 2013).", 
        "132": "GloVe is proposed by (Pennington et al., 2014), which seeks the advantages of the WRL models based on statistics and those based on prediction.", 
        "133": "Moreover, we propose another model, Maximum Selection over Target Model (MST), for further comparison inspired by (Chen et al., 2014).", 
        "134": "It represents the current word embeddings with only the most probable sense according to the contexts, instead of viewing a word as a particular distribution over all its senses similar to that of SAT.", 
        "135": "For a fair comparison, we train these models with the same experimental settings and with their best parameters.", 
        "136": "As for the parameter settings, we set the context window size K = 8 as the upper bound, and during training, the window size is dynamically selected ranging from 1 to 8 randomly.", 
        "137": "We set the dimensions of word, sense and sememe embeddings to be the same 200.", 
        "138": "For\n2https://www.sogou.com/labs/resource/ t.php\n3https://github.com/Leonard-Xu/CWE/ tree/master/data\nlearning rate \u03b1, its initial value is 0.025 and will descend through iterations.", 
        "139": "We set the number of negative samples to be 25.", 
        "140": "We also set a lower bound of word frequency as 50, and in the training set, those words less frequent than this bound will be filtered out.", 
        "141": "For SAT, we set K \u2032 = 2.", 
        "142": "4.3 Word Similarity  The task of word similarity aims to evaluate the quality of word representations by comparing the similarity ranks of word pairs computed by WRL models with the ranks given by dataset.", 
        "143": "WRL models typically compute word similarities according to their distances in the semantic space.", 
        "144": "4.3.1 Evaluation Protocol  In experiments, we choose the cosine similarity between two word embeddings to rank word pairs.", 
        "145": "For evaluation, we compute the Spearman correlation between the ranks of models and the ranks of human judgments.", 
        "146": "4.3.2 Experiment Results  Table 1 shows the results of these models for word similarity computation.", 
        "147": "From the results we can observe that:\n(1) Our SAT model outperforms other models, including all baselines, on both two test sets.", 
        "148": "This indicates that, by utilizing sememe annotation properly, our model can better capture the semantic relations of words, and learn more accurate word embeddings.", 
        "149": "(2) The SSA model represents a word with the average of its sememe embeddings.", 
        "150": "In general, SSA model performs slightly better than baselines, which tentatively proves that sememe information is helpful.", 
        "151": "The reason is that words which share common sememe embeddings will benefit from each other.", 
        "152": "Especially, those words with lower frequency, which cannot be learned sufficiently using conventional WRL models, in contrast, can\nobtain better word embeddings from SSA simply because their sememe embeddings can be trained sufficiently through other words.", 
        "153": "(3) The SAT model performs much better than SSA and SAC.", 
        "154": "This indicates that SAT can obtain more precise sense distribution of a word.", 
        "155": "The reason has been mentioned above that, different from SAC using only one target word as attention for WSD, SAT adopts richer contextual information as attention for WSD.", 
        "156": "(4) SAT works better than MST, and we can conclude that a soft disambiguation over senses prevents inevitable errors when selecting only one most-probable sense.", 
        "157": "The result makes sense because, for many words, their various senses are not always entirely different from each other, but share some common elements.", 
        "158": "In some contexts, a single sense may not convey the exact meaning of this word.", 
        "159": "4.4 Word Analogy  Word analogy inference is another widely-used task to evaluate the quality of WRL models (Mikolov et al., 2013).", 
        "160": "4.4.1 Evaluation Protocol  The dataset proposed by (Chen et al., 2015) consists of 1, 124 analogies, which contains three analogy types: (1) capitals of countries (Capital), 677 groups; (2) states/provinces of cities (City), 175 groups; (3) family words (Relationship), 272 groups.", 
        "161": "Given an analogy group of words (w1, w2, w3, w4), WRL models usually get w2\u2212w1+w3 equal to w4.", 
        "162": "Hence for word analogy inference, we suppose w4 is missing, and WRL models will rank all candidate words according to their scores as follows:\nR(w) = cos(w2 \u2212w1 +w3,w), (10)\nand select the top-ranked word as the answer.", 
        "163": "For word analogy inference, we consider two evaluation metrics: (1) Accuracy.", 
        "164": "For each analogy group, a WRL model selects the top-ranked word w = argmaxw R(w), which is judged as positive if w = w4.", 
        "165": "The percentage of positive samples is regarded as the accuracy score for this WRL model.", 
        "166": "(2) Mean Rank.", 
        "167": "For each analogy group, a WRL model will assign a rank for the gold standard word w4 according to the scores computed by Eq.", 
        "168": "(10).", 
        "169": "We use the mean rank of all gold standard words as the evaluation metric.", 
        "170": "4.4.2 Experiment Results  Table 2 shows the evaluation results of these models for word analogy inference.", 
        "171": "From the table, we can observe that:\n(1) The SAT model performs best among all models, and the superiority is more significant than that on word similarity computation.", 
        "172": "This indicates that SAT will enhance the modeling of implicit relations between word embeddings in the semantic space.", 
        "173": "The reason is that sememes annotated to word senses have encoded these word relations.", 
        "174": "For example, capital and Cuba are two sememes of the word \u201cHavana\u201d, which provide explicit semantic relations between the words \u201cCuba\u201d and \u201cHavana\u201d.", 
        "175": "(2) The SAT model does well on both classes of Capital and City, because some words in these classes have low frequencies, while their sememes occur so many times that sememe embeddings can be learned sufficiently.", 
        "176": "With these sememe embeddings, these low-frequent words can be learned more efficiently by SAT.", 
        "177": "(3) It seems that CBOW works better than SAT on Relationship class.", 
        "178": "Whereas for the mean rank, CBOW gets the worst results, which indicates the performance of CBOW is unstable.", 
        "179": "On the contrary, although the accuracy of SAT is a bit lower than that of CBOW, SAT seldom gives an outrageous prediction.", 
        "180": "In most wrong cas-\nes, SAT predicts the word \u201cgrandfather\u201d instead of \u201cgrandmother\u201d, which is not completely nonsense, because in HowNet the words \u201cgrandmother\u201d, \u201cgrandfather\u201d, \u201cgrandma\u201d and some other similar words share four common sememes while only one sememe of them are different.", 
        "181": "These similar sememes make the attention process less discriminative with each other.", 
        "182": "But for the wrong cases of CBOW, we find that many mistakes are about words with low frequencies, such as \u201cstepdaughter\u201d which occurs merely for 358 times.", 
        "183": "Considering sememes may relieve this problem.", 
        "184": "4.5 Case study  The above experiments verify the effectiveness of our models for WRL.", 
        "185": "Here we show some examples of sememes, senses and words for case study.", 
        "186": "4.5.1 Word Sense Disambiguation  To demonstrate the validity of Sememe Attention, we select three attention results in training set, as shown in Table 3.", 
        "187": "In this table, the first rows of three examples are word-sense-sememe structures of each word.", 
        "188": "For instance, in the third example, the word has two senses, contingent and troops; contingent has one sememe community, while troops has one sememe army.", 
        "189": "The three examples all indicate that our models can estimate appropriate distributions of senses for a word given a context.", 
        "190": "4.5.2 Effect of Context Words for Attention  We demonstrate the effect of context words for attention in Table.", 
        "191": "4.", 
        "192": "The word \u201cHavana\u201d consists of four sememes, among which two sememes capital and Cuba describe distinct attributes of the word from different aspects.", 
        "193": "Here, we list three different context words \u201cCuba\u201d, \u201cRussia\u201d and \u201ccigar\u201d.", 
        "194": "Given the context word \u201cCuba\u201d, both sememes get high weights, indicating their contributions to the meaning of \u201cHavana\u201d in this context.", 
        "195": "The context word \u201cRussia\u201d is more relevant to the sememe capital.", 
        "196": "When the context word is \u201ccigar\u201d, the sememe Cuba has more influence, because cigar is a famous specialty of Cuba.", 
        "197": "From these examples, we can conclude that our Sememe Attention can accurately capture the word meanings in complicated contexts.", 
        "198": "5 Conclusion and Future Work  In this paper, we propose a novel method to model sememe information for learning better word representations.", 
        "199": "Specifically, we utilize sememe information to represent various senses of each word and propose Sememe Attention to select appropriate senses in contexts automatically.", 
        "200": "We evaluate our models on word similarity and word analogy, and results show the advantages of our SememeEncoded WRL models.", 
        "201": "We also analyze several cases in WSD and WRL, which confirms our models are capable of selecting appropriate word senses with the favor of sememe attention.", 
        "202": "We will explore the following research directions in future: (1) The sememe information in HowNet is annotated with hierarchical structure\nand relations, which have not been considered in our framework.", 
        "203": "We will explore to utilize these annotations for better WRL.", 
        "204": "(2) We believe the idea of sememes is universal and could be wellfunctioned beyond languages.", 
        "205": "We will explore the effectiveness of sememe information for WRL in other languages.", 
        "206": "Acknowledgments  This work is supported by the 973 Program (No.", 
        "207": "2014CB340501), the National Natural Science Foundation of China (NSFC No.", 
        "208": "61572273, 61661146007), and the Key Technologies Research and Development Program of China (No.", 
        "209": "2014BAK04B03).", 
        "210": "This work is also funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFC TRR-169."
    }, 
    "document_id": "P17-1187.pdf.json"
}
