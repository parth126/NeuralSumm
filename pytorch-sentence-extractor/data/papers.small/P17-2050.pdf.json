{
    "abstract_sentences": {
        "1": "We design and release BONIE, the first open numerical relation extractor, for extracting Open IE tuples where one of the arguments is a number or a quantity-unit phrase.", 
        "2": "BONIE uses bootstrapping to learn the specific dependency patterns that express numerical relations in a sentence.", 
        "3": "BONIE\u2019s novelty lies in task-specific customizations, such as inferring implicit relations, which are clear due to context such as units (for e.g., \u2018square kilometers\u2019 suggests area, even if the word \u2018area\u2019 is missing in the sentence).", 
        "4": "BONIE obtains 1.5x yield and 15 point precision gain on numerical facts over a state-of-the-art Open IE system."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317\u2013323 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2050  1 Introduction  Open Information Extraction (Open IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary (Etzioni et al., 2008; Mausam, 2016), by constructing the relation phrases and arguments from within the sentences themselves.", 
        "2": "Early works on Open IE such as REVERB (Etzioni et al., 2011) extract verbmediated relations via a handful of human-defined patterns.", 
        "3": "OLLIE improves recall by learning dependency patterns, using bootstrapping over REVERB extractions (Mausam et al., 2012).", 
        "4": "Open IE 4.2, a state-of-the-art open information extractor, is based on a combination of SRLIE, a verbmediated extractor over SRL frames (Christensen et al., 2011), and RELNOUN 2.0, which performs special linguistic processing for extraction from complex noun phrases (Pal and Mausam, 2016).", 
        "5": "1\n\u2217Most work was done when the author was a graduate student at IIT Delhi.", 
        "6": "1https://github.com/knowitall/openie\nIn this work, we present and release2 the first system for open numerical extraction, which we name BONIE for Bootstrapping-based Open Numerical Information Extractor.", 
        "7": "It is important to note that existing Open IE systems, like Open IE 4.2, may also extract numerical facts.", 
        "8": "However, they are oblivious to the presence of numbers in arguments.", 
        "9": "Therefore, they may miss important extractions and may not always output the best numerical facts.", 
        "10": "Table 1 compares extractions generated by Open IE 4.2 and BONIE on some of the sample sentences.", 
        "11": "At a high level BONIE follows OLLIE\u2019s design of identifying seed facts, constructing training data by bootstrapping sentences that may mention a seed fact, pattern learning and ranking.", 
        "12": "Madaan et al (2016) note that bootstrapping for numerical IE is challenging; it can lead to high noise and missed recall, since numbers can easily match out of context, and numbers may not match due to approximations.", 
        "13": "In response, similar to most previous works (e.g., LUCHS (Hoffmann et al., 2010)) BONIE matches a number if it is within a percentage threshold.", 
        "14": "Additionally, BONIE uses a quantity extractor (Roy et al., 2015), which provides\n2Available at https://github.com/Open-NRE\n317\nthe units mentioned in the sentence \u2013 BONIE bootstraps a sentence only when the units match.", 
        "15": "When compared to OLLIE, BONIE contributes several numerical IE specific customizations.", 
        "16": "(1) Since no open facts are available for this task, we first manually define a set of high-precision seed patterns, which are run over a large corpus to generate seed facts.", 
        "17": "(2) Not all seeds are fit for bootstrapping \u2013 many don\u2019t even have an entity as first argument.", 
        "18": "We develop heuristics to identify an informative subset from these.", 
        "19": "After bootstrapping and pattern learning, we find that we are missing important tuples.", 
        "20": "E.g., sentence #3 in Table 1 above has no explicit relation word \u2013 the relation \u201chas length of\u201d is implicit via the adjective \u2018long\u2019.", 
        "21": "And, sentence #5 expresses the relation \u2018area\u2019 via the units.", 
        "22": "(3) BONIE identifies implicit relations using additional processing of units and adjectives.", 
        "23": "(4) Finally, BONIE can tag a quantity as count and prepends \u201cnumber of\u201d in the relation phrase (sentence #2).", 
        "24": "2 Related Work  One of the first Open IE systems to obtain substantial recall is OLLIE (Mausam et al., 2012), which is a pattern learning approach based on a bootstrapped training data using high precision verb-based extractions.", 
        "25": "Other methods augment the linguistic knowledge in the systems \u2013 Exemplar (de Sa\u0301 Mesquita et al., 2013) adds new rules over dependency parses, SRLIE develops extraction logic over SRL frames (Christensen et al., 2011).", 
        "26": "Several works identify clauses and operate over restructured sentences (Schmidek and Barbosa, 2014; Corro and Gemulla, 2013; Bast and Haussmann, 2013).", 
        "27": "Other approaches use tree kernels (Xu et al., 2013), qualia-based patterns (Xavier et al., 2015), and simple within-sentence inference (Bast and Haussmann, 2014).", 
        "28": "However, none of them handle numbers specifically, and hence do not work for our problem.", 
        "29": "Numerical Relations: Numbers play an important role in extracting information from text.", 
        "30": "Early works have seen people working on understanding numbers that express temporal information (Ling and Weld, 2010).", 
        "31": "More recently, the focus has been on numbers that express physical quantities or measures, either mentioned in text (Chaganty and Liang, 2016) or in the context of web tables (Ibrahim et al., 2016; Neumaier et al., 2016), or on numbers that represent cardinalities of relations (Mirza et al., 2017).", 
        "32": "One of the prior works that applies to generic numerical relations is LUCHS (Hoffmann et al., 2010), where the system uses distant supervision to create 5,000 relation extractors, which included numerical relations as well.", 
        "33": "Researchers have also specifically developed numerical relation extractors to extract those relations where one of the arguments is a quantity (Vlachos and Riedel, 2015; Intxaurrondo et al., 2015; Madaan et al., 2016).", 
        "34": "However, all of them extract only an ontology relation, and hence are not directly applicable to Open IE.", 
        "35": "3 Open Numerical Relation Extraction  The goal of Open Numerical Relation Extraction is to process a sentence that has a quantity mention in it, and extract any tuple of the form (Arg1, relation phrase, Arg2) where Arg2 (or Arg1) is a quantity.", 
        "36": "As a first step, BONIE learns patterns where Arg2 is a quantity, as most English sentences tend to express numerical facts in active voice.", 
        "37": "Figure 1 outlines BONIE\u2019s algorithm, which operates in two phases: training and extraction.", 
        "38": "BONIE\u2019s training includes creation of seed facts, generation of training data via bootstrapping, and pattern learning over dependency parses.", 
        "39": "In the extraction phrase, BONIE performs pattern matching and parse-based expansion to construct numerical tuples.", 
        "40": "These numerical tuples are made more coherent by a novel relation construction step.", 
        "41": "As an example, the sentence \u201cIndia has a population of 1.2 billion\u201d matches seed pattern #2 (from Figure 2) to create a seed fact (India; population; 1.2 billion; null).", 
        "42": "This \u2018null\u2019 represents that the quantity needs no unit.", 
        "43": "While bootstrapping, this seed fact may match a sentence \u201cIndia is the second most populous country in the world, with a population of 1.25 billion.\u201d in the corpus.", 
        "44": "This training example will help learn a new pattern.3 This pattern, when applied to the sentence \u201cMicrosoft Windows is the most popular operating system, with a customer base of 300 million users\u201d, will extract (Microsoft Windows; has customer base of; 300 million users).", 
        "45": "While BONIE\u2019s skeleton broadly resembles that of OLLIE\u2019s (Mausam et al., 2012), it brings in customizations specific to the problem of numerical extraction such as a modified pattern language, heuristics for generating high quality seed set and training data, special processing for non-noun relations, and a novel relation construction step.", 
        "46": "We now describe BONIE\u2019s algorithm in detail.", 
        "47": "3.1 Generation of Seed Facts  Since open numerical facts are not readily available, we first write a handful of high-precision dependency patterns (see Figure 2 for a list).", 
        "48": "Each dependency pattern encodes the minimal sub-tree of the dependency parse connecting the relation, quantity and argument in that sentence.", 
        "49": "BONIE encodes a node in a pattern via \u2018<depLabel>#<word>#<POSTag>\u2019, where \u2018depLabel\u2019 is the edge connecting the node to its parent, \u2018word\u2019 is the word at the node, \u2018POSTag\u2019 is its part of speech tag; \u2018#\u2019 is a delimiter separating them.", 
        "50": "{rel}, {arg} and {quantity} in the patterns are placeholders for relation, argument and quantity headwords, respectively.", 
        "51": "BONIE generates seed facts by parsing the corpus and matching seed patterns with the parse.", 
        "52": "In case of a successful match, a seed fact of the form (arg headword; relation headword; quantity; unit) is generated.", 
        "53": "Argument and relation headwords are extracted directly from the parse.", 
        "54": "For the other two, it uses Illinois Quantifier (Roy et al., 2015), which returns both the quantity and unit separately.", 
        "55": "Since seed facts form the basis of our training task, they must be as clean as possible \u2013 BONIE\n3<(#is#verb)<(nsubj#{arg}#nnp|nn)(prep#with#in)< (pobj#{rel}#nnp|nn)<(prep#of#in)<(pobj#{quantity}#.+) >>>>>\nadds several filters to reduce noise.", 
        "56": "It considers a seed fact as valid only when the quantity node in the pattern is within some quantity span given by Illinois Quantifier.", 
        "57": "It also rejects any fact whose argument is not a proper noun.", 
        "58": "After these filters it gets high-precision extractions, but not necessarily good seeds \u2013 many seeds are generic, which may easily match unrelated sentences.", 
        "59": "E.g., (Michael; drove; 20; kms) isn\u2019t a good seed, since \u2018Michael\u2019 isn\u2019t specific, and could erroneously match sentences mentioning another Michael with some unrelated reference of a 20 km drive.", 
        "60": "To improve the set, BONIE checks for the presence of a seed fact in Yago KB (Hoffart et al., 2013) and keeps only those that are common.", 
        "61": "Since Yago has many numerical facts for height, area, latitude, GDP, etc., this gives BONIE a diverse set of clean facts for further training.", 
        "62": "Finally, some numerical facts may be expressed without using a nominal relation word.", 
        "63": "BONIE uses WordNet (Miller, 1995) to generate new seeds from such seed facts using the derivationally related noun form of the relation headword.", 
        "64": "For example, (Brown ; tall ; 13 ; inches) gets transformed to (Brown; height; 13; inches), which gets added as a seed fact.", 
        "65": "3.2 Bootstrapping  Similar to OLLIE, BONIE finds sentences that contain all words in a seed fact and generates (sentence, fact) pairs.", 
        "66": "But unlike OLLIE, BONIE has quantities and units, and matching them as words isn\u2019t appropriate.", 
        "67": "Illinois Quantifier performs an internal normalization for both, e.g, changes \u2018dollars\u2019 and \u2018$\u2019 to \u2018US$\u2019, and \u2018%\u2019 to \u2018percent\u2019.", 
        "68": "Since seed facts also have normalized units, we run Illinois quantifier on candidate sentences and match normalized units directly.", 
        "69": "Moreover, BONIE maintains a percentage threshold \u03b4 to control the amount of allowed difference between quantities in the sentence and seed fact.", 
        "70": "Once all constituents of a fact match with a sentence, BONIE generates the (sentence, fact) pair.", 
        "71": "3.3 Open Pattern Learning  For each (sentence, fact) pair, BONIE parses the sentence, and replaces the argument and relation words of the fact with \u2018{arg}\u2019 and \u2018{rel}\u2019 placeholders.", 
        "72": "For quantity and unit words, BONIE replaces the one at a higher level in the parse with \u2018{quantity}\u2019.", 
        "73": "The minimal path containing \u2018{arg}\u2019, \u2018{rel}\u2019 and \u2018{quantity}\u2019 is learned as a pattern.", 
        "74": "Since quantity and unit are typically expected to remain close to each other in a sentence, BONIE rejects all such patterns where the distance between them exceeds a certain threshold value.", 
        "75": "Some patterns are learned with specific words such as \u2018contains\u2019 in example (partial) <(#contains#verb)<(dobj#{quantity}#.+)<... We believe that this pattern should work with all inflections and synonyms of \u2018contain\u2019, BONIE uses WordNet to expand the pattern by including all inflections and synset synonyms.", 
        "76": "Each pattern is scored based on the number of times it is learned from the data.", 
        "77": "3.4 Constructing Extractions  After matching a pattern to a new sentence, similar to OLLIE, arg/rel phrases are completed by expanding the extracted headnouns on poss, det, num, neg, amod, quantmod, nn, and pobj edges.", 
        "78": "If one of the children of the argument headword is a prep, rcmod or partmod edge, the whole subtree under that is extracted.", 
        "79": "Quantity phrase is extracted by Illinois quantifier, but if any sibling node of the quantity node is connected by a prep edge, with the word \u2018of\u2019, BONIE expands the entire subtree below it.", 
        "80": "This allows \u201c10 percent of 100 dollars\u201d to be included in the quantity phrase.", 
        "81": "Relation Phrase Construction: Whenever the relation headword is an adjective or an adverb, BONIE uses WordNet to get its derivationally related noun form and that becomes the new relation.", 
        "82": "This transforms the tuple (Donald Trump ; old ; about 70 years) to (Donald Trump ; has age of ; about 70 years).", 
        "83": "Sometimes, sentences don\u2019t use a numerical relation word \u2013 it is obvious from the units.", 
        "84": "E.g., sentence #4 on page 1 expresses the \u2018area\u2019 relation implicitly.", 
        "85": "BONIE infers these implicit relations using the unit analysis in UnitTagger (Sarawagi and Chakrabarti, 2014).", 
        "86": "Whenever BONIE sees a unit (sq kms) getting mistreated as a relation it uses UnitTagger to infer relations from units and postprocesses the extraction accordingly.", 
        "87": "The ex-\ntraction, as a result changes from (James Valley; has sq kms of ; 5 of fruit orchards) to (James Valley ; has area of fruit orchards ; 5 sq kms).", 
        "88": "Finally, in cases when a plural noun relation word also appears as a unit in the quantifier, BONIE hypothesizes that it is a count extraction, and prepends \u2018number of\u2019 to the relation headword and removes the unit from the quantity phrase.", 
        "89": "E.g., (Microsoft; has employees; 100,000 employees) from sentence #2 becomes (Microsoft; has number of employees; 100,000).", 
        "90": "4 Experiments  We build BONIE over data from ClueWeb12,4 filtered so as to keep only the sentences that contain numbers.", 
        "91": "We further remove those where quantity represents a date, time, or duration, and where the quantity is accompanied by document words like \u2018Section\u2019, \u2018Table\u2019, or \u2018Figure\u2019.", 
        "92": "We use the dependency parser from ClearNLP5.", 
        "93": "We generate about 21,000 seed facts from roughly 20 million numerical sentences.", 
        "94": "These are matched against 7 million numerical sentences obtaining about 18,500 (sentence, fact) pairs.", 
        "95": "We tried different values of \u03b4 (the matching threshold) and found results to not be sensitive as long as \u03b4 varies in the range of 2% to 5%.", 
        "96": "So we set \u03b4=2% during the final evaluation.", 
        "97": "The distance threshold between the quantity and unit mentioned in Section 2.3 is set to 3 and is based on our general understanding of parse trees.", 
        "98": "BONIE learns around 7,000 new patterns.", 
        "99": "Since pattern frequency is a good indicator of pattern quality (Wu and Weld, 2010), we rank the patterns on the basis of frequency and take the top 1,000 patterns for further analysis.", 
        "100": "We find that almost all patterns beyond the top 1,000 are learned only once or twice on our training set.", 
        "101": "Our decision to ignore all patterns beyond the top 1,000 is so that we have a support of at least three for each pattern.", 
        "102": "We sample a random testset of 2,000 numerical sentences from ClueWeb12 (not used in training).", 
        "103": "Two annotators with NLP experience annotate each extraction for correctness.", 
        "104": "We obtain an inter-annotator agreement of 97%, and report the results on the subset where both annotators agree.", 
        "105": "Since there are no open numerical extractors available, we compare BONIE against an Open IE system and another closed numerical IE system.", 
        "106": "4http://www.lemurproject.org/clueweb12.php/ 5https://github.com/clir/clearnlp\nTable 2 reports the precisions and yields of all systems.", 
        "107": "We first compare against numerical tuples from Open IE 4.2,6 a publicly available state-ofthe-art Open IE system that combines SRLIE and RELNOUN.", 
        "108": "BONIE outputs a much higher precision and yield on numerical facts, as compared to Open IE 4.2.", 
        "109": "We also compare against NumberRule, a state-of-the-art closed numerical IE system (Madaan et al., 2016).", 
        "110": "NumberRule7 can be quickly re-targeted to any new semantic relation by inputting keywords.", 
        "111": "We feed all of Yago numerical relation words as keywords to NumberRule, but still find that it was able to generate only 12 extractions on our testset.", 
        "112": "We also perform additional ablation study to evaluate the value of each component.", 
        "113": "Just the seed patterns themselves have a significantly higher precision but much smaller yield.", 
        "114": "This is expected, since the seeds must be highly precise for bootstrapping.", 
        "115": "If Yago matching and other seed filtering heuristics are turned off, the precision of the system goes down drastically due to a very noisy bootstrapped set.", 
        "116": "If the post-processing of relation phrase construction is turned off, there is a 5 point precision loss and about 7% yield reduction due to some incorrect extracted tuples, which are corrected by post-processing.", 
        "117": "Finally,\n6https://github.com/knowitall/openie 7https://github.com/NEO-IE\nWordnet-based expansion has marginal increase in yield and slight precision loss.", 
        "118": "Open IE 4.2 associates a confidence value with each extraction - ranking against which generates a precision-yield curve.", 
        "119": "For BONIE, we rank the patterns in such a way that the seed patterns are at the top, followed by the learned patterns.", 
        "120": "The learned patterns are ordered based on their frequencies.", 
        "121": "Figure 3 reports the curves for both the systems and we find that BONIE has a larger area under the curve as compared to Open IE 4.2.", 
        "122": "Estimating recall in Open IE is difficult since it requires annotators to exhaustively tag all open extractions in a sentence.", 
        "123": "To get an estimate, an author manually tagged 100 sentences with all numerical extractions.", 
        "124": "We find that BONIE\u2019s recall is about 48%.", 
        "125": "Two-thirds of missed recall is because of missing conjuncts.", 
        "126": "E.g., it misses the tuple relating retirement age with 68 years in \u201cThe retirement age for men is 65 years and 68 years for women.\u201d Other missed recall is due to complexity of sentences or inaccuracy of parsers.", 
        "127": "5 Conclusions  We release BONIE 8, the first open numerical relation extractor and other resources for further research.", 
        "128": "BONIE is based on bootstrapping and pattern learning and follows previous similar works such as OLLIE.", 
        "129": "However, for effective bootstrapping and training, it implements various customizations specific to numerical relations in curation of seed fact set, matching of sentences, and construction of relation phrase at the time of extraction.", 
        "130": "BONIE significantly outperforms both open non-numerical IE, and closed numerical IE systems with 1.5x yield and 15 point precision gain over a state-of-the-art Open IE system.", 
        "131": "We find that better conjunction processing is an important future step for improving BONIE\u2019s recall even further.", 
        "132": "Acknowledgements  This work is supported by Google language understanding and knowledge discovery focused research grants, a Bloomberg award, a Visvesvaraya faculty award by Govt.", 
        "133": "of India and an IRD seed grant at I.I.T, Delhi.", 
        "134": "We also thank Microsoft for the Microsoft Azure sponsorship and a Microsoft Travel grant in support of the work.", 
        "135": "8Available at https://github.com/Open-NRE"
    }, 
    "document_id": "P17-2050.pdf.json"
}
