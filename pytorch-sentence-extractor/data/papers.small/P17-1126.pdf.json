{
    "abstract_sentences": {
        "1": "This paper presents a novel encoderdecoder model for automatically generating market comments from stock prices.", 
        "2": "The model first encodes both shortand long-term series of stock prices so that it can mention shortand long-term changes in stock prices.", 
        "3": "In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices.", 
        "4": "Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1374\u20131384 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1126  1 Introduction  Various industries such as finance, pharmaceuticals, and telecommunications have been increasingly providing opportunities to treat various types of large-scale numerical time-series data.", 
        "2": "Such data are hard for non-specialists to interpret in detail and time-consuming even for specialists to construe.", 
        "3": "As a result, there has been a growing interest in automatically generating concise descriptions of such data, i.e., data summarization.", 
        "4": "This interest in data summarization is encouraged by the recent development of neural network-based text generation methods.", 
        "5": "Given an appropriate architecture, a neural network can generate a sentence that is mostly grammatical and semantically reasonable.", 
        "6": "In this study, we focus on the task of generating market comments from a time-series of stock prices.", 
        "7": "We adopt an encoder-decoder model (Sutskever et al., 2014) and exploit its capability to learn to capture the behavior of the input and generate a description of it.", 
        "8": "Although encoderdecodermodels can learn to do this, they need to be\nprovided with an appropriate network-architecture and necessary information.", 
        "9": "We use Figure 1 to illustrate the characteristic problems of comment generation for time-series of stock prices.", 
        "10": "The figure shows the Nikkei Stock Average (Nikkei 225, or simply Nikkei), which is a stock market index calculated from 225 selected issues, on some consecutive trading days accompanied by the market comments made at some specific time points in the span.", 
        "11": "The first problem is that market comments do not merely describe the increase and decrease of the price.", 
        "12": "They also often describe how the price changes compared with the previous period, such as \u201ccontinues to fall\u201d in (3) of Figure 1, \u201cturns to rise\u201d in (2), and \u201crebound\u201d in (6).", 
        "13": "Market comments sometimes describe the change in price compared with the prices in the previous week.", 
        "14": "The second problem is that market comments also\n1374\ncontain expressions that depend on their delivery time: e.g., \u201copens with\u201d in (1), \u201cclosing price of the morning session\u201d in (3), and \u201cbeginning of the afternoon session\u201d in (4).", 
        "15": "The third problem is that market comments typically contain numerical values, which often cannot be copied from the input prices.", 
        "16": "Such numerical values probably cannot be generated as other words are generated by the standard decoder.", 
        "17": "This difficulty can be easily understood as analogous with the difficulty of generating named entities by encoder-decoder models.", 
        "18": "To derive such values, the model needs arithmetic operations such as subtraction as in examples (3) and (6)mentioning the difference in price and rounding as in example (5).", 
        "19": "To address these problems, we present a novel encoder-decoder model to automatically generate market comments from stock prices.", 
        "20": "To address the first problem of capturing various types of change in different time scales, the model first encodes data consisting of both short- and long-term time-series, where a multi-layer perceptron, a recurrent neural network, or a convolutional network is adopted as a basic encoder.", 
        "21": "In the decoding phase, we feed our model with the delivery time of the market comment to generate the expressions depending on time of day to address the second problem.", 
        "22": "To address the third problem regarding with numerical values mentioned in the generated text, we allow our model to choose an arithmetic operation such as subtraction or rounding instead of generating a word.", 
        "23": "The proposed methods are evaluated on the task of generating Japanese market comments on the Nikkei Stock Average.", 
        "24": "Automatic evaluation with BLEU score (Papineni et al., 2002) and F-score of time-dependent expressions reveals that our model outperforms a baseline encoder-decoder model significantly.", 
        "25": "Furthermore, human assessment and error analysis prove that our best model generates characteristic expressions discussed above almost perfectly, approaching the fluency and the informativeness of human-generated market comments.", 
        "26": "2 Related Work  The task of generating descriptions from timeseries or structured data has been tackled in various domains such as weather forecasts (Belz, 2007; Angeli et al., 2010), healthcare (Portet et al., 2009; Banaee et al., 2013b), and sports (Liang et al., 2009).", 
        "27": "Traditionally, many studies used hand-\ncrafted rules (Goldberg et al., 1994; Dale et al., 2003; Reiter et al., 2005).", 
        "28": "On the other hand, interest has recently been growing in automatically learning a correspondence relationship from data to text and generating a description of this relationship since large-scale data in diversified formats have become easy to acquire.", 
        "29": "In fact, a data-driven approach has been extensively studied nowadays for various tasks such as image caption generation (Vinyals et al., 2015) and weather forecast generation (Mei et al., 2016b).", 
        "30": "The task, called data-to-text or concept-to-text, is generally divided into two subtasks: content selection and surface realization.", 
        "31": "Whereas previous studies tackled the subtasks separately (Barzilay and Lapata, 2005; Wong and Mooney, 2007; Lu et al., 2009), recent work has focused on solving them jointly using a single framework (Chen and Mooney, 2008; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2012, 2013).", 
        "32": "More recently, there has been some work on an encoder-decoder model (Sutskever et al., 2014) for generating a description from time-series or structured data to solve the subtasks jointly in a single framework, and this model has been proven to be useful (Mei et al., 2016b; Lebret et al., 2016).", 
        "33": "However, the task of generating a description from numerical time-series data presents difficulties such as the second and third problems mentioned in Section 1.", 
        "34": "For the second problem, the model needs to be fed with information on delivery time.", 
        "35": "Also, the model needs arithmetic operations such as subtraction for the third problem because even if we simply apply a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) to the model, it cannot derive a calculated value such as (3), (5), or (6) in Figure 1 from input.", 
        "36": "Thus, in this work, we tackle these problems and develop a model on the basis of the encoder-decoder model that can mention a specific numerical value by referring to the input data or producing a processed value with mathematical calculation and mention time-dependent expressions by incorporating the information on delivery time into its decoder.", 
        "37": "There has also been some work on generating market comments.", 
        "38": "Kukich (1983) developed a system consisting of rule-based components for generating stock reports from a database of daily stock quotes.", 
        "39": "Although she used several components individually and had to define a number of rules for the generation, our encoder-decoder model can\nperform it with fewer and simpler rules for the calculation.", 
        "40": "Aoki and Kobayashi (2016) developed a method on the basis of a weighted bi-gram language model for automatically describing trends of time-series data such as the Nikkei Stock Average.", 
        "41": "However, they did not attempt to refer to specific numerical values such as closing prices and amounts of rises in price although such descriptions are often used in market comments as shown in Figure 1 (3), (5), and (6).", 
        "42": "In contrast, we present a novel approach to generate natural language descriptions of time-series data that can not only able to describe trends of the data but also mention specific numerical values by referring to the time-series data.", 
        "43": "3 Generating Market Comments  To generate market comments on stock prices, we introduce an encoder-decoder model.", 
        "44": "Encoderdecoder models have been widely used and proven useful in various tasks of natural language generation such as machine translation (Cho et al., 2014) and text summarization (Rush et al., 2015).", 
        "45": "Our task is similar to these tasks in that the system takes sequential data and generates text.", 
        "46": "Therefore, it is natural to use an encoder-decoder model in modeling stock prices.", 
        "47": "Figure 2 illustrates our model.", 
        "48": "In describing time-series data, the model is expected to capture various types of change and important values in the given sequence, such as absolute or relative changes and maximum or minimum value, in different time-scales.", 
        "49": "Moreover, it is necessary to generate time-dependent comments and numerical values that require arithmetic operations for derivation, such as \u201cThe closing price of the morning session decreases by 5 yen...\u201d.", 
        "50": "To achieve these, we present three strategies that alter the standard encoder-decoder model.", 
        "51": "First (Section 3.1), we use several encoding methods for time-series data, as in (1) of Figure 2, to capture the changes and important values.", 
        "52": "Second (Section 3.2), we incorporate delivery-time information into the decoder, as in (2) of Figure 2, to generate time-dependent comments.", 
        "53": "For the decoder, we use a recurrent neural network language model (RNNLM) (Mikolov et al., 2010), which is widely used in language generation tasks.", 
        "54": "Finally (Section 3.3), we extend the decoder to estimate arithmetic operations, as in (3) of Figure 2, to generate numerical values in market comments.", 
        "55": "3.1 Encoding Numerical Time-Series Data  We prepare short- and long-term data, using the five-minute chart of Nikkei 225.", 
        "56": "A vector for short-term data consists of the prices of one trading day and has N elements.", 
        "57": "We denote it as xshort = ( xshort, i ) N\u22121 i=0 .", 
        "58": "On the other hand, a vector for long-term data consists of the closing prices of the M preceding trading days.", 
        "59": "It is denoted as xlong = ( xlong, i )M\u22121 i=0 .", 
        "60": "Data are commonly preprocessed to remove noise and enhance generalizability of a model (Zhang and Qi, 2005; Banaee et al., 2013a).", 
        "61": "We use two preprocessing methods: standardization and moving reference.", 
        "62": "Standardization substitutes each element xi of input x by\nxstdi = xi \u2212 \u00b5 \u03c3 , (1)\nwhere \u00b5 and \u03c3 are the mean and standard deviation of the values in the training data, respectively.", 
        "63": "Standardized values are less affected by scale.", 
        "64": "The second method, moving reference (Freitas et al., 2009), substitutes each element xi of input x by\nxmovei = xi \u2212 ri, (2)\nwhere ri is the closing price of the previous trading day of x.", 
        "65": "This is introduced to capture price fluctuations from the previous day.", 
        "66": "By applying one of the preprocessing methods to xshort and xlong, we obtain two vectors of preprocessed values lshort and llong.", 
        "67": "Given these, each encoder emits the corresponding hidden states hshort and hlong.", 
        "68": "After obtaining the hidden states, we concatenate the two vectors of the preprocessed values and the outputs of the encoders as a multilevel representation of the input time-series data.", 
        "69": "The multi-level representation is an approach developed by Mei et al.", 
        "70": "(2016a) that enable the decoder to take into account both the high-level representation, e.g., hshort, hlong, and the low-level representation, e.g., lshort, llong, at the same time.", 
        "71": "They have shown that it improves performance in terms of selecting salient objects in input data.", 
        "72": "We thus set the initial hidden state s0 of the decoder as\ns0 = lshort \u2295 llong \u2295 hshort \u2295 hlong, (3)\nwhere \u2295 is the concatenation operator.", 
        "73": "When we use both preprocessing methods, we have four preprocessed input vectors: lmoveshort , l std short, lmovelong , and l std long.", 
        "74": "In this case, we introduce four encoders, and set the initial hidden state s0 of the decoder as\ns0 = l move short \u2295 lstdshort \u2295 lmovelong \u2295 lstdlong \u2295 hmoveshort \u2295 hstdshort \u2295 hmovelong \u2295 hstdlong.", 
        "75": "(4)\nSince several encoding methods can be used for the time-series data, we use any one of the three conventional neural networks: Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), or Recurrent Neural Network (RNN) with Long Short-Term Memory cells (Hochreiter and Schmidhuber, 1997).", 
        "76": "In the experiments, we empirically evaluate and compare the encoding methods.", 
        "77": "3.2 Incorporating Time Embedding  Even if identical sequences of values are observed, comments usually vary in accordance with price history or the time they are observed.", 
        "78": "For instance, when the market opens, comments usually mention how much the stock price has increased or decreased compared with the closing price of the previous trading day, as in (1) and (3) in Figure 1.", 
        "79": "Our model creates vectors called time embedding vectors T on the basis of the time when the\ncomment is delivered (e.g., 9:00 a.m. or 3:00 p.m.).", 
        "80": "Then a time embedding vector is added to each hidden state s j in decoding so that words are generated depending on time.", 
        "81": "This mechanism is inspired by speaker embedding introduced by Li et al.", 
        "82": "(2016).", 
        "83": "They use an encoder-decoder model for a conversational agent that inherits the characteristics of a speaker, such as his/her manner of speaking.", 
        "84": "They encode speaker-specific information (e.g., dialect, age, and gender) into speaker embedding vectors and used them in decoding.", 
        "85": "3.3 Estimation of Arithmetic Operations  Text generation systems based on language models such as RNNLM often generate erroneous words for named entities; that is, they often mention a similar but incorrect entity, e.g., Nissan for Toyota.", 
        "86": "To overcome this problem, Gulcehre et al.", 
        "87": "(2016) developed a text generation method called copy mechanism.", 
        "88": "The method copies rare words missing from the vocabulary from a given sequence of words using an attention mechanism, and emits the copied words.", 
        "89": "Market comments often mention numerical values that appear in the input data, but they also mention values obtained through arithmetic operations, such as differences in prices as in (3) and (6) in Figure 1, or rounded values as in (5).", 
        "90": "Thus, another problem arises: what type of operation is suitable for text to be generated?", 
        "91": "In this work, we solve this problem by extending the idea of copy mechanism.", 
        "92": "To enable our model to generate text with values calculated from input values, we add generalization tags to the vocabulary used in the model.", 
        "93": "Each generalization tag represents a type of arithmetic operation.", 
        "94": "When a generalization tag is emitted, the model performs the operations on the designated values in accordance with the tag, replaces the tag with the calculated value, and finally outputs text containing numerical values.", 
        "95": "For preprocessing, we replace each numerical value appearing in the market comments in the training data with generalization tags such as <price1>.", 
        "96": "The tag for a numerical value depends on what the value stands for in the text.", 
        "97": "Table 1 displays all the tags and the corresponding types of calculation.", 
        "98": "To illustrate, suppose a market comment says\n(a) Nikkei rebounds.", 
        "99": "The closing price of the morning session is 16,610 yen, which is 227 yen higher.", 
        "100": "Since this comment omits the phrase \u201cthan the\nclosing price of the previous day\u201d, 227 in this example indicates the difference between the closing price of the previous trading day xlong, M\u22121 and the latest price xshort, N\u22121 denoted by z in Table 1.", 
        "101": "Therefore, we replace 227 with the tag <price1>.", 
        "102": "Likewise, we replace 16,610 with <price6> because it represents the latest price z.", 
        "103": "To find the optimal tag for each value, we try all the types of operations listed in Table 1 using the values appearing in the text, i.e., 227 and 16,610 in this case.", 
        "104": "Then, we select the tag that has the operation that yields the value closest to the original one.", 
        "105": "In prediction, the model first generates a tentative comment, which includes tags as well as words.", 
        "106": "Suppose that the input vectors are xshort and xlong, with xshort, N\u22121 = 14508 and xlong, M\u22121 = 14612, and that the model generates the comment below:\n(b) Nikkei opens turning down.", 
        "107": "The loss exceeds <price2> yen, and it falls to the <price7> yen level.", 
        "108": "Since the tag <price2> represents \u201cthe difference between xshort, N\u22121 and xlong, M\u22121 rounded down to the nearest 10\u201d, we replace the tag with 100.", 
        "109": "Similarly, we replace <price7>, which is \u201cthe last price xshort, N\u22121 rounded down to the nearest 100\u201d, with 14,500.", 
        "110": "Finally, we have a market comment containing the numbers as below:\n(c) Nikkei opens turning down.", 
        "111": "The loss exceeds 100 yen, and it falls to the 14,500 yen level.", 
        "112": "4 Experiments    4.1 Experimental Settings  We used the five-minute chart of Nikkei 225 from March 2013 to October 2016 as numerical timeseries data, which were collected from IBI-Square Stocks1, and 7,351 descriptions as market comments, which are written in Japanese and provided by Nikkei QUICK News.", 
        "113": "We divided the dataset into three parts: 5,880 for training, 730 for validation, and 741 for testing.", 
        "114": "For a human evaluation, we randomly selected 100 comments and their time-series data included in the test set.", 
        "115": "We set N = 62, which is the number of time steps for stock prices for one trading day, and M = 7, which is the number of the time steps for closing prices of the preceding trading days.", 
        "116": "We used Adam (Kingma and Ba, 2015) for optimization with a learning rate of 0.001 and a mini-batch size of 100.", 
        "117": "The dimensions of word embeddings, time embeddings, and hidden states for both the encoder and decoder are set to 128, 64, and 256, respectively.", 
        "118": "For CNN, we used a single convolutional layer and set the filter size to 3.", 
        "119": "In the experiments, we conducted three types of evaluation: two for automatic evaluation, and one for human evaluation.", 
        "120": "For one automatic evaluation, we used BLEU (Papineni et al., 2002) to measure the matching degree between the market comments written by humans as references and output comments generated by our model.", 
        "121": "We applied paired bootstrap resampling (Koehn, 2004) for a significance test.", 
        "122": "For the other automatic evaluation metric, we calculate F-measures for time-dependent expressions, using market comments written by humans as references, to investigate whether our model can correctly output timedependent expressions such as \u201copen with\u201d and describe how the price changes compared with the previous period referring to the series of preceding prices such as \u201ccontinual fall\u201d.", 
        "123": "Specifically, we calculate F-measures for 13 expressions shown in Figure 3.", 
        "124": "For the human evaluation, we recruited a specialist in financial engineering as a judge to evaluate the quality of generated market comments.", 
        "125": "To evaluate the difference in the quality of generated comments between our models and human, we showed both system-generated and humangenerated market comments together with their\n1http://www.ibi-square.jp/index.htm\ntime-series data consisting of xshort and xlong, without letting the judge know which comment is generated by which method.", 
        "126": "We asked the judge to give each market comment two scores: one for informativeness and one for fluency.", 
        "127": "Both scores have two levels, 0 or 1, where 1 indicates high informativeness or fluency.", 
        "128": "For informativeness, the judge used both generated comments and their input stock prices to rate the comments.", 
        "129": "Specifically, if the judge deem that a generated comment describes an important price movement or an outline of the movement properly, such comments are considered to be informative.", 
        "130": "For fluency, the judge read only the generated comments and rate them in terms of readability, regardless of their content of the comment.", 
        "131": "In addition, since some of the market comments written by humans sometimes include external information such as \u201cNikkei opens with a continual fall as yen pressures exporters\u201d, we also asked the judge to ignore the correctness of external information mentioned in comments, for the sake of fairness in comparison, because external information cannot be retrieved from the time-series data.", 
        "132": "To assess the effectiveness of the techniques we introduced, we conducted experiments with 11 models.", 
        "133": "Table 2 shows an overview of the models\nwe compared.", 
        "134": "We compared three types of models: a baseline, full models (e.g., mlp-enc), and ablatedmodels (e.g., -short).", 
        "135": "For example, -short is a model that does not use the short-term time series.", 
        "136": "4.2 Results  Table 3 shows the BLEU scores on the test set.", 
        "137": "Figure 3 presents the F-measure of the models for each phrase.", 
        "138": "We also present output examples with human-generated market comments (Human) for reference in Figure 4.", 
        "139": "In the results for the automatic evaluation in BLEU, the model using both MLP as encoders and all the techniques we developed, mlp-enc, outperformed baseline and the other models.", 
        "140": "The BLEU scores and F-measure values revealed differences among themodels usingMLP, CNN, or RNN (mlp-\nenc, cnn-enc, rnn-enc).", 
        "141": "In the comparison between the models that took two types of the time-series data xshort, xlong as input (e.g.,mlp-enc or rnn-enc) and the models that only used one of them (-short, -long), the models using both types of data such as mlp-enc and rnn-enc gained higher BLEU scores than -short and -long.", 
        "142": "Also, the models that encoded the two types of time-series data to capture their short- and long-term changes correctly output more expressions that described the changes such as \u201cturn to rise\u201d, \u201ccontinue to fall\u201d, and \u201crebound\u201d than -short and -long as shown in Figure 3.", 
        "143": "According to the comparison between prepro-\ncessing methods, mlp-enc, which used both standardization and moving reference as preprocessing methods, obtained a higher BLEU score than the models that used neither (-std, -move).", 
        "144": "In terms of the F-measure values, mlp-enc output phrases mentioning changes more appropriately and therefore achieved the higher values than the other two models as in \u201cturn to rise\u201d or \u201cturn to fall\u201d in Figure 3.", 
        "145": "Furthermore, we found that the BLEU score of -multi, which did not use the multi-level representation of the data, was inferior.", 
        "146": "In other words, incorporating the multi-level representation along with an output of an encoder into a decoder seems\nto contribute to improving the automatic evaluation and producing a better representation of the input data.", 
        "147": "baseline and -num output numerical values as \u201cwords\u201d from the vocabulary for RNNLM because these models do not use any arithmetic operation.", 
        "148": "Therefore, there were many cases including <unk> that should be output as a numerical value as shown in Figure 4 (a).", 
        "149": "We found that -num had a lower BLEU score than themodels such asmlp-enc and -std that used arithmetic operations.", 
        "150": "Furthermore, we observed that the models with arithmetic operations correctly generated stock prices in most cases.", 
        "151": "By comparing -time, which did not incorporate time-embeddings into a decoder, and other models such as mlp-enc with respect to the F-measure of expressions depending on delivery time (e.g., \u201copen with\u201d or \u201cclosing session\u201d), we found that themodels that took time information into account, such as mlp-enc, generated those phrases more accurately than -time.", 
        "152": "Moreover, we analyzed the effect of different sizes of training data.", 
        "153": "Figure 5 shows BLEU scores of market comments generated by our models for each size of training data on the validation set.", 
        "154": "According to the results, we found that the BLEU scores for the models saturated when we used 3000 training data.", 
        "155": "In addition, there was not much difference in convergence speed among the models.", 
        "156": "The human evaluation results in Table 4 indicate that market comments generated by our model (mlp-enc) achieved a quality comparable even to that of market comments written by humans.", 
        "157": "Moreover, we found that mlp-enc signifi-\ncantly outperformed baseline in terms of informativeness but was outperformed by baseline in terms of fluency.", 
        "158": "The reason was that mlp-enc occasionally generated a market comment such as \u201cNikkei gains more than 0 yen\u201d because of an error in the prediction of the operation, and such comments were not considered not to be fluent or informative by the judge, although most of comments generated bymlp-encwere as fluent as those of baseline.", 
        "159": "Note that baseline does not generate expressions like \u201c0 yen\u201d because they are not normally used in market comments and so not included in the vocabulary.", 
        "160": "Therefore, the judge considered all the comments generated by baseline to be fluent.", 
        "161": "For another possibility to enhance our model, we have to consider that the model should mention a difference or gain for a duration from when to when.", 
        "162": "For example, our current model sometimes generated a market comment such as \u201cNikkei gains more than 200 yen\u201d, although Nikkei actually gained more than 300 yen.", 
        "163": "Such a comment is not incorrect but is imprecise.", 
        "164": "Therefore, we consider that a mechanism is needed to select the period to be mentioned when the model generates a comment to this problem and increase the generalizability of our model for generating a description from various time-series data.", 
        "165": "5 Conclusion and Future Work  In this study, we presented a novel encoder-decoder model to automatically generate market comments from numerical time-series data of stock prices, using the Nikkei Stock Average as an example.", 
        "166": "Descriptions of numerical time-series data written by humans such as market comments have several writing style characteristics.", 
        "167": "For example, (1) content to be mentioned in the market comments varies depending on short- or long-term changes of the time-series data, (2) expressions depending on delivery time at which text is written are used, and (3) numerical values obtained through arith-\nmetic operations applied to the input data are often described.", 
        "168": "We developed approaches for generating comments that have these characteristics and showed the effectiveness of the proposed model.", 
        "169": "In future work, we plan to apply our model to descriptions of time-series data in various domains such as weather forecasts and sports, which share the above writing-style characteristics.", 
        "170": "We also plan to use multiple time-series as input such as multiple brands of stock.", 
        "171": "Acknowledgements  This paper is based on results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO)."
    }, 
    "document_id": "P17-1126.pdf.json"
}
