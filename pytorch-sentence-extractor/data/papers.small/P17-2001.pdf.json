{
    "abstract_sentences": {
        "1": "Temporal relation classification is becoming an active research field.", 
        "2": "Lots of methods have been proposed, while most of them focus on extracting features from external resources.", 
        "3": "Less attention has been paid to a significant advance in a closely related task: relation extraction.", 
        "4": "In this work, we borrow a state-of-the-art method in relation extraction by adopting bidirectional long short-term memory (BiLSTM) along dependency paths (DP).", 
        "5": "We make a \u201ccommon root\u201d assumption to extend DP representations of cross-sentence links.", 
        "6": "In the final comparison to two stateof-the-art systems on TimeBank-Dense, our model achieves comparable performance, without using external knowledge and manually annotated attributes of entities (class, tense, polarity, etc.", 
        "7": ")."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1\u20136 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2001  1 Introduction  Recently, the need for extracting temporal information from text is motivated rapidly by many NLP tasks such as: question answering (QA), information extraction (IE), etc.", 
        "2": "Along with the TimeBank1 (Pustejovsky et al., 2003) and other temporal information annotated corpora, a series of temporal evaluation challenges (TempEval1,2,3) (Verhagen et al., 2009, 2010; UzZaman et al., 2012) are attracting growing research efforts.", 
        "3": "Temporal relation classification is a task to identify the pairs of temporal entities (events or temporal expressions) that have a temporal link and classify the temporal relations between them.", 
        "4": "For instance, we show an event-event (E-E) link with \u2018DURING\u2019 type in (i), an event-time (E-T) link\n1https://catalog.ldc.upenn.edu/LDC2006T08\nwith \u2018INCLUDES\u2019 type in (ii) and an event-DCT (document creation time, E-D) with \u2018BEFORE\u2019 type in (iii).", 
        "5": "(i) There was no hint of trouble in the last conversation between controllers and TWA pilot Steven Snyder.", 
        "6": "(ii) In Washington today, the Federal Aviation Administration released air traffic control tapes.", 
        "7": "(iii) The U.S. Navy has 27 ships in the maritime barricade of Iraq.", 
        "8": "Marcu and Echihabi (2002) propose an approach considering word-based pairs as useful features.", 
        "9": "The following researchers (Laokulrat et al., 2013; Chambers et al., 2014; Mani et al., 2006; D\u2019Souza and Ng, 2013) focus on extracting lexical, syntactic or semantic information from various external knowledge bases such as: WordNet (Miller, 1995) and VerbOcean (Chklovski and Pantel, 2004).", 
        "10": "However, these feature based methods rely on hand-crafted efforts and external resources.", 
        "11": "In addition, these works require the features of entity attributes (class, tense, polarity, etc.", 
        "12": "), which are manually annotated to achieve high performance.", 
        "13": "Consequently, they are hard to obtain in practical application scenarios.", 
        "14": "In relation extraction, there is an explosion of the works done with the dependency path (DP) based methods, which employ various models along dependency paths (Bunescu and Mooney, 2005; Plank and Moschitti, 2013).", 
        "15": "In recent years, the DP-based neural networks (Socher et al., 2011; Xu et al., 2015a,b) show state-of-the-art performance, with less requirements on explicit features.", 
        "16": "Intuitively, the DP-based approaches have the potential to classify temporal relations.", 
        "17": "Both relation extraction and temporal relation classification require the identification of relation-\n1\nship between entities in texts.", 
        "18": "However, temporal relation classification is more challenging, since it includes three different type of entities: \u2018event\u2019, \u2018time expression\u2019 and DCT.", 
        "19": "Cross-sentence links also add additional complexity into the task.", 
        "20": "Due to the outstanding performance of DP-based neural networks revealed in relation extraction, we borrow this state-of-the-art approach to temporal relation classification.", 
        "21": "In Section 2 of this paper, we review related work and introduce TimeBank-Dense.", 
        "22": "We discuss the cross-sentence link problem and the architectures of our E-E, E-T and E-D classifiers in Section 3.", 
        "23": "In Section 4, the experiments are performed on TimeBank-Dense and we compare our model to the baseline and two state-of-the-art systems.", 
        "24": "The final conclusion is made in Section 5.", 
        "25": "2 Background    2.1 Related Work  Current state-of-the-art temporal relation classifiers exploit a variety of features.", 
        "26": "Laokulrat et al.", 
        "27": "(2013); Chambers et al.", 
        "28": "(2014) extract lexical and morphological features derived from WordNet synsets.", 
        "29": "Mani et al.", 
        "30": "(2006); D\u2019Souza and Ng (2013) incorporate semantic relations between verbs from VerbOcean as features.", 
        "31": "In addition, most of the systems include the entity attributes (Figure 1) specified in TimeML 2 as basic features, which actually need heavy human annotations.", 
        "32": "In this work, we push this work into a more practical level by using only word, part-of-speech (POS), dependency parsing information, without incorporating entity attributes, as well as any other external resources.", 
        "33": "In relation extraction, Bunescu and Mooney (2005) propose an observation that a relation can be captured by the shortest dependency path\n2http://timeml.org/\n(SDP) between the two entities in the entire dependency graph.", 
        "34": "Plank and Moschitti (2013) extract syntactic and semantic information in a tree kernel.", 
        "35": "Following this line, researchers (Socher et al., 2011; Xu et al., 2015a,b) achieve state-of-the-art performance by building various neural networks over dependency path.", 
        "36": "Our system is similar to the work by Xu et al.", 
        "37": "(2015b).", 
        "38": "They perform LSTM with max pooling separately on each feature channel along dependency path.", 
        "39": "In contrast, our system adopts bidirectional LSTM on the concatenation of feature embeddings.", 
        "40": "2.2 TimeBank-Dense  In the original TimeBank, temporal links have been created on those pairs with semantic connections, which led to a sparse annotation style.", 
        "41": "Cassidy et al.", 
        "42": "(2014) 3 propose a mechanism to force annotators to create complete graphs over the entities in neighboring sentences.", 
        "43": "Compared to 6,418 links in 183 TimeBank documents, TimeBankDense achieves greater density with 12,715 links in 36 documents.", 
        "44": "We follow a similar experiment setting to the other two systems (Mirza and Tonelli, 2016; Chambers et al., 2014) with the same 9 documents\n3https://www.usna.edu/Users/cs/nchamber/caevo\nas test data and the others as training data (15% of training data is split as validation data for early stopping).", 
        "45": "3 The Proposed Method    3.1 Cross-sentence Dependency Paths  Intuitively, the dependency path based idea can be introduced into the temporal relation classification task.", 
        "46": "However, around 64% E-E, E-T links in TimeBank-Dense are with the ends in two neighboring sentences, called cross-sentence links.", 
        "47": "A crucial obstacle is how to represent the dependency path of a cross-sentence link.", 
        "48": "In this work, we make a naive assumption that two neighboring sentences share a \u201ccommon root\u201d.", 
        "49": "Therefore, a cross-sentence dependency path can be represented as two shortest dependency path branches from the ends to the \u201ccommon root\u201d, as shown in Figure 2.", 
        "50": "Stanford CoreNLP4 is used to parsing syntactic structures of sentences in this work.", 
        "51": "3.2 Temporal Relation Classifiers  Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a natural choice for processing sequential dependency paths.", 
        "52": "As the reversed order also takes useful information, a backward representation can be achieved by feeding LSTM with the same input in reverse.", 
        "53": "We adopt the concatenation of the forward and backward LSTMs outputs, referred to as bidirectional LSTM (Graves and Schmidhuber, 2005).", 
        "54": "Figure 3a shows the neural network architecture of our E-E, E-T classifier.", 
        "55": "Given an E-E or E-T temporal link, our system first generates two SDP branches: 1) the source entity to common root, 2) the target entity to common root.", 
        "56": "For each word along a SDP branch, concatenation of word, POS and dependency relation (DEP) embeddings (word-level) is fed into Bi-LSTM.", 
        "57": "The forward and backward outputs of both source and target branches are all concatenated, and fed into a fully connected hidden units layer.", 
        "58": "The final Softmax layer generates multi-class predictions.", 
        "59": "Since an E-D link contains single event SDP branch, our system applies a similar architecture, but with single branch Bi-LSTM with outputs fed into the penultimate hidden layer, as shown in Figure 3b.", 
        "60": "In this work, we use word2vec5 (Mikolov et al., 4http://stanfordnlp.github.io/CoreNLP/ 5https://code.google.com/archive/p/word2vec/\n2013a,b) to train 200-dimensions word embeddings on English Gigaword 4th edition with skipgram model and other default settings.", 
        "61": "For either of POS or DEP, we adopt the 50-dimensions lookup table initialized randomly.", 
        "62": "4 Experiments    4.1 Hyper-parameters and Cross-validation  The grid search exploring a full hyper-parameter space takes time for three classifiers (E-E, E-T and E-D).", 
        "63": "Empirically, we set each single LSTM output with the same dimensions (equal to 300) as the concatenation of word, POS, DEP embeddings.", 
        "64": "The hidden layer is set as 200-dimensions.", 
        "65": "Our system adopts dependency paths as input, which means that the entities in the same sentences contain highly covered word sequence input.", 
        "66": "Simple cross-validation (CV) on links can not reflect the generalization ability of our model correctly.", 
        "67": "We use a grouped 5-fold CV based on the source entity ids (document id + sentence id) of links.", 
        "68": "This schema can reduce bias separately in either the source SDP or the target SDP.", 
        "69": "Although document level CV can avoid this issue, it\u2019s not feasible for TimeBank-Dense because it contains only 27 training documents.", 
        "70": "Early stopping is used to save the best model based on the validation data.", 
        "71": "In each run of the 5-fold cross-validation, we split 80% of \u2018original training\u2019 as \u2018tentative training\u2019 and 20% as \u2018tentative test\u2019.", 
        "72": "85% of \u2018tentative training\u2019 is used to learning and 15% is used for validation.", 
        "73": "We also adopt early stopping in the final system on the validation data (15% of \u2018original training\u2019).", 
        "74": "The patience is set as 10.", 
        "75": "Dropout (Srivastava et al., 2014) recently is proved to be an useful approach to prevent neural networks from over-fitting.", 
        "76": "We adopt dropout\nseparately after the following layers: embeddings, LSTM, and hidden layer to investigate the impact of dropout on performance.", 
        "77": "Table 1 shows the best CV results recorded in tuning dropout.", 
        "78": "The hyperparameter setting with the best CV performance is adopted in the final system.", 
        "79": "4.2 Overall Performance  Recently, Mirza and Tonelli (2016) report state-ofthe-art performance on TimeBank-Dense.", 
        "80": "They show the new attempt to mine the value of lowdimensions word embeddings by concatenating them with sparse traditional features.", 
        "81": "Their traditional features include entity attributes, temporal signals, semantic information of WordNet, etc., which means it\u2019s a hard setting for challenging their performance.", 
        "82": "In Table 2 and 3, \u2018Mirza\u2019 denotes their system.", 
        "83": "Table 2 shows the detailed comparison to\ntheir work.", 
        "84": "Our system achieves higher performance on \u2018AFTER\u2019, \u2018VAGUE\u2019, while lower on \u2018BEFORE\u2019, \u2018INCLUDES\u2019 (5% of all data) and \u2018IS INCLUDED\u2019 (4% of all data).", 
        "85": "It is likely that their rich traditional features help the classifiers to capture more minority-class links.", 
        "86": "On the whole, our system reaches better \u2018Overall\u2019 on both E-E and E-D. As their E-T classifier does not include word embeddings, the E-T results are not listed.", 
        "87": "The final comparison is shown in Table 3.", 
        "88": "An one-layer fully connected hidden units baseline (200-dimensions) with word, POS embeddings as input (without any dependency information) is provided.", 
        "89": "The significant out-performance of our proposed model over the baseline indicates the effectiveness of the dependency path information and our Bi-LSTM in classifying temporal links.", 
        "90": "As a hybrid system, \u2018CAEVO\u2019 (Chambers et al., 2014) includes hand-crafted rules for their E-T and E-D classifiers.", 
        "91": "For instance, the temporal prepositions in, on, over, during, and within indicate \u2018IN INCLUDED\u2019 relations.", 
        "92": "Their system is superior in E-T and E-D. \u2019Miza\u2019 takes the pure feature-\nbased methods and performs slightly better in EE and overall, compared to \u2018CAEVO\u2019.", 
        "93": "Our system shows the highest scores in E-E and overall among the four systems.", 
        "94": "In general, our system achieves comparable performance to two state-ofthe-art systems, without using any hand-crafted features, rules, or external resources.", 
        "95": "5 Conclusion  We borrow the idea of the dependency path based neural networks into temporal relation classification.", 
        "96": "A \u201ccommon root\u201d assumption adapts our model to cross-sentence links.", 
        "97": "Our model adopts bidirectional LSTM for capturing both forward and backward orders information.", 
        "98": "We observe the significant benefit of the DP-based Bi-LSTM model by comparing it to the baseline.", 
        "99": "Our model achieves comparable performance to two state-ofthe-art systems without using any explicit features (class, tense, polarity, etc.)", 
        "100": "or external resources, which indicates that our model can capture such information automatically.", 
        "101": "6 Acknowledgments  We thank the anonymous reviewers for the insightful comments."
    }, 
    "document_id": "P17-2001.pdf.json"
}
