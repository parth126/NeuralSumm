{
    "abstract_sentences": {
        "1": "We present a new framework for evaluating extractive summarizers, which is based on a principled representation as optimization problem.", 
        "2": "We prove that every extractive summarizer can be decomposed into an objective function and an optimization technique.", 
        "3": "We perform a comparative analysis and evaluation of several objective functions embedded in wellknown summarizers regarding their correlation with human judgments.", 
        "4": "Our comparison of these correlations across two datasets yields surprising insights into the role and performance of objective functions in the different summarizers."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 26\u201331 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2005  1 Introduction  The task of extractive summarization (ES) can naturally be cast as a discrete optimization problem where the text source is considered as a set of sentences and the summary is created by selecting an optimal subset of the sentences under a length constraint (McDonald, 2007; Lin and Bilmes, 2011).", 
        "2": "In this work, we go one step further and mathematically prove that ES is equivalent to the problem of choosing (i) an objective function \u03b8 for scoring system summaries, and (ii) an optimizer O.", 
        "3": "We use (\u03b8, O) to denote the resulting decomposition of any extractive summarizer.", 
        "4": "Our proposed decomposition enables a principled analysis and evaluation of existing summarizers, and addresses a major issue in the current evaluation of ES.", 
        "5": "This issue concerns the traditional \u201cintrinsic\u201d evaluation comparing system summaries against human reference summaries.", 
        "6": "This kind of evaluation is actually an end-to-end evaluation of summarization systems which is performed after \u03b8 has been optimized by O.", 
        "7": "This is highly problematic\nfrom an evaluation point of view, because first, \u03b8 is typically not optimized exactly, and second, there might be side-effects caused by the particular optimization technique O, e.g., a sentence extracted to maximize \u03b8 might be suitable because of other properties not included in \u03b8.", 
        "8": "Moreover, the commonly used evaluation metric ROUGE yields a noisy surrogate evaluation (despite its good correlation with human judgments) compared to the much more meaningful evaluation based on human judgments.", 
        "9": "As a result, the current end-toend evaluation does not provide any insights into the task of automatic summarization.", 
        "10": "The (\u03b8,O) decomposition we propose addresses this issue: it enables a well-defined and principled evaluation of extractive summarizers on the level of their components \u03b8 and O.", 
        "11": "In this work, we focus on the analysis and evaluation of \u03b8, because \u03b8 is a model of the quality indicators of a summary, and thus crucial in order to understand the properties of \u201cgood\u201d summaries.", 
        "12": "Specifically, we compare \u03b8 functions of different summarizers by measuring the correlation of their \u03b8 functions with human judgments.", 
        "13": "Our goal is to provide an evaluation framework which the research community could build upon in future research to identify the best possible \u03b8 and use it in optimization-based systems.", 
        "14": "We believe that the identification of such a \u03b8 is the central question of summarization, because this optimal \u03b8 would represent an optimal definition of summary quality both from an algorithmic point of view and from the human perspective.", 
        "15": "In summary, our contribution is twofold: (i) We present a novel and principled evaluation framework for ES which allows evaluating the objective function and the optimization technique separately and independently.", 
        "16": "(ii) We compare wellknown summarization systems regarding their implicit choices of \u03b8 by measuring the correlation\n26\nof their \u03b8 functions with human judgments on two datasets from the Text Analysis Conference (TAC).", 
        "17": "Our comparative evaluation yields surprising results and shows that extractive summarization is not solved yet.", 
        "18": "The code used in our experiments, including a general evaluation tool is available at github.com/UKPLab/acl2017-theta_ evaluation_summarization.", 
        "19": "2 Evaluation Framework    2.1 (\u03b8,O) decomposition  Let D = {si} be a document collection considered as a set of sentences.", 
        "20": "A summary S is then a subset of D, or we can say that S is an element of P(D), the power set of D. Objective function We define an objective function to be a function that takes a summary of the document collection D and outputs a score:\n\u03b8 : P(D) \u2192 R S 7\u2192 \u03b8D(S) (1)\nOptimizer Then the task of ES is to select the set of sentences S\u2217 with maximal \u03b8(S\u2217) under a length constraint:\nS\u2217 = argmax S \u03b8(S)\nlen(S) = \u2211\ns\u2208S len(s) \u2264 c (2)\nWe use O to denote the technique which solves this optimization problem.", 
        "21": "O is an operator which takes an objective function \u03b8 from the set of all objective functions \u0398 and a document collection D from the set of all document collections D, and outputs a summary S\u2217:\nO : \u0398\u00d7D \u2192 S (\u03b8,D) 7\u2192 S\u2217 (3)\nDecomposition Theorem Now we show that the problem of ES is equivalent to the problem of choosing a decomposition (\u03b8, O).", 
        "22": "We formalize an extractive summarizer \u03c3 as a set function which takes a document collection D \u2208 D and outputs a summary SD,\u03c3 \u2208 P(D).", 
        "23": "With this formalism, it is clear that every (\u03b8,O) tuple forms a summarizer because O(\u03b8, \u00b7) produces a summary from a document collection.", 
        "24": "But the other direction is also true: for every extractive summarizer there exists at least one tuple (\u03b8, O) which perfectly describes the summarizer:\nTheorem 1 \u2200\u03c3, \u2203(\u03b8,O) such that: \u2200D \u2208 D, \u03c3(D) = O(\u03b8,D)\nThis theorem is quite intuitive, especially since it is common to use a similar decomposition in optimization-based summarization systems.", 
        "25": "In the next section we illustrate the theorem by way of several examples, and provide a rigorous proof of the existence in the supplemental material.", 
        "26": "2.2 Examples of \u03b8  We analyze a range of different summarizers regarding their (mostly implicit) \u03b8. ICSI (Gillick and Favre, 2009) is a global linear optimization that extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents.", 
        "27": "ICSI has been among the best systems in a classical ROUGE evaluation (Hong et al., 2014).", 
        "28": "For ICSI, the identification of \u03b8 is trivial because it was formulated as an optimization task.", 
        "29": "If ci is the i-th bigram selected in the summary and wi its weight computed from D, then:\n\u03b8ICSI(S) = \u2211\nci\u2208S ci \u2217 wi (4)\nLexRank (Erkan and Radev, 2004) is a wellknown graph-based approach.", 
        "30": "A similarity graph G(V,E) is constructed where V is the set of sentences and an edge eij is drawn between sentences vi and vj if and only if the cosine similarity between them is above a given threshold.", 
        "31": "Sentences are scored according to their PageRank score inG.", 
        "32": "We observe that \u03b8LexRank is given by:\n\u03b8LexRank(S) = \u2211\ns\u2208S PRG(s) (5)\nwhere PR is the PageRank score of sentence s. KL-Greedy (Haghighi and Vanderwende, 2009) minimizes the Kullback Leibler (KL) divergence between the word distributions in the summary and D (i.e \u03b8KL = \u2212KL).", 
        "33": "Recently, Peyrard and Eckle-Kohler (2016) optimized KL and Jensen Shannon (JS) divergence with a genetic algorithm.", 
        "34": "In this work, we use KL and JS for both unigram and bigram distributions.", 
        "35": "LSA (Steinberger and Jezek, 2004) is an approach involving a dimensionality reduction of the termdocument matrix via Singular Value Decomposition (SVD).", 
        "36": "The sentences extracted should cover the most important latent topics:\n\u03b8LSA = \u2211\nt\u2208S \u03bbt (6)\nwhere t is a latent topic identified by SVD on the term-document matrix and \u03bbt the associated singular value.", 
        "37": "Edmundson (Edmundson, 1969) is an older heuristic method which scores sentences according to cue-phrases, overlap with title, term frequency and sentence position.", 
        "38": "\u03b8Edmundson is simply a weighted sum of these heuristics.", 
        "39": "TF?IDF (Luhn, 1958) scores sentences with the TF*IDF of their terms.", 
        "40": "The best sentences are then greedily extracted.", 
        "41": "We use both the unigram and bigram versions in our experiments.", 
        "42": "3 Experiments  Now we compare the summarizers analyzed above by measuring the correlation of their \u03b8 functions with human judgments.", 
        "43": "Datasets We use two multi-document summarization datasets from the Text Analysis Conference (TAC) shared task: TAC-2008 and TAC2009.1 TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively.", 
        "44": "Each topic consists of 10 news articles to be summarized in a maximum of 100 words.", 
        "45": "We use only the so-called initial summaries (A summaries), but not the update part.", 
        "46": "For each topic, there are 4 human reference summaries along with a manually created Pyramid set.", 
        "47": "In both editions, all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for readability, content selection (with Pyramid) and overall responsiveness.", 
        "48": "At the time of the shared tasks, 57 systems were submitted to TAC-2008 and 55 to TAC-2009.", 
        "49": "For our experiments, we use the Pyramid and the responsiveness annotations.", 
        "50": "System Comparison For each \u03b8, we compute the scores of all system and all manual summaries for any given topic.", 
        "51": "These scores are compared with the human scores.", 
        "52": "We include the manual summaries in our computation because this yields a more diverse set of summaries with a wider range of scores.", 
        "53": "Since an ideal summarizer would create summaries as well as humans, an ideal \u03b8 would also be able to correctly score human summaries with high scores.", 
        "54": "For comparison, we also report the correlation between pyramid and responsiveness.", 
        "55": "Correlations are measured with 3 metrics: Pear1http://tac.nist.gov/2009/ Summarization/, http://tac.nist.gov/2008/ Summarization/\nson\u2019s r, Spearman\u2019s \u03c1 and Normalized Discounted Cumulative Gain (Ndcg).", 
        "56": "Pearson\u2019s r is a value correlation metric which depicts linear relationships between the scores produced by \u03b8 and the human judgments.", 
        "57": "Spearman\u2019s \u03c1 is a rank correlation metric which compares the ordering of systems induced by \u03b8 and the ordering of systems induced by human judgments.", 
        "58": "Ndcg is a metric that compares ranked lists and puts more emphasis on the top elements by logarithmic decay weighting.", 
        "59": "Intuitively, it captures how well \u03b8 can recognize the best summaries.", 
        "60": "The optimization scenario benefits from high Ndcg scores because only summaries with high \u03b8 scores are extracted.", 
        "61": "Previous work on correlation analysis averaged scores over topics for each system and then computed the correlation between averaged scores (Louis and Nenkova, 2013; Nenkova et al., 2007).", 
        "62": "An alternative and more natural option which we use here is to compute the correlation for each topic and average these correlations over topics (CORRELATION-AVERAGE).", 
        "63": "Since we want to estimate how well \u03b8 functions measure the quality of summaries, we find the summary level averaging more meaningful.", 
        "64": "Analysis The results of our correlation analysis are presented in Table 1.", 
        "65": "In our (\u03b8,O) formulation, the end-to-end approach maps a set of documents to exactly one summary selected by the system.", 
        "66": "We call the (classical and well known) evaluation of this single summary end-to-end evaluation because it measures the end product of the system.", 
        "67": "This is in contrast to our proposed evaluation of the assumption made by individual summarizers shown in Table 1.", 
        "68": "A system summary was extracted by a given system because it was high scoring using its \u03b8, but we ask the question whether optimizing this \u03b8 made sense in the first place.", 
        "69": "We first observe that scores are relatively low.", 
        "70": "Summarization is not a solved problem and the systems we investigated can not identify correctly what makes a good summary.", 
        "71": "This is in contrast to the picture in the classical end-to-end evaluation with ROUGE where state-of-the-art systems score relatively high.", 
        "72": "Some Ndcg scores are higher (for TAC-2008) which explains why these systems can extract relatively good summaries in the end-toend evaluation.", 
        "73": "In this classical evaluation, only the single best summary is evaluated, which means that a system does not need to be able to rank all\npossible summaries correctly.", 
        "74": "We see that systems with high end-to-end ROUGE scores (according to Hong et al.", 
        "75": "(2014)) do not necessarily have a good model of summary quality.", 
        "76": "Indeed, the best performing \u03b8 functions are not part of the systems performing best with ROUGE.", 
        "77": "For example, ICSI is the best system according to ROUGE, but it is not clear that it has the best model of summary quality.", 
        "78": "In TAC-2009, LexRank, LSA and the heuristic Edmundson have better correlations with human judgments.", 
        "79": "The difference with end-to-end evaluation might stem from the fact that ICSI solves the optimization problem exactly, while LexRank and Edmundson use greedy optimizers.", 
        "80": "There might also be some side-effects from which ICSI profits: extracting sentences to improve \u03b8 might lead to accidentally selecting suitable sentences, because \u03b8 can merely correlate well with properties of good summaries, while not modeling these properties itself.", 
        "81": "It is worth noting that systems perform differently on TAC2009 and TAC2008.", 
        "82": "There are several differences between TAC2008 and TAC2009 like redundancy level or guidelines for annotations; for example, responsiveness is scored out of 5 in 2008 and out of 10 in 2009.", 
        "83": "The LSA summarizer ranks among the best systems in TAC2009 with pearson\u2019s r but is closer to the worst systems in TAC2008.", 
        "84": "While this is difficult to explain we hypothesize that the model of summary quality from LSA is sensitive to the slight variations and therefore not robust.", 
        "85": "In general, any system which claims to have a better \u03b8 than previous works should indeed report results on several datasets to ensure robustness and generality.", 
        "86": "Interestingly, we observe that the correlation between Pyramid and responsiveness is better than in\nany system, but still not particularly high.", 
        "87": "Responsiveness is an overall annotation while Pyramid is a manual measure of content only.", 
        "88": "These results confirm the intuition that humans take into account much more aspects when evaluating summaries.", 
        "89": "4 Related Work and Discussion  While correlation analyses on human judgment data have been performed in the context of validating automatic summary evaluation metrics (Louis and Nenkova, 2013; Nenkova et al., 2007; Lin, 2004), there is no prior work which uses these data for a principled comparison of summarizers.", 
        "90": "Much previous work focused on efficient optimizers O, such as ILP, which impose constraints on the \u03b8 function.", 
        "91": "Linear (Gillick and Favre, 2009) and submodular (Lin and Bilmes, 2011) \u03b8 functions are widespread in the summarization community because they can be optimized efficiently and effectively via ILP (Schrijver, 1986) and the greedy algorithm for submodularity (Fujishige, 2005).", 
        "92": "A greedy approach is often used when \u03b8 does not have convenient properties that can be leveraged by a classical optimizer (Haghighi and Vanderwende, 2009).", 
        "93": "Such interdependencies of O and \u03b8 limit the expressiveness of \u03b8.", 
        "94": "However, realistic \u03b8 functions are unlikely to be linear or submodular, and in the well-studied field of optimization there exist a range of different techniques developed to tackle difficult combinatorial problems (Schrijver, 2003; Blum and Roli, 2003).", 
        "95": "A recent example of such a technique adapted to extractive summarization are meta-heuristics used to optimize non-linear, non-submodular objective functions (Peyrard and Eckle-Kohler, 2016).", 
        "96": "Other methods like Markov Chain Monte Carlo (Metropolis et al., 1953) or Monte-Carlo Tree Search (Suttner and Ertel, 1991; Silver et al., 2016) could also be adapted to summarization and thus become realistic choices for O.", 
        "97": "General purpose optimization techniques are especially appealing, because they offer a decoupling of \u03b8 and O and allow investigating complex \u03b8 functions without making any assumption on their mathematical properties.", 
        "98": "In particular, this supports future work on identifying an \u201coptimal\u201d \u03b8 as a model of relevant quality aspects of a summary.", 
        "99": "5 Conclusion  We presented a novel evaluation framework for ES which is based on the proof that ES is equivalent to the problem of choosing an objective function \u03b8 and an optimizer O.", 
        "100": "This principled and welldefined framework allows evaluating \u03b8 and O of any extractive summarizer \u2013 separately and independently.", 
        "101": "We believe that our framework can serve as a basis for future work on identifying an \u201coptimal\u201d \u03b8 function, which would provide an answer to the central question of what are the properties of a \u201cgood\u201d summary.", 
        "102": "Acknowledgments  This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group \u201cAdaptive Preparation of Information from Heterogeneous Sources\u201d (AIPHES) under grant No.", 
        "103": "GRK 1994/1, and via the GermanIsraeli Project Cooperation (DIP, grant No.", 
        "104": "GU 798/17-1)."
    }, 
    "document_id": "P17-2005.pdf.json"
}
