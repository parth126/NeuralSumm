{
    "abstract_sentences": {
        "1": "Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media.", 
        "2": "We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-ofthe-art parser trained on the Singlish treebank.", 
        "3": "Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies.", 
        "4": "To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages.", 
        "5": "We make both our annotation and parser available for further research."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1732\u20131744 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1159  1 Introduction  Languages evolve temporally and geographically, both in vocabulary as well as in syntactic structures.", 
        "2": "When major languages such as English or French are adopted in another culture as the primary language, they often mix with existing languages or dialects in that culture and evolve into a stable language called a creole.", 
        "3": "Examples of creoles include the French-based Haitian Creole, and Colloquial Singaporean English (Singlish) (MianLian and Platt, 1993), an English-based creole.", 
        "4": "While the majority of the natural language processing (NLP) research attention has been focused on the major languages, little work has been done on adapting the components to creoles.", 
        "5": "One notable body of work originated from the featured\ntranslation task of the EMNLP 2011 Workshop on Statistical Machine Translation (WMT11) to translate Haitian Creole SMS messages sent during the 2010 Haitian earthquake.", 
        "6": "This work highlights the importance of NLP tools on creoles in crisis situations for emergency relief (Hu et al., 2011; Hewavitharana et al., 2011).", 
        "7": "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media.", 
        "8": "Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009).", 
        "9": "Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4.", 
        "10": "For example, Seah et al.", 
        "11": "(2015) adapted the Socher et al.", 
        "12": "(2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser.", 
        "13": "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish.", 
        "14": "To address this issue, we start with investigating the linguistic characteristics of Singlish and specifically the causes of difficulties for understanding Singlish with English syntax.", 
        "15": "We found that, despite the obvious attribute of inheriting a large portion of basic vocabularies and grammars from English, Singlish not only imports terms from regional languages and dialects, its lexical\n1We follow Leimgruber (2011) in using \u201cgrammar\u201d to describe \u201csyntactic constructions\u201d and we do not differentiate the two expressions in this paper.", 
        "16": "1732\nsemantics and syntax also deviate significantly from English (Leimgruber, 2009, 2011).", 
        "17": "We categorize the challenges and formalize their interpretation using Universal Dependencies (Nivre et al., 2016), which extends to the creation of a Singlish dependency treebank with 1,200 sentences.", 
        "18": "Based on the intricate relationship between Singlish and English, we build a Singlish parser by leveraging knowledge of English syntax as a basis.", 
        "19": "This overall approach is illustrated in Figure 1.", 
        "20": "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2017), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.", 
        "21": "Since POS tags are important features for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking.", 
        "22": "Results show that English syntax knowledge brings 51.50% and 25.01% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.47% unlabeled attachment score (UAS) and 77.76% labeled attachment score (LAS).", 
        "23": "We make our Singlish dependency treebank, the source code for training a dependency parser and the trained model for the parser with the best performance freely available online2.", 
        "24": "2https://github.com/wanghm92/Sing_Par  2 Related Work  Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017).", 
        "25": "In particular, the biaffine attention method of Dozat and Manning (2017) uses deep bi-directional long short-term memory (bi-LSTM) networks for highorder non-linear feature extraction, producing the highest-performing graph-based English dependency parser.", 
        "26": "We adopt this model as the basis for our Singlish parser.", 
        "27": "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).", 
        "28": "Seminal work employed statistical models.", 
        "29": "McDonald et al.", 
        "30": "(2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language.", 
        "31": "Subsequent work considered syntactic similarities between languages for better feature transfer (Ta\u0308ckstro\u0308m et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).", 
        "32": "Recently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016).", 
        "33": "The basic idea is to map the word embedding spaces between different languages into the same vector space, by using sentence-aligned bilingual data.", 
        "34": "This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016).", 
        "35": "Our work is similar to these methods in using a neural network model for knowledge sharing between different languages.", 
        "36": "However, ours is different in the use of a neural stacking model, which respects the distributional differences between Singlish and English words.", 
        "37": "This empirically gives higher accuracies for Singlish.", 
        "38": "Neural stacking was previously used for cross-annotation (Chen et al., 2016) and crosstask (Zhang and Weiss, 2016) joint-modelling on monolingual treebanks.", 
        "39": "To the best of our knowledge, we are the first to employ it on cross-lingual\nfeature transfer from resource-rich languages to improve dependency parsing for low-resource languages.", 
        "40": "Besides these three dimensions in dealing with heterogeneous text data, another popular area of research is on the topic of domain adaption, which is commonly associated with crosslingual problems (Nivre et al., 2007).", 
        "41": "While this large strand of work is remotely related to ours, we do not describe them in details.", 
        "42": "Unsupervised rule-based approaches also offer an competitive alternative for cross-lingual dependency parsing (Naseem et al., 2010; Gillenwater et al., 2010; Gelling et al., 2012; S\u00f8gaard, 2012a,b; Mart\u0131\u0301nez Alonso et al., 2017), and recently been benchmarked for the Universal Dependencies formalism by exploiting the linguistic constraints in the Universal Dependencies to improve the robustness against error propagation and domain adaption (Mart\u0131\u0301nez Alonso et al., 2017).", 
        "43": "However, we choose a data-driven supervised approach given the relatively higher parsing accuracy owing to the availability of resourceful treebanks from the Universal Dependencies project.", 
        "44": "3 Singlish Dependency Treebank    3.1 Universal Dependencies for Singlish  Since English is the major genesis of Singlish, we choose English as the source of lexical feature transfer to assist Singlish dependency parsing.", 
        "45": "Universal Dependencies provides a set of multilingual treebanks with cross-lingually consistent dependency-based lexicalist annotations, designed to aid development and evaluation for cross-lingual systems, such as multilingual parsers (Nivre et al., 2016).", 
        "46": "The current version of Universal Dependencies comprises not only major treebanks for 47 languages but also their siblings for domain-specific corpora and dialects.", 
        "47": "With the aligned initiatives for creating transfer-learning-friendly treebanks, we adopt the Universal Dependencies protocol for constructing the Singlish dependency treebank, both as a new resource for the low-resource languages and to facilitate knowledge transfer from English.", 
        "48": "On top of the general Universal Dependencies guidelines, English-specific dependency relation definitions including additional subtypes are employed as the default standards for annotating the Singlish dependency treebank, unless augmented or redefined when necessary.", 
        "49": "The latest English\ncorpus in Universal Dependencies v1.43 collection is constructed from the English Web Treebank (Bies et al., 2012), comprising of web media texts, which potentially smooths the knowledge transfer to our target Singlish texts in similar domains.", 
        "50": "The statistics of this dataset, from which we obtain English syntactic knowledge, is shown in Table 1 and we refer to this corpus as UD-Eng.", 
        "51": "This corpus uses 47 dependency relations and we show below how to conform to the same standard while adapting to unique Singlish grammars.", 
        "52": "3.2 Challenges and Solutions for Annotating Singlish  The deviations of Singlish from English come from both the lexical and the grammatical levels (Leimgruber, 2009, 2011), which bring challenges for analysis on Singlish using English NLP tools.", 
        "53": "The former involves imported vocabularies from the first languages of the local people and the latter can be represented by a set of relatively localized features which collectively form 5 unique grammars of Singlish according to Leimgruber (2011).", 
        "54": "We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions while ensuring consistency with the annotations in other non-English UD treebanks, which are explained with examples as follows.", 
        "55": "Imported vocabulary: Singlish borrows a number of words and expressions from its nonEnglish origins (Leimgruber, 2009, 2011), such as \u201cKiasu\u201d, which originates from Hokkien meaning \u201cvery anxious not to miss an opportunity\u201d.4 These imported terms often constitute out-of-vocabulary (OOV) words with respect to a standard English treebank and result in difficulties for using English-trained tools on Singlish.", 
        "56": "All borrowed words are annotated based on their usages in Singlish, which mainly inherit the POS from their genesis languages.", 
        "57": "Table A4 in Appendix A\n3Only guidelines for Universal Dependencies v2 but not the English corpus is available when this work is completed.", 
        "58": "4Definition by the Oxford living Dictionaries for English.", 
        "59": "summarizes all borrowed terms in our treebank.", 
        "60": "Topic-prominence: This type of sentences start with establishing its topic, which often serves as the default one that the rest of the sentence refers to, and they typically employ an object-subjectverb sentence structure (Leimgruber, 2009, 2011).", 
        "61": "In particular, three subtypes of topic-prominence are observed in the Singlish dependency treebank and their annotations are addressed as follows:\nFirst, topics framed as clausal arguments at the beginning of the sentence are labeled as \u201ccsubj\u201d (clausal subject), as shown by \u201cDrive this car\u201d of (1) in Figure 2, which is consistent with the dependency relations in its Chinese translation.", 
        "62": "Second, noun phrases used to modify the predicate with the absence of a preposition is regarded as a \u201cnsubj\u201d (nominal subject).", 
        "63": "Similarly, this is a common order of words used in Chinese and one example is the \u201cSG\u201d of (2) in Figure 2.", 
        "64": "Third, prepositional phrases moved in front are still treated as \u201cnmod\u201d (nominal modifier) of their\nintended heads, following the exact definition but as a Singlish-specific form of exemplification, as shown by the \u201cInside tent\u201d of (3) in Figure 2.", 
        "65": "Although the \u201cdislocated\u201d (dislocated elements) relation in UD is also used for preposed elements, but it captures the ones \u201cthat do not fulfill the usual core grammatical relations of a sentence\u201d and \u201cnot for a topic-marked noun that is also the subject of the sentence\u201d (Nivre et al., 2016).", 
        "66": "In these three scenarios, the topic words or phrases are in relatively closer grammatical relations to the predicate, as subjects or modifiers.", 
        "67": "Copula deletion: Imported from the corresponding Chinese sentence structure, this copula verb is often optional and even deleted in Singlish, which is one of its diagnostic characteristics (Leimgruber, 2009, 2011).", 
        "68": "In UD-Eng standards, predicative \u201cbe\u201d is the only verb used as a copula and it often depends on its complement to avoid copular head.", 
        "69": "This is explicitly designed in UD to promote parallelism for zero-copula phenomenon in languages such as Russian, Japanese, and Arabic.", 
        "70": "The deleted copula and its \u201ccop\u201d (copula) arcs are simply ignored, as shown by (4) in Figure 2.", 
        "71": "NP deletion: Noun-phrase (NP) deletion often results in null subjects or objects.", 
        "72": "It may be regarded as a branch of \u201cTopic-prominence\u201d but is a distinctive feature of Singlish with relatively high frequency of usage (Leimgruber, 2011).", 
        "73": "NP deletion is also common in pronoun-dropping languages such as Spanish and Italian, where the anaphora can be morphologically inferred.", 
        "74": "In one example, \u201cVorrei ora entrare brevemente nel merito.\u201d5, from the Italian treebank in UD, \u201cVorrei\u201d means \u201cI would like to\u201d and depends on the sentence root, \u201centrare\u201d, with the \u201caux\u201d(auxiliary) relation, where the subject \u201cI\u201d is absent but implicitly understood.", 
        "75": "Similarly, we do not recover such relations since the deleted NP imposes negligible alteration to the dependency tree, as exemplified by (5) in Figure 2.", 
        "76": "Inversion: Inversion in Singlish involves either keeping the subject and verb in interrogative sentences in the same order as in statements, or tag questions in polar interrogatives (Leimgruber, 2011).", 
        "77": "The former also exists in non-English languages, such as Spanish and Italian, where the subject can prepose the verb in questions (La-\n5In English: (I) would now like to enter briefly on the merit (of the discussion).", 
        "78": "housse and Lamiroy, 2012).", 
        "79": "This simply involves a change of word orders and thus requires no special treatments.", 
        "80": "On the other hand, tag questions should be carefully analyzed in two scenarios.", 
        "81": "One type is in the form of \u201cisn\u2019t it?\u201d or \u201chaven\u2019t you?\u201d, which are dependents of the sentence root with the \u201cparataxis\u201d relation.6 The other type is exemplified as \u201cright?\u201d, and its Singlish equivalent \u201ctio boh?\u201d (a transliteration from Hokkien) are labeled with the \u201cdiscourse\u201d (discourse element) relation with respect to the sentence root.", 
        "82": "See example (6) in Figure 2.", 
        "83": "Discourse particles: Usage of clausal-final discourse particles, which originates from Hokkien and Cantonese, is one of the most typical feature of Singlish (Leimgruber, 2009, 2011; Lim, 2007).", 
        "84": "All discourse particles that appear in our treebank are summarized in Table A3 in Appendix A with the imported vocabulary:.", 
        "85": "These words express the tone of the sentence and thus have the \u201cINTJ\u201d (interjection) POS tag and depend on the root of the sentence or clause labeled with \u201cdiscourse\u201d, as is shown by the \u201cleh\u201d of (3) in Figure 2.", 
        "86": "The word \u201cone\u201d is a special instance of this type with the sole purpose being a tone marker in Singlish but not English, as shown by (7) in Figure 2.", 
        "87": "3.3 Data Selection and Annotation  Data Source: Singlish is used in written form mainly in social media and local Internet forums.", 
        "88": "After comparison, we chose the SG Talk Forum7 as our data source due to its relative abundance in Singlish contents.", 
        "89": "We crawled 84,459 posts using the Scrapy framework8 from pages dated up to 25th December 2016, retaining sentences of length between 5 and 50, which total 58,310.", 
        "90": "Sentences are reversely sorted according to the log likelihood of the sentence given by an English language model trained using the KenLM toolkit (Heafield et al., 2013)9 normalized by the sentence length, so that those most different from standard English can be chosen.", 
        "91": "Among the top 10,000 sentences, 1,977 sentences contain unique Singlish vocabularies defined by The\n6In UD: Relation between the main verb of a clause and other sentential elements, such as sentential parenthetical clause, or adjacent sentences without any explicit coordination or subordination.", 
        "92": "7http://sgTalk.com 8https://scrapy.org/ 9Trained using the afp eng and xin eng sources of English\nGigaword Fifth Edition (Gigaword).", 
        "93": "Coxford Singlish Dictionary10, A Dictionary of Singlish and Singapore English11, and the Singlish Vocabulary Wikipedia page12.", 
        "94": "The average normalized log likelihood of these 10,000 sentences is -5.81, and the same measure for all sentences in UD-Eng is -4.81.", 
        "95": "This means these sentences with Singlish contents are 10 times less probable expressed as standard English than the UD-Eng contents in the web domain.", 
        "96": "This contrast indicates the degree of lexical deviation of Singlish from English.", 
        "97": "We chose 1,200 sentences from the first 10,000.", 
        "98": "More than 70% of the selected sentences are observed to consist of the Singlish grammars and imported vocabularies described in section 3.2.", 
        "99": "Thus the evaluations on this treebank can reflect the performance of various POS taggers and parsers on Singlish in general.", 
        "100": "Annotation: The chosen texts are divided by random selection into training, development, and testing sets according to the proportion of sentences in the training, development, and test division for UD-Eng, as summarized in Table 1.", 
        "101": "The sentences are tokenized using the NLTK Tokenizer,13 and then annotated using the Dependency Viewer.14 In total, all 17 UD-Eng POS tags and 41 out of the 47 UD-Eng dependency labels are present in the Singlish dependency treebank.", 
        "102": "Besides, 100 sentences are randomly selected and double annotated by one of the coauthors, and the inter-annotator agreement has a 97.76% accuracy on POS tagging and a 93.44% UAS and a 89.63% LAS for dependency parsing.", 
        "103": "A full summary of the numbers of occurrences of each POS tag and dependency label are included in Appendix A.", 
        "104": "4 Part-of-Speech Tagging  In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al.", 
        "105": "(2016), depicted in Figure 3.", 
        "106": "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).", 
        "107": "10http://72.5.72.93/html/lexec.php 11http://www.singlishdictionary.com 12https://en.wikipedia.org/wiki/\nSinglish_vocabulary 13http://www.nltk.org/api/nltk.", 
        "108": "tokenize.html 14http://nlp.nju.edu.cn/tanggc/tools/ DependencyViewer.exe\nBased on this English POS tagging model, we train a POS tagger for Singlish using the featurelevel neural stacking model of Chen et al.", 
        "109": "(2016).", 
        "110": "Both the English and Singlish models consist of an input layer, a feature layer, and an output layer.", 
        "111": "4.1 Base Bi-LSTM-CRF POS Tagger  Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al.", 
        "112": "(2014).", 
        "113": "Following Chen et al.", 
        "114": "(2016), the input layer produces a dense representation for the current input token by concatenating its word vector and the ones for its surrounding context tokens in a window of finite size.", 
        "115": "Feature Layer: This layer employs a bi-LSTM network to encode the input into a sequence of hidden vectors that embody global contextual information.", 
        "116": "Following Chen et al.", 
        "117": "(2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).", 
        "118": "Output layer: This is a CRF layer to predict the POS tags for the input words by maximizing the conditional probability of the sequence of tags given input sentence.", 
        "119": "4.2 POS Tagger with Neural Stacking  We adopt the deep integration neural stacking structure presented in Chen et al.", 
        "120": "(2016).", 
        "121": "As shown in Figure 4, the distributed vector representation for the target word at the input layer of the Singlish Tagger is augmented by concatenating the emission vector produced by the English Tagger with the original word and character-based embeddings, before applying the concatenation within a context window in section 4.1.", 
        "122": "During training, loss is back-propagated to all trainable parameters\nin both the Singlish Tagger and the pre-trained feature layer of the base English Tagger.", 
        "123": "At test time, the input sentence is fed to the integrated tagger model as a whole for inference.", 
        "124": "4.3 Results  We use the publicly available source code15 by Chen et al.", 
        "125": "(2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011).", 
        "126": "We set the hidden layer size to 300, the initial learning rate for Adagrad (Duchi et al., 2011) to 0.01, the regularization parameter \u03bb to 10\u22126, and the dropout rate to 15%.", 
        "127": "The tagger gives 94.84% accuracy on the UD-Eng test set after 24 epochs, chosen according to development tests, which is comparable to the stateof-the-art accuracy of 95.17% reported by Plank et al.", 
        "128": "(2016).", 
        "129": "We use these settings to perform 10- fold jackknifing of POS tagging on the UD-Eng training set, with an average accuracy of 95.60%.", 
        "130": "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.", 
        "131": "However, due to limited amount of training data, the\n15https://github.com/chenhongshen/ NNHetSeq\ntagging accuracy is not satisfactory even with a larger dropout rate to avoid over-fitting.", 
        "132": "In contrast, the neural stacking structure on top of the English base model trained on UD-Eng achieves a POS tagging accuracy of 89.50%16, which corresponds to a 51.50% relative error reduction over the baseline Singlish model, as shown in Table 2.", 
        "133": "We use this for 10-fold jackknifing on Singlish parsing training data, and tagging the Singlish development and test data.", 
        "134": "5 Dependency Parsing  We adopt the Dozat and Manning (2017) parser17 as our base model, as displayed in Figure 5, and apply neural stacking to achieve improvements over the baseline parser.", 
        "135": "Both the base and neural stacking models consist of an input layer, a feature layer, and an output layer.", 
        "136": "5.1 Base Parser with Bi-affine Attentions  Input Layer: This layer encodes the current input word by concatenating a pre-trained word embedding with a trainable word embedding and POS tag embedding from the respective lookup tables.", 
        "137": "Feature Layer: The two recurrent vectors produced by the multi-layer bi-LSTM network from each input vector are concatenated and mapped to multiple feature vectors in lower-dimension space by a set of parallel multilayer perceptron (MLP)\n16We empirically find that using ICE-SIN embeddings in neural stacking model performs better than using English SENNA embeddings.", 
        "138": "Similar findings are found for the parser, of which more details are given in section 6.", 
        "139": "17https://github.com/tdozat/Parser\nlayers.", 
        "140": "Following Dozat and Manning (2017), we adopt Cif-LSTM cells (Greff et al., 2016).", 
        "141": "Output Layer: This layer applies biaffine transformation on the feature vectors to calculate the score of the directed arcs between every pair of words.", 
        "142": "The inferred trees for input sentence are formed by choosing the head with the highest score for each word and a cross-entropy loss is calculated to update the model parameters.", 
        "143": "5.2 Parser with Neural Stacking  Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, trainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word.", 
        "144": "In order to further preserve syntactic knowledge retained by the English Tagger, the feature vectors from its MLP layer is added to the ones produced by the Singlish Parser, as illustrated in Figure 6, and the scoring tensor of the Singlish Parser is initialized with the one from the trained English Tagger.", 
        "145": "Loss is back-propagated by reversely traversing all forward paths to all trainable parameter for training and the whole model is used collectively for inference.", 
        "146": "6 Experiments    6.1 Experimental Settings  We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2017).", 
        "147": "It achieves an UAS of 88.83% and a LAS of 85.20%, which are close to the state-of-the-art 85.90% LAS on UD-Eng reported by Ammar et al.", 
        "148": "(2016), and the main difference is caused by us not using fine-grained POS tags.", 
        "149": "We apply the same settings for a baseline Singlish parser.", 
        "150": "We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best.", 
        "151": "Thus we stick to all default hyper-parameters in Dozat and Manning (2017) for training the Singlish parsers.", 
        "152": "We experimented with different word embeddings, as with the raw text sources summarized in Table 3 and further described in section 6.2.", 
        "153": "When using the neural stacking model, we fix the model configuration for the base English parser model and choose the size of the hidden vector and the number of bi-LSTM layers stacked on top based on the performance on the development set.", 
        "154": "It turns out that a 1-layer bi-LSTM with 900 hidden dimension performs the best, where the bigger hidden layer accommodates the elongated input vector to the stacked bi-LSTM and the fewer number of recurrent layers avoids over-fitting on the small Singlish dependency treebank, given the deep bi-LSTM English parser network at the bottom.", 
        "155": "The evaluation of the neural stacking model is further described in section 6.3.", 
        "156": "6.2 Investigating Distributed Lexical Characteristics  In order to learn characteristics of distributed lexical semantics for Singlish, we compare performances of the Singlish dependency parser using several sets of pre-trained word embeddings: GloVe6B, large-scale English word embeddings18; ICE-SIN, Singlish word embeddings trained using GloVe (Pennington et al., 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings.", 
        "157": "First, the English Giga100M embeddings marginally improve the Singlish parser from the baseline without pre-trained embeddings and also using the UD-Eng parser directly on Singlish, represented as \u201cENG-on-SIN\u201d in Table 4.", 
        "158": "With much more English lexical semantics being fed to the Singlish parser using the English GloVe6B embeddings, further enhancement is achieved.", 
        "159": "Nevertheless, the Singlish ICE-SIN embeddings lead to even more improvement, with 13.78% relative error reduction, compared with 7.04% using the English Giga100M embeddings and 9.16% using the English GloVe6B embeddings, despite the huge difference in sizes in the latter case.", 
        "160": "This demonstrates the distributional differences between Singlish and English tokens, even though they share a large vocabulary.", 
        "161": "More detailed comparison is described in section 6.4.", 
        "162": "6.3 Knowledge Transfer Using Neural Stacking  We train a parser with neural stacking and Singlish ICE-SIN embeddings, which achieves the best performance among all the models, with a UAS of 84.47%, represented as \u201cStack-ICE-SIN\u201d in Table 4, which corresponds to 25.01% relative error reduction compared to the baseline.", 
        "163": "This demonstrates that knowledge from English can be successfully incorporated to boost the Singlish parser.", 
        "164": "To further evaluate the effectiveness of the neural stacking model, we also trained a base model with the combination of UD-Eng and the Singlish tree-\n18Trained with Wikipedia 2014 the Gigaword.", 
        "165": "Downloadable from http://nlp.stanford.edu/data/ glove.6B.zip\nbank, represented as \u201cENG-plus-SIN\u201d in Table 4, which is still outperformed by the neural stacking model.", 
        "166": "Besides, we performed a 5-cross-fold validation for the base parser with Singlish ICE-SIN embeddings and the parser using neural stacking, where half of the held-out fold is used as the development set.", 
        "167": "The average UAS and LAS across the 5 folds shown in Table 5 and the relative error reduction on average 23.61% suggest that the overall improvement from knowledge transfer using neural stacking remains consistent.", 
        "168": "This significant improvement is further explained in section 6.4.", 
        "169": "6.4 Improvements over Grammar Types  To analyze the sources of improvements for Singlish parsing using different model configurations, we conduct error analysis over 5 syntactic categories19, including 4 types of grammars mentioned in section 3.220, and 1 for all other cases, including sentences containing imported vocabularies but expressed in basic English syntax.", 
        "170": "The number of sentences and the results in each group of the test set are shown in Table 6.", 
        "171": "The neural stacking model leads to the biggest improvement over all categories except for a tie UAS performance on \u201cNP Deletion\u201d cases, which explains the significant overall improvement.", 
        "172": "Comparing the base model with ICE-SIN embeddings with the base parser trained on UD-Eng, which contain syntactic and semantic knowledge in Singlish and English, respectively, the former outperforms the latter on all 4 types of Singlish grammars but not for the remaining samples.", 
        "173": "This suggests that the base English parser mainly contributes to analyzing basic English syntax, while the base Singlish parser models unique Singlish grammars better.", 
        "174": "Similar trends are also observed on the base model using the English Giga100M embeddings, but the overall performances are not as good as\n19Multiple labels are allowed for one sentence.", 
        "175": "20The \u201cInversion\u201d type of grammar is not analyzed since\nthere is only 1 such sentence in the test set.", 
        "176": "using ICE-SIN embeddings, especially over basic English syntax where it undermines the performance to a greater extent.", 
        "177": "This suggests that only limited English distributed lexical semantic information can be integrated to help modelling Singlish syntactic knowledge due to the differences in distributed lexical semantics.", 
        "178": "7 Conclusion  We have investigated dependency parsing for Singlish, an important English-based creole language, through annotations of a Singlish dependency treebank with 10,986 words and building an enhanced parser by leveraging on knowledge transferred from a 20-times-bigger English treebank of Universal Dependencies.", 
        "179": "We demonstrate the effectiveness of using neural stacking for feature transfer by boosting the Singlish dependency parsing performance to from UAS 79.29% to UAS 84.47%, with a 25.01% relative error reduction over the parser with all available Singlish resources.", 
        "180": "We release the annotated Singlish dependency treebank, the trained model and the source code for the parser with free public access.", 
        "181": "Possible future work include expanding the investigation to other regional languages such as Malay and Indonesian.", 
        "182": "Acknowledgments  Yue Zhang is the corresponding author.", 
        "183": "This research is supported by IGDSS1603031 from Temasek Laboratories@SUTD.", 
        "184": "We appreciate anonymous reviewers for their insightful comments, which helped to improve the paper, and Zhiyang Teng, Jiangming Liu, Yupeng Liu, and Enrico Santus for their constructive discussions.", 
        "185": "A Statistics of Singlish Dependency Treebank  POS Tags ADJ 782 INTJ 556 PUNCT 1604 ADP 490 NOUN 1779 SCONJ 126 ADV 941 NUM 153 SYM 11 AUX 429 PART 355 VERB 1704 CONJ 167 PRON 682 X 10 DET 387 PROPN 810\nTable A1: Statistics of POS tags\nDependency labels acl 37 dobj 612 acl:relcl 29 expl 10 advcl 194 iobj 15 advmod 859 list 10 appos 18 mwe 105 amod 423 name 117 aux 377 neg 261 auxpass 47 nmod 398 case 463 nmod:npmod 26 cc 167 nmod:poss 153 ccomp 138 nmod:tmod 81 compound 420 nsubj 1005 compound:prt 30 nsubjpass 34 conj 238 nummod 94 cop 152 mark 275 csubj 30 parataxis 241 det 304 punct 1607 det:predet 7 remnant 17 discourse 552 vocative 41 dislocated 2 xcomp 190\nTable A2: Statistics of dependency labels\nah aiyah ba hah / har / huh hiak hiak hiak hor huat la / lah lau leh loh / lor ma / mah wahlow / wah lau wa / wah ya ya walaneh / wah lan eh\nTable A3: List of discourse particles\nA-B act blur ah beng ah ne angpow arrowed ang ku kueh angmoh/ang moh ahpek / ah peks atas boh/bo boho jiak boh pian buay lin chu buen kuey C chai tow kway chao ah beng chap chye png char kway teow chee cheong fun / che cheong fen cheesepie cheong / chiong chiam / cham chiak liao bee / jiao liao bee chio ching chong chio bu / chiobu chui chop chop chow-angmoh chwee kueh D-F dey diam diam die kock standing die pain pain dun eat grass flip prata fried beehoon G gahmen / garment gam geylang gone case gong kia goreng pisang gui H-J hai si lang heng hiong hoot Hosay / ho say how lian jepun kia / jepun kias jialat / jia lak / jia lat K ka kaki kong kaki song kancheong kateks kautim kay kiang kayu kee chia kee siao kelong kena / kana kiam kiasu ki seow kkj kong si mi kopi kopi lui kopi-o kosong koyok ku ku bird L lagi lai liao laksa lao jio kong lao sai lau chwee nua liao / ler like dat / like that lim peh lobang M mahjong kaki makan masak masak mati mee mee pok mee rebus mee siam mee sua mei mei N-S nasi lemak pang sai piak sabo sai same same sia sianz / sian sia suay sibeh siew dai siew siew dai simi taisee soon kuey sotong suay / suey swee T tahan tak pakai te te kee tong tua tikopeh tio tio pian/dio pian talk cock / talk cock sing song U-Z umm zai up lorry / up one\u2019s lorry xiao zhun / buay zhun\nTable A4: List of imported vocabularies"
    }, 
    "document_id": "P17-1159.pdf.json"
}
