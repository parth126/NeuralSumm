{
    "abstract_sentences": {
        "1": "In this paper, we define a measure of dependency between two random variables, based on the Jensen-Shannon (JS) divergence between their joint distribution and the product of their marginal distributions.", 
        "2": "Then, we show that word2vec\u2019s skip-gram with negative sampling embedding algorithm finds the optimal low-dimensional approximation of this JS dependency measure between the words and their contexts.", 
        "3": "The gap between the optimal score and the low-dimensional approximation is demonstrated on a standard text corpus."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 167\u2013171 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2026  1 Introduction  Continuous word representations, derived from unlabeled text, have proven useful in many NLP tasks.", 
        "2": "Such word representations (or embeddings) associate a low-dimensional, real-valued vector with each word, typically induced via neural language models or matrix factorization.", 
        "3": "Substantial benefit arises when embeddings can be efficiently trained on large volumes of data.", 
        "4": "Hence the recent considerable interest in the continuous bag-of-words (CBOW) and skip-gram with negative sampling (SGNS) models, described in (Mikolov et al., 2013), as implemented in the opensource toolkit word2vec.", 
        "5": "These models are based on a relatively simple log-linear method and avoid hidden layers typical to neural networks.", 
        "6": "Consequently, they can be trained to produce high-quality word embeddings on large corpora like the entirety of English Wikipedia in several hours, compared to days or even weeks in the case of other continuous models.", 
        "7": "Recent studies obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity extraction (Passos et al., 2014)\nand dependency parsing (Bansal et al., 2014).", 
        "8": "In recent years, there were several attempts to mathematically interpret word embedding models (Arora et al., 2016; Pennington et al., 2014; Stratos et al., 2015).", 
        "9": "Our study pursues this established line of work, attempting to explain the objective function of the SGNS word embedding algorithm.", 
        "10": "In the SGNS model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context.", 
        "11": "The objective function is a binary logistic regression classifier that treats a word and its observed context as a positive example, and a word and a randomly sampled context as a negative example.", 
        "12": "Levy and Goldberg (2014) offered a motivation for this function by showing that it obtains its global maximum value at the word-context pointwise mutual information (PMI) matrix.", 
        "13": "In this study, we take their analysis one step further and provide an informationtheoretical interpretation of the SGNS objective function.", 
        "14": "In Section 2, we define a new measure of mutual information between random variables based the Jensen-Shennon divergence (Lin, 1991) instead of the KL divergence.", 
        "15": "In Section 3, we show that the value of the SGNS objective computed at the PMI matrix is this information measure.", 
        "16": "We then derive an explicit expression for the information loss caused by the low-dimensional embedding learned by the SGNS algorithm.", 
        "17": "Finally, in Section 4, we illustrate this by computing the information loss caused by actual SGNS embeddings learned on a standard text corpus.", 
        "18": "2 A Dependency Measure based on Jensen-Shannon  In this section, we define a dependency measure between two random variables, which is based on the Jensen-Shannon divergence.", 
        "19": "Later, in Section 3, we show how it relates to the SGNS objective function.", 
        "20": "167\nThere are several standard methods of measuring the distance between two discrete probability distributions, defined on a given finite set A.", 
        "21": "The Kullback-Leibler (KL) divergence of a distribution p from a distribution q is defined as follows: KL(p||q) =\u2211i\u2208A pi log piqi .", 
        "22": "The mutual information between two jointly distributed random variables X and Y is defined as the KL divergence of the joint distribution p(x, y) from the product p(x)p(y) of the marginal distributions of X and Y, i.e.", 
        "23": "I(X;Y ) = KL(p(x, y)||p(x)p(y)).", 
        "24": "The Jensen-Shannon (JS) divergence (Lin, 1991) between distributions p and q is:\nJS\u03b1(p, q) =\u03b1KL(p||r) + (1\u2212\u03b1)KL(q||r) (1)\n= H(r)\u2212 \u03b1H(p)\u2212 (1\u2212\u03b1)H(q) such that 0 < \u03b1 < 1, r = \u03b1p+ (1\u2212 \u03b1)q and H is the entropy function (i.e.", 
        "25": "H(p) = \u2212\u2211i pi log pi).", 
        "26": "Unlike KL divergence, JS divergence is bounded from above and 0 \u2264 JS\u03b1(p, q) \u2264 1.", 
        "27": "We next propose a new measure for mutualinformation using the JS-divergence between p(x, y) and p(x)p(y) instead of the KL-divergence.", 
        "28": "We define the Jensen-Shannon Mutual information (JSMI) as follows:\nJSMI\u03b1(X,Y ) = JS\u03b1(p(x, y), p(x)p(y)).", 
        "29": "(2)\nIt can be easily verified that X and Y are independent if and only if JSMI\u03b1(X,Y ) = 0.", 
        "30": "We next derive an alternative definition of the JSMI dependency measure.", 
        "31": "Assume we choose between the two distributions, p(x, y) and the product of marginal distributions p(x)p(y), according to a binary random variableZ, such that p(Z = 1) = \u03b1.", 
        "32": "We first sample a binary value for Z and next, we sample a r.v.", 
        "33": "W as follows:\np(W =(x, y)|Z)= { p(x)p(y) if Z=0 p(x, y) if Z=1.", 
        "34": "(3) The divergence measure JSMI\u03b1(X,Y ) can be alternatively defined in terms of mutual information between W and Z.", 
        "35": "The mutual-information between W and Z is:\nI(W;Z) = H(W )\u2212 \u2211\ni=0,1\np(Z= i)H(W |Z= i)\n= H(\u03b1p(x, y) + (1\u2212\u03b1)p(x)p(y))\n\u2212\u03b1H(p(x, y))\u2212 (1\u2212\u03b1)H(p(x)p(y)).", 
        "36": "Eq.", 
        "37": "(1) thus implies that:\nJSMI\u03b1(X,Y ) = I(W ;Z).", 
        "38": "(4)\nApplying Bayes rule we obtain:\np(Z=1|W =(x, y)) (5)\n= \u03b1p(x, y)\n\u03b1p(x, y) + (1\u2212\u03b1)p(x)p(y)\n= 1\n1 + exp(\u2212 log( \u03b1p(x,y)(1\u2212\u03b1)p(x)p(y))) = \u03c3(pmix,y)\nsuch that \u03c3(u) = 11+exp(\u2212u) is the sigmoid function and\npmix,y = log p(x, y)\np(x)p(y) + log\n\u03b1\n1\u2212\u03b1 (6)\nis a shifted version of the PMI function.", 
        "39": "Equations (4) and (5) imply that:\nJSMI\u03b1(X,Y ) = H(Z)\u2212H(Z|W ) (7)\n= h(\u03b1)+\u03b1 \u2211\nx,y\np(x, y) log \u03c3(pmix,y)\n+(1\u2212\u03b1) \u2211\nx,y\np(x)p(y) log \u03c3(\u2212pmix,y)\nsuch that h(\u03b1) = \u2212\u03b1 log(\u03b1) \u2212 (1\u2212\u03b1) log(1\u2212\u03b1) is the binary entropy function.", 
        "40": "3 The Skip-Gram Embedding Algorithm  The SGNS embedding algorithm (Mikolov et al., 2013) represents each word x and each context y as d-dimensional vectors ~x and ~y, with the purpose that words that are \u201csimilar\u201d to each other will have similar vector representations.", 
        "41": "We can represent a given d-dimensional embedding by a matrix m, such that m(x, y) = ~x \u00b7 ~y.", 
        "42": "The rank of the embedding matrix m is (at most) d.\nLet p(x, y) be the normalized number of cooccurrences of word x and context-word y in a given corpus and let p(x) and p(y) be the corresponding unigram distributions.", 
        "43": "Consider a binary classifier that treats a word and its observed context as a positive example, and a word and a randomly sampled context as a negative example.", 
        "44": "The classification is made based on the embedding in such a way that the probability that (x, y) is a positive example is \u03c3(~x \u00b7 ~y).", 
        "45": "The objective function ideally maximized by the SGNS word embedding\nalgorithm is the expectation of the log-likelihood function of the embedding:\nS(m) = h( 1\nk+1 ) +\n1\nk+1\n\u2211\nx,y\np(x, y) log \u03c3(~x \u00b7 ~y)\n+ k\nk+1\n\u2211\nx,y\np(x)p(y) log \u03c3(\u2212~x \u00b7 ~y).", 
        "46": "(8) Note that the term h( 1k+1), which does not appear in the original SGNS objective function (Mikolov et al., 2013), is a constant number that was added here to simplify the following presentation.", 
        "47": "The sparsity of p(x, y) (which is obtained as normalized counts from a given learning corpus) makes it feasible to compute the second term of (8).", 
        "48": "The number of summed-over elements in the third term of (8), however, is quadratic in the size of the vocabulary, making it hard to compute.", 
        "49": "Therefore, in practice, we can approximate the expectation by sampling of \u2018negative\u2019 examples.", 
        "50": "The actual SGNS score, then, is:\nS(m) \u2248 h( 1 k+1 ) + 1 k+1 \u00b7 1 n\nn\u2211\nt=1\n(log \u03c3(~xt \u00b7 ~yt)\n+\nk\u2211\ni=1\nlog \u03c3(\u2212~xt \u00b7 ~yti)).", 
        "51": "(9) such that t goes over all the word-context pairs in a given corpus.", 
        "52": "The negative examples yti are created for each pair (xt, yt) by drawing k random contexts from the context-word distribution p(y).", 
        "53": "As pointed out in (Levy et al., 2015), k has two distinct functions in the SGNS objective function.", 
        "54": "First, it is used to better estimate the distribution of negative examples.", 
        "55": "Second, it is used as a weight on the probability of observing a positive example versus a negative example; a higher k means that negative examples are more probable.", 
        "56": "We can compute the SGNS score function S(m) for every real-valued matrix m = (mx,y).", 
        "57": "Levy and Goldberg (2014) showed that the function achieves its global maximal value when for each word-pair (x, y) the inner product of the embedding vectors ~x \u00b7 ~y is equal to pmi(x, y).", 
        "58": "In other words they showed that S(m) \u2264 S(pmi) for every matrix m. We next show that the value of the function S(m) at its maximum point, the PMI matrix, has a concrete interpretation, namely it is exactly the Jensen-Shannon Mutual Information (JSMI) between words and their contexts.", 
        "59": "Theorem 1: The value of the SGNS score with k negative samples (8) at the PMI matrix satisfies:\nS(pmi) = JSMI\u03b1(X,Y )\nsuch that \u03b1 = 1k+1 .", 
        "60": "Proof: It can be easily verified that by substituting \u03b1 = 1k+1 in the definition of JSMI (Eq.", 
        "61": "(7)), we exactly obtain the SGNS score (8) at the PMI matrix.", 
        "62": "2\nLevy and Goldberg (2014) showed that SGNS\u2019s objective achieves its maximal value at the PMI matrix.", 
        "63": "However, this result reveals nothing about the more interesting lower dimensional case, where the PMI matrix factorization is forced to compress the joint distribution and thereby learn a meaningful embedding.", 
        "64": "We next derive an explicit description of the approximation criterion that quantifies the gap between S(m) and S(pmi).", 
        "65": "Given the word co-occurrences joint distribution p(x, y), we obtained in Eq.", 
        "66": "(5) a conditional distribution on the alphabet of (Z,W ) as follows:\np(Z=1|W =(x, y)) = \u03c3(pmix,y).", 
        "67": "In a similar way, given any matrixm, we can define a conditional distribution pm on the alphabet of (Z,W ) as follows:\npm(Z=1|W =(x, y)) = \u03c3(mx,y).", 
        "68": "Note that in the special case where m is the PMI matrix, ppmi(z|w) coincides with the original p(z|w) that was defined in Eq.", 
        "69": "(5).", 
        "70": "Theorem 2: The difference between the SGNS score at the PMI matrix and the SGNS score at a given matrix m can be written as:\nS(pmi)\u2212 S(m) = KL(ppmi(Z|W )||pm(Z|W )) (10) Proof:\nS(pmi)\u2212S(m) = \u2211\nx,y\n(\u03b1p(x, y) log \u03c3(pmix,y) \u03c3(mx,y)\n+(1\u2212\u03b1)p(x)p(y) log \u03c3(\u2212pmix,y) \u03c3(\u2212mx,y) )\n= \u2211\nx,y\n(\u03b1p(x, y) log ppmi(Z=1|x, y) pm(Z=1|x, y)\n+(1\u2212\u03b1)p(x)p(y) log ppmi(Z=0|x, y) pm(Z=0|x, y) )\n= \u2211\nw,z\np(W =w,Z=z) log ppmi(Z=z|W =w) pm(Z=z|W =w)\n= KL(ppmi(Z|W )||pm(Z|W )).2 The KL divergence between two distributions is always non-negative and is zero only if the two distributions are the same.", 
        "71": "Therefore, we rederive the results of (Levy and Goldberg, 2014) that S(pmi) = maxm S(m).", 
        "72": "Theorem 2 can be viewed as an instance of the well-known connection between maximizing log-likelihood and minimizing KL divergence between the estimated and the true data-generating distribution.", 
        "73": "In this case, the true distribution is the pmi-based classifier ppmi(Z|W ).", 
        "74": "Combining theorems 1 and 2 we obtain that S(m) \u2264 JSMI\u03b1(X,Y ) for every low-dimensional embedding matrix.", 
        "75": "The difference JSMI\u03b1(X,Y )\u2212 S(m) is the information loss caused by the lowdimensional embedding.", 
        "76": "We can view it as a Jensen-Shannon variant of the information bottleneck principle (Tishby et al., 1999; Globerson et al., 2007) that is defined in terms of the KL divergence.", 
        "77": "The optimal d-dimensional embedding, is the best d-dimensional approximation of the JSMI dependency measure in the sense that it minimizes the information loss.", 
        "78": "The JSMI is the upper bound that any embedding can obtain.", 
        "79": "To illustrate that, in the next section we compute the JSMI between words and their contexts based on a standard text corpus and show the information gap between the JSMI and the actual SGNS score as a function of the embedding dimension d.\nFrom Theorem 2 we can also derive an explicit information-theoretic interpretation of the score function S(m) (7) as the difference between two KL-divergence terms:\nS(m) = S(pmi)\u2212 (S(pmi)\u2212 S(m)) =\nI(Z;W )\u2212 (S(pmi)\u2212 S(m)) = KL(p(Z|W )||p(Z))\u2212 KL(p(Z|W )||pm(Z|W ))\nThe word embedding problem can be also viewed as a factorization of the PMI matrix.", 
        "80": "Previous works suggested other criteria for matrix factorization such as least-squares (Eckart and Young, 1936) and KL-divergence between the original matrix and the low-rank matrix approximation (Lee and Seung, 2000).", 
        "81": "We have shown that the SGNS algorithm factorizes the PMI matrix based on the JSMI-based criterion stated in Eq.", 
        "82": "(10).", 
        "83": "4 Experiments  In this section we use word2vec to train real skipgram with negative sampling (SGNS) embedding models.", 
        "84": "By measuring the value of their objective function and comparing it against the optimal one using exact PMI values, we demonstrate how a well-trained model minimizes the difference in Eq.", 
        "85": "(10).", 
        "86": "We note that this is an intrinsic measure that does not necessarily reflect the usefulness of the learned embeddings for other tasks.", 
        "87": "We used the Penn Tree Bank (PTB), a popular small-scale corpus, for our experiments.", 
        "88": "A version of this dataset is available from Tomas Mikolov.1 It consists of 929K training words with a 10K word vocabulary, which we used to train our models.", 
        "89": "To learn the SGNS word embeddings, we used word2vec\u2019s default parameter values: windowsize = 5, min-count = 5, and number of negative samples k = 5.", 
        "90": "We varied the dimensionality of the embeddings and the number of training iterations performed.", 
        "91": "Once the models were trained, we measured their score (9) on the training corpus.", 
        "92": "Based on the same learning corpus, we computed S(pmi) = JSMI\u03b1(X,Y ) for \u03b1 = 1k+1 = 1/6.", 
        "93": "Note that p(x, y) = 0 implies that pmix,y = \u2212\u221e and therefore log \u03c3(\u2212pmix,y) = 0.", 
        "94": "Hence, as in the second term, to compute the third term of S(m) (8) for the case of m = pmi, we can sum only\n1http://www.fit.vutbr.cz/~imikolov/ rnnlm/simple-examples.tgz\nover the positive pairs (x, y) that actually appear in the corpus.2 In other words, for the special case m = pmi, it is feasible to compute the exact score (8) and not just its approximation (9) that is based on negative sampling.", 
        "95": "Figure 1 illustrates the optimal PMI-based score, compared with the scores obtained by different models with varied embedding dimensionality and number of training iterations.", 
        "96": "As can be seen, the embeddings score gets close to the optimal value using higher dimensionality and more training iterations, but doesn\u2019t surpass it.", 
        "97": "5 Conclusion  In this study, we developed a new correlation measure between random variables, denoted JSMI.", 
        "98": "This measure is based on the JS divergence and differs from the standard mutual information measure that is based on the KL divergence.", 
        "99": "We showed that the optimization of skip-gram embeddings with negative sampling finds the best low-dimensional approximation of the JSMI measure.", 
        "100": "Thus, we provided an information theory framework that hopefully contributes to a better understanding of this embedding algorithm.", 
        "101": "Furthermore, although we focused here on the case of word-context joint distributions, the connection we haven shown between the PMI matrix and the JSMI function is valid for every joint distribution of two random variables.", 
        "102": "Acknowledgments  This work is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
    }, 
    "document_id": "P17-2026.pdf.json"
}
