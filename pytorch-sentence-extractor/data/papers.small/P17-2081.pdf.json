{
    "abstract_sentences": {
        "1": "We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset.", 
        "2": "We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD.", 
        "3": "For WikiQA, our model outperforms the previous best model by more than 8%.", 
        "4": "We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision, through quantitative results and visual analysis.", 
        "5": "We also show that a similar transfer learning procedure achieves the state of the art on an entailment task."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 510\u2013517 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2081  1 Introduction  Question answering (QA) is a long-standing challenge in NLP, and the community has introduced several paradigms and datasets for the task over the past few years.", 
        "2": "These paradigms differ from each other in the type of questions and answers and the size of the training data, from a few hundreds to millions of examples.", 
        "3": "We are particularly interested in the contextaware QA paradigm, where the answer to each question can be obtained by referring to its accompanying context (paragraph or a list of sentences).", 
        "4": "Under this setting, the two most notable types of supervisions are coarse sentence-level and finegrained span-level.", 
        "5": "In sentence-level QA, the task is to pick sentences that are most relevant to the question among a list of candidates (Yang et al., 2015).", 
        "6": "In span-level QA, the task is to locate the\n\u2217 All work was done while the author was an exchange student at University of Washington.", 
        "7": "smallest span in the given paragraph that answers the question (Rajpurkar et al., 2016).", 
        "8": "In this paper, we address coarser, sentencelevel QA through a standard transfer learning1 technique of a model trained on a large, spansupervised QA dataset.", 
        "9": "We demonstrate that the target task not only benefits from the scale of the source dataset but also the capability of the finegrained span supervision to better learn syntactic and lexical information.", 
        "10": "For the source dataset, we pretrain on SQuAD (Rajpurkar et al., 2016), a recentlyreleased, span-supervised QA dataset.", 
        "11": "For the source and target models, we adopt BiDAF (Seo et al., 2017), one of the top-performing models in the dataset\u2019s leaderboard.", 
        "12": "For the target datasets, we evaluate on two recent QA datasets, WikiQA (Yang et al., 2015) and SemEval 2016 (Task 3A) (Nakov et al., 2016), which possess sufficiently different characteristics from that of SQuAD.", 
        "13": "Our results show 8% improvement in WikiQA and 1% improevement in SemEval.", 
        "14": "In addition, we report state-of-the-art results on recognizing textual entailment (RTE) in SICK (Marelli et al., 2014) with a similar transfer learning procedure.", 
        "15": "2 Background and Data  Modern machine learning models, especially deep neural networks, often significantly benefit from transfer learning.", 
        "16": "In computer vision, deep convolutional neural networks trained on a large image classification dataset such as ImageNet (Deng et al., 2009) have proved to be useful for initializing models on other vision tasks, such as object detection (Zeiler and Fergus, 2014).", 
        "17": "In nat-\n1 The borderline between transfer learning and domain adaptation is often ambiguous (Mou et al., 2016).", 
        "18": "We choose the term \u201ctransfer learning\u201d because we also adapt the pretrained QA model to an entirely different task, RTE.", 
        "19": "510\nural language processing, domain adaptation has traditionally been an important topic for syntactic parsing (McClosky et al., 2010) and named entity recognition (Chiticariu et al., 2010), among others.", 
        "20": "With the popularity of distributed representation, pre-trained word embedding models such as word2vec (Mikolov et al., 2013b,a) and glove (Pennington et al., 2014) are also widely used for natural language tasks (Karpathy and Fei-Fei, 2015; Kumar et al., 2016).", 
        "21": "Instead of these, we initialize our models from a QA dataset and show how standard transfer learning can achieve stateof-the-art in target QA datasets.", 
        "22": "There have been several QA paradigms in NLP, which can be categorized by the context and supervision used to answer questions.", 
        "23": "This context can range from structured and confined knowledge bases (Berant et al., 2013) to unstructured and unbounded natural language form (e.g., documents on the web (Voorhees and Tice, 2000)) and unstructured, but restricted in size (e.g., a paragraph or multiple sentences (Hermann et al., 2015)).", 
        "24": "The recent advances in neural question answering lead to numerous datasets and successful models in these paradigms (Rajpurkar et al., 2016; Yang et al., 2015; Nguyen et al., 2016; Trischler et al., 2016).", 
        "25": "The answer types in these datasets are largely divided into three categories: sentencelevel, in-context span, and generation.", 
        "26": "In this paper, we specifically focus on the former two and show that span-supervised models can better learn syntactic and lexical features.", 
        "27": "Among these datasets, we briefly describe three QA datasets to be used for the experiments in this paper.", 
        "28": "We also give the description of an RTE dataset for an example of a non-QA task.", 
        "29": "Refer to Table 1 to see the examples of the datasets.", 
        "30": "SQuAD (Rajpurkar et al., 2016) is a recent spanbased QA dataset, containing 100k/10k train/dev examples.", 
        "31": "Each example is a pair of context para-\ngraph from Wikipedia and a question created by a human, and the answer is a span in the context.", 
        "32": "SQUAD-T is our modification of SQuAD dataset to allow for sentence selection QA.", 
        "33": "(\u2018T\u2019 for senTence).", 
        "34": "We split the context paragraph into sentences and formulate the task as classifying whether each sentence contains the answer.", 
        "35": "This enables us to make a fair comparison between pretraining with span-supervised and sentencesupervised QA datasets.", 
        "36": "WikiQA (Yang et al., 2015) is a sentence-level QA dataset, containing 1.9k/0.3k train/dev answerable examples.", 
        "37": "Each example consists of a real user\u2019s Bing query and a snippet of a Wikipedia article retrieved by Bing, containing 18.6 sentences on average.", 
        "38": "The task is to classify whether each sentence provides the answer to the query.", 
        "39": "SemEval 2016 (Task 3A) (Nakov et al., 2016) is a sentence-level QA dataset, containing 1.8k/0.2k/0.3k train/dev/test examples.", 
        "40": "Each example consists of a community question by a user and 10 comments.", 
        "41": "The task is to classify whether each comment is relevant to the question.", 
        "42": "SICK (Marelli et al., 2014) is a dataset for recognizing textual entailment (RTE), containing 4.5K/0.5K/5.0K train/dev/test examples.", 
        "43": "Each example consists of a hypothesis and a premise, and the goal is to determine if the premise is entailed by, contradicts, or is neutral to the hypothesis (hence classification problem).", 
        "44": "We also report results on SICK to show that span-supervised QA dataset can be also useful for non-QA datasets.", 
        "45": "3 Model  Among numerous models proposed for spanlevel QA tasks (Xiong et al., 2017; Wang and Jiang, 2017b), we adopt an open-sourced model, BiDAF2 (Seo et al., 2017).", 
        "46": "2https://allenai.github.io/bi-att-flow\nBiDAF.", 
        "47": "The inputs to the model are a question q, and a context paragraph x.", 
        "48": "Then the model selects the best answer span, which is argmax(i,j) y start i y end j , where i <= j.", 
        "49": "Here, ystarti and y end i are start and end position probabilities of i-th element, respectively.", 
        "50": "Here, we briefly describe the answer module which is important for transfer learning to sentence-level QA.", 
        "51": "The input to the answer module is a sequence of vectors {hi} each of which encodes enough information about the i-th context word and its relationship with its surrounding words and the question words.", 
        "52": "Then the role of the answer module is to map each vector hi to its start and end position probabilities, ystarti and y end i .", 
        "53": "BiDAF-T refers to the modified version of BiDAF to make it compatible with sentence-level QA.", 
        "54": "(\u2018T\u2019 for senTence).", 
        "55": "In this task, the inputs are a question q and a list of sentences, x1, .", 
        "56": ".", 
        "57": ".", 
        "58": ",xT , where T is the number of the sentences.", 
        "59": "Note that, unlike BiDAF, which outputs single answer per example, Here we need to output a C-way classification for each k-th sentence.", 
        "60": "Since BiDAF is a span-selection model, it cannot be directly used for sentence-level classification.", 
        "61": "Hence we replace the original answer module of BiDAF with a different answer module, and keep the other modules identical to those of BiDAF.", 
        "62": "Given the input to the new answer module, {hk1, .", 
        "63": ".", 
        "64": ".", 
        "65": ",hkN}, where the superscript is the sentence index (1 \u2264 k \u2264 T ), we obtain the C\u2013way classification scores for the k-th sentence, y\u0303k \u2208 [0, 1]C via max-pooling method:\ny\u0303k = softmax(Wmax(hk1, .", 
        "66": ".", 
        "67": ".", 
        "68": ",h k N ) + b) (1)\nwhere W \u2208 RC\u00d7d,b \u2208 RC are trainable weight matrix and bias, respectively, and max() function is applied elementwise.", 
        "69": "For WikiQA and SemEval 2016, the number of classes (C) is 2, i.e.", 
        "70": "each sentence (or comment) is either relevant or not relevant.", 
        "71": "Since some of the metrics used for these datasets require full ranking, we use the predicted probability for \u201crelevant\u201d label to rank the sentences.", 
        "72": "Note that BiDAF-T can be also used for the RTE dataset, where we can consider the hypothesis as a question and the premise as a context sentence (T = 1), and classify each example into \u2018entailment\u2019, \u2018neutral\u2019, or \u2018contradiction\u2019 (C = 3).", 
        "73": "Transfer Learning.", 
        "74": "Transfer learning between the same model architectures3 is straightforward: we first initialize the weights of the target model with the weights of the source model pretrained on the source dataset, and then we further train (finetune) on the target model with the target dataset.", 
        "75": "To transfer from BiDAF (on SQuAD) to BiDAFT, we transfer all the weights of the identical modules, and initialize the new answer module in BiDAF-T with random values.", 
        "76": "For more training details, refer to Appendix A.", 
        "77": "4 Experiments  Question Answering Results.", 
        "78": "Table 2 reports the state-of-the-art results of our transfer learning on WikiQA and SemEval-2016 and the performance of previous models as well as several ablations that use no pretraining or no finetuning.", 
        "79": "There are multiple interesting observations from Table 2 as follows: (a) If we only train the BiDAF-T model on the target datasets with no pretraining (first row of Table 2), the results are poor.", 
        "80": "This shows the importance of both pretraining and finetuning.", 
        "81": "(b) Pretraining on SQuAD and SQuAD-T with no finetuning (second and third row) achieves results close to the state-of-the-art in the WikiQA dataset, but not in SemEval-2016.", 
        "82": "Interestingly, our result on SemEval-2016 is not better than only training without transfer learning.", 
        "83": "We conjecture that this is due to the significant difference between the domain of SemEval-2016 and\n3 Strictly speaking, this is a domain adaptation scenario.", 
        "84": "that of SQuAD, which are from community and Wikipedia, respectively.", 
        "85": "(c) Pretraining on SQuAD and SQuAD-T with finetuning (fourth and fifth row) significantly outperforms (by more than 5%) the highest-rank systems on WikiQA.", 
        "86": "It also outperforms the second ranking system in SemEval-2016 and is only 1% behind the first ranking system.", 
        "87": "(d) Transfer learning models achieve better results with pretraining on span-level supervision (SQuAD) than coarser sentence-level supervision (SQuAD-T).4\nFinally, we also use the ensemble of 12 different training runs on the same BiDAF architecture, which obtains the state of the art in both datasets.", 
        "88": "This system outperforms the highest-ranking system in WikiQA by more than 8% and the best system in SemEval-2016 by 1% in every metric.", 
        "89": "It is important to note that, while we definitely benefit from the scale of SQuAD for transfer learning to smaller WikiQA, given the gap between SQuADT and SQuAD (> 3%), we see a clear sign that span-supervision plays a significant role well.", 
        "90": "Varying the size of pretraining dataset.", 
        "91": "We vary the size of SQuAD dataset used during pretraining, and test on WikiQA with finetuning.", 
        "92": "Results are shown in Table 3.", 
        "93": "As expected, MAP on WikiQA drops as the size of SQuAD decreases.", 
        "94": "It is worth noting that pretraining on SQuAD-T (Table 2) yields 0.5 point lower MAP than pretraining on 50% of SQuAD.", 
        "95": "In other words, roughly speaking, span-level supervision data is worth more than twice the size of sentence-level supervision data for the purpose of pretraining.", 
        "96": "Also, even a small size of fine-grained supervision data helps; pretraining with 12.5% of SQuAD gives an advantage of more than 7 points than no pretraining.", 
        "97": "Analysis.", 
        "98": "Figure 1 shows the latently-learned attention maps between the question and one of the context sentences from a WikiQA example in Table 1.", 
        "99": "The top map is pretrained on SQuADT (corresponding to SQuAD-T&Yes in Table 2) and the bottom map is pretrained on SQuAD (SQuAD&Yes).", 
        "100": "The more red the color, the higher\n4We additionally perform Mann-Whitney U Test and McNemars Test to show the statistical significance of the advantage of span-level pretraining over sentence-level pretraining.", 
        "101": "For WikiQA, the advantage is statistically significant with the confidence levels of 97.1% and 99.6%, respectively.", 
        "102": "For SemEval, we obtain the confidence levels of 97.8% and 99.9%, respectively.", 
        "103": "the relevance between the words.", 
        "104": "There are two interesting observations here.", 
        "105": "First, in SQuAD-pretrained model (bottom), we see a high correspondence between question\u2019s airbus and context\u2019s aircraft and aerospace, but the SQuAD-T-pretrained model fails to learn such correspondence.", 
        "106": "Second, we see that the attention map of the SQuAD-pretrained model is more sparse, indicating that it is able to more precisely localize correspondence between question and context words.", 
        "107": "In fact, we compare the sparsity of WikiQA test examples in SQuAD&Y and SQuAD-T&Y.", 
        "108": "Following Hurley and Rickard (2009), the sparsity of an attention map is defined by\nsparsity = | {x \u2208 V|x \u2264 } |\n|V| (2)\nwhere V is a set of values between 0 and 1 in attention map, and is a small value which we define 0.01 for here.", 
        "109": "A histogram of the sparsity is shown in Figure 2.", 
        "110": "There is a large gap in the average sparsity of WikiQA test examples between SQuAD&Yes and SQuAD-T&Yes, which are 0.84 and 0.56, respectively.", 
        "111": "More analyses including error analysis and more visualizations are shown in Appendix B.\nEntailment Results.", 
        "112": "In addition to QA experiments, we also show that the models trained on span-supervised QA can be useful for textual entailment task (RTE).", 
        "113": "Table 4 shows the trans-\nfer learning results of BiDAF-T on SICK dataset (Marelli et al., 2014), with various pretraining routines.", 
        "114": "Note that SNLI (Bowman et al., 2015) is a similar task to SICK and is significantly larger (150K/10K/10K train/dev/test examples).", 
        "115": "Here we highlight three observations: (a) BiDAF-T pretrained on SQuAD outperforms that without any pretraining by 6% and that pretrained on SQuAD-T by 2%, which demonstrates that the transfer learning from large span-based QA gives a clear improvement.", 
        "116": "(b) Pretraining on SQuAD+SNLI outperforms pretraining on SNLI only.", 
        "117": "Given that SNLI is larger than SQuAD, the difference in their performance is a strong indicator that we are benefiting from not only the scale of SQuAD, but also the fine-grained supervision that it provides.", 
        "118": "(c) We outperform the previous state of the art by 2% with the ensemble of SQuAD+SNLI pretraining routine.", 
        "119": "It is worth noting that Mou et al.", 
        "120": "(2016) also shows improvement on SICK by pretraining on SNLI.", 
        "121": "5 Conclusion  In this paper, we show state-of-the-art results on WikiQA and SemEval-2016 (Task 3A) as well as an entailment task, SICK, outperforming previous results by 8%, 1%, and 2%, respectively.", 
        "122": "We show that question answering with sentence-level supervision can greatly benefit from standard transfer learning of a question answering model trained on a large, span-level supervision.", 
        "123": "We additionally show that such transfer learning can be applicable in other NLP tasks such as textual entailment.", 
        "124": "Acknowledgments  This research was supported by the NSF (IIS 1616112), Allen Institute for AI (66-9175), Allen Distinguished Investigator Award, and Google Research Faculty Award.", 
        "125": "We thank the anonymous reviewers for their helpful comments.", 
        "126": "A Training details  Parameters.", 
        "127": "For pretraining BiDAF on SQuAD, we follow the exact same procedure in Seo et al.", 
        "128": "(2017).", 
        "129": "For pretraining BiDAF-T on SQuAD-T, we use the same hyperparameters for all modules except the answer module, for which we use the hidden state size of 200.", 
        "130": "The learning rate is controlled by AdaDelta (Zeiler, 2012) with the initial learning rate of 0.5 and minibatch size of 50.", 
        "131": "We maintain the moving averages of all weights of the model with the exponential decay rate of 0.999 during training and use them at test.", 
        "132": "The loss function is the cross entropy between y\u0303k and the one-hot vector of the correct classification.", 
        "133": "Convergence.", 
        "134": "For all settings, we train models until performance on development set continue to decrease for 5k steps.", 
        "135": "Table 5 shows the median selected step on each setting.", 
        "136": "B More Analysis  Attention maps.", 
        "137": "We show some more examples of attention maps in Figure 3.", 
        "138": "(Top) We see high correspondence between same word from question and context such as senator and john, in SQuAD-pretrained model, but the SQuAD-Tpretrained model fails to learn such correspondence.", 
        "139": "(Bottom) We see high correspondence between stems from question and stem from context (left) as well as plant from question and plants from context (right), in SQuADpretrained model, but the SQuAD-T-pretrained model fails to learn such correspondence.", 
        "140": "Error Analysis.", 
        "141": "Table 7 shows the comparison between answers by SQuAD-T-pretrained model and SQuAD-pretrained model on the example\nof WikiQA and SemEval-2016 from Table 1.", 
        "142": "On WikiQA, SQuAD-T-pretrained model selects C2 instead of the groundtruth answer C1.", 
        "143": "On SemEval-2016, SQuAD-pretrained model ranks C3 (bad comment) higher than C2 (good comment).", 
        "144": "In addition, we sampled 100 example randomly\nfrom WikiQA and SemEval-2016, and classified them into 6 categories(Table 6).", 
        "145": "In Table 8, we compare the performance on these WikiQA examples by SQuAD-T-pretrained model and SQuADpretrained model.", 
        "146": "It shows that span supervision clearly helps answering questions on Category 1 and 2, which are easier to answer, with answering\ncorrectly on most of the questions in Category 1.", 
        "147": "Similarly, we show the comparison of the performance on classified examples of the model without pretraining and SQuAD-pretrained model on SemEval-2016.", 
        "148": "It also shows that span supervision helps answering questions asking information or opinion/recommendation.", 
        "149": "C More Results  SQuAD-T. To better understand SQuAD-T dataset, we show the performance BiDAF-T with different training routines.", 
        "150": "We get MAP 89.46 and accuracy 85.34% with SQuAD-trained BiDAF model, and MAP 90.18 and accuracy 84.69% with SQuAD-T-trained BiDAF-T model.", 
        "151": "There is no large gap between the two models, as each paragraph of SQuAD-T has 5 sentences on average, which makes the classification problem easier than WikiQA.", 
        "152": "SNLI.", 
        "153": "Other larger RTE datasets such as SNLI also benefit from transfer learning, although the improvement is smaller.", 
        "154": "We confirm the improvement by showing that the result on SNLI when pretraining on SQuAD with BiDAF is 82.6%, which is slightly higher than that of the model pretrained on SQuAD-T (81.6%).", 
        "155": "This, however, did not outperform the state of the art (88.8%) by Wang et al.", 
        "156": "(2017).", 
        "157": "This is mostly because BiDAF (or BiDAF-T) is a QA model, which is not designed for RTE tasks."
    }, 
    "document_id": "P17-2081.pdf.json"
}
