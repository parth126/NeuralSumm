{
    "abstract_sentences": {
        "1": "Counterfactual statements, describing events that did not occur and their consequents, have been studied in areas including problem-solving, affect management, and behavior regulation.", 
        "2": "People with more counterfactual thinking tend to perceive life events as more personally meaningful.", 
        "3": "Nevertheless, counterfactuals have not been studied in computational linguistics.", 
        "4": "We create a counterfactual tweet dataset and explore approaches for detecting counterfactuals using rule-based and supervised statistical approaches.", 
        "5": "A combined rule-based and statistical approach yielded the best results (F1 = 0.77) outperforming either approach used alone."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 654\u2013658 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2103  1 Introduction  Counterfactuals describe events that did not occur, and what would have happened (or not happened), had the event occurred (e.g., \u201cIf I hadn\u2019t broken my arm, I never would have met her.\u201d).", 
        "2": "More precisely, counterfactual conditionals have the form \u201cIf it had been the case that A (or not A), it would have been the case that B (or not B).\u201d\nCounterfactuals have been studied in many different domains.", 
        "3": "Logicians and philosophers focus on literally logical relations between the antecedent and consequent of counterfactual forms and the outcomes (Goodman, 1947).", 
        "4": "In contrast, political scientists usually conduct counterfactual thought experiments for hypothetical tests on historical events, policies, or other aspects of a society and assess them (Tetlock, 1996).", 
        "5": "Counterfactual thoughts are defined, especially in psychology, as mental representations of alternatives to past events, actions, or states.", 
        "6": "Their use\nhas been explored for correlations with many different demographics (age, gender) and psychological variables (depression, religiosity) (Kray et al., 2010; Markman and Miller, 2006).", 
        "7": "Counterfactual thinking has been linked to perceiving life events as more meaningful, fated, and even as influenced by the divine (Kray et al., 2010; Buffone et al., 2016), as well as with problem-solving, because imagining alternate outcomes can easily bring to mind the steps needed for improvement (Epstude and Roese, 2008; Roese, 1994).", 
        "8": "It has also been shown to be associated with affect management, particularly when imagining realities that are worse than what actually happened (Epstude and Roese, 2008; Roese, 1994)\nDespite the extensive research on counterfactual thinking, counterfactual language forms have not been studied in computational linguistics.", 
        "9": "Language-based models to recognize counterfactual thinking in social media would potentially allow for psychological analysis on users based on their everyday language, avoiding the high expense of capturing counterfactual thinking at a large scale using traditional psychological assessments.", 
        "10": "Therefore, in this paper, we build a languagebased model to recognize counterfactual forms in social media texts of Twitter and Facebook.", 
        "11": "There are many challenges for this task.", 
        "12": "First, counterfactual statements have a low base rate; we found only 2% of status updates on Facebook and 1% of tweets contain counterfactual statements.", 
        "13": "Secondly, counterfactual statements can take on many forms in natural language.1 For example, they may or may not use explicit if- or then- clauses (e.g, consider \u201cIf I had not met him then I would be better off\u201d versus \u201cI wish I had not met him\u201d).", 
        "14": "1Simply looking for words like \u2018if\u2019 fails to produce useful results; only 2 percent of sentences of tweets containing \u2019if\u2019 are counterfactuals.", 
        "15": "654\nThe low base rate and high variability of natural language counterfactuals in social media texts make them difficult recognize using simple linguistic or statistical features.", 
        "16": "We address theses challenges by using a combined rule-based and statistical approach.", 
        "17": "Key to our success is defining seven sub-types of counterfactuals, allowing better coverage of rarer sub-types.", 
        "18": "2 Related Work  Identifying counterfactuals is in many ways similar to identifying discourse relations.", 
        "19": "In terms of relation classification, the counterfactual conditionals can be viewed as a subset of Condition type of Contingency class in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) or the Condition relation of Rhetorical Structure Theory (RST) (Mann and Thompson, 1987).", 
        "20": "Also, like all discourse relations in the PDTB, counterfactuals have implicit and explicit forms, and so cannot by uniquely identified by the presence of specific words.", 
        "21": "There have been many researchers who have tried end-to-end discourse relation parsing with the PDTB and RST(Biran and McKeown, 2015; Lin et al., 2009; Ji and Eisenstein, 2014).", 
        "22": "Many of them used dependency parsing or constituency parsing for argument detection or elementary discourse unit (EDU) segmentation to infer the relation between them.", 
        "23": "However, the short lengths and poor quality of parses of social media texts make dependency constituents unreliable.2 For example, posters frequently drop the subject of a sentence.", 
        "24": "Other work mostly focuses on relation classification with an assumption that arguments of the given relations are already identified (Park and Cardie, 2012; Pitler et al., 2009).", 
        "25": "They explore various learning algorithms and types of features in the given arguments of discourse relations.", 
        "26": "Then, they report which combinations give the best performance of each discourse relation.", 
        "27": "Our work, while possible to view as a task in discourse relation classification, focuses on critical features of counterfactuals rather than on accurate demarcation of each argument of the relation.", 
        "28": "Most downstream applications, such as\n2In our preliminary experiments on our causality tweet dataset (\u03ba = 0.61), Lin\u2019s parser (Lin et al., 2009) obtained 0.45 F1 while a linear support vector machine (SVM) with n-gram obtained 0.58 F1 for causality detection.", 
        "29": "psychological studies, require knowing the presence/absence of counterfactuals rather than their exact extent.", 
        "30": "3 Method  We use a combination of a rule-based approach and a supervised classifier to capture counterfactual statements from Twitter.", 
        "31": "3.1 Data Set  No existing corpus of counterfactual statements was available, so we collected our own data set, starting from a random set of tweets from May 2014 and July 2014.", 
        "32": "As noted previously, couterfactual statements are rare, so we first limited the random tweet set to 1,637 containing keywords3 that can signal counterfactuals (Train and Test row from Table 1).", 
        "33": "Keywords were in part based on prior literature on spontaneous counterfactual generation, such as should have, could have, at least, if only, or next time (Sanna and Turley, 1996).", 
        "34": "We identified further counterfactual forms (e.g.", 
        "35": "wish) based on visual inspection of the data.", 
        "36": "Next we used the overall list of keywords to draw samples of 500 Tweets for further visual inspection.", 
        "37": "Words or phrases which had an unreasonably high false positive rate for containing counterfactuals were eliminated.", 
        "38": "Well-trained annotators then manually labeled each of the 1,637 tweets for counterfactuals with a 10% postive rate, results in 166 counterfactuals and 1,471 negative samples.", 
        "39": "A random set of 500 of these instances were used in training and the rest were reserved for testing.", 
        "40": "To build out our training set to capture examples of all forms of counterfactuals, we added a train supplement from random tweets from 2012 \u2013 at least thirty tweets from each of seven counterfactual forms we defined for our statistical model using the regular expressions4 with brown-clusters and the tweet PTB tagging model (described next).", 
        "41": "With this process, we enabled the model to be less biased towards only the samples with the counterfactual cue phrases used for data collection.", 
        "42": "Additionally, the model learned syntactically different forms of counterfactuals identified in prior work.", 
        "43": "To evaluate counterfactual form annotation, inter-annotator agreement was established on 1,637 tweets with a\n3e.g., \u2018should\u2019, \u2018shulda\u2019; full list available in our supplementary data.", 
        "44": "4The regular expression table is included in our supplementary data.", 
        "45": "second rater with achieving \u03ba = 0.774 and human annotation F1 0.791.", 
        "46": "3.2 Classification  We first use a rule-based model to capture counterfactual patterns from social media texts.", 
        "47": "We then use a statistical model (Linear SVM) to increase precision by identifying tricky false positives with forms similar to counterfactuals (e.g., \u201dwish you the best\u201d).", 
        "48": "Rule-based Classification.", 
        "49": "Our rule-based approach is based on seven forms of counterfactuals (Table 2).", 
        "50": "Central to our method is our theorizing, based on reading the literature, especially (Kray et al., 2010) and examining many counterfactual examples, that counterfactuals come in seven different forms, shown with examples in (Table 2).", 
        "51": "First, we remove sentences ending in question marks predicted as \u2018end of sentence\u2019 by the tweet part-of-speech (POS) tagger (Gimpel et al., 2011).", 
        "52": "We then use pattern matching with regular expression using a combination of cue phrases (bold), POS tags, and word clusters.", 
        "53": "The word clusters, based on a set of Twitter Brown clusters5 are used to capture the numerous variations of words in social media texts (e.g., \u2018shuldve\u2019 for \u2018should have\u2019).", 
        "54": "This approach requires matching both the token and its part-of-speech, since the POS tag of each token is important for counterfactual form.", 
        "55": "The rule-based approach is also useful in that it allows us to detect the arguments for counterfactual relations; conditional statement and consequent statement from Conjunctive Normal/ Converse form and Verb Inversion form, one counterfactual statement from Wish Verb and Could / Would / Should have.", 
        "56": "We customized Biran\u2019s demarcation methods using the first verb phrase or the connective as a boundary to capture the more informative argument of the statement: For one argument detection, we demarcate from the cue phrase (e.g., would have) to the end of sentence.", 
        "57": "For two arguments, we demarcate from condi-\n5http://www.cs.cmu.edu/\u02dcark/TweetNLP/ clusters/50mpaths2\ntional word (e.g., if, unless) to the end of statement or before the start of the second verb phrase.", 
        "58": "Part of Speech Tagging We use the Penn Treebank (PTB)-style Tweet POS tags6 instead of Tweet POS tags (Gimpel et al., 2011) as it contains more fine-grained categories and yields higher accuracy of pattern matching.", 
        "59": "For instance, Tweet POS tags do not differentiate modal verbs, past tense verbs, and other types of verbs, but categorize all of them as \u2018V\u2019.", 
        "60": "However, in many forms of counterfactuals, the distinction between modal verbs and past particles from other types of verbs are critical (e.g., in Should / Could / Would Have forms).", 
        "61": "Finally, we conduct a postprocessing on the Tweet POS parsing results for the more accurate prediction.", 
        "62": "First, we delete RT tags along with the token since it is not informative for our task.", 
        "63": "Then, we convert \u2018USR\u2019 to nouns because the word token tagged as \u2018user\u2019 usually plays the role of a common noun from the discourse relation perspective.", 
        "64": "Additionally, in order to enhance the POS tagging, we use the brown clusters to tag empirical variations of modal verbs as \u2018MD\u2019 and we define \u2018CCJ\u2019, a new tag to distinguish conditional conjunctions (i.e.", 
        "65": "Brown clusters for \u2018if\u2019) from other types of conjunctions.", 
        "66": "Statistical Modeling.", 
        "67": "Each counterfactual form has a different number of arguments for the relation, and different types of features that cause the most errors.", 
        "68": "Therefore, we analyze the errors of each form separately and use different approaches expected to ensure the best performance.", 
        "69": "If a tweet matches rules for counterfactual forms 1, 2, 3, 4, or 5, it is further classified using a statistical model trained with features of sequential words (n-gram) and POS tags of demarcated arguments and the whole sentence.", 
        "70": "A statistical model is expected to capture some implicit relations between arguments as well as lexical and part-of-speech patterns, but may also hurt performance in situations where the rulebased approach achieves high precision.", 
        "71": "Therefore, we applied statistical approaches to counterfactual forms which cannot be easily differentiated by their superficial patterns.", 
        "72": "These forms were selected by both theoretical and empirical analysis; we discuss these forms further in our evaluation section.", 
        "73": "6http://www.cs.cmu.edu/\u02dcark/TweetNLP/ model.ritter_ptb_alldata_fixed.20130723  4 Evaluation  As discussed, counterfactuals are not easily identified by rules or specific words.", 
        "74": "Given their low base rate and multiplicity of forms, traditional machine learning approaches trained on a random tweet sample tend to label all tweets as the most frequent class (non-counterfactual).", 
        "75": "Use of a counterfactual-enriched training set increases precision, but still gives a low F1 on the imbalanced test set.", 
        "76": "Thus, in order to make the classifier robust to the imbalanced dataset, we designed a rule-based model with counterfactual forms, which resulted in significantly lower false negative rate of the statistical model.", 
        "77": "Moreover, the rule-based model captured more positive samples of all possible forms, despite its lack of presence in the training set.", 
        "78": "This results in a substantial increase in overall performance F1.", 
        "79": "However, the precision is extremely low because of its incapability to detect negative sample with subtle differences inside the pattern.", 
        "80": "A combined approach therefor gives the best result.", 
        "81": "As Table 3 shows, the statistical model obtained the highest precision, while the rule-based model obtained, by a large margin, the highest recall.", 
        "82": "However, our whole pipeline (\u2018CF Parser\u2019 in Table 3) obtained the best overall performance with the combination of both approaches.", 
        "83": "For Wish Verb form prediction gets a big performance boost from the statistical model because of highly frequent false positives which have counterfactual-like forms such as birthday wishes or new year\u2019s day wishes.", 
        "84": "Among samples classified as Wish Verb form the counterfactual prediction F1 increased from 0.82 to 0.90 after the final\nprediction by the statistical model.", 
        "85": "Finally, we conducted an ablation test to analyze how each process of the pipeline affects the overall performance of the classifier (Table 4).", 
        "86": "The argument detection was less effective (F1 0.01 drop) than we expected due to the relatively simple and concise structure of tweets in general (Args in Table 4).", 
        "87": "Using only n-grams as features for the statistical model without PTB-style Tweet POS tags gives a relatively large drop (0.02) from F1.", 
        "88": "From the grammatical perspective, n-grams are less informative than POS tags for counterfactuals especially considering that there are so many variations of each word token in social media (e.g., \u2018clda\u2019, \u2018coulda\u2019, and \u2018couldve\u2019 for \u2018could have\u2019).", 
        "89": "We examined how the statistical model affected the final performance of each counterfactual form.", 
        "90": "The model we used for filtering out frequent false positives (e.g., birth day wishes) of Wish Verb form caused 0.03 F1 drop when it is removed.", 
        "91": "Also, the models trained with twoargument-relation forms (Conjunctive Normal / Converse, Modal Normal, and Verb Inversion) caused 0.04 F1 drop when they are removed from the pipeline, since the classifier cannot use subtle relations between arguments for its counterfactual prediction.", 
        "92": "5 Conclusion  This is the first work to identify counterfactuals in social media, a task we hope more people will address.", 
        "93": "Our best results came from combining rulebased methods that exploit a theory of the different forms of counterfactual with focused statistical\nmethods for reclassification of challenging forms.", 
        "94": "Our counterfactual predictor can now be applied to large collections of tweets and Facebook posts from people of known education, religiosity, political orientation, well-being, and other attributes of interest to psychologists and political scientists, allowing further study of their theories of counterfactual use."
    }, 
    "document_id": "P17-2103.pdf.json"
}
