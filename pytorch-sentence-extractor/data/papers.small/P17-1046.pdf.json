{
    "abstract_sentences": {
        "1": "We study response selection for multiturn conversation in retrieval-based chatbots.", 
        "2": "Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information.", 
        "3": "We propose a sequential matching network (SMN) to address both problems.", 
        "4": "SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.", 
        "5": "The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances.", 
        "6": "The final matching score is calculated with the hidden states of the RNN.", 
        "7": "An empirical study on two public data sets shows that SMN can significantly outperform stateof-the-art methods for response selection in multi-turn conversation."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 496\u2013505 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1046  1 Introduction  Conversational agents include task-oriented dialog systems and non-task-oriented chatbots.", 
        "2": "Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011).", 
        "3": "Existing work on building chatbots includes generation -based methods and retrieval-based methods.", 
        "4": "Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for\n\u2217 Corresponding Author\nthe current conversation from a repository with response selection algorithms.", 
        "5": "While most existing work on retrieval-based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario.", 
        "6": "In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.", 
        "7": "The key to response selection lies in inputresponse matching.", 
        "8": "Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also matching between responses and utterances in previous turns.", 
        "9": "The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in context, which is crucial to selecting a proper response and leveraging relevant information in matching; and (2) how to model relationships among the utterances in the context.", 
        "10": "Table 1 illustrates the challenges with an example.", 
        "11": "First, \u201chold a drum class\u201d and \u201cdrum\u201d in context are very important.", 
        "12": "Without them, one may find responses relevant to the message (i.e., the fifth utterance of the context) but nonsense in the context (e.g., \u201cwhat lessons do you want?\u201d).", 
        "13": "Second, the message highly depends on the second utterance in the context, and\n496\nthe order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses.", 
        "14": "Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).", 
        "15": "We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way.", 
        "16": "The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector.", 
        "17": "Thus, responses in these models connect with the context until the final step in matching.", 
        "18": "To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector.", 
        "19": "The matching vectors are then accumulated in the utterances\u2019 temporal order to model their relationships.", 
        "20": "The final matching degree is computed with the accumulation of the matching vectors.", 
        "21": "Specifically, for each utterance-response pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the word embeddings and the hidden states of a recurrent neural network with gated recurrent units (GRU) (Chung et al., 2014) respectively.", 
        "22": "The two matrices capture important matching information in the pair on a word level and a segment (word subsequence) level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices.", 
        "23": "By this means, important information from multiple levels of granularity in context is recognized under sufficient supervision from the response and carried into matching with minimal loss.", 
        "24": "The matching vectors are then uploaded to another GRU to form a matching score for the context and the response.", 
        "25": "The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in context.", 
        "26": "It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching.", 
        "27": "The matching degree of the context and the response is computed by a logit\nmodel with the hidden states of the GRU.", 
        "28": "SMN extends the powerful \u201c2D\u201d matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage of both important information in utterance-response pairs and relationships among utterances being sufficiently preserved and leveraged in matching.", 
        "29": "We test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale publicly available English data set for research in multi-turn conversation.", 
        "30": "The results show that our model can significantly outperform state-ofthe-art methods, and improvement to the best baseline model on R10@1 is over 6%.", 
        "31": "In addition to the Ubuntu corpus, we create a human-labeled Chinese data set, namely the Douban Conversation Corpus, and test our model on it.", 
        "32": "In contrast to the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from the open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges.", 
        "33": "On this data, our model improves the best baseline model by 3% on R10@1 and 4% on P@1.", 
        "34": "As far as we know, Douban Conversation Corpus is the first human-labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus.", 
        "35": "We have released Douban Conversation Corups and our source code at https://github.com/MarkWuNLP/ MultiTurnResponseSelection\nOur contributions in this paper are three-folds: (1) the proposal of a new context based matching model for multi-turn response selection in retrieval-based chatbots; (2) the publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets.", 
        "36": "2 Related Work  Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Ji et al., 2014) has drawn significant attention.", 
        "37": "Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation-based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing et al.,\n2016; Serban et al., 2016).", 
        "38": "Our work is a retrievalbased method, in which we study context-based response selection.", 
        "39": "Early studies of retrieval-based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).", 
        "40": "Recently, researchers have begun to pay attention to multi-turn conversation.", 
        "41": "For example, Lowe et al.", 
        "42": "(2015) match a response with the literal concatenation of context utterances.", 
        "43": "Yan et al.", 
        "44": "(2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture.", 
        "45": "Zhou et al.", 
        "46": "(2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view.", 
        "47": "Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.", 
        "48": "3 Sequential Matching Network    3.1 Problem Formalization  Suppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, .", 
        "49": ".", 
        "50": ".", 
        "51": ", ui,ni} represents a conversation context with {ui,k}nik=1 as utterances.", 
        "52": "ri is a response candidate and yi \u2208 {0, 1} denotes a label.", 
        "53": "yi = 1 means ri is a proper response for si, otherwise yi = 0.", 
        "54": "Our goal is to learn a matching model g(\u00b7, \u00b7) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r.  3.2 Model Overview  We propose a sequential matching network (SMN) to model g(\u00b7, \u00b7).", 
        "55": "Figure 1 gives the architecture.", 
        "56": "SMN first decomposes context-response matching into several utterance-response pair matching and then all pairs matching are accumulated as a context based matching through a recurrent neural network.", 
        "57": "SMN consists of three layers.", 
        "58": "The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution, pooling and encoded in a matching vector.", 
        "59": "The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context.", 
        "60": "The third layer calculates the final matching score with the hidden states of the second layer.", 
        "61": "SMN enjoys several advantages over existing models.", 
        "62": "First, a response candidate can match each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss.", 
        "63": "Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful for response selection in each utterance can be well identified and extracted.", 
        "64": "Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.", 
        "65": "By taking utterance relationships into account, SMN extends the \u201c2D\u201d matching that has proven effective in text pair matching for single-turn response selection to sequential \u201c2D\u201d matching for\ncontext based matching in response selection for multi-turn conversation.", 
        "66": "In the following sections, we will describe details of the three layers.", 
        "67": "3.3 Utterance-Response Matching  Given an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U = [eu,1, .", 
        "68": ".", 
        "69": ".", 
        "70": ", eu,nu ] and R = [er,1, .", 
        "71": ".", 
        "72": ".", 
        "73": ", er,nr ] respectively, where eu,i, er,i \u2208 Rd are the embeddings of the i-th word of u and r respectively.", 
        "74": "U \u2208 Rd\u00d7nu and R \u2208 Rd\u00d7nr are then used to construct a word-word similarity matrix M1 \u2208 Rnu\u00d7nr and a sequence-sequence similarity matrix M2 \u2208 Rnu\u00d7nr which are two input channels of a convolutional neural network (CNN).", 
        "75": "The CNN distills important matching information from the matrices and encodes the information into a matching vector v.\nSpecifically, \u2200i, j, the (i, j)-th element of M1 is defined by\ne1,i,j = e > u,i \u00b7 er,j .", 
        "76": "(1)\nM1 models the matching between u and r on a word level.", 
        "77": "To construct M2, we first employ a GRU to transform U and R to hidden vectors.", 
        "78": "Suppose that Hu = [hu,1, .", 
        "79": ".", 
        "80": ".", 
        "81": ", hu,nu ] are the hidden vectors of U, then \u2200i, hu,i \u2208 Rm is defined by\nzi = \u03c3(Wzeu,i +Uzhu,i\u22121) ri = \u03c3(Wreu,i +Urhu,i\u22121)\nh\u0303u,i = tanh(Wheu,i +Uh(ri hu,i\u22121)) hu,i = zi h\u0303u,i + (1\u2212 zi) hu,i\u22121, (2)\nwhere hu,0 = 0, zi and ri are an update gate and a reset gate respectively, \u03c3(\u00b7) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters.", 
        "82": "Similarly, we have Hr = [hr,1, .", 
        "83": ".", 
        "84": ".", 
        "85": ", hr,nr ] as the hidden vectors of R. Then, \u2200i, j, the (i, j)-th element of M2 is defined by\ne2,i,j = h > u,iAhr,j , (3)\nwhere A \u2208 Rm\u00d7m is a linear transformation.", 
        "86": "\u2200i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector.", 
        "87": "Therefore, M2 models the matching between u and r on a segment level.", 
        "88": "M1 and M2 are then processed by a CNN to form v. \u2200f = 1, 2, CNN regards Mf as\nan input channel, and alternates convolution and max-pooling operations.", 
        "89": "Suppose that z(l,f) =[ z (l,f) i,j ] I(l,f)\u00d7J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) = Mf , \u2200f = 1, 2.", 
        "90": "On the convolution layer, we employ a 2D convolution operation with a window size r (l,f) w \u00d7 r(l,f)h , and define z (l,f) i,j as\nz (l,f) i,j = \u03c3(\nFl\u22121\u2211\nf \u2032=0\nr (l,f) w\u2211\ns=0\nr (l,f) h\u2211\nt=0\nW (l,f) s,t \u00b7 z(l\u22121,f \u2032) i+s,j+t + b l,k), (4)\nwhere \u03c3(\u00b7) is a ReLU, W(l,f) \u2208 Rr(l,f)w \u00d7r(l,f)h and bl,k are parameters, and Fl\u22121 is the number of feature maps on the (l \u2212 1)-th layer.", 
        "91": "A max pooling operation follows a convolution operation and can be formulated as\nz (l,f) i,j = max\np (l,f) w >s\u22650 max p (l,f) h >t\u22650 zi+s,j+t, (5)\nwhere p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively.", 
        "92": "The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v \u2208 Rq.", 
        "93": "According to Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful for recognizing the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices.", 
        "94": "These areas will be transformed and selected by convolution and pooling operations and carry important information in the utterance to the matching vector.", 
        "95": "This is how our model identifies important information in context and leverage it in matching under the supervision of the response.", 
        "96": "We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.", 
        "97": "3.4 Matching Accumulation  Suppose that [v1, .", 
        "98": ".", 
        "99": ".", 
        "100": ", vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, .", 
        "101": ".", 
        "102": ".", 
        "103": ", vn] as an input and encodes the matching sequence into its hidden states Hm = [h \u2032 1, .", 
        "104": ".", 
        "105": ".", 
        "106": ", h \u2032 n] \u2208 Rq\u00d7n with a detailed parameterization similar to Equation (2).", 
        "107": "This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the\ncontext; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching.", 
        "108": "Moreover, from Equation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out.", 
        "109": "3.5 Matching Prediction and Learning  With [h\u20321, .", 
        "110": ".", 
        "111": ".", 
        "112": ", h \u2032 n], we define g(s, r) as\ng(s, r) = softmax(W2L[h \u2032 1, .", 
        "113": ".", 
        "114": ".", 
        "115": ", h \u2032 n] + b2), (6)\nwhere W2 and b2 are parameters.", 
        "116": "We consider three parameterizations for L[h\u20321, .", 
        "117": ".", 
        "118": ".", 
        "119": ", h \u2032 n]: (1) only the last hidden state is used.", 
        "120": "Then L[h\u20321, .", 
        "121": ".", 
        "122": ".", 
        "123": ", h \u2032 n] = h \u2032 n. (2) the hidden states are linearly combined.", 
        "124": "Then, L[h\u20321, .", 
        "125": ".", 
        "126": ".", 
        "127": ", h \u2032 n] =\u2211n\ni=1wih \u2032 i, where wi \u2208 R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states.", 
        "128": "Then, L[h\u20321, .", 
        "129": ".", 
        "130": ".", 
        "131": ", h \u2032 n] is defined as\nti = tanh(W1,1hui,nu +W1,2h \u2032 i + b1), \u03b1i = exp(t>i ts)\u2211 i(exp(t > i ts)) ,\nL[h\u20321, .", 
        "132": ".", 
        "133": ".", 
        "134": ", h \u2032 n] =\nn\u2211\ni=1\n\u03b1ih \u2032 i, (7)\nwhere W1,1 \u2208 Rq\u00d7m,W1,2 \u2208 Rq\u00d7q and b1 \u2208 Rq are parameters.", 
        "135": "h\u2032i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively.", 
        "136": "ts \u2208 Rq is a virtual context vector which is randomly initialized and jointly learned in training.", 
        "137": "Both (2) and (3) aim to learn weights for {h\u20321, .", 
        "138": ".", 
        "139": ".", 
        "140": ", h\u2032n} from training data and highlight the effect of important matching vectors in the final matching.", 
        "141": "The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors.", 
        "142": "We denote our model with the three parameterizations of L[h\u20321, .", 
        "143": ".", 
        "144": ".", 
        "145": ", h \u2032 n] as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.", 
        "146": "We learn g(\u00b7, \u00b7) by minimizing cross entropy withD.", 
        "147": "Let \u0398 denote the parameters of SMN, then the objective function L(D,\u0398) of learning can be\nformulated as\n\u2212 N\u2211\ni=1\n[yilog(g(si, ri)) + (1\u2212 yi)log(1\u2212 g(si, ri))] .", 
        "148": "(8)  4 Response Candidate Retrieval  In practice, a retrieval-based chatbot, to apply the matching approach to the response selection, one needs to retrieve a number of response candidates from an index beforehand.", 
        "149": "While candidate retrieval is not the focus of the paper, it is an important step in a real system.", 
        "150": "In this work, we exploit a heuristic method to obtain response candidates from the index.", 
        "151": "Given a message un with {u1, .", 
        "152": ".", 
        "153": ".", 
        "154": ", un\u22121} utterances in its previous turns, we extract the top 5 keywords from {u1, .", 
        "155": ".", 
        "156": ".", 
        "157": ", un\u22121} based on their tf-idf scores1 and expand un with the keywords.", 
        "158": "Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index.", 
        "159": "Finally, we use g(s, r) to rerank the candidates and return the top one as a response to the context.", 
        "160": "5 Experiments  We tested our model on a publicly available English data set and a Chinese data set published with this paper.", 
        "161": "5.1 Ubuntu Corpus  The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of the Ubuntu Forum.", 
        "162": "The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for testing.", 
        "163": "Positive responses are true responses from humans, and negative ones are randomly sampled.", 
        "164": "The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and testing.", 
        "165": "We used the copy shared by Xu et al.", 
        "166": "(2016) 2 in which numbers, urls, and paths are replaced by special placeholders.", 
        "167": "We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.", 
        "168": "1Tf is word frequency in the context, while idf is calculated using the entire index.", 
        "169": "2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0  5.2 Douban Conversation Corpus  The Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment.", 
        "170": "To further verify the efficacy of our model, we created a new data set with open domain conversations, called the Douban Conversation Corpus.", 
        "171": "Response candidates in the test set of the Douban Conversation Corpus are collected following the procedure of a retrieval-based chatbot and are labeled by human judges.", 
        "172": "It simulates the real scenario of a retrievalbased chatbot.", 
        "173": "We publish it to research communities to facilitate the research of multi-turn response selection.", 
        "174": "Specifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China.", 
        "175": "We randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap between the three sets.", 
        "176": "For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response.", 
        "177": "There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.", 
        "178": "To create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5.", 
        "179": "We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs.", 
        "180": "We recruited three labelers to judge if a candidate is a proper response to the context.", 
        "181": "A proper response means the response can naturally reply to the message given the whole context.", 
        "182": "Each pair received three labels and the majority of the labels were taken as the final decision.", 
        "183": "Table 2 gives the statistics of the three sets.", 
        "184": "Note that the Fleiss\u2019 kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.", 
        "185": "Besides Rn@ks, we also followed the conven-\n3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/\ntion of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics.", 
        "186": "We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation.", 
        "187": "When using the labeled set, we removed conversations with all negative responses or all positive responses, as models make no difference with them.", 
        "188": "There are 6, 670 contextresponse pairs left in the test set.", 
        "189": "5.3 Baseline  We considered the following baselines:\nBasic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.", 
        "190": "Multi-view: the model proposed by Zhou et al.", 
        "191": "(2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.", 
        "192": "Deep learning to respond (DL2R): the model proposed by Yan et al.", 
        "193": "(2016) that reformulates the message with other utterances in the context.", 
        "194": "Advanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3.", 
        "195": "Multi-Channel is a simple version of our model without considering utterance relationships.", 
        "196": "We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp.", 
        "197": "5.4 Parameter Tuning  For baseline models, if their results are available in existing literature (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures.", 
        "198": "All models were implemented using Theano (Theano Development Team, 2016).", 
        "199": "Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200.", 
        "200": "For Multi-Channel and layer one of our model, we set the dimensionality of the hidden states of GRU as 200.", 
        "201": "We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally.", 
        "202": "The number of feature maps is 8.", 
        "203": "In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50.", 
        "204": "The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU.", 
        "205": "The initial learning rate is 0.001, and the parameters of Adam, \u03b21 and \u03b22 are 0.9 and 0.999 respectively.", 
        "206": "We employed early-stopping as a regularization strategy.", 
        "207": "Models were trained in minibatches with a batch size of 200, and the maximum utterance length is 50.", 
        "208": "We set the maximum context length (i.e., number of utterances) as 10, because the performance of models does not improve on contexts longer than 10 (details are shown in the Section 5.6).", 
        "209": "We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances.", 
        "210": "5.5 Evaluation Results  Table 3 shows the evaluation results on the two data sets.", 
        "211": "Our models outperform baselines\ngreatly in terms of all metrics on both data sets, with the improvements being statistically significant (t-test with p-value \u2264 0.01, except R10@5 on Douban Corpus).", 
        "212": "Even the state-of-the-art singleturn matching models perform much worse than our models.", 
        "213": "The results demonstrate that one cannot neglect utterance relationships and simply perform multi-turn response selection by concatenating utterances together.", 
        "214": "Our models achieve significant improvements over Multi-View, which justified our \u201cmatching first\u201d strategy.", 
        "215": "DL2R is worse than our models, indicating that utterance reformulation with heuristic rules is not a good method for utilizing context information.", 
        "216": "Rn@ks are low on the Douban Corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33).", 
        "217": "SMNdynamic is only slightly better than SMNstatic and SMNlast.", 
        "218": "The reason might be that the GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of an attention mechanism is not obvious for the task at hand.", 
        "219": "5.6 Further Analysis  Visualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4.", 
        "220": "The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how?", 
        "221": "u4: are the files all\nin the same directory?", 
        "222": "u5: yes they all are; r: then the command glebihan should extract them all from/to that directory}.", 
        "223": "It is from the test set and our model successfully ranked the correct response to the top position.", 
        "224": "Due to space limitation, we only visualized M1, M2 and the update gate (i.e.", 
        "225": "z) in Figure 2.", 
        "226": "We can see that in u1 important words including \u201cunzip\u201d, \u201crar\u201d, \u201cfiles\u201d are recognized and carried to matching by \u201ccommand\u201d, \u201cextract\u201d, and \u201cdirectory\u201d in r, while u3 is almost useless and thus little information is extracted from it.", 
        "227": "u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost \u201cclosed\u201d to keep the information from u1 and r until the final state.", 
        "228": "Model ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4.", 
        "229": "First, replacing the multi-channel \u201c2D\u201d matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically.", 
        "230": "This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair.", 
        "231": "Together with the visualization, we can conclude that \u201c2D\u201d matching plays a key role in the \u201cmatching first\u201d strategy as it captures the important matching information in each pair with minimal loss.", 
        "232": "Second, the performance drops slightly when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA).", 
        "233": "This indicates that utterance relationships are useful.", 
        "234": "Finally, we left only one channel in matching\nand found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on the Douban Corpus).", 
        "235": "Performance across context length: we study how our model (SMNlast) performs across the length of contexts.", 
        "236": "Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus.", 
        "237": "Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger.", 
        "238": "The results demonstrate that our model can well capture the dependencies, especially long dependencies, among utterances in contexts.", 
        "239": "Maximum context length: we investigate the influence of maximum context length for SMN.", 
        "240": "Figure 4 shows the performance of SMN on Ubuntu Corpus and Douban Corpus with respect to maximum context length.", 
        "241": "From Figure 4, we find that performance improves significantly when the maximum context length is lower than 5, and becomes stable after the context length reaches 10.", 
        "242": "This indicates that context information is important for multi-turn response selection, and we can set the maximum context length as 10 to balance effectiveness and efficiency.", 
        "243": "Error analysis: although SMN outperforms baseline methods on the two data sets, there are\nstill several problems that cannot be handled perfectly.", 
        "244": "(1) Logical consistency.", 
        "245": "SMN models the context and response on the semantic level, but pays little attention to logical consistency.", 
        "246": "This leads to several DSATs in the Douban Corpus.", 
        "247": "For example, given a context {a: Does anyone know Newton jogging shoes?", 
        "248": "b: 100 RMB on Taobao.", 
        "249": "a: I know that.", 
        "250": "I do not want to buy it because that is a fake which is made in Qingdao ,b: Is it the only reason you do not want to buy it?", 
        "251": "}, SMN gives a large score to the response { It is not a fake.", 
        "252": "I just worry about the date of manufacture}.", 
        "253": "The response is inconsistent with the context on logic, as it claims that the jogging shoes are not fake.", 
        "254": "In the future, we shall explore the logic consistency problem in retrieval-based chatbots.", 
        "255": "(2) No correct candidates after retrieval.", 
        "256": "In the experiment, we prepared 1000 contexts for testing, but only 667 contexts have correct candidates after candidate response retrieval.", 
        "257": "This indicates that there is still room for candidate retrieval components to improve, and only expanding the input message with several keywords in context may not be a perfect approach for candidate retrieval.", 
        "258": "In the future, we will consider advanced methods for retrieving candidates.", 
        "259": "6 Conclusion and Future Work  We present a new context based model for multiturn response selection in retrieval-based chatbots.", 
        "260": "Experiment results on open data sets show that the model can significantly outperform the stateof-the-art methods.", 
        "261": "Besides, we publish the first human-labeled multi-turn response selection data set to research communities.", 
        "262": "In the future, we shall study how to model logical consistency of responses and improve candidate retrieval.", 
        "263": "7 Acknowledgment  We appreciate valuable comments provided by anonymous reviewers and our discussions with Zhao Yan.", 
        "264": "This work was supported by the National Natural Science Foundation of China (Grand Nos.", 
        "265": "61672081, U1636211, 61370126), Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-2016001), National High Technology Research and Development Program of China (No.2015AA016004), and the Fund of the State Key Laboratory of Software Development Environment (No.SKLSDE-2015ZX-16)."
    }, 
    "document_id": "P17-1046.pdf.json"
}
