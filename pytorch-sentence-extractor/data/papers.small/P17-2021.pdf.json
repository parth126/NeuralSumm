{
    "abstract_sentences": {
        "1": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees.", 
        "2": "Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset.", 
        "3": "An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline.", 
        "4": "A smallscale human evaluation also showed an advantage to the syntax-aware system."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 132\u2013140 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2021  1 Introduction and Model  Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al., 2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches (Koehn, 2010).", 
        "2": "NMT models usually do not make explicit use of syntactic information about the languages at hand.", 
        "3": "However, a large body of work was dedicated to syntax-based SMT (Williams et al., 2016).", 
        "4": "One prominent approach to syntaxbased SMT is string-to-tree (S2T) translation (Yamada and Knight, 2001, 2002), in which a sourcelanguage string is translated into a target-language tree.", 
        "5": "S2T approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and target languages.", 
        "6": "The main formalism driving current S2T SMT systems is GHKM rules (Galley et al., 2004, 2006), which are\nsynchronous transduction grammar (STSG) fragments, extracted from word-aligned sentence pairs with syntactic trees on one side.", 
        "7": "The GHKM translation rules allow flexible reordering on all levels of the parse-tree.", 
        "8": "We suggest that NMT can also benefit from the incorporation of syntactic knowledge, and propose a simple method of performing string-to-tree neural machine translation.", 
        "9": "Our method is inspired by recent works in syntactic parsing, which model trees as sequences (Vinyals et al., 2015; Choe and Charniak, 2016).", 
        "10": "Namely, we translate a source sentence into a linearized, lexicalized constituency tree, as demonstrated in Figure 2.", 
        "11": "Figure 1 shows a translation from our neural S2T model compared to one from a vanilla NMT model for the same source sentence, as well as the attention-induced word alignments of the two models.", 
        "12": "Note that the linearized trees we predict are different in their structure from those in Vinyals et al.", 
        "13": "(2015) as instead of having part of speech tags as terminals, they contain the words of the translated sentence.", 
        "14": "We intentionally omit the POS informa-\n132\ntion as including it would result in significantly longer sequences.", 
        "15": "The S2T model is trained on parallel corpora in which the target sentences are automatically parsed.", 
        "16": "Since this modeling keeps the form of a sequence-to-sequence learning task, we can employ the conventional attention-based sequence to sequence paradigm (Bahdanau et al., 2014) as-is, while enriching the output with syntactic information.", 
        "17": "Related Work Some recent works did propose to incorporate syntactic or other linguistic knowledge into NMT systems, although mainly on the source side: Eriguchi et al.", 
        "18": "(2016a,b) replace the encoder in an attention-based model with a Tree-LSTM (Tai et al., 2015) over a constituency parse tree; Bastings et al.", 
        "19": "(2017) encoded sentences using graph-convolutional networks over dependency trees; Sennrich and Haddow (2016) proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al.", 
        "20": "(2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the source sentence; Stahlberg et al.", 
        "21": "(2016) proposed a hybrid approach in which translations are scored by combining scores from an NMT system with scores from a Hiero (Chiang, 2005, 2007) system.", 
        "22": "Shi et al.", 
        "23": "(2016) explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to predict syntactic information like constituency trees, voice and tense with high accuracy.", 
        "24": "In parallel and highly related to our work, Eriguchi et al.", 
        "25": "(2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al.", 
        "26": "(2017) incorporated target syntax by predicting CCG tags serialized into the target translation.", 
        "27": "Our work differs from those by modeling syntax using constituency trees, as was previously common in the \u201ctraditional\u201d syntaxbased machine translation literature.", 
        "28": "2 Experiments & Results  Experimental Setup We first experiment in a resource-rich setting by using the German-English\nportion of the WMT16 news translation task (Bojar et al., 2016), with 4.5 million sentence pairs.", 
        "29": "We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al.", 
        "30": "(2017).", 
        "31": "In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.2\nIn each experiment we train two models.", 
        "32": "A baseline model (bpe2bpe), trained to translate from the source language sentences to English sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized constituency trees as shown in Figure 2.", 
        "33": "Words are segmented into sub-word units using the BPE model we learn on the raw parallel data.", 
        "34": "We use the NEMATUS (Sennrich et al., 2017)3 implementation of an attention-based NMT model.4 We trained the models until there was no improvement on the development set in 10 consecutive checkpoints.", 
        "35": "Note that the only difference between the baseline and the bpe2tree model is the syntactic information, as they have a nearly-identical amount of model parameters (the only additional parameters to the syntax-aware system are the embeddings for the brackets of the trees).", 
        "36": "For all models we report results of the best performing single model on the dev-set (newstest2013+newstest2014 in the resource rich setting, newstest2015 in the rest, as measured by BLEU) when translating newstest2015 and newstest2016, similarly to Sennrich et al.", 
        "37": "(2016a); Eriguchi et al.", 
        "38": "(2017).", 
        "39": "To evaluate the string-totree translations we derive the surface form by removing the symbols that stand for non-terminals in the tree, followed by merging the sub-words.", 
        "40": "We also report the results of an ensemble of the last 5 checkpoints saved during each model training.", 
        "41": "We compute BLEU scores using the\n1https://github.com/BLLIP/bllip-parser 2https://github.com/rsennrich/\nsubword-nmt 3https://github.com/rsennrich/nematus 4Further technical details of the setup and training are available in the supplementary material.", 
        "42": "mteval-v13a.pl script from the Moses toolkit (Koehn et al., 2007).", 
        "43": "Results As shown in Table 1, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) perform similarly in terms of BLEU on newstest2015.", 
        "44": "On newstest2016 we witness an advantage to the bpe2tree model.", 
        "45": "A similar trend is found when evaluating the model ensembles: while they improve results for both models, we again see an advantage to the bpe2tree model on newstest2016.", 
        "46": "Table 2 shows the results in the low-resource setting, where the bpe2tree model is consistently better than the bpe2bpe baseline.", 
        "47": "We find this interesting as the syntax-aware system performs a much harder task (predicting trees on top of the translations, thus handling much longer output sequences) while having a nearly-identical amount of model parameters.", 
        "48": "In order to better understand where or how the syntactic information improves translation quality, we perform a closer analysis of the WMT16 experiment.", 
        "49": "3 Analysis  The Resulting Trees Our model produced valid trees for 5970 out of 6003 sentences in the development set.", 
        "50": "While we did not perform an in-depth error-analysis, the trees seem to follow the syntax of English, and most choices seem reasonable.", 
        "51": "Quantifying Reordering English and German differ in word order, requiring a significant amount of reordering to generate a fluent translation.", 
        "52": "A major benefit of S2T models in SMT is facilitating reordering.", 
        "53": "Does this also hold for our neural S2T model?", 
        "54": "We compare the amount of reordering in the bpe2bpe and bpe2tree models using a distortion score based on the alignments derived from the attention weights of the corresponding systems.", 
        "55": "We first convert the attention weights to hard alignments by taking for each target word the source word with highest attention weight.", 
        "56": "For an n-word target sentence t and source sentence s let a(i) be the position of the source word aligned to the target word in position i.", 
        "57": "We define:\nd(s, t) = 1\nn\nn\u2211\ni=2\n|a(i)\u2212 a(i\u2212 1)|\nFor example, for the translations in Figure 1, the above score for the bpe2tree model is 2.73, while the score for the bpe2bpe model is 1.27 as the bpe2tree model did more reordering.", 
        "58": "Note that for the bpe2tree model we compute the score only on tokens which correspond to terminals (words or sub-words) in the tree.", 
        "59": "We compute this score for each source-target pair on newstest2015 for each model.", 
        "60": "Figure 3 shows a histogram of the binned score counts.", 
        "61": "The bpe2tree model has more translations with distortion scores in bins 1- onward and significantly less translations in the least-reordering bin (0) when compared to the bpe2bpe model, indicating that the syntactic information encouraged the model to perform more reordering.5 Figure 4 tracks the distortion scores throughout the learning process, plotting the average dev-set scores for the model checkpoints saved every 30k updates.", 
        "62": "Interestingly, both models obey to the following trend: open with a relatively high distortion score, followed by a steep decrease, and from there ascend gradually.", 
        "63": "The bpe2tree model usually has a higher distortion score during training, as we would expect after our previous findings from Figure 3.", 
        "64": "Tying Reordering and Syntax The bpe2tree model generates translations with their constituency tree and their attention-derived alignments.", 
        "65": "We can use this information to extract GHKM rules (Galley et al., 2004).6 We derive\n5We also note that in bins 4-6 the bpe2bpe model had slightly more translations, but this was not consistent among different runs, unlike the gaps in bins 0-3 which were consistent and contain most of the translations.", 
        "66": "6github.com/joshua-decoder/galley-ghkm\nhard alignments for that purpose by treating every source/target token-pair with attention score above 0.5 as an alignment.", 
        "67": "Extracting rules from the dev-set predictions resulted in 233,657 rules, where 22,914 of them (9.8%) included reordering, i.e.", 
        "68": "contained variables ordered differently in the source and the target.", 
        "69": "We grouped the rules by their LHS (corresponding to a target syntactic structure), and sorted them by the total number of RHS (corresponding to a source sequential structure) with reordering.", 
        "70": "Table 3 shows the top 10 extracted LHS, together with the top-5 RHS, for each rule.", 
        "71": "The most common rule, VP(x0:TER x1:NP) \u2192 x1 x0, found in 184 sentences in the dev set (8.4%), is indicating that the sequence x1 x0 in German was reordered to form a verb phrase in English, in which x0 is a terminal and x1 is a noun phrase.", 
        "72": "The extracted GHKM rules reveal very sensible German-English reordering patterns.", 
        "73": "Relative Constructions Browsing the produced trees hints at a tendency of the syntax-aware model to favor using relative-clause structures and subordination over other syntactic constructions (i.e., \u201cseveral cameras that are all priced...\u201d vs. \u201cseveral cameras, all priced...\u201d).", 
        "74": "To quantify this, we count the English relative pronouns (who, which, that7, whom, whose) found in the newstest2015 translations of each model and in the reference translations, as shown in Figure 5.", 
        "75": "The bpe2tree model produces more relative constructions compared to the bpe2bpe model, and both models produce more such constructions than found in the reference.", 
        "76": "Main Verbs While not discussed until this point, the generated opening and closing brackets also have attention weights, providing another opportunity to to peak into the model\u2019s behavior.", 
        "77": "Figure 6 in the supplementary material presents an example of a complete attention matrix, including the syntactic brackets.", 
        "78": "While making full sense of the attention patterns of the syntactic elements remains a challenge, one clear trend is that opening the very first bracket of the sentence consistently attends to the main verb or to structural markers (i.e.", 
        "79": "question marks, hyphens) in the source sentence, suggesting a planning-ahead behavior of the decoder.", 
        "80": "The underlines in Table 4 correspond to the source word attended by the first opening bracket, and the target word this source word was\n7\u201dthat\u201d also functions as a determiner.", 
        "81": "We do not distinguish the two cases.", 
        "82": "most strongly aligned to.", 
        "83": "In general, we find the alignments from the syntax-based system more sensible (i.e.", 
        "84": "in Figure 1 \u2013 the bpe2bpe alignments are off-by-1).", 
        "85": "Qualitative Analysis and Human Evaluations The bpe2tree translations read better than their bpe2bpe counterparts, both syntactically and semantically, and we highlight some examples which demonstrate this.", 
        "86": "Table 4 lists some representative examples, highlighting improvements that correspond to syntactic phenomena involving reordering or global structure.", 
        "87": "We also performed a small-scale human-evaluation using mechanical turk on the first 500 sentences in the dev-set.", 
        "88": "Further details are available in the supplementary material.", 
        "89": "The results are summarized in the following table:\n2bpe weakly better 100 2bpe strongly better 54 2tree weakly better 122 2tree strongly better 64 both good 26 both bad 3 disagree 131\nAs can be seen, in 186 cases (37.2%) the human evaluators preferred the bpe2tree translations, vs. 154 cases (30.8%) for bpe2bpe, with the rest of the cases (30%) being neutral.", 
        "90": "4 Conclusions and Future Work  We present a simple string-to-tree neural translation model, and show it produces results which are better than those of a neural string-to-string model.", 
        "91": "While this work shows syntactic information about the target side can be beneficial for NMT, this paper only scratches the surface with what can be done on the subject.", 
        "92": "First, better models can be proposed to alleviate the long sequence problem in the linearized approach or allow a more natural tree decoding scheme (Alvarez-Melis and Jaakkola, 2017).", 
        "93": "Comparing our approach to other syntax aware NMT models like Eriguchi et al.", 
        "94": "(2017) and Nadejde et al.", 
        "95": "(2017) may also be of interest.", 
        "96": "A Contrastive evaluation (Sennrich, 2016) of a syntax-aware system vs. a syntax-agnostic system may also shed light on the benefits of incorporating syntax into NMT.", 
        "97": "Acknowledgments  This work was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and The Israeli Science Foundation (grant number 1555/15).", 
        "98": "A Supplementary Material  Data The English side of the corpus was tokenized (into Penn treebank format) and truecased using the scripts provided in Moses (Koehn et al., 2007).", 
        "99": "We ran the BPE process on a concatenation of the source and target corpus, with 89500 BPE operations in the WMT experiment and with 45k operations in the other experiments.", 
        "100": "This resulted in an input vocabulary of 84924 tokens and an output vocabulary of 78499 tokens in the WMT16 experiment.", 
        "101": "The linearized constituency trees are obtained by simply replacing the POS tags in the parse trees with the corresponding word or subwords.", 
        "102": "The output vocabulary in the bpe2tree models includes the target subwords and the tree symbols which correspond to an opening or closing of a specific phrase type.", 
        "103": "Hyperparameters The word embedding size was set to 500/256 and the encoder and decoder sizes were set to 1024/256 (WMT16/other experiments).", 
        "104": "For optimization we used Adadelta (Zeiler, 2012) with minibatch size of 40.", 
        "105": "For decoding we used beam search with a beam size of 12.", 
        "106": "We trained the bpe2tree WMT16 model on sequences with a maximum length of 150 tokens (the average length for a linearized tree in the training set was about 50 tokens).", 
        "107": "It was trained for two weeks on a single Nvidia TitanX GPU.", 
        "108": "The bpe2bpe WMT16 model was trained on sequences with a maximum length of 50 tokens, and with minibatch size of 80.", 
        "109": "It was trained for one week on a single Nvidia TitanX GPU.", 
        "110": "Only in the low-resource experiments we applied dropout as described in Sennrich et al.", 
        "111": "(2016a) for RomanianEnglish.", 
        "112": "Human Evaluation We performed humanevaluation on the Mechnical Turk platform.", 
        "113": "Each sentence was evaluated using two annotators.", 
        "114": "For each sentence, we presented the annotators with the English reference sentence, followed by the outputs of the two systems.", 
        "115": "The German source was not shown, and the two system\u2019s outputs were shown in random order.", 
        "116": "The annotators were instructed to answer \u201cWhich of the two sentences, in your view, is a better portrayal of the the reference sentence.\u201d They were then given 6 options: \u201csent 1 is better\u201d, \u201csent 2 is better\u201d, \u201csent 1 is a little better\u201d, \u201csent 2 is a little better\u201d, \u201cboth sentences are equally good\u201d, \u201cboth sentences are equally bad\u201d.", 
        "117": "We then ignore differences between \u201cbetter\u201d and\n\u201ca little better\u201d.", 
        "118": "We count as \u201cstrongly better\u201d the cases where both annotators indicated the same sentence as better, as \u201cweakly better\u201d the cases were one annotator chose a sentence and the other indicated they are both good/bad.", 
        "119": "Other cases are treated as either \u201cboth good\u201d / \u201cboth bad\u201d or as disagreements.", 
        "120": "Additional Output Examples from both models, in the format of Figure 1.", 
        "121": "Notice the improved translation and alignment quality in the tree-based translations, as well as the overall high structural quality of the resulting trees.", 
        "122": "The few syntactic mistakes in these examples are attachment errors of SBAR and PP phrases, which will also challenge dedicated parsers."
    }, 
    "document_id": "P17-2021.pdf.json"
}
