{
    "abstract_sentences": {
        "1": "To enable human-robot communication and collaboration, previous works represent grounded verb semantics as the potential change of state to the physical world caused by these verbs.", 
        "2": "Grounded verb semantics are acquired mainly based on the parallel data of the use of a verb phrase and its corresponding sequences of primitive actions demonstrated by humans.", 
        "3": "The rich interaction between teachers and students that is considered important in learning new skills has not yet been explored.", 
        "4": "To address this limitation, this paper presents a new interactive learning approach that allows robots to proactively engage in interaction with human partners by asking good questions to learn models for grounded verb semantics.", 
        "5": "The proposed approach uses reinforcement learning to allow the robot to acquire an optimal policy for its question-asking behaviors by maximizing the long-term reward.", 
        "6": "Our empirical results have shown that the interactive learning approach leads to more reliable models for grounded verb semantics, especially in the noisy environment which is full of uncertainties.", 
        "7": "Compared to previous work, the models acquired from interactive learning result in a 48% to 145% performance gain when applied in new situations."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1634\u20131644 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1150  1 Introduction  In communication with cognitive robots, one of the challenges is that robots do not have sufficient linguistic or world knowledge as humans do.", 
        "2": "For example, if a human asks a robot to boil the water but the robot has no knowledge what this verb\nphrase means and how this verb phrase relates to its own actuator, the robot will not be able to execute this command.", 
        "3": "Thus it is important for robots to continuously learn the meanings of new verbs and how the verbs are grounded to its underlying action representations from its human partners.", 
        "4": "To support learning of grounded verb semantics, previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016) rely on multiple instances of human demonstrations of corresponding actions.", 
        "5": "From these demonstrations, robots capture the state change of the environment caused by the actions and represent verb semantics as the desired goal state.", 
        "6": "One advantage of such state-based representation is that, when robots encounter the same verbs/commands in a new situation, the desired goal state will trigger the action planner to automatically plan a sequence of primitive actions to execute the command.", 
        "7": "While the state-based verb semantics provides an important link to connect verbs to the robot\u2019s actuator, previous works also present several limitations.", 
        "8": "First of all, previous approaches were developed under the assumption of perfect perception of the environment (She et al., 2014; Misra et al., 2015; She and Chai, 2016).", 
        "9": "However, this assumption does not hold in real-world situated interaction.", 
        "10": "The robot\u2019s representation of the environment is often incomplete and error-prone due to its limited sensing capabilities.", 
        "11": "Thus it is not clear whether previous approaches can scale up to handle noisy and incomplete environment.", 
        "12": "Second, most previous works rely on multiple demonstration examples to acquire grounded verb models.", 
        "13": "Each demonstration is simply a sequence of primitive actions associated with a verb.", 
        "14": "No other type of interaction between humans and robots is explored.", 
        "15": "Previous cognitive studies (Bransford et al., 2000) on how people learn have shown that social interaction (e.g., conver-\n1634\nsation with teachers) can enhance student learning experience and improve learning outcomes.", 
        "16": "For robotic learning, previous work (Cakmak and Thomaz, 2012) has also demonstrated the necessity of question answering in the learning process.", 
        "17": "Thus, in our view, interactive learning beyond demonstration of primitive actions should play a vital role in the robot\u2019s acquisition of more reliable models of grounded verb semantics.", 
        "18": "This is especially important because the robot\u2019s perception of the world is noisy and incomplete, human language can be ambiguous, and the robot may lack the relevant linguistic or world knowledge during the learning process.", 
        "19": "To address these limitations, we have developed a new interactive learning approach where robots actively engage with humans to acquire models of grounded verb semantics.", 
        "20": "Our approach explores the space of interactive question answering between humans and robots during the learning process.", 
        "21": "In particular, motivated by previous work on robot learning (Cakmak and Thomaz, 2012), we designed a set of questions that are pertinent to verb semantic representations.", 
        "22": "We further applied reinforcement learning to learn an optimal policy that guides the robot in deciding when to ask what questions.", 
        "23": "Our empirical results have shown that this interactive learning process leads to more reliable representations of grounded verb semantics, which contribute to significantly better action performance in new situations.", 
        "24": "When the environment is noisy and uncertain (as in a realistic situation), the models acquired from interactive learning result in a performance gain between 48% and 145% when applied in new situations.", 
        "25": "Our results further demonstrate that the interaction policy acquired from reinforcement learning leads to the most efficient interaction and the most reliable verb models.", 
        "26": "2 Related Work  To enable human-robot communication and collaboration, recent years have seen an increasing amount of works which aim to learn semantics of language that are grounded to agents\u2019 perception (Gorniak and Roy, 2007; Tellex et al., 2014; Kim and Mooney, 2012; Matuszek et al., 2012a; Liu et al., 2014; Liu and Chai, 2015; Thomason et al., 2015, 2016; Yang et al., 2016; Gao et al., 2016) and action (Matuszek et al., 2012b; Artzi and Zettlemoyer, 2013; She et al., 2014; Misra\net al., 2014, 2015; She and Chai, 2016).", 
        "27": "Specifically for verb semantics, recent works explored the connection between verbs and action planning (She et al., 2014; Misra et al., 2014, 2015; She and Chai, 2016), for example, by representing grounded verbs semantics as the desired goal state of the physical world that is a result of the corresponding actions.", 
        "28": "Such representations are learned based on example actions demonstrated by humans.", 
        "29": "Once acquired, these representations will allow agents to interpret verbs/commands issued by humans in new situations and apply action planning to execute actions.", 
        "30": "Given its clear advantage in connecting verbs with actions, our work also applies the state-based representation for verb semantics.", 
        "31": "However, we have developed a new approach which goes beyond learning from demonstrated examples by exploring how rich interaction between humans and agents can be used to acquire models for grounded verb semantics.", 
        "32": "This approach was motivated by previous cognitive studies (Bransford et al., 2000) on how people learn as well as recent findings on robot skill learning (Cakmak and Thomaz, 2012).", 
        "33": "One of the principles for human learning is that \u201clearning is enhanced through socially supported interactions\u201d.", 
        "34": "Studies have shown that social interaction with teachers and peers (e.g., substantive conversation) can enhance student learning experience and improve learning outcomes.", 
        "35": "In recent work on interactive robot learning of new skills (Cakmak and Thomaz, 2012), researchers identified three types of questions that can be used by a human/robot student to enhance learning outcomes: 1) demonstration query (i.e., asking for a full or partial demonstration of the task), 2) label query (i.e., asking whether an execution is correct), and 3) feature query (i.e., asking for a specific feature or aspect of the task).", 
        "36": "Inspired by these previous findings, our work explores interactive learning to acquire grounded verb semantics.", 
        "37": "In particular, we aim to address when to ask what questions during interaction to improve learning.", 
        "38": "3 Acquisition of Grounded Verb Semantics  This section gives a brief review on acquisition of grounded verb semantics and illustrates the differences between previous approaches and our approach using interactive learning.", 
        "39": "3.1 State-based Representation  As shown in Figure 1, the verb semantics (e.g., boil(x)) is represented by the goal state (e.g., Status(x, TempHigh)) which is the result of the demonstrated primitive actions.", 
        "40": "Given the verb phrase boil the water (i.e., Li), the human teaches the robot how to accomplish the corresponding action based on a sequence of primitive actions\n\u2212\u2192Ai.", 
        "41": "By comparing the final environment E \u2032i with the initial environment Ei, the robot is able to identify the state change of the environment, which becomes a hypothesis of goal state to represent verb semantics.", 
        "42": "Compared to procedure-based representations, the state-based representation supports automated planning at the execution time.", 
        "43": "It is environment-independent and more generalizable.", 
        "44": "In (She and Chai, 2016), instead of one hypothesis, it maintains a specific-to-general hypothesis space as shown in Figure 2 to capture all goal hypotheses of a particular verb frame.", 
        "45": "Specifically, it assumes that one verb frame may lead to different outcomes under different environments, where each possible outcome is represented by one node in the hierarchical graph and each node is a conjunction of multiple atomic fluents.", 
        "46": "1\nGiven a language command (i.e., a verb phrase), a robot will engage in the following processes:\n\u2022 Execution.", 
        "47": "In this process, the robot will select a hypothesis from the space of hypotheses that is most relevant to the current situation and use the corresponding goal state to plan for actions to execute.", 
        "48": "\u2022 Learning.", 
        "49": "When the robot fails to select a hypothesis or fails to execute the action, it will ask the human for a demonstration.", 
        "50": "1In this work, we assume the set of atomic fluents representing environment state are given and do not address the question of whether these predicates are adequate to represent a domain.", 
        "51": "Based on the demonstrated actions, the robot will learn a new representation (i.e., new nodes) and update the hypothesis space.", 
        "52": "3.2 Noisy Environment  Previous works represent the environment Ei as a conjunction of grounded state fluents.", 
        "53": "Each fluent consists of a predicate and one or more arguments (i.e., objects in the physical world, or object status), representing one aspect of the perceived environment.", 
        "54": "An example of a fluent is \u201cHas(Kettle1,WATER)\u201d meaning object Kettle1 has some water inside, where Has is the predicate, and Kettle1 and WATER are arguments.", 
        "55": "The set of fluents include the status of the robot (e.g., Grasping(Kettle1)), the status of different objects (e.g., Status(WATER, TempHigh)), and relations between objects (e.g., On(Kettle1, Stove)).", 
        "56": "One limitation of the previous works is that the envi-\nronment has a perfect, deterministic representation, as shown in Figure 1.", 
        "57": "This is clearly not the case in the realistic physical world.", 
        "58": "In reality, given limitations of sensor capabilities, the environment representation is often partial, error prone, and full of uncertainties.", 
        "59": "Figure 3 shows an example of a more realistic representation where each fluent comes with a confidence between 0 and 1 to indicate how likely that particular fluent can be detected in the current environment.", 
        "60": "Thus, it is unclear whether the previous work is able to handle representations with uncertainties.", 
        "61": "Our interactive learning approach aims to address these uncertainties through interactive question answering with human partners.", 
        "62": "4 Interactive Learning    4.1 Framework of Interactive Learning  Figure 4 shows a general framework for interactive learning of action verbs.", 
        "63": "It aims to support a life-long learning cycle for robots, where the robot can continuously (1) engage in collaboration and\ncommunication with humans based on its existing knowledge; (2) acquire new verbs by learning from humans and experiencing the change of the world (i.e., grounded verb semantics as in this work); and (3) learn how to interact (i.e., update interaction policies).", 
        "64": "The lifelong learning cycle is composed by a sequence of interactive learning episodes (Episode 1, 2...) where each episode consists of either an execution phase or a learning phase or both.", 
        "65": "The execution phase starts with a human request for action (e.g., boil the water).", 
        "66": "According to its interaction policy, the robotic agent may choose to ask one or more questions (i.e., Q+i ) and wait for human answers (i.e., A+i ), or select a hypothesis from its existing knowledge base to execute the command (i.e., Execute).", 
        "67": "With the human feedback of the execution, the robot can update its interaction policy and existing knowledge.", 
        "68": "In the learning phase, the robot can initiate the learning by requesting a demonstration from the human.", 
        "69": "After the human performs the task,\nthe robotic agent can either choose to update its knowledge if it feels confident, or it can choose to ask the human one or more questions before updating its knowledge.", 
        "70": "4.2 Examples of Interactive Learning  Table 1 illustrates the differences between the previous approach that acquires verb models based solely on demonstrations and our current work that acquires models based on interactive learning.", 
        "71": "As shown in Table 1, under the demonstration setting, humans only provide a demonstration of primitive actions and there\u2019s no interactive question answering.", 
        "72": "In the interactive learning setting, the robot can proactively choose to ask questions regarding the uncertainties either about the environment (e.g., R1), the goal (e.g., R2), or the demonstrations (e.g., R6).", 
        "73": "Our hypothesis is that rich interactions based on question answering will allow the robot to learn more reliable models for grounded verb semantics, especially in a noisy environment.", 
        "74": "Then the question is how to manage such interaction: when to ask and what questions to ask to most efficiently acquire reliable models and apply them in execution.", 
        "75": "Next we describe the application of reinforcement learning to manage interactive question answering for both the execution phase and the learning phase.", 
        "76": "4.3 Formulation of Interactive Learning  Markov Decision Process (MDP) and its closely related Reinforcement Learning (RL) have been applied to sequential decision-making problems in dynamic domains with uncertainties, e.g., dialogue/interaction management (Singh et al., 2002; Paek and Pieraccini, 2008; Williams and Zweig, 2016), mapping language commands to actions (Branavan et al., 2009), interactive robot learning (Knox and Stone, 2011), and interactive information retrieval (Li et al., 2017).", 
        "77": "In this work, we formulate the choice of when to ask what questions during interaction as a sequential decisionmaking problem and apply reinforcement learning to acquire an optimal policy to manage interaction.", 
        "78": "Specifically, each of the execution and learning phases is governed by one policy (i.e., \u03b8E and \u03b8D), which is updated by the reinforcement learning algorithm.", 
        "79": "The use of RL intends to obtain optimal policies that can lead to the highest long-term reward by balancing the cost of interaction (e.g., the length of interaction and difficulties of questions) and the quality of the acquired models.", 
        "80": "The\nreinforcement formulation for both the execution phase and the learning phase are described below.", 
        "81": "State For the execution phase, each state se \u2208 SE is a five tuple: se = <l, e,KB,Grd,Goal>.", 
        "82": "l is a language command, including a verb and multiple noun phrases extracted by the Stanford parser.", 
        "83": "For example, the command \u201cMicrowave the ramen\u201d is represented as l = microwave(ramen).", 
        "84": "The environment e is a probabilistic representation of the currently perceived physical world, consisting of a set of grounded fluents and the confidence of perceiving each fluent (an example is shown in Figure 3).", 
        "85": "KB stands for the existing knowledge of verb models.", 
        "86": "Grd accounts for the agent\u2019s current belief of object grounding: the probability of each noun in the l being grounded to different objects.", 
        "87": "Goal represents the agent\u2019s belief of different goal state hypotheses of the current command.", 
        "88": "Within one interaction episode, command l and knowledge KB will stay the same, while e, Grd, and Goal may change accordingly due to interactive question answering and robot actions.", 
        "89": "In the execution phase, Grd and Goal are initialized with existing knowledge of learned verb models.", 
        "90": "For the learning phase, a state sd \u2208 SD is a four tuple: sd = <l, estart, eend, Grd>.", 
        "91": "estart and eend stands for the environment before the demonstration and after the demonstration.", 
        "92": "Action Motivated by previous studies on how humans ask questions while learning new skills (Cakmak and Thomaz, 2012), the agent\u2019s question set includes two categories: yes/no questions and wh- questions.", 
        "93": "These questions are designed to address ambiguities in noun phrase grounding, uncertain environment sensing, and goal states.", 
        "94": "They are domain independent in nature.", 
        "95": "For example, one of the questions is np grd ynq(n, o).", 
        "96": "It is a yes/no question asking whether the noun phrase n refers to an object o (e.g., \u201cI see a silver object, is that the pot?\u201d).", 
        "97": "Other questions are env pred ynq(p) (i.e., whether a fluent p is present in the environment; e.g., \u201cIs the microwave door open?\u201d) and goal pred ynq(p) (i.e., whether a predicate p should be part of the goal; \u201cShould the pot be on a pot stand?\u201d).", 
        "98": "Table 2 lists all the actions available in the execution and learning phases.", 
        "99": "The select hypo action (i.e., select a goal hypothesis to execute) is only for the execution.", 
        "100": "Ideally, after asking questions, the agent should be more likely to select a goal hy-\npothesis that best describes the current situation.", 
        "101": "For the learning phase, the include fluent(\u2227p) action forms a goal hypothesis by conjoining a set of fluents ps where each p should have high probability of being part of the goal.", 
        "102": "Transition The transition function takes action a in state s, and gives the next state s\u2032 according to human feedback.", 
        "103": "Note that the command l does not change during interaction.", 
        "104": "But the agent\u2019s belief of environment e, object grounding Grd, and goal hypotheses Goal is changed according to the questions and human answers.", 
        "105": "For example, suppose the agent asks whether noun phrase n refers to the object o, if the human confirms it, the probability of n being grounded to o becomes 1.0, otherwise it will become 0.0.", 
        "106": "Reward Finding a good reward function is a hard problem in reinforcement learning.", 
        "107": "Our current approach has followed the general practice in the spoken dialogue community (Schatzmann et al., 2006; Fang et al., 2014; Su et al., 2016).", 
        "108": "The immediate robot questions are assigned small costs to favor shorter and more efficient interaction.", 
        "109": "Furthermore, motivated by how humans ask\n1According to the study in (Cakmak and Thomaz, 2012), the frequency of y/n questions used by humans is about 6.5 times the frequency of open questions (wh question), which motivates our assignment of -6.5 to wh questions.", 
        "110": "2bulk np grd ynq asks multiple object grounding all at once.", 
        "111": "This is harder to answer than asking for a single np.", 
        "112": "Therefore, its cost is assigned three times of the other yes/no questions.", 
        "113": "Algorithm 1: Policy learning.", 
        "114": "The execution and learning phases share the same learning process, but with\ndifferent state s, action a spaces, and feature vectors \u03c6.", 
        "115": "The eend is only available to the learning phase.", 
        "116": "Input : e, l (, eend);\nFeature function \u03c6; Old policy \u03b8 (i.e., a weight vector) Verb Goal States HypothesesH;\nInitialize : state s initialized with e, l (, eend); first action a \u223c P (a|s; \u03b8) with greedy\n1 while s is not terminal do 2 Take action a, receive reward r; 3 s\u2032 = T (s, a); 4 Choose a\u2032 \u223c P (a\u2032|s\u2032; \u03b8) with greedy; \u03b4 \u2190 r + \u03b3 \u00b7 \u03b8T \u00b7 \u03c6(s\u2032, a\u2032)\u2212 \u03b8T \u00b7 \u03c6(s, a); 5 \u03b8 \u2190 \u03b8 + \u03b4 \u00b7 \u03b7 \u00b7 \u03c6(s, a); 6 end 7 if s terminates with positive feedback then 8 UpdateH; 9 end\nOutput : UpdatedH and \u03b8.\nquestions (Cakmak and Thomaz, 2012), yes/no questions are easier for a human to answer than the open questions (e.g., wh-questions) and thus are given smaller costs.", 
        "117": "A large positive reward is given at the end of interaction when the task is completed successfully.", 
        "118": "Detailed reward assignment for different actions are shown in Table 2.", 
        "119": "Learning The SARSA algorithm with linear function approximation is utilized to update policies \u03b8E and \u03b8D (Sutton and Barto, 1998).", 
        "120": "Specifically, the objective of training is to learn an optimal value function Q(s, a) (i.e., the expected cu-\nmulative reward of taking action a in a state s).", 
        "121": "This value function is approximated by a linear function Q(s, a) = \u03b8\u1d40 \u00b7 \u03c6(s, a), where \u03c6(s, a) is a feature vector and \u03b8 is a weight updated during training.", 
        "122": "Details of the algorithm is shown in Algorithm 1.", 
        "123": "During testing, the agent can take an action a that maximizes the Q value at a state s. Feature Example features used by the two phases are listed in Table 3.", 
        "124": "These features intend to capture different dimensions of information such as specific types of questions, how well noun phrases are grounded to the environment, uncertainties of the environment, and consistencies between a hypothesis and the current environment.", 
        "125": "5 Evaluation    5.1 Experiment Setup  Dataset.", 
        "126": "To evaluate our approach, we utilized the benchmark made available by (Misra et al., 2015).", 
        "127": "Individual language commands and corresponding action sequences are extracted similarly as (She and Chai, 2016).", 
        "128": "This dataset includes common tasks in the kitchen and living room domains, where each data instance comes with a language command (e.g., \u201cboil the water\u201d, \u201cthrow the beer into the trashcan\u201d) and the corresponding sequence of primitive actions.", 
        "129": "In total, there are 979 instances, including 75 different verbs and 215 different noun phrases.", 
        "130": "The length of primitive action sequences range from 1 to 51 with an average of 4.82 (+/-4.8).", 
        "131": "We divided the dataset into three groups: (1) 200 data instances were used by reinforcement learning to acquire optimal inter-\naction policies; (2) 600 data instances were used by different approaches (i.e., previous approaches and our interactive learning approach) to acquire grounded verb semantics models; and (3) 179 data instances were used as testing data to evaluate the learned verb models.", 
        "132": "The performance on applying the learned models to execute actions for the testing data is reported.", 
        "133": "To learn interaction policies, a simulated human model is created from the dataset (Schatzmann et al., 2006) to continuously interact with the robot learner3.", 
        "134": "This simulated user can answer the robot\u2019s different types of questions and make decisions on whether the robot\u2019s execution is correct.", 
        "135": "During policy learning, one data instance can be used multiple times.", 
        "136": "At each time, the interaction sequence is different due to exploitation and exploration in RL in selecting the next action.", 
        "137": "The RL discount factor \u03b3 is set to 0.99, the in - greedy is 0.1, and the learning rate is 0.01.", 
        "138": "Noisy Environment Representation.", 
        "139": "The original data provided by (Misra et al., 2015) is based on the assumption that environment sensing is perfect and deterministic.", 
        "140": "To enable incomplete and noisy environment representation, for each fluent (e.g., grasping(Cup3), near(robot1, Cup3)) in the original data, we independently sampled a confidence value to simulate the likelihood that a particular fluent can be detected correctly from the environment.", 
        "141": "We applied the following four different variations in sampling the confidence values, which correspond to different levels of sensor reliability.", 
        "142": "(1) PerfectEnv represents the most reliable sensor.", 
        "143": "If a fluent is true in the original data, its sampled confidence is 1, and 0 otherwise.", 
        "144": "(2) NormStd3 represents a relatively reliable sensor.", 
        "145": "For each fluent in the original environment, a confidence is sampled according to a normal distribution N (1, 0.32) with an interval [0,1].", 
        "146": "This distribution has a large probability of sampling a number larger than 0.5, meaning the corresponding fluent is still more likely to be true.", 
        "147": "(3) NormStd5 represents a less reliable sensor.", 
        "148": "The sampling distribution is N (1, 0.52), which has a larger probability of generating a number smaller than 0.5 compared to NormStd3.", 
        "149": "3In our future work, interacting with real humans will be conducted through Amazon Mechanical Turk.", 
        "150": "And the policies acquired with a simulated user in this work will be used as initial policies.", 
        "151": "(4) UniEnv represents an unreliable sensor.", 
        "152": "Each number is sampled with a uniform distribution between 0 and 1.", 
        "153": "This means the sensor works randomly.", 
        "154": "A fluent has a equal change to be true or false no matter what the true environment is.", 
        "155": "Evaluation Metrics.", 
        "156": "We used the same evaluation metrics as in the previous works (Misra et al., 2015; She and Chai, 2016) to evaluate the performance of applying the learned models to testing instances on action planning.", 
        "157": "\u2022 IED: Instruction Editing Distance.", 
        "158": "This is a number between 0 and 1 measuring the similarity between the predicted action sequence and the ground-truth action sequence.", 
        "159": "IED equals 1 if the two sequences are exactly the same.", 
        "160": "\u2022 SJI: State Jaccard Index.", 
        "161": "This is a number between 0 and 1 measuring the similarity between the predicted and the ground-truth state changes.", 
        "162": "SJI equals 1 if action planning leads to exactly the same state change as in the ground-truth.", 
        "163": "Configurations.", 
        "164": "To understand the role of interactive learning in model acquisition and action planning, we first compared the interactive learning approach with the previous leading approach (presented as She16).", 
        "165": "To further evaluate the interaction policies acquired by reinforcement learning, we also compared the learned policy (i.e., RLPolicy) with the following two baseline policies:\n\u2022 RandomPolicy which randomly selects questions to ask during interaction.", 
        "166": "\u2022 ManualPolicy which continuously asks for yes/no confirmations (i.e., object grounding questions (GroundQ), environment questions (EnvQ), goal prediction questions (GoalQ)) until there\u2019s no more questions before making a decision on model acquisition or action execution.", 
        "167": "5.2 Results    5.2.1 The Effect of Interactive Learning  Table 4 shows the performance comparison on the testing data between the previous approach She16 and our interactive learning approach based on environment representations with different levels of noise.", 
        "168": "The verb models acquired by interactive learning perform better consistently across all four\nenvironment conditions.", 
        "169": "When the environment becomes noisy (i.e., NormStd3, NormStd5, and UniEnv), the performance of She16 that only relies on demonstrations decreases significantly.", 
        "170": "While the interactive learning improves the performance under the perfect environment condition, its effect in noisy environment is more remarkable.", 
        "171": "It leads to a significant performance gain between 48% and 145%.", 
        "172": "These results validate our hypothesis that interactive question answering can help to alleviate the problem of uncertainties in environment representation and goal prediction.", 
        "173": "Figure 5 shows the performance of the various learned models on the testing data, based on a varying number of training instances and different interaction policies.", 
        "174": "The interactive learning guided by the policy acquired from RL outperforms the previous approach She16.", 
        "175": "The RL policy slightly outperforms interactive learning using manually defined policy (i.e., ManualPolicy).", 
        "176": "However, as shown in the next section, the Man-\nualPolicy results in much longer interaction (i.e., more questions) than the RL acquired policy.", 
        "177": "5.2.2 Comparison of Interaction Policies  Table 5 compares the performance of different interaction policies.", 
        "178": "It shows the average number of questions asked under different policies.", 
        "179": "It is not surprising the RandomPolicy has the worst performance.", 
        "180": "For the ManualPolicy, its performance is similar to the RLPolicy.", 
        "181": "However, the average interaction length of ManualPolicy is 6.792, which is much longer than the RLPolicy (which is 3.127).", 
        "182": "These results further demonstrate that the policy learned from RL enables efficient interactions and the acquisition of more reliable verb models.", 
        "183": "6 Conclusion  Robots live in a noisy environment.", 
        "184": "Due to the limitations in their external sensors, their representations of the shared environment can be error prone and full of uncertainties.", 
        "185": "As shown in previous work (Moura\u0303o et al., 2012), learning action models from the noisy and incomplete observation of the world is extremely challenging.", 
        "186": "The same problem applies to the acquisition of verb semantics that are grounded to the perceived world.", 
        "187": "To address this problem, this paper presents an interactive learning approach which aims to handle uncertainties of the environment as well as incompleteness and conflicts in state representation by asking human partners intelligent questions.", 
        "188": "The interaction strategies are learned through reinforcement learning.", 
        "189": "Our empirical results have shown a significant improvement in model acquisition and action prediction.", 
        "190": "When applying the learned models in new situations, the models ac-\nquired through interactive learning leads to over 140% performance gain in noisy environment.", 
        "191": "The current investigation also has several limitations.", 
        "192": "As in previous works, we assume the world can be described by a closed set of predicates.", 
        "193": "This causes significant simplification for the physical world.", 
        "194": "One of the important questions to address in the future is how to learn new predicates through interaction with humans.", 
        "195": "Another limitation is that the current utility function is learned based on a set of pre-identified features.", 
        "196": "Future work can explore deep neural network to alleviate feature engineering.", 
        "197": "As cognitive robots start to enter our daily lives, data-driven approaches to learning may not be possible in new situations.", 
        "198": "Human partners who work side-by-side with these cognitive robots are great resources that the robots can directly learn from.", 
        "199": "Recent years have seen an increasing amount of work on task learning from human partners (Saunders et al., 2006; Chernova and Veloso, 2008; Cantrell et al., 2012; Mohan et al., 2013; Asada et al., 2009; Mohseni-Kabir et al., 2015; Nejati et al., 2006; Liu et al., 2016).", 
        "200": "Our future work will incorporate interactive learning of verb semantics with task learning to enable autonomy that can learn by communicating with humans.", 
        "201": "Acknowledgments  This work was supported by the National Science Foundation (IIS-1208390 and IIS-1617682) and the DARPA SIMPLEX program under a subcontract from UCLA (N66001-15-C-4035).", 
        "202": "The authors would like to thank Dipendra K. Misra and colleagues for providing the evaluation data, and the anonymous reviewers for valuable comments."
    }, 
    "document_id": "P17-1150.pdf.json"
}
