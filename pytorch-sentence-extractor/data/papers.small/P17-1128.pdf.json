{
    "abstract_sentences": {
        "1": "Finding the correct hypernyms for entities is essential for taxonomy learning, finegrained entity categorization, knowledge base construction, etc.", 
        "2": "Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately.", 
        "3": "Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in the embedding space directly.", 
        "4": "It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules.", 
        "5": "Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1394\u20131404 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1128  1 Introduction  A hypernym of an entity characterizes the type or the class of the entity.", 
        "2": "For example, the word country is the hypernym of the entity Canada.", 
        "3": "The accurate prediction of hypernyms benefits a variety of NLP tasks, such as taxonomy learning (Wu et al., 2012; Fu et al., 2014), fine-grained entity categorization (Ren et al., 2016), knowledge base construction (Suchanek et al., 2007), etc.", 
        "4": "In previous work, the detection of hypernyms requires lexical, syntactic and/or semantic analysis of relations between entities and their respective hypernyms from a language-specific knowledge source.", 
        "5": "For example, Hearst (1992) is the pioneer work to extract is-a relations from a text corpus based on handcraft patterns.", 
        "6": "The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow\n\u2217Corresponding author.", 
        "7": "et al., 2004; Ritter et al., 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al., 2010; Lenci and Benotto, 2012; Shwartz et al., 2016).", 
        "8": "While these approaches have relatively high precision over English corpora, extracting hypernyms for entities is still challenging for Chinese.", 
        "9": "From the linguistic perspective, Chinese is a lower-resourced language with very flexible expressions and grammatical rules (Wang et al., 2015).", 
        "10": "For instance, there are no word spaces, explicit tenses and voices, and distinctions between singular and plural forms in Chinese.", 
        "11": "The order of words can be changed flexibly in sentences.", 
        "12": "Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language (Fu et al., 2014; Wang et al., 2015; Wang and He, 2016).", 
        "13": "Based on such conditions, several classification methods are proposed to distinguish is-a and notis-a relations based on Chinese encyclopedias (Lu et al., 2015; Li et al., 2015).", 
        "14": "Similar to Princeton WordNet, a few Chinese wordnets have also been developed (Huang et al., 2004; Xu et al., 2008; Wang and Bond, 2013).", 
        "15": "The most recent approaches for Chinese is-a relation extraction (Fu et al., 2014; Wang and He, 2016) use word embedding based linear projection models to map embeddings of hyponyms to those of their hypernyms, which outperform previous algorithms.", 
        "16": "However, we argue that these projection-based methods may have three potential limitations: (i) Only positive is-a relations are used for projection learning.", 
        "17": "The distinctions between is-a and not-is-a relations in the embedding space are not modeled.", 
        "18": "(ii) These methods lack the capacity to encode linguistic rules, which are designed by linguists and usually have high precision.", 
        "19": "(iii) It assumes that the linguistic regularities of is-a rela-\n1394\ntions can be solely captured by single or multiple linear projection models.", 
        "20": "In this paper, we address these limitations by a two-stage transductive learning approach.", 
        "21": "It distinguishes is-a and not-is-a relations given a Chinese word/phrase pair as input.", 
        "22": "In the initial stage, we train linear projection models on positive and negative training data separately and predict isa relations jointly.", 
        "23": "In the transductive learning stage, the initial prediction results, linguistic rules and the non-linear mappings from entities to hypernyms are optimized simultaneously in a unified framework.", 
        "24": "This optimization problem can be efficiently solved by blockwise gradient descent.", 
        "25": "We evaluate our method over two public datasets and show that it outperforms state-of-the-art approaches for Chinese hypernym prediction.", 
        "26": "The rest of this paper is organized as follows.", 
        "27": "We summarize the related work in Section 2.", 
        "28": "Our approach is introduced in Section 3.", 
        "29": "Experimental results are presented in Section 4.", 
        "30": "We conclude our paper in Section 5.", 
        "31": "2 Related Work  In this section, we overview the related work on hypernym prediction and discuss the challenges of Chinese hypernym detection.", 
        "32": "Pattern based methods identify is-a relations from texts by handcraft or automatically generated patterns.", 
        "33": "Hearst patterns (Hearst, 1992) are lexical patterns in English that are employed to extract isa relations for taxonomy construction (Wu et al., 2012).", 
        "34": "Automatic approaches mostly use iterative learning paradigms such that the system learns new is-a relations and patterns simultaneously.", 
        "35": "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010).", 
        "36": "To avoid \u201csemantic drift\u201d in iterations, Snow et al.", 
        "37": "(2004) train a hypernym classifier based on syntactic features based on parse trees.", 
        "38": "Carlson et al.", 
        "39": "(2010) exploit multiple learners to extract relations via coupled learning.", 
        "40": "These approaches are not effective for Chinese for two reasons: i) Chinese is-a relations are expressed in a highly flexible manner (Fu et al., 2014) and ii) the accuracy of basic NLP tasks such as dependency parsing still need improvement for Chinese (Li et al., 2013).", 
        "41": "Inference based methods take advantage of distributional similarity measures (DSM) to infer relations between words.", 
        "42": "They assume that a\nhypernym may appear in all contexts of the hyponyms and a hyponym can only appear in part of the contexts of its hypernyms.", 
        "43": "In previous work, Kotlerman et al.", 
        "44": "(2010) design directional DSMs to model the asymmetric property of is-a relations.", 
        "45": "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014).", 
        "46": "Shwartz et al.", 
        "47": "(2016) combine dependency parsing and DSM to improve the performance of hypernymy detection.", 
        "48": "The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexible and sparse.", 
        "49": "Encyclopedia based methods take encyclopedias as knowledge sources to construct taxonomies.", 
        "50": "Ponzetto and Strube (2007) design features from multiple aspects to predict is-a relations between entities and categories in English Wikipedia.", 
        "51": "The taxonomy in YAGO (Suchanek et al., 2007) is constructed by linking conceptual categories in Wikipedia to WordNet synsets (Miller, 1995).", 
        "52": "For Chinese, Li et al.", 
        "53": "(2015) propose an SVM-based approach to build a large Chinese taxonomy from Wikipedia.", 
        "54": "Similar classification based algorithms are presented in (Fu et al., 2013; Lu et al., 2015).", 
        "55": "Due to the lack of Chinese version of WordNet, several Chinese semantic dictionaries have been conducted, such as Sinica BOW (Huang et al., 2004), SEW (Xu et al., 2008), COW (Wang and Bond, 2013), etc.", 
        "56": "These approaches have higher accuracy than mining hypernym relations from texts directly.", 
        "57": "However, they heavily rely on existing knowledge sources and are difficult to extend to different domains.", 
        "58": "To tackle these challenges, word embedding based methods directly model the task of hypernym prediction as learning a mapping from entity vectors to their respective hypernym vectors in the embedding space.", 
        "59": "The vectors can be pretrained by neural language models (Mikolov et al., 2013).", 
        "60": "For the Chinese language, Fu et al.", 
        "61": "(2014) train piecewise linear projection models based on a Chinese thesaurus.", 
        "62": "The state-of-the-art method (Wang and He, 2016) combines an iterative learning procedure and Chinese Hearst-style patterns to improve the performance of projection models.", 
        "63": "They can reduce data noise by avoiding direct parsing of Chinese texts, but still capture the linguistic regularities of is-a relations based on word embeddings.", 
        "64": "Additionally, several work aims to study how to combine word embeddings for re-\nlation classification, such as (Mirza and Tonelli, 2016).", 
        "65": "In our paper, we extend these approaches by modeling non-linear mappings from entities to hypernyms and adding linguistic rules via a unified transductive learning framework.", 
        "66": "3 Proposed Approach  This section begins with a brief overview of our approach.", 
        "67": "After that, the detailed steps and the learning algorithm are introduced in detail.", 
        "68": "3.1 Overview  Given a word/phrase pair (xi, yi), the goal of our task is to learn a classification model to predict whether yi is the hypernym of xi.", 
        "69": "As illustrated in Figure 1, our approach has two stages: initial stage and transductive learning stage.", 
        "70": "The input is a positive is-a set D+, a negative is-a set D\u2212 and an unlabeled set DU , all of which are the collections of word/phrase pairs.", 
        "71": "Denote xi as the embedding vector of word xi, pre-trained and stored in a lookup table.", 
        "72": "In the initial stage, we train a linear projection model over D+ such that for each (xi, yi) \u2208 D+, a projection matrix maps the entity vector xi to its hypernym vector yi.", 
        "73": "A similar model is also trained over D\u2212.", 
        "74": "Based on the two models, we estimate the prediction score and the confidence score for each (xi, yi) \u2208 DU .", 
        "75": "In the transductive learning stage, a joint optimization problem is formed to learn the final prediction score for each (xi, yi) \u2208 DU .", 
        "76": "It aims to minimize the prediction errors based on the human labeled data, the initial model prediction and linguistic rules.", 
        "77": "It also employs nonlinear mappings to capture linguistic regularities of is-a relations other than linear projections.", 
        "78": "3.2 Initial Model Training  The initial stage models how entities are mapped to their hypernyms or non-hypernyms by projection learning.", 
        "79": "We first train a Skip-gram model (Mikolov et al., 2013) to learn word embeddings over a large text corpus.", 
        "80": "Inspired by (Fu et al., 2014; Wang and He, 2016), for each (xi, yi) \u2208 D+, we assume there is a positive projection model such that M+xi \u2248 yi where M+ is an |xi|\u00d7|xi| projection matrix1.", 
        "81": "However, this model does not capture the semantics of not-is-a relations.", 
        "82": "Thus, we learn a negative projection model M\u2212xi \u2248 yi where (xi, yi) \u2208 D\u2212.", 
        "83": "This approach is equivalent to learning two separate translation models within the same semantic space.", 
        "84": "For parameter estimation, we minimize the two following objectives:\nJ(M+) = 1\n2\n\u2211\n(xi,yi)\u2208D+ \u2016M+xi\u2212yi\u201622+\n\u03bb 2 \u2016M+\u20162F\nJ(M\u2212) = 1\n2\n\u2211\n(xi,yi)\u2208D\u2212 \u2016M\u2212xi\u2212yi\u201622+\n\u03bb 2 \u2016M\u2212\u20162F\nwhere \u03bb > 0 is a Tikhonov regularization parameter (Golub et al., 1999).", 
        "85": "In the testing phase, for each (xi, yi) \u2208 DU , denote d+(xi, yi) = \u2016M+xi \u2212 yi\u20162 and d\u2212(xi, yi) = \u2016M\u2212xi\u2212yi\u20162.", 
        "86": "The prediction score is defined as:\nscore(xi, yi) = tanh(d \u2212(xi, yi)\u2212 d+(xi, yi))\nwhere score(xi, yi) \u2208 (\u22121, 1).", 
        "87": "Higher prediction score indicates there is a larger probability of an is-a relation between xi and yi.", 
        "88": "We choose the hyperbolic tangent function rather than the sigmoid function to avoid the widespread saturation of sigmoid function (Menon et al., 1996).", 
        "89": "Because the semantics of Chinese is-a and not-is-a relations are complicated and difficult to model (Fu et al., 2014), we do not impose explicit connections between M+ and M\u2212 and let the algorithm learn the parameters automantically.", 
        "90": "The difference between d+(xi, yi) and d\u2212(xi, yi) can be also used to indicate whether the models are confident enough to make a prediction.", 
        "91": "1We have also examined piecewise linear projection models proposed in (Fu et al., 2014; Wang and He, 2016) as the initial models for transductive learning.", 
        "92": "However, we found that this practice is less efficient and the performance does not improve significantly.", 
        "93": "In this paper, we calculate the confidence score as:\nconf(xi, yi) = |d+(xi, yi)\u2212 d\u2212(xi, yi)|\nmax{d+(xi, yi), d\u2212(xi, yi)}\nwhere conf(xi, yi) \u2208 (0, 1).", 
        "94": "Higher confidence score means that there is a larger probability that the models can predict whether there is an is-a relation between xi and yi correctly.", 
        "95": "This score gives different data instances different weights in the transductive learning stage.", 
        "96": "3.3 Transductive Non-linear Learning  Although linear projection methods are effective for Chinese hypernym prediction, it does not encode non-linear transformation and only leverages the positive data.", 
        "97": "We present an optimization framework for non-linear mapping utilizing both labeled and unlabeled data and linguistic rules by transductive learning (Gammerman et al., 1998; Chapelle et al., 2006).", 
        "98": "Let Fi be the final prediction score of the word/phrase pair (xi, yi).", 
        "99": "In the initialization stage of our algorithm, we set Fi = 1 if (xi, yi) \u2208 D+, Fi = \u22121 if (xi, yi) \u2208 D\u2212 and set Fi randomly in (\u22121, 1) if (xi, yi) \u2208 DU .", 
        "100": "In matrix representation, denote F as the m\u00d7 1 final prediction vector where m = |D+| + |D\u2212| + |DU |.", 
        "101": "Fi is the ith element in F. The three components in our transductive learning model are as follows:  3.3.1 Initial Prediction  Denote S as an m\u00d71 initial prediction vector.", 
        "102": "We set Si = 1 if (xi, yi) \u2208 D+, Si = \u22121 if (xi, yi) \u2208 D\u2212 and Si = score(xi, yi) if (xi, yi) \u2208 DU .", 
        "103": "In order to encode the confidence of model prediction, we define W as an m \u00d7m diagonal weight matrix.", 
        "104": "The element in the ith row and the jth column of W is set as follows:\nWi,j =    conf(xi, yi) i = j, (xi, yi) \u2208 DU 1 i = j, (xi, yi) \u2208 D+ \u222aD\u2212 0 Otherwise\nThe objective function is defined as: Os = \u2016W(F\u2212S)\u201622, which encodes the hypothesis that the final prediction should be similar to the initial prediction for unlabeled data or human labeling for training data.", 
        "105": "The weight matrix W gives the largest weight (i.e., 1) to all the pairs in D+ \u222aD\u2212 and a larger weight to the pair (xi, yi) \u2208 DU if the initial prediction is more confident.", 
        "106": "3.3.2 Linguistic Rules  Although linguistic rules can only cover a few circumstances, they are effective to guide the learning process.", 
        "107": "For Chinese hypernym prediction, Li et al.", 
        "108": "(2015) study the word formation of conceptual categories in Chinese Wikipedia.", 
        "109": "In our model, let C be the collection of linguistic rules.", 
        "110": "\u03b3i is the true positive (or negative) rate with respect to the respective positive (or negative) rule ci \u2208 C, estimated over the training set.", 
        "111": "Considering the word formation of Chinese entities and hypernyms, we design one positive rule (i.e., P1) and two negative rules (i.e., N1 and N2), shown in Table 1.", 
        "112": "Let R be an m \u00d7 1 linguistic rule vector and Ri is the ith element in R. For training data, we set Ri = 1 if (xi, yi) \u2208 D+ and Ri = \u22121 if (xi, yi) \u2208 D\u2212, which follows the same settings as those in S. For unlabeled pairs that do not match any linguistic rules in C, we update Ri = Fi in each iteration of the learning process, meaning no loss for errors imposed in this part.", 
        "113": "For other conditions, denote C(xi,yi) \u2286 C as the collection of rules that (xi, yi) matches.", 
        "114": "If C(xi,yi) are positive rules, we set Ri as follows:\nRi = max{Fi, max cj\u2208C(xi,yi) \u03b3j}\nSimilarly, if C(xi,yi) are negative rules, we have:\nRi = \u2212max{\u2212Fi, max cj\u2208C(xi,yi) \u03b3j}\nwhich means Fi receives a penalty only if Fi < maxcj\u2208C(xi,yi) \u03b3j for pairs that match positive rules or Fi > \u2212maxcj\u2208C(xi,yi) \u03b3j for negative rules2.", 
        "115": "The objective function is: Or = \u2016F\u2212R\u201622.", 
        "116": "In this way, our model can integrate arbitrary \u201csoft\u201d constraints, making it robust to false positives or negatives introduced by these rules.", 
        "117": "3.3.3 Non-linear Learning  TransLP is a transductive label propagation framework (Liu and Yang, 2015) for link prediction, previously used for applications such as text classification (Xu et al., 2016).", 
        "118": "In our work, we extend their work for our task, modeling non-linear mappings from entities to hypernyms.", 
        "119": "2We do not consider the cases where a pair matches both positive and negative rules because such cases are very rare, and even non-existent in our datasets.", 
        "120": "However, our method can deal with these cases by using some simple heuristics.", 
        "121": "For example, we can update Ri using either of the following two ways: i) Ri = Fi and ii) Ri = Fi +\n\u2211 cj\u2208C(xi,yi) \u03b3j .", 
        "122": "For is-a relations, we find that if y is the hypernym of x, it is likely that y is the hypernym of entities that are semantically close to x.", 
        "123": "For example, if we know United States is a country, we can infer country is the hypernym of similar entities such as Canada, Australia, etc.", 
        "124": "This intuition can be encoded in the similarity of the two pairs pi = (xi, yi) and pj = (xj , yj):\nsim(pi, pj) =\n{ cos(xi,xj) yi = yj\n0 otherwise (1)\nwhere xi is the embedding vector of xi3.", 
        "125": "This similarity indicates there exists a nonlinear mapping from entities to hypernyms, which can not be encoded in linear projection based methods (Fu et al., 2014; Wang and He, 2016).", 
        "126": "Based on TransLP (Liu and Yang, 2015), this intuition can be model as propagating class labels (is-a or not-is-a) of labeled word/phrase pairs to similar unlabeled ones based on Eq.", 
        "127": "(1).", 
        "128": "For example, the score of is-a relations between United State and country will propagate to pairs such as (Canada, country) and (Australia, country) by random walks.", 
        "129": "Denote F\u2217 as the optimal solution of the problem min Os + Or.", 
        "130": "Inspired by (Liu and Yang, 2015; Xu et al., 2016), we can add a Gaussian prior N(F\u2217,\u03a3) to F where \u03a3 is the covariance matrix and \u03a3i,j = sim(pi, pj).", 
        "131": "Hence the optimization objective of this part is defined as: On = FT\u03a3\u22121F which is linearly proportional to the negative likelihood of the Gaussian random field prior.", 
        "132": "This means we minimize the training error and encourage F to have a smooth propagation with respect to the similarities among pairs defined by Eq.", 
        "133": "(1) at the same time.", 
        "134": "3We only consider the similarity between entities and not candidate hypernyms because the similar rule for candidate hypernyms is not true.", 
        "135": "For example, nouns close to country in our Skip-gram model are region, department, etc.", 
        "136": "They are not all correct hypernyms of United States, Canada, Australia, etc.", 
        "137": "3.3.4 Joint Optimization  By combining the three components together, we minimize the following function:\nJ(F) = Os + Or + \u00b51 2 On + \u00b52 2 \u2016F\u201622 (2)\nwhere \u2016F\u201622 imposes an additional smooth l2regularization on F. \u00b51 and \u00b52 are regularization parameters that can be tuned manually.", 
        "138": "Based on the convexity of the optimization problem, we can learn the optimal values of F is via gradient descent.", 
        "139": "The derivative of F with respect to J(F) is:\ndJ(F) dF = W2(F\u2212S)+(F\u2212R)+\u00b51\u03a3\u22121F+\u00b52F\nwhich is computationally expensive when m is large.", 
        "140": "After W2, S, R and \u03a3\u22121 are pre-computed, the runtime complexity of the loop of gradient descent is O(tm2) where t is the number of iterations.", 
        "141": "To speed up the learning process, we introduce a blockwise gradient descent technique.", 
        "142": "From the definition of Eq.", 
        "143": "(2), we can see that the optimal values of Fi and Fj with respect to (xi, yi) and (xj , yj) are irrelevant if yi 6= yj .", 
        "144": "Therefore, the original optimization problem can be decomposed and solved separately according to different candidate hypernyms.", 
        "145": "LetH be the collection of candidate hypernyms in DU .", 
        "146": "For each h \u2208 H , denote Dh as the collection of word/phase pairs in D+ \u222a D\u2212 \u222a DU that share the same candidate hypernym h. The original problem can be decomposed into |H| optimization subproblems over Dh for each h \u2208 H .", 
        "147": "Denote Wh, Sh, Rh, Fh and \u03a3h as the weight matrix, the initial prediction vector, the rule prediction vector, the final prediction vector and the entity similarity covariance matrix with respect Dh.", 
        "148": "The objective function can be rewritten as:\nJ(F) = \u2211\nh\u2208H J\u0303(Fh) where\nJ\u0303(Fh) = \u2016Wh(Fh \u2212 Sh)\u201622 + \u2016Fh \u2212Rh\u201622 + \u00b51 2 FTh\u03a3 \u22121 h Fh + \u00b52 2 \u2016Fh\u201622\nWe additionally use (n) to denote the values of matrices or vectors in the nth iteration.", 
        "149": "F(n)h is iteratively updated based on the following equation:\nF (n+1) h = F (n) h \u2212 \u03b7 \u00b7\ndJ\u0303(F(n)h )\ndF(n)h\nwhere \u03b7 is the learning rate.", 
        "150": "To this end, we present the learning algorithm in Algorithm 1.", 
        "151": "Algorithm 1 Learning Algorithm 1: Initialize Wh and Sh based on the initial pre-\ndiction model; 2: Randomly initialize F(0)h ; 3: Compute \u03a3\u22121h based on entity similarities; 4: Initialize counter n = 1; 5: for each linguistic rule ci \u2208 C do 6: Estimate \u03b3i over the training set; 7: end for 8: while \u2016F(n)h \u2212 F (n+1) h \u20162 < 10\u22123 do 9: Compute R(n)h based on C and F (n) h ;\n10: Calculate dJ\u0303(F (n) h )\ndF(n)h = W2h(F (n) h \u2212 Sh) +\n(F (n) h \u2212R (n) h ) + \u00b51\u03a3 \u22121 h F (n) h + \u00b52F (n) h ;\n11: Compute F(n+1)h for the next iteration:\nF (n+1) h = F (n) h \u2212 \u03b7 \u00b7\ndJ\u0303(F(n)h )\ndF(n)h ;\n12: Update counter n = n+ 1; 13: end while 14: return Final prediction vector F(n+1)h ;\nThe runtime complexity of this algorithm is O( \u2211 h\u2208Dh th|Dh|\n2) where th is the number of iterations to solve the subproblem over Dh.", 
        "152": "Although we do not know the upper bounds on the numbers of iterations of these two learning techniques, the runtime complexity can be reduced by blockwise gradient descent for two reasons: i)\u2211\nh\u2208Dh |Dh| \u2264 m and ii) th has a large probability to be smaller than t due to the smaller number of data instances.", 
        "153": "This technique can be also viewed as optimizing Eq.", 
        "154": "(2) based on blockwise matrix computation.", 
        "155": "Finally, for each (xi, yi) \u2208 DU , we predict that yi is a hypernym of xi if Fi > \u03b8 where \u03b8 \u2208 (\u22121, 1) is a threshold tuned on the development set.", 
        "156": "4 Experiments  In this section, we conduct experiments to evaluate our method.", 
        "157": "Section 4.1 to Section 4.5 report the experimental steps on Chinese datasets.", 
        "158": "We present the performance on English datasets in Section 4.6 and a discussion in Section 4.7.", 
        "159": "4.1 Experimental Data  We have two collections of Chinese word/phase pairs as ground truth datasets.", 
        "160": "Each pair is labeled with an is-a or not-is-a tag.", 
        "161": "The first one (denoted as FD) is from Fu et al.", 
        "162": "(2014), containing 1,391 is-a pairs and 4,294 not-is-a pairs, which is the first publicly available dataset to evaluate this task.", 
        "163": "The second one (denoted as BK) is larger in size and crawled from Baidu Baike by ourselves, consisting of <entity, category> pairs.", 
        "164": "For each pair in BK, we ask multiple human annotators to label the tag and discard the pair with inconsistent labels by different annotators.", 
        "165": "In total, it contains 3,870 is-a pairs and 3,582 not-is-a pairs4.", 
        "166": "The Chinese text corpus is extracted from the contents of 1.2M entity pages from Baidu Baike5, a Chinese online encyclopedia.", 
        "167": "It contains approximately 1.1B words.", 
        "168": "We use the open source toolkit Ansj6 for Chinese word segmentation.", 
        "169": "Chinese words/phrases in our test sets may consist of multiple Chinese characters.", 
        "170": "We treat such word/phrase as a whole to learn embeddings, instead of using character-level embeddings.", 
        "171": "In the following experiments, we use 60% of the data for training, 20% for development and 20% for testing, partitioned randomly.", 
        "172": "By rotating the 5-fold subsets of the datasets, we report the performance of each method on average.", 
        "173": "4.2 Parameter Analysis  The word embeddings are pre-trained by ourselves on the Chinese corpus.", 
        "174": "In total, we obtain the 100- dimensional embedding vectors of 5.8M distinct words.", 
        "175": "The regularization parameters are set to \u03bb = 10\u22123 and \u00b51 = \u00b52 = 10\u22124, fine tuned on the development set.", 
        "176": "The choice of \u03b8 reflects the precision-recall trade-off in our model.", 
        "177": "A larger value of \u03b8 means we pay more attention to precision rather than recall.", 
        "178": "Figure 2 illustrates the precision-recall curves\n4https://chywang.github.io/data/acl17.zip 5https://baike.baidu.com/ 6https://github.com/NLPchina/ansj seg/\non both datasets.", 
        "179": "It can be seen that the performance of our method is generally better in BK than FD.", 
        "180": "The most probable cause is that BK is a large dataset with more \u201cbalanced\u201d numbers of positive and negative data.", 
        "181": "Finally, \u03b8 is set to 0.05 on FD and 0.1 on BK.", 
        "182": "4.3 Performance  In a series of previous work (Fu et al., 2013, 2014; Wang and He, 2016), several pattern-based, inference-based and encyclopedia-based is-a relation extraction methods for English have been implemented for the Chinese language.", 
        "183": "As their experiments show, these methods achieve the Fmeasure of lower than 60% in most cases, which are not suggested to be strong baselines for Chinese hypernym prediction.", 
        "184": "Interested readers may refer to their papers for the experimental results.", 
        "185": "To make the convincing conclusion, we employ two recent state-of-the-art approaches for Chinese is-a relation identification (Fu et al., 2014; Wang and He, 2016) as baselines.", 
        "186": "We also take the word embedding based classification approach (Mirza and Tonelli, 2016)7 and Chinese Wikipedia based\n7Although the experiments in their paper are mostly related to temporal relations, the method can be applied to is-a\nSVM model (Li et al., 2015) as baselines to predict is-a relations between words8.", 
        "187": "The experimental results are illustrated in Table 2.", 
        "188": "For Fu et al.", 
        "189": "(2014), we test the performance using a linear projection model (denoted as S in Table 2) and piecewise projection models (P).", 
        "190": "It shows that the semantics of is-a relations are better modeled by multiple projection models, with a slight improvement in F-measure.", 
        "191": "By combining iterative projection models and pattern-based validation, the most recent approach (Wang and He, 2016) increases the F-measure by 4% and 2% in two datasets.", 
        "192": "In this method, the patternbased statistics are calculated using the same corpus over which we train word embedding models.", 
        "193": "The main reason of the improvement may be that the projection models have a better generalization power by applying an iterative learning paradigm.", 
        "194": "Mirza and Tonelli (2016) is implemented using three different strategies in combining the word vectors of a pair: i) concatenation xi \u2295 yi (derelations without modification.", 
        "195": "8Previously, these methods used different knowledge sources to train models and thus the results in their papers are not directly comparable with ours.", 
        "196": "To make fair comparison, we take the training data as the same knowledge source to train models for all methods.", 
        "197": "noted as C), ii) addition xi + yi (A) and iii) subtraction xi \u2212 yi (S).", 
        "198": "As seen, the classification models using addition and subtraction have similar performance in two datasets, while the concatenation strategy outperforms previous two approaches.", 
        "199": "Although Li et al.", 
        "200": "(2015) achieve a high performance in their dataset, this method does not perform well in ours.", 
        "201": "The most likely cause is that the features in that work are designed specifically for the Chinese Wikipedia category system.", 
        "202": "Our initial model has a higher accuracy than all the baselines.", 
        "203": "By utilizing the transductive learning framework, we boost the F-measure by 1.7% and 2.1%, respectively.", 
        "204": "Therefore, our method is effective to predict hypernyms of Chinese entities.", 
        "205": "We further conduct statistical tests which show our method significantly (p < 0.01) improves the Fmeasure over the state-of-the-art method (Wang and He, 2016).", 
        "206": "4.4 Effectiveness of Linguistic Rules  To illustrate the effectiveness of linguistic rules, we present the true positive (or negative) rate by using one positive (or negative) rule solely, shown in Table 4.", 
        "207": "These values serve as \u03b3is in the transductive learning stage.", 
        "208": "The results indicate that these rules have high precision (over 90%) over both datasets for our task.", 
        "209": "We state that currently we only use a few handcraft linguistic rules in our work.", 
        "210": "The proposed approach is a general framework that can encode arbitrary numbers of rules and in any language.", 
        "211": "4.5 Error Analysis and Case Studies  We analyze correct and error cases in the experiments.", 
        "212": "Some examples of prediction results are shown in Table 3.", 
        "213": "We can see that our method is generally effective.", 
        "214": "However, some mistakes occur mostly because it is difficult to distinguish strict is-a and topic-of relations.", 
        "215": "For example, the entity Nuclear Reactor is semantically close to Nuclear Energy.", 
        "216": "The error statistics show that such kind of errors account for approximately 80.2% and 78.6% in two test sets, respectively.", 
        "217": "Based on the literature study, we find that such problem has been also reported in (Fu et al., 2013; Wang and He, 2016).", 
        "218": "To reduce such errors, we employ the Chinese thematic lexicon based on Li et al.", 
        "219": "(2015) in the transductive learning stage but the coverage is still limited.", 
        "220": "Two possible solutions are: i) adding more negative training data of this kind; and ii) constructing a large-scale thematic lexicon automatically from the Web.", 
        "221": "4.6 Experiments on English Datasets  To examine how our method can benefit hypernym prediction for the English language, we use two standard datasets in this paper.", 
        "222": "The first one is a benchmark dataset for distributional semantic evaluation, i.e., BLESS (Baroni and Lenci, 2011).", 
        "223": "Because the number of pairs in BLESS is relatively small, we also use the Shwartz (Shwartz et al., 2016) dataset.", 
        "224": "In the experiments, we treat the HYPER relations as positive data (1,337 pairs) and randomly sample 30% of the RANDOM relations as negative data (3,754 pairs) in BLESS.", 
        "225": "To create a relatively balanced dataset, we take the random split of Shwartz as input and use only 30% of the negative pairs.", 
        "226": "The dataset contains 14,135 positive pairs and 16,956 negative pairs.", 
        "227": "We use English Wikipedia as the text corpus to estimate the\nstatistics, and the pre-trained embedding vectors of English words9.", 
        "228": "For comparison, we test all the baselines over English datasets except Li et al.", 
        "229": "(2015).", 
        "230": "This is because most features in Li et al.", 
        "231": "(2015) can only be used in the Chinese environment.", 
        "232": "To implement Wang and He (2016) for English, we use the original Hearst patterns (Hearst, 1992) to perform relation selection and do not consider not-is-a patterns.", 
        "233": "We also take two recent DSM based approaches (Lenci and Benotto, 2012; Santus et al., 2014) as baselines.", 
        "234": "As for our own method, we do not use linguistic rules in Table 1 for English.", 
        "235": "The results are illustrated in Table 5.", 
        "236": "As seen, our method is superior to all the baselines over BLESS, with an F-measure of 81.9%.", 
        "237": "In Shwartz, while the approach (Mirza and Tonelli, 2016) has the highest F-measure of 80.1%, our method is generally comparable to theirs and outperforms others.", 
        "238": "The results suggest that although our method is not necessarily the state-of-the-art for English hypernym prediction, it has several potential applications.", 
        "239": "Refer to Section 4.7 for discussion.", 
        "240": "4.7 Discussion  From the experiments, we can see that the proposed approach outperforms the state-of-the-art methods for Chinese hypernym prediction.", 
        "241": "Although the English language is not our focus, our approach still has relatively high performance.", 
        "242": "Additionally, our work has potential values for the following applications:\n\u2022 Domain-specific or Context-sparse Relation Extraction.", 
        "243": "If the task is to predict re-\n9http://nlp.stanford.edu/projects/glove/\nlations between words when it is related to a specific domain or the contexts are sparse, even for English, traditional pattern-based methods are likely to fail.", 
        "244": "Our method can predict the existence of relations without explicit textual patterns and requires a relatively small amount of pairs as training data.", 
        "245": "\u2022 Under-resourced Language Learning.", 
        "246": "Our method can be adapted for relation extraction in languages with flexible expressions, few knowledge resources and/or lowperformance NLP tools.", 
        "247": "Our method does not require deep NLP parsing of sentences in a text corpus and thus the performance is not affected by parsing errors.", 
        "248": "5 Conclusion  In summary, this paper introduces a transuctive learning approach for Chinese hypernym prediction.", 
        "249": "By modeling linear projection models, linguistic rules and non-linear mappings, our method is able to identify Chinese hypernyms with high accuracy.", 
        "250": "Experiments show that the performance of our method outperforms previous approaches.", 
        "251": "We also discuss the potential applications of our method besides Chinese hypernym prediction.", 
        "252": "In our work, the candidate Chinese hyponyms and hypernyms are extracted from user generated categories.", 
        "253": "In the future, we will study how to construct a taxonomy from texts in Chinese.", 
        "254": "Acknowledgements  This work is supported by the National Key Research and Development Program of China under Grant No.", 
        "255": "2016YFB1000904."
    }, 
    "document_id": "P17-1128.pdf.json"
}
