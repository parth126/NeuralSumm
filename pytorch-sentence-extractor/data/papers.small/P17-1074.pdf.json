{
    "abstract_sentences": {
        "1": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated.", 
        "2": "To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rulebased framework.", 
        "3": "This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets.", 
        "4": "Human experts rated the automatic edits as \u201cGood\u201d or \u201cAcceptable\u201d in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 793\u2013805 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1074  1 Introduction  Grammatical Error Correction (GEC) systems are often only evaluated in terms of overall performance because system hypotheses are not annotated.", 
        "2": "This can be misleading however, and a system that performs poorly overall may in fact outperform others at specific error types.", 
        "3": "This is significant because a robust specialised system is actually more desirable than a mediocre general system.", 
        "4": "Without an error type analysis however, this information is completely unknown.", 
        "5": "The main aim of this paper is hence to rectify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information.", 
        "6": "This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback\nto non-native learners.", 
        "7": "Given that different corpora are also annotated according to different standards, we also attempted to standardise existing datasets under a common error type framework.", 
        "8": "Our approach consists of two main steps.", 
        "9": "First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016) and second, we classify them according to a new, rule-based framework that relies solely on dataset-agnostic information such as lemma and part-of-speech.", 
        "10": "We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT)1, by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical error correction (Ng et al., 2014).", 
        "11": "It is worth mentioning that despite an increased interest in GEC evaluation in recent years (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015; Grundkiewicz et al., 2015; Sakaguchi et al., 2016), ERRANT is the only toolkit currently capable of producing error types scores.", 
        "12": "2 Edit Extraction  The first stage of automatic annotation is edit extraction.", 
        "13": "Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits.", 
        "14": "This is fundamentally an alignment problem:\n1https://github.com/chrisjbryant/errant\n793\nThe first attempt at automatic edit extraction was made by Swanson and Yamangil (2012), who simply used the Levenshtein distance to align parallel original and corrected sentences.", 
        "15": "As the Levenshtein distance only aligns individual tokens however, they also merged all adjacent nonmatches in an effort to capture multi-token edits.", 
        "16": "Xue and Hwa (2014) subsequently improved on Swanson and Yamangil\u2019s work by training a maximum entropy classifier to predict whether edits should be merged or not.", 
        "17": "Most recently, Felice et al.", 
        "18": "(2016) proposed a new method of edit extraction using a linguistically-enhanced alignment algorithm supported by a set of merging rules.", 
        "19": "More specifically, they incorporated various linguistic information, such as part-of-speech and lemma, into the cost function of the Damerau-Levenshtein2 algorithm to make it more likely that tokens with similar linguistic properties aligned.", 
        "20": "This approach ultimately proved most effective at approximating human edits in several datasets (80-85% F1), and so we use it in the present study.", 
        "21": "3 Automatic Error Typing  Having extracted the edits, the next step is to assign them error types.", 
        "22": "While Swanson and Yamangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this approach is that such classifiers are biased towards their particular training corpora.", 
        "23": "For example, a classifier trained on the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) is unlikely to perform as well on the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier and Ng, 2012) or vice versa, because both corpora have been annotated according to different standards (cf.", 
        "24": "Xue and Hwa (2014)).", 
        "25": "Instead, a dataset-agnostic error type classifier is much more desirable.", 
        "26": "3.1 A Rule-Based Error Type Framework  To solve this problem, we took inspiration from Swanson and Yamangil\u2019s (2012) observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags.", 
        "27": "We then added another rule to similarly differentiate between Missing, Unnecessary and Replace-\n2Damerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g.", 
        "28": "AB\u2192BA\nment errors depending on whether tokens were inserted, deleted or substituted.", 
        "29": "Finally, we extended our approach to classify errors that are not well-characterised by POS, such as Spelling or Word Order, and ultimately assigned all error types based solely on automatically-obtained, objective properties of the data.", 
        "30": "In total, we wrote roughly 50 rules.", 
        "31": "While many of them are very straightforward, significant attention was paid to discriminating between different kinds of verb errors.", 
        "32": "For example, despite all having the same correction, the following sentences contain different types of common learner errors:\n(a) He IS asleep now.", 
        "33": "[IS\u2192 is]: orthography (b) He iss asleep now.", 
        "34": "[iss\u2192 is]: spelling (c) He has asleep now.", 
        "35": "[has\u2192 is]: verb (d) He being asleep now.", 
        "36": "[being\u2192 is]: form (e) He was asleep now.", 
        "37": "[was\u2192 is]: tense (f) He are asleep now.", 
        "38": "[are\u2192 is]: SVA\nTo handle these cases, we hence wrote the following ordered rules:\n1.", 
        "39": "Are the lower case forms of both sides of the edit the same?", 
        "40": "(a)\n2.", 
        "41": "Is the original token a real word?", 
        "42": "(b)\n3.", 
        "43": "Do both sides of the edit have the same lemma?", 
        "44": "(c)\n4.", 
        "45": "Is one side of the edit a gerund (VBG) or participle (VBN)?", 
        "46": "(d)\n5.", 
        "47": "Is one side of the edit in the past tense (VBD)?", 
        "48": "(e)\n6.", 
        "49": "Is one side of the edit in the 3rd person present tense (VBZ)?", 
        "50": "(f)\nWhile the final three rules could certainly be reordered, we informally found the above sequence performed best during development.", 
        "51": "It is also worth mentioning that this is a somewhat simplified example and that there are additional rules to discriminate between auxiliary verbs, main verbs and multi verb expressions.", 
        "52": "Nevertheless, the above case exemplifies our approach, and a more complete description of all rules is provided with the software.", 
        "53": "3.2 A Dataset-Agnostic Classifier  One of the key strengths of a rule-based approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset independent and does not require labelled training data.", 
        "54": "This is in contrast with machine learning approaches which not only learn dataset specific biases, but also presuppose the existence of sufficient quantities of training data.", 
        "55": "A second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned a particular error category.", 
        "56": "In contrast, human and machine learning classification decisions are often much less transparent.", 
        "57": "Finally, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent.", 
        "58": "3.3 Automatic Markup  The prerequisites for our rule-based classifier are that each token in both the original and corrected\nsentence is POS tagged, lemmatized, stemmed and dependency parsed.", 
        "59": "We use spaCy3 v1.7.3 for all but the stemming, which is performed by the Lancaster Stemmer in NLTK.4 Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy\u2019s Penn Treebank style tags to the coarser set of Universal Dependency tags.5 We use the latest Hunspell GB-large word list6 to help classify non-word errors.", 
        "60": "The marked-up tokens in an edit span are then input to the classifier and an error type is returned.", 
        "61": "3.4 Error Categories  The complete list of 25 error types in our new framework is shown in Table 2.", 
        "62": "Note that most of them can be prefixed with \u2018M:\u2019, \u2018R:\u2019 or \u2018U:\u2019, depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable\n3https://spacy.io/ 4http://www.nltk.org/ 5http://universaldependencies.org/tagset-conversion/\nen-penn-uposf.html 6https://sourceforge.net/projects/wordlist/files/speller/ 2017.01.22/\nevaluation at different levels of granularity (see Appendix A for all valid combinations).", 
        "63": "This means we can choose to evaluate, for example, only replacement errors (anything prefixed by \u2018R:\u2019), only noun errors (anything suffixed with \u2018NOUN\u2019) or only replacement noun errors (\u2018R:NOUN\u2019).", 
        "64": "This flexibility allows us to make more detailed observations about different aspects of system performance.", 
        "65": "One caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could\u2192 should] a tense error, when it might otherwise be considered a modal error.", 
        "66": "The reason we do not call it a modal error, however, is because it would then become less clear how to handle other cases such as [can \u2192 should] and [has eaten \u2192 should eat], which might be considered a more complex combination of modal and tense error.", 
        "67": "As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality.", 
        "68": "3.5 Classifier Evaluation  As our new error scheme is based solely on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance.", 
        "69": "For this reason, we instead carried out a small-scale manual evaluation, where we simply asked 5 GEC researchers to rate the appropriateness of the predicted error types for 200 randomly chosen edits in context (100 from FCE-test and 100 from CoNLL-2014) as \u201cGood\u201d, \u201cAcceptable\u201d or \u201cBad\u201d.", 
        "70": "\u201cGood\u2019 meant the chosen type was the most appropriate for the given edit, \u201cAcceptable\u201d meant the chosen type was appropriate, but probably not optimum, while \u201cBad\u201d meant the chosen type was not appropriate for the edit.", 
        "71": "Raters were warned that the edit boundaries had been determined automatically and hence might be unusual, but that they should focus on the appropriateness of the error type regardless of whether they agreed with the boundary or not.", 
        "72": "It is worth stating that the main purpose of this evaluation was not to evaluate the specific strengths and weaknesses of the classifier, but rather ascertain how well humans believed the predicted error types characterised each edit.", 
        "73": "GEC is known to be a highly subjective task (Bryant and\nNg, 2015) and so we were more interested in overall judgements than specific disagreements.", 
        "74": "The results from this evaluation are shown in Table 3.", 
        "75": "Significantly, all 5 raters considered at least 95% of the predicted error types to be either \u201cGood\u201d or \u201cAcceptable\u201d, despite the degree of noise introduced by automatic edit extraction.", 
        "76": "Furthermore, whenever raters judged an edit as \u201cBad\u201d, this could usually be traced back to a POS or parse error; e.g.", 
        "77": "[ring \u2192 rings] might be considered a NOUN:NUM or VERB:SVA error depending on whether the POS tagger considered both sides of the edit nouns or verbs.", 
        "78": "Interannotator agreement was also good at 0.724 \u03bafree (Randolph, 2005).", 
        "79": "In contrast, although incomparable on account of the different metric and error scheme, the best results using machine learning were between 50- 70% F1 (Felice et al., 2016).", 
        "80": "Ultimately however, we believe the high scores awarded by the raters validates the efficacy of our rule-based approach.", 
        "81": "4 Error Type Scoring  Having described how to automatically annotate parallel sentences with ERRANT, we now also have a method to annotate system hypotheses; this is the first step towards an error type evaluation.", 
        "82": "Since no scorer is currently capable of calculating error type performance however (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015), we instead built our own.", 
        "83": "Fortunately, one benefit of explicitly annotating system hypotheses is that it makes evaluation much more straightforward.", 
        "84": "In particular, for each sentence, we only need to compare the edits in the hypothesis against the edits in each respective reference and measure the overlap.", 
        "85": "Any edit with the same span and correction in both files is hence a\ntrue positive (TP), while unmatched edits in the hypothesis and references are false positives (FP) and false negatives (FN) respectively.", 
        "86": "These results can then be grouped by error type for the purposes of error type evaluation.", 
        "87": "Finally, it is worth noting that this scorer is much simpler than other scorers in GEC which typically incorporate edit extraction or alignment directly into their algorithms.", 
        "88": "Our approach, on the other hand, treats edit extraction and evaluation as separate tasks.", 
        "89": "4.1 Gold Reference vs. Auto Reference  Before evaluating an automatically annotated hypothesis against its reference, we must also address another mismatch: namely that hypothesis edits must be extracted and classified automatically, while reference edits are typically extracted and classified manually using a different framework.", 
        "90": "Since evaluation is now reduced to a straightforward comparison between two files however, it is especially important that the hypothesis and references are both processed in the same way.", 
        "91": "For instance, a hypothesis edit [have eating \u2192 has eaten] will not match the reference edits [have \u2192 has] and [eating \u2192 eaten] because the former is one edit while the latter is two edits, even though they equate to the same thing.", 
        "92": "To solve this problem, we can reprocess the references in the same way as the hypotheses.", 
        "93": "In other words, we can apply ERRANT to the references such that each reference edit is subject to the same automatic extraction and classification criteria as each hypothesis edit.", 
        "94": "While it may seem unorthodox to discard gold reference information in favour of automatic reference information, this is necessary to minimise the difference between hypothesis and reference edits and also standardise error type annotations.", 
        "95": "To show that automatic references are feasible alternatives to gold references, we evaluated each team in the CoNLL-2014 shared task using both types of reference with the M2 scorer (Dahlmeier and Ng, 2012), the de facto standard of GEC evaluation, and our own scorer.", 
        "96": "Table 4 hence shows that there is little difference between the overall scores for each team, and we formally validated this hypothesis for precision, recall and F0.5 by means of bootstrap significance testing (Efron and Tibshirani, 1993).", 
        "97": "Ultimately, we found no statistically significant difference\nbetween automatic and gold references (1,000 iterations, p > .05) which leads us to conclude that our automatic references are qualitatively as good as human references.", 
        "98": "4.2 Comparison with the M2 Scorer  Despite using the same metric, Table 4 also shows that the M2 scorer tends to produce slightly higher F0.5 scores than our own.", 
        "99": "This initially led us to believe that our scorer was underestimating performance, but we subsequently found that instead the M2 scorer tends to overestimate performance (cf.", 
        "100": "Felice and Briscoe (2015) and Napoles et al.", 
        "101": "(2015)).", 
        "102": "In particular, given a choice between matching [have eating \u2192 has eaten] from Annotator 1 or [have \u2192 has] and [eating \u2192 eaten] from Annotator 2, the M2 scorer will always choose Annotator 2 because two true positives (TP) are worth more than one.", 
        "103": "Similarly, whenever the scorer encounters two false positives (FP) within a certain distance of each other,7 it merges them and treats them as one false positive; e.g.", 
        "104": "[is a cat \u2192 are a cats] is selected over [is\u2192 are] and [cat \u2192 cats] even though these edits are best handled separately.", 
        "105": "In other words, the M2 scorer exploits its dynamic edit boundary prediction to artificially maximise true positives and minimise false positives and hence produce slightly inflated scores.", 
        "106": "7The distance is controlled by the max unchanged words parameter which is set to 2 by default.", 
        "107": "5 CoNLL-2014 Shared Task Analysis  To demonstrate the value of ERRANT, we applied it to the data produced in the CoNLL-2014 shared task (Ng et al., 2014).", 
        "108": "Specifically, we automatically annotated all the system hypotheses and official reference files.8 Although ERRANT can be applied to any dataset of parallel sentences, we chose to evaluate on CoNLL-2014 because it represents the largest collection of publicly available GEC system output.", 
        "109": "For more information about the systems in CoNLL-2014, we refer the reader to the shared task paper.", 
        "110": "5.1 Edit Operation  In our first category experiment, we simply investigated the performance of each system in terms of Missing, Replacement and Unnecessary edits.", 
        "111": "The results are shown in Table 5 with additional information in Appendix B, Table 10.", 
        "112": "The most surprising result is that five teams (AMU, IPN, PKU, RAC, UFC) failed to correct any unnecessary token errors at all.", 
        "113": "This is noteworthy because unnecessary token errors account for roughly 25% of all errors in the CoNLL-2014 test data and so failing to address them significantly limits a system\u2019s maximum performance.", 
        "114": "While the reason for this is clear in some cases, e.g.", 
        "115": "UFC\u2019s rule-based system was never designed to tackle unnecessary tokens (Gupta, 2014), it is less clear in others, e.g.", 
        "116": "there is no obvious reason why AMU\u2019s SMT system failed to learn when\n8http://www.comp.nus.edu.sg/\u223cnlp/conll14st.html\nto delete tokens (Junczys-Dowmunt and Grundkiewicz, 2014).", 
        "117": "AMU\u2019s result is especially remarkable given that their system still came 3rd overall despite this limitation.", 
        "118": "In contrast, CUUI\u2019s classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB\u2019s hybrid MT approach (Felice et al., 2014) significantly outperformed all others in terms of missing token errors.", 
        "119": "It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016).", 
        "120": "5.2 General Error Types  Table 6 shows precision, recall and F0.5 for each of the error types in our proposed framework for each team in CoNLL-2014.", 
        "121": "As some error types are more common than others, we also provide the TP, FP and FN counts used to make this table in Appendix B, Table 11.", 
        "122": "Overall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU, who scored highest in 6 categories.", 
        "123": "All but 3 teams (IITB, IPN and POST) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types.", 
        "124": "Only CAMB attempted to correct at least 1 error from every category.", 
        "125": "Other interesting observations we can make from this table include:\n\u2022 Despite the prevalence of spell checkers nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance.", 
        "126": "\u2022 Although several teams built specialised classifiers for DET and PREP errors, CAMB\u2019s hybrid MT approach still outperformed them.", 
        "127": "This might be because the classifiers were trained using a different error type framework however.", 
        "128": "\u2022 CUUI\u2019s classifiers significantly outperformed all other approaches at ORTH and VERB:FORM errors.", 
        "129": "This suggests classifiers are well-suited to these error types.", 
        "130": "\u2022 Although UFC\u2019s rule-based approach was the best at VERB:SVA errors, CUUI\u2019s classifier was not very far behind.", 
        "131": "\u2022 Only AMU managed to correct any CONJ errors.", 
        "132": "\u2022 Content word errors (i.e.", 
        "133": "ADJ, ADV, NOUN and VERB) were unsurprisingly very difficult for all teams.", 
        "134": "5.3 Detailed Error Types  In addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail.", 
        "135": "For example, Table 7 shows the breakdown of Determiner errors for two teams using different approaches in terms of edit operation.", 
        "136": "Note that this is a representative example of detailed error type performance, as an analysis of all error type combinations for all teams would take up too much space.", 
        "137": "While CAMB\u2019s hybrid MT approach achieved a higher score than CUUI\u2019s classifier overall, our more detailed evaluation reveals that CUUI actually outperformed CAMB at Replacement Determiner errors.", 
        "138": "We also learn that CAMB scored twice as highly on M:DET and U:DET than it did on R:DET and that CUUI\u2019s significantly higher U:DET recall was offset by a lower precision.", 
        "139": "Ultimately, this shows that even though one approach might be better than another overall, different approaches may still have complementary strengths.", 
        "140": "5.4 Multi Token Errors  Another benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size.", 
        "141": "Table 8 hence shows the overall performance for each team at correcting multitoken edits, where a multi-token edit is an edit that has at least two tokens on either side.", 
        "142": "In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).", 
        "143": "In general, teams did not do well at multi-token edits.", 
        "144": "In fact only three teams achieved scores greater than 10% F0.5 and all of them used MT (AMU, CAMB, UMC).", 
        "145": "This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016).", 
        "146": "If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available.", 
        "147": "5.5 Detection vs.", 
        "148": "Correction  Another important aspect of GEC that is seldom reported in the literature is that of error detection; i.e.", 
        "149": "the extent to which a system can identify erroneous tokens in text.", 
        "150": "This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed correction in a manner similar to Recognition evaluation in the HOO shared tasks for GEC (Dale and Kilgarriff, 2011).", 
        "151": "Figure 1 hence shows how each team\u2019s score for detection differed in relation to their score for correction.", 
        "152": "While CAMB scored highest for detection overall, it is interesting to note that CUUI ultimately performed slightly better than CAMB at correction.", 
        "153": "This suggests CUUI was more successful at correcting the errors they detected than CAMB.", 
        "154": "In contrast, IPN and PKU are notable for detecting significantly more errors than they were able to correct.", 
        "155": "Nevertheless, a system\u2019s ability to detect errors, even if it is unable to correct them, is still likely to be valuable information to a learner (Rei and Yannakoudakis, 2016).", 
        "156": "Finally, although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection.", 
        "157": "6 Conclusion  In this paper, we described ERRANT, a grammatical ERRor ANnotation Toolkit designed to au-\ntomatically annotate parallel error correction data with explicit edit spans and error type information.", 
        "158": "ERRANT can be used to not only facilitate a detailed error type evaluation in GEC, but also to standardise existing error correction corpora and reduce annotator workload.", 
        "159": "We release ERRANT with this paper.", 
        "160": "Our approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits.", 
        "161": "This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas.", 
        "162": "A small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either \u201cGood\u201d (85%) or \u201cAcceptable\u201d (10%).", 
        "163": "We demonstrated the value of ERRANT by carrying out a detailed evaluation of system error type performance for all teams in the CoNLL2014 shared task on Grammatical Error Correction.", 
        "164": "We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance.", 
        "165": "B TP, FP and FN counts for various CoNLL-2014 results    A Complete list of valid error code combinations"
    }, 
    "document_id": "P17-1074.pdf.json"
}
