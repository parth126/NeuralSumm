{
    "abstract_sentences": {
        "1": "A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form.", 
        "2": "In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets.", 
        "3": "We find that we can substantially improve parsing accuracy by training a single sequence-tosequence model over multiple KBs, when providing an encoding of the domain at decoding time.", 
        "4": "Our model achieves state-ofthe-art performance on the OVERNIGHT dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 623\u2013628 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2098  1 Introduction  Semantic parsing is concerned with translating language utterances into executable logical forms and constitutes a key technology for developing conversational interfaces (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant and Liang, 2015).", 
        "2": "A fundamental obstacle to widespread use of semantic parsers is the high cost of annotating logical forms in new domains.", 
        "3": "To tackle this problem, prior work suggested strategies such as training from denotations (Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013), from paraphrases (Berant and Liang, 2014; Wang et al., 2015) and from declarative sentences (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014).", 
        "4": "In this paper, we suggest an orthogonal solution: to pool examples from multiple datasets in different domains, each corresponding to a separate knowledge-base (KB), and train a model over all examples.", 
        "5": "This is motivated by an observation that while KBs differ in their entities and properties, the structure of language composition repeats across domains (Figure 1).", 
        "6": "E.g., a superlative in language will correspond to an \u2018argmax\u2019, and a verb followed by a noun often denotes a join operation.", 
        "7": "A model that shares information across domains can improve generalization compared to a model that is trained on a single domain only.", 
        "8": "Recently, Jia and Liang (2016) and Dong and Lapata (2016) proposed sequence-to-sequence models for semantic parsing.", 
        "9": "Such neural models substantially facilitate information sharing, as both language and logical form are represented with similar abstract vector representations in all domains.", 
        "10": "We build on their work and examine models that share representations across domains during encoding of language and decoding of logical form, inspired by work on domain adaptation\n623\n(Daume III, 2007) and multi-task learning (Caruana, 1997; Collobert et al., 2011; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2016).", 
        "11": "We find that by providing the decoder with a representation of the domain, we can train a single model over multiple domains and substantially improve accuracy compared to models trained on each domain separately.", 
        "12": "On the OVERNIGHT dataset, this improves accuracy from 75.6% to 79.6%, setting a new state-of-the-art, while reducing the number of parameters by a factor of 7.", 
        "13": "To our knowledge, this work is the first to train a semantic parser over multiple KBs.", 
        "14": "2 Problem Setup  We briefly review the model presented by Jia and Liang (2016), which we base our model on.", 
        "15": "Semantic parsing can be viewed as a sequenceto-sequence problem (Sutskever et al., 2014), where a sequence of input language tokens x = x1, .", 
        "16": ".", 
        "17": ".", 
        "18": ", xm is mapped to a sequence of output logical tokens y1, .", 
        "19": ".", 
        "20": ".", 
        "21": ", yn .", 
        "22": "The encoder converts x1, .", 
        "23": ".", 
        "24": ".", 
        "25": ", xm into a sequence of context sensitive embeddings b1, .", 
        "26": ".", 
        "27": ".", 
        "28": ", bm using a bidirectional RNN (Bahdanau et al., 2015): a forward RNN generates hidden states hF1 , .", 
        "29": ".", 
        "30": ".", 
        "31": ", h F m by applying the LSTM recurrence (Hochreiter and Schmidhuber, 1997): hFi = LSTM(\u03c6 (in)(xi), h F i\u22121), where \u03c6\n(in) is an embedding function mapping a word xi to a fixed-dimensional vector.", 
        "32": "A backward RNN similarly generates hidden states hBm, .", 
        "33": ".", 
        "34": ".", 
        "35": ", h B 1 by processing the input sequence in reverse.", 
        "36": "Finally, for each input position i, the representation bi is the concatenation [hFi , h B i ] .", 
        "37": "An attention-based decoder (Bahdanau et al., 2015; Luong et al., 2015) generates output tokens one at a time.", 
        "38": "At each time step j, it generates yj based on the current hidden state sj , then updates the hidden state sj+1 based on sj and yj .", 
        "39": "Formally, the decoder is defined by the following equations:\ns1 = tanh(W (s)[hFm, h B 1 ]),\neji = s > j W (a)bi,\n\u03b1ji = exp(eji)\u2211m i\u2032=1 eji\u2032 ,\ncj =\nm\u2211\ni=1\n\u03b1jibi,\np(yj = w | x, y1:j\u22121) \u221d exp(U [sj , cj ]), sj+1 = LSTM([\u03c6 (out)(yj), cj ], sj),\n(1)\nwhere i \u2208 {1, .", 
        "40": ".", 
        "41": ".", 
        "42": ",m} and j \u2208 {1, .", 
        "43": ".", 
        "44": ".", 
        "45": ", n}.", 
        "46": "The matrices W (s), W (a), U , and the embedding function \u03c6(out) are decoder parameters.", 
        "47": "We also employ attention-based copying as described by Jia and Liang (2016), but omit details for brevity.", 
        "48": "The entire model is trained end-to-end by maximizing p(y | x) = \u220fnj=1 p(yj | x, y1:j\u22121).", 
        "49": "3 Models over Multiple KBs  In this paper, we focus on a setting where we have access to K training sets from different domains, and each domain corresponds to a different KB.", 
        "50": "In all domains the input is a language utterance and the label is a logical form (we assume annotated logical forms can be converted to a single formal language such as lambda-DCS in Figure 1).", 
        "51": "While the mapping from words to KB constants is specific to each domain, we expect that the manner in which language expresses composition of meaning to be shared across domains.", 
        "52": "We now describe architectures that share information between the encoders and decoders of different domains.", 
        "53": "3.1 One-to-one model  This model is similar to the baseline model described in Section 2.", 
        "54": "As illustrated in Figure 2, it consists of a single encoder and a single decoder, which are used to generate outputs for all domains.", 
        "55": "Thus, all model parameters are shared across domains, and the model is trained from all examples.", 
        "56": "Note that the number of parameters does not depend on the number of domains K.\nSince there is no explicit representation of the domain that is being decoded, the model must learn to identify the domain given only the input.", 
        "57": "To alleviate that, we encode the k\u2019th domain by a one-hot vector dk \u2208 RK .", 
        "58": "At each step, the decoder updates the hidden state conditioned on the domain\u2019s one-hot vector, as well as on the previous hidden state, the output token and the context.", 
        "59": "Formally, for domain k, Equation 1 is changed:1\nsj+1 = LSTM([\u03c6 (out)(yj), cj , dk], sj).", 
        "60": "(2)\nRecently Johnson et al.", 
        "61": "(2016) used a similar intuition for neural machine translation, where they added an artificial token at the beginning of each source sentence to specify the target language.", 
        "62": "We implemented their approach and compare to it in Section 4.", 
        "63": "1For simplicity, we omit the domain index k from our notation whenever it can be inferred from context.", 
        "64": "Since we have one decoder for multiple domains, tokens which are not in the domain vocabulary could possibly be generated.", 
        "65": "We prevent that at test time by excluding out-of-domain tokens before the softmax (p(yj | x, y1:j\u22121)) takes place.", 
        "66": "3.2 Many-to-many model  In this model, we keep a separate encoder and decoder for every domain, but augment the model with an additional encoder that consumes examples from all domains (see Figure 2).", 
        "67": "This is motivated by prior work on domain adaptation (Daume III, 2007; Blitzer et al., 2011), where each example has a representation that captures domain-specific aspects of the example and a representation that captures domain-general aspects.", 
        "68": "In our case, this is achieved by encoding examples with a domainspecific encoder as well as a domain-general encoder, and passing both representations to the decoder.", 
        "69": "Formally, we now have K + 1 encoders and K decoders, and denote by hF,ki , h B,k i , b k i the forward state, backward state and their concatenation at position i (the domain-general encoder has index K + 1).", 
        "70": "The hidden state of the decoder in domain k is initialized from the domain-specific and domain-general encoder:\ns1 = tanh(W (s)[hF,km , h B,k 1 , h F,K+1 m , h B,K+1 1 ]).", 
        "71": "Then, we compute unnormalized attention scores based on both encoders, and represent the language context with both domain-general and domain-specific representations.", 
        "72": "Equation 1 for domain k is changed as follows:\neji = s > j W (a)[bki , b K+1 i ],\ncj =\nm\u2211\ni=1\n\u03b1ji[b k i , b K+1 i ].", 
        "73": "In this model, the number of encoding parameters grows by a factor of 1k , and the number of decoding parameters grows by less than a factor of 2.", 
        "74": "3.3 One-to-many model  Here, a single encoder is shared, while we keep a separate decoder for each domain.", 
        "75": "The shared encoder captures the fact that the input in each domain is a sequence of English words.", 
        "76": "The domainspecific decoders learn to output tokens from the right domain vocabulary.", 
        "77": "4 Experiments    4.1 Data  We evaluated our system on the OVERNIGHT semantic parsing dataset, which contains 13, 682 examples of language utterances paired with logical forms across eight domains.", 
        "78": "OVERNIGHT was constructed by generating logical forms from a grammar and annotating them with language through crowdsourcing.", 
        "79": "We evaluated on the same train/test split as Wang et al.", 
        "80": "(2015), using the same accuracy metric, that is, the proportion of questions for which the denotations of the predicted and gold logical forms are equal.", 
        "81": "4.2 Implementation Details  We replicate the experimental setup of Jia and Liang (2016): We used the same hyper-parameters without tuning; we used 200 hidden units and 100- dimensional word vectors; we initialized parameters uniformly within the interval [\u22120.1, 0.1], and maximized the log likelihood of the correct logical form with stochastic gradient descent.", 
        "82": "We trained the model for 30 epochs with an initial learning rate of 0.1, and halved the learning rate every 5 epochs, starting from epoch 15.", 
        "83": "We replaced word vectors for words that occur only once in the training set with a universal <unk> word vector.", 
        "84": "At test time, we used beam search with beam size 5.", 
        "85": "We then picked the highest-scoring logical form that does not yield an executor error when its denotation is computed.", 
        "86": "Our models were implemented in Theano (Bergstra et al., 2010).", 
        "87": "4.3 Results  For our main result, we trained on all eight domains all models described in Section 3: ONE2ONE, DOMAINENCODING and INPUTTOKEN representing respectively the basic one-toone model, with extensions of one-hot domain encoding or an extra input token, as described in Section 3.1.", 
        "88": "MANY2MANY and ONE2MANY are the models described in Sections 3.2 and 3.3, respectively.", 
        "89": "INDEP is the baseline sequence-tosequence model described in Section 2, which trained independently on each domain.", 
        "90": "Results show (Table 1) that training on multiple KBs improves average accuracy over all domains for all our proposed models, and that performance improves as more parameters are shared.", 
        "91": "Our strongest results come when parameter sharing is maximal (i.e., single encoder and single decoder), coupled with a one-hot domain representation at decoding time (DOMAINENCODING).", 
        "92": "In this case accuracy improves not only on average, but also for each domain separately.", 
        "93": "Moreover, the number of model parameters necessary for training the model is reduced by a factor of 7.", 
        "94": "Our baseline, INDEP, is a reimplementation of the NORECOMBINATION model described in Jia and Liang (2016), which achieved average accuracy of 75.8% (corresponds to our 75.6% result).", 
        "95": "Jia and Liang (2016) also introduced a framework for generating new training examples in a single domain through recombination.", 
        "96": "Their model that uses the most training data achieved state-of-theart average accuracy of 77.5% on OVERNIGHT.", 
        "97": "We show that by training over multiple KBs we can achieve higher average accuracy, and our best model, DOMAINENCODING, sets a new state-ofthe-art average accuracy of 79.6%.", 
        "98": "Figure 3 shows a learning curve for all models on the test set, when training on a fraction of the training data.", 
        "99": "We observe that the difference between models that share parameters (INPUTTOKEN, ONE2ONE and DOMAINENCODING) and models that keep most of the pa-\nrameters separate (INDEP, MANY2MANY and ONE2MANY) is especially pronounced when the amount of data is small, reaching a difference of more than 15 accuracy point with 10% of the training data.", 
        "100": "This highlights the importance of using additional data from a similar distribution without increasing the number of parameters when there is little data.", 
        "101": "The learning curve also suggests that the MANY2MANY model improves considerably as the amount of data increases, and it would be interesting to examine its performance on larger datasets.", 
        "102": "4.4 Analysis  Learning a semantic parser involves mapping language phrases to KB constants, as well as learning how language composition corresponds to logical form composition.", 
        "103": "We hypothesized that the main benefit of training on multiple KBs lies in learning about compositionality.", 
        "104": "To verify that, we append the domain index to the name of every constant in every KB, and therefore constant names are disjoint across datasets.", 
        "105": "We train DOMAINENCODING on this dataset and obtain an accuracy of 79.1% (comparing to 79.6%), which hints that most of the gain is attributed to compositionality rather than mapping of language to KB constants.", 
        "106": "We also inspected cases where DOMAINEN-\nCODING performed better than INDEP, by analyzing errors on a development set (20% of the training data).", 
        "107": "We found 45 cases where INDEP makes an error (and DOMAINENCODING does not) by predicting a wrong comparative or superlative structure (e.g., > instead of \u2265).", 
        "108": "However, the opposite case occurs only 29 times.", 
        "109": "This reiterates how we learn structural linguistic regularities when sharing parameters.", 
        "110": "Lastly, we observed that the domain\u2019s training set size negatively correlates with its relative improvement in performance (DOMAINENCODING accuracy compared to INDEP), where Spearman\u2019s \u03c1 = \u22120.86.", 
        "111": "This could be explained by the tendency of smaller domains to cover a smaller fraction of structural regularities in language, thus, they gain more by sharing information.", 
        "112": "5 Conclusion  In this paper we address the challenge of obtaining training data for semantic parsing from a new perspective.", 
        "113": "We propose that one can improve parsing accuracy by training models over multiple KBs and demonstrate this on the eight domains of the OVERNIGHT dataset.", 
        "114": "In future work, we would like to further reduce the burden of data gathering by training characterlevel models that learn to map language phrases to KB constants across datasets, and by pre-training language side models that improve the encoder from data that is independent of the KB.", 
        "115": "We also plan to apply this method on datasets where only denotations are provided rather than logical forms.", 
        "116": "Reproducibility\nAll code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets.", 
        "117": "codalab.org/worksheets/ 0xdec998f58deb4829aba80fbf49f69236/.", 
        "118": "Acknowledgments  We thank Shimi Salant and the anonymous reviewers for their constructive feedback.", 
        "119": "This work was partially supported by the Israel Science Foundation, grant 942/16."
    }, 
    "document_id": "P17-2098.pdf.json"
}
