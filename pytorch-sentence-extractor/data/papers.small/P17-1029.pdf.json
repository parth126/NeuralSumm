{
    "abstract_sentences": {
        "1": "Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels.", 
        "2": "In this paper we propose multi-space variational encoderdecoders, a new model for labeled sequence transduction with semi-supervised learning.", 
        "3": "The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data.", 
        "4": "Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data.", 
        "5": "On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-ofart results by a large margin for the majority of languages.1"
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 310\u2013320 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1029  1 Introduction  This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels.", 
        "2": "Several examples of these tasks exist in prior work: using labels to moderate politeness in machine translation results (Sennrich et al., 2016), modifying the output language of a machine translation system (Johnson et al., 2016), or controlling the length of a summary in summarization (Kikuchi et al., 2016).", 
        "3": "In particular, however, we are motivated by the task of morphological reinflection (Cotterell et al.,\n1An implementation of our model are available at https://github.com/violet-zct/ MSVED-morph-reinflection.", 
        "4": "2016), which we will use as an example in our description and test bed for our models.", 
        "5": "In morphologically rich languages, different affixes (i.e.", 
        "6": "prefixes, infixes, suffixes) can be combined with the lemma to reflect various syntactic and semantic features of a word.", 
        "7": "The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation (Chahuneau et al., 2013; Toutanova et al., 2008) or information retrieval (Darwish and Oard, 2007) in these languages.", 
        "8": "As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging subtask of handling morphology as a whole, in which we take as input an inflected form (in the example, \u201cplaying\u201d) and labels representing the desired form (\u201cpos=Verb, tense=Past\u201d) and must generate the desired form (\u201cplayed\u201d).", 
        "9": "Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics (Taji et al., 2016), as well as learning-based approaches using alignment and extracted transduction rules (Durrett and DeNero, 2013; Alegria and Etxeberria, 2016; Nicolai et al., 2016).", 
        "10": "There have also been methods proposed using neural sequenceto-sequence models (Faruqui et al., 2016; Kann et al., 2016; Ostling, 2016), and currently ensembles of attentional encoder-decoder models (Kann and Schu\u0308tze, 2016a,b) have achieved state-of-art results on this task.", 
        "11": "One feature of these neural models however, is that they are trained in a\n310\nlargely supervised fashion (top of Fig.", 
        "12": "1), using data explicitly labeled with the input sequence and labels, along with the output representation.", 
        "13": "Needless to say, the ability to obtain this annotated data for many languages is limited.", 
        "14": "However, we can expect that for most languages we can obtain large amounts of unlabeled surface forms that may allow for semi-supervised learning over this unlabeled data (entirety of Fig.", 
        "15": "1).2\nIn this work, we propose a new framework for labeled sequence transduction problems: multi-space variational encoder-decoders (MSVED, \u00a73.3).", 
        "16": "MSVEDs employ continuous or discrete latent variables belonging to multiple separate probability distributions3 to explain the observed data.", 
        "17": "In the example of morphological reinflection, we introduce a vector of continuous random variables that represent the lemma of the source and target words, and also one discrete random variable for each of the labels, which are on the source or the target side.", 
        "18": "This model has the advantage of both providing a powerful modeling framework for supervised learning, and allowing for learning in an unsupervised setting.", 
        "19": "For labeled data, we maximize the variational lower bound on the marginal log likelihood of the data and annotated labels.", 
        "20": "For unlabeled data, we train an auto-encoder to reconstruct a word conditioned on its lemma and morphological labels.", 
        "21": "While these labels are unavailable, a set of discrete latent variables are associated with each unlabeled word.", 
        "22": "Afterwards we can perform posterior inference on these latent variables and maximize the variational lower bound on the marginal log likelihood of data.", 
        "23": "Experiments on the SIGMORPHON morphological reinflection task (Cotterell et al., 2016) find that our model beats the state-of-the-art for a single model in the majority of languages, and is particularly effective in languages with more complicated inflectional phenomena.", 
        "24": "Further, we find that semi-supervised learning allows for significant further gains.", 
        "25": "Finally, qualitative evaluation of lemma representations finds that our model is able to learn lemma embeddings that match with human intuition.", 
        "26": "2Faruqui et al.", 
        "27": "(2016) have attempted a limited form of semi-supervised learning by re-ranking with a standard ngram language model, but this is not integrated with the learning process for the neural model and gains are limited.", 
        "28": "3Analogous to multi-space hidden Markov models (Tokuda et al., 2002)  2 Labeled Sequence Transduction  In this section, we first present some notations regarding labeled sequence transduction problems in general, then describe a particular instantiation for morphological reinflection.", 
        "29": "Notation: Labeled sequence transduction problems involve transforming a source sequence x(s) into a target sequence x(t), with some labels describing the particular variety of transformation to be performed.", 
        "30": "We use discrete variables y (t) 1 , y (t) 2 , \u00b7 \u00b7 \u00b7 , y (t) K to denote the labels associated with each target sequence, where K is the total number of labels.", 
        "31": "Let y(t) = [y(t)1 , y (t) 2 , \u00b7 \u00b7 \u00b7 , y (t) K ] denote a vector of these discrete variables.", 
        "32": "Each discrete variable y(t)k represents a categorical feature pertaining to the target sequence, and has a set of possible labels.", 
        "33": "In the later sections, we also use y(t) and y(t)k to denote discrete latent variables corresponding to these labels.", 
        "34": "Given a source sequence x(s) and a set of associated target labels y(t), our goal is to generate a target sequence x(t) that exhibits the features specified by y(t) using a probabilistic model p(x(t)|x(s),y(t)).", 
        "35": "The best target sequence x\u0302(t) is then given by:\nx\u0302(t) = argmax x(t) p(x(t)|x(s),y(t)).", 
        "36": "(1)\nMorphological Reinflection Problem: In morphological reinflection, the source sequence x(s) consists of the characters in an inflected word (e.g., \u201cplayed\u201d), while the associated labels y(t) describe some linguistic features (e.g., y(t)pos = Verb, y(t)tense = Past) that we hope to realize in the target.", 
        "37": "The target sequence x(t) is therefore the characters of the re-inflected form of the source word (e.g., \u201cplayed\u201d) that satisfy the linguistic features specified by y(t).", 
        "38": "For this task, each discrete variable y(t)k has a set of possible labels (e.g.", 
        "39": "pos=V, pos=ADJ, etc) and follows a multinomial distribution.", 
        "40": "3 Proposed Method    3.1 Preliminaries: Variational Autoencoder  As mentioned above, our proposed model uses probabilistic latent variables in a model based on neural networks.", 
        "41": "The variational autoencoder (Kingma and Welling, 2014) is an efficient way to handle (continuous) latent variables in neural\nmodels.", 
        "42": "We describe it briefly here, and interested readers can refer to Doersch (2016) for details.", 
        "43": "The VAE learns a generative model of the probability p(x|z) of observed data x given a latent variable z, and simultaneously uses a recognition model q(z|x) at learning time to estimate z for a particular observation x (Fig.", 
        "44": "2(a)).", 
        "45": "q(\u00b7) and p(\u00b7) are modeled using neural networks parameterized by \u03c6 and \u03b8 respectively, and these parameters are learned by maximizing the variational lower bound on the marginal log likelihood of data:\nlog p\u03b8(x) \u2265 Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z)]\u2212 KL(q\u03c6(z|x)||p(z)) (2)\nThe KL-divergence term (a standard feature of variational methods) ensures that the distributions estimated by the recognition model q\u03c6(z|x) do not deviate far from our prior probability p(z) of the values of the latent variables.", 
        "46": "To optimize the parameters with gradient descent, Kingma and Welling (2014) introduce a reparameterization trick that allows for training using simple backpropagation w.r.t.", 
        "47": "the Gaussian latent variables z.", 
        "48": "Specifically, we can express z as a deterministic variable z = g\u03c6( ,x) where is an independent Gaussian noise variable \u223c N (0, 1).", 
        "49": "The mean \u00b5 and the variance \u03c32 of z are reparameterized by the differentiable functions w.r.t.", 
        "50": "\u03c6.", 
        "51": "Thus, instead of generating z from q\u03c6(z|x), we sample the auxiliary variable and obtain z = \u00b5\u03c6(x)+\u03c3\u03c6(x)\u25e6 , which enables gradients to backpropagate through \u03c6.", 
        "52": "3.2 Multi-space Variational Autoencoders  As an intermediate step to our full model, we next describe a generative model for a single sequence with both continuous and discrete latent variables, the multi-space variational auto-encoder (MSVAE).", 
        "53": "MSVAEs are a combination of two threads of previous work: deep generative models with both continuous/discrete latent variables for classification problems (Kingma et al., 2014;\nMaal\u00f8e et al., 2016) and VAEs with only continuous variables for sequential data (Bowman et al., 2016; Chung et al., 2015; Zhang et al., 2016; Fabius and van Amersfoort, 2014; Bayer and Osendorfer, 2014).", 
        "54": "In MSVAEs, we have an observed sequence x, continuous latent variables z like the VAE, as well as discrete variables y.", 
        "55": "In the case of the morphology example, x can be interpreted as an inflected word to be generated.", 
        "56": "y is a vector representing its linguistic labels, either annotated by an annotator in the observed case, or unannotated in the unobserved case.", 
        "57": "z is a vector of latent continuous variables, e.g.", 
        "58": "a latent embedding of the lemma that captures all the information about x that is not already represented in labels y.\nMSVAE: Because inflected words can be naturally thought of as \u201clemma+morphological labels\u201d, to interpret a word, we resort to discrete and continuous latent variables that represent the linguistic labels and the lemma respectively.", 
        "59": "In this case when the labels of the sequence y is not observed, we perform inference over possible linguistic labels and these inferred labels are referenced in generating x.", 
        "60": "The generative model p\u03b8(x,y, z) = p(z)p\u03c0(y)p\u03b8(x|y, z) is defined as:\np(z) = N (z|0, I) (3) p\u03c0(y) = \u220f\nk\nCat(yk|\u03c0k) (4)\np\u03b8(x|y, z) = f(x;y, z, \u03b8).", 
        "61": "(5)\nLike the standard VAE, we assume the prior of the latent variable z is a diagonal Gaussian distribution with zero mean and unit variance.", 
        "62": "We assume that each variable in y is independent, resulting in a factorized distribution in Eq.", 
        "63": "4, where Cat(yk|\u03c0k) is a multinomial distribution with parameters \u03c0k.", 
        "64": "For the purposes of this study, we set these to a uniform distribution \u03c0k,j = 1 |\u03c0k| .", 
        "65": "f(x;y, z, \u03b8) calculates the likelihood of x, a function parametrized by deep neural networks.", 
        "66": "Specifically, we employ an RNN decoder to generate the target word conditioned on the lemma variable z and linguistic labels y, detailed in \u00a75.", 
        "67": "When inferring the latent variables from the given data x, we assume the joint distribution of latent variables z and y has a factorized form, i.e.", 
        "68": "q(z,y|x) = q(z|x)q(y|x) as shown in Fig.", 
        "69": "2(c).", 
        "70": "The inference model is defined as follows:\nq\u03c6(z|x) = N (z|\u00b5\u03c6(x), diag(\u03c32\u03c6(x))) (6) q\u03c6(y|x) = \u220f\nk\nq\u03c6(yk|x)\n= \u220f\nk\nCat(yk|\u03c0\u03c6(x)) (7)\nwhere the inference distribution over z is a diagonal Gaussian distribution with mean and variance parameterized by neural networks.", 
        "71": "The inference model q(y|x) on labels y has the form of a discriminative classifier that generates a set of multinomial probability vectors \u03c0\u03c6(x) over all labels for each tag yk.", 
        "72": "We represent each multinomial distribution q(yk|x) with an MLP.", 
        "73": "The MSVAE is trained by maximizing the following variational lower bound U(x) on the objective for unlabeled data:\nlog p\u03b8(x) \u2265 E(y,z)\u223cq\u03c6(y,z|x) log p\u03b8(x,y, z)\nq\u03c6(y, z|x) = Ey\u223cq\u03c6(y|x)[Ez\u223cq\u03c6(z|x)[log p\u03b8(x|z,y)] \u2212 KL(q\u03c6(z|x)||p(z)) + log p\u03c0(y) \u2212 log q\u03c6(y|x)] = U(x) (8)\nNote that this introduction of discrete variables requires more sophisticated optimization algorithms, which we will discuss in \u00a74.1.", 
        "74": "Labeled MSVAE: When y is observed as shown in Fig.", 
        "75": "2(b), we maximize the following variational lower bound on the marginal log likelihood of the data and the labels:\nlog p\u03b8(x,y) \u2265 Ez\u223cq\u03c6(z|x) log p\u03b8(x,y, z)\nq\u03c6(z|x) =\nEz\u223cq\u03c6(z|x)[log p\u03b8(x|y, z) + log p\u03c0(y)] \u2212 KL(q\u03c6(z|x)||p(z)) (9)\nwhich is a simple extension to Eq.", 
        "76": "2.", 
        "77": "Note that when labels are not observed, the inference model q\u03c6(y|x) has the form of a discriminative classifier, thus we can use observed labels as the supervision signal to learn a better classifier.", 
        "78": "In this case we also minimize the following cross entropy as the classification loss:\nD(x,y) = E(x,y)\u223cpl(x,y)[\u2212 log q\u03c6(y|x)] (10)\nwhere pl(x,y) is the distribution of labeled data.", 
        "79": "This is a form of multi-task learning, as this additional loss also informs the learning of our representations.", 
        "80": "3.3 Multi-space Variational Encoder-Decoders  Finally, we discuss the full proposed method: the multi-space variational encoder-decoder (MSVED), which generates the target x(t) from the source x(s) and labels y(t).", 
        "81": "Again, we discuss two cases of this model: labels of the target sequence are observed and not observed.", 
        "82": "MSVED: The graphical model for the MSVED is given in Fig.", 
        "83": "2 (e).", 
        "84": "Because the labels of target sequence are not observed, once again we treat them as discrete latent variables and make inference on the these labels conditioned on the target sequence.", 
        "85": "The generative process for the MSVED is very similar to that of the MSVAE with one important exception: while the standard MSVAE conditions the recognition model q(z|x) on x, then generates x itself, the MSVED conditions the recognition model q(z|x(s)) on the source x(s), then generates the target x(t).", 
        "86": "Because only the recognition model is changed, the generative equations for p\u03b8(x\n(t),y(t), z) are exactly the same as Eqs.", 
        "87": "3\u20135 with x(t) swapped for x and y(t) swapped for y.", 
        "88": "The variational lower bound on the conditional log likelihood, however, is affected by the recognition model, and thus is computed as:\nlog p\u03b8(x (t)|x(s))\n\u2265E(y(t),z)\u223cq\u03c6(y(t),z|x(s),x(t)) log p\u03b8(x (t),y(t), z|x(s)) q\u03c6(y(t), z|x(s),x(t))\n=Ey(t)\u223cq\u03c6(y(t)|x(t))[Ez\u223cq\u03c6(z|x(s))[log p\u03b8(x (t)|y(t), z)]\n\u2212 KL(q\u03c6(z|x(s))||p(z)) + log p\u03c0(y(t)) \u2212 log q\u03c6(y(t)|x(t))] = Lu(x(t)|x(s)) (11)\nLabeled MSVED: When the complete form of x(s), y(t), and x(t) is observed in our training data, the graphical model of the labeled MSVED model is illustrated in Fig.", 
        "89": "2 (d).", 
        "90": "We maximize the variational lower bound on the conditional log likelihood of observing x(t) and y(t) as follows:\nlog p\u03b8(x (t),y(t)|x(s))\n\u2265 Ez\u223cq\u03c6(z|x(s)) log p\u03b8(x (t),y(t), z|x(s)) q\u03c6(z|x(s))\n= Ez\u223cq\u03c6(z|x(s))[log p\u03b8(x (t)|y(t), z) + log p\u03c0(y(t))]\u2212\nKL(q\u03c6(z|x(s))||p(z)) = Ll(x(t),y(t)|x(s)) (12)  4 Learning MSVED  Now that we have described our overall model, we discuss details of the learning process that prove\nuseful to its success.", 
        "91": "4.1 Learning Discrete Latent Variables  One challenge in training our model is that it is not trivial to perform back-propagation through discrete random variables, and thus it is difficult to learn in the models containing discrete tags such as MSVAE or MSVED.4 To alleviate this problem, we use the recently proposed Gumbel-Softmax trick (Maddison et al., 2014; Gumbel and Lieblein, 1954) to create a differentiable estimator for categorical variables.", 
        "92": "The Gumbel-Max trick (Gumbel and Lieblein, 1954) offers a simple way to draw samples from a categorical distribution with class probabilities \u03c01, \u03c02, \u00b7 \u00b7 \u00b7 by using the argmax operation as follows: one hot(argmaxi[gi + log \u03c0i]), where g1, g2, \u00b7 \u00b7 \u00b7 are i.i.d.", 
        "93": "samples drawn from the Gumbel(0,1) distribution.5 When making inferences on the morphological labels y1, y2, \u00b7 \u00b7 \u00b7 , the GumbelMax trick can be approximated by the continuous softmax function with temperature \u03c4 to generate a sample vector y\u0302i for each label i:\ny\u0302ij = exp((log(\u03c0ij) + gij)/\u03c4)\u2211Ni k=1 exp((log(\u03c0ik) + gik)/\u03c4\n(13)\nwhereNi is the number of classes of label i.", 
        "94": "When \u03c4 approaches zero, the generated sample y\u0302i becomes a one-hot vector.", 
        "95": "When \u03c4 > 0, y\u0302i is smooth w.r.t \u03c0i.", 
        "96": "In experiments, we start with a relatively large temperature and decrease it gradually.", 
        "97": "4.2 Learning Continuous Latent Variables  MSVED aims at generating the target sequence conditioned on the latent variable z and the target labels y(t).", 
        "98": "This requires the encoder to generate an informative representation z encoding the content of the x(s).", 
        "99": "However, the variational lower bound in our loss function contains the KL-divergence between the approximate posterior q\u03c6(z|x) and the prior p(z), which is relatively easy to learn compared with learning to generate output from a latent representation.", 
        "100": "We observe that with the vanilla implementation the KL cost quickly decreases to near zero, setting q\u03c6(z|x) equal to standard normal distribution.", 
        "101": "In\n4 Kingma et al.", 
        "102": "(2014) solve this problem by marginalizing over all labels, but this is infeasible in our case where we have an exponential number of label combinations.", 
        "103": "5The Gumbel (0,1) distribution can be sampled by first drawing u \u223c Uniform(0,1) and computing g = \u2212 log(\u2212 log(u)).", 
        "104": "this case, the RNN decoder can easily rely on the true output of last time step during training to decode the next token, which degenerates into an RNN language model.", 
        "105": "Hence, the latent variables are ignored by the decoder and cannot encode any useful information.", 
        "106": "The latent variable z learns an undesirable distribution that coincides with the imposed prior distribution but has no contribution to the decoder.", 
        "107": "To force the decoder to use the latent variables, we take the following two approaches which are similar to Bowman et al.", 
        "108": "(2016).", 
        "109": "KL-Divergence Annealing: We add a coefficient \u03bb to the KL cost and gradually anneal it from zero to a predefined threshold \u03bbm.", 
        "110": "At the early stage of training, we set \u03bb to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost.", 
        "111": "Although we are not optimizing the tight variational lower bound, the model balances well between generation and regularization.", 
        "112": "This technique can also be seen in (Koc\u030cisky\u0300 et al., 2016; Miao and Blunsom, 2016).", 
        "113": "Input Dropout in the Decoder: Besides annealing the KL cost, we also randomly drop out the input token with a probability of \u03b2 at each time step of the decoder during learning.", 
        "114": "The previous ground-truth token embedding is replaced with a zero vector when dropped.", 
        "115": "In this way, the RNN decoder could not fully rely on the ground-truth previous token, which ensures that the decoder uses information encoded in the latent variables.", 
        "116": "5 Architecture for Morphological  Reinflection\nTraining details: For the morphological reinflection task, our supervised training data consists of source x(s), target x(t), and target tags y(t).", 
        "117": "We test three variants of our model trained using different types of data and different loss functions.", 
        "118": "First, the single-directional supervised model (SDSup) is purely supervised: it only decodes the target word from the given source word with the loss function Ll(x(t),y(t)|x(s)) from Eq.", 
        "119": "12.", 
        "120": "Second, the bi-directional supervised model (BDSup) is trained in both directions: decoding the target word from the source word and decoding the source word from the target word, which corresponds to the loss function Ll(x(t),y(t)|x(s)) + Lu(x(s)|x(t)) using Eqs.", 
        "121": "11\u201312.", 
        "122": "Finally, the semisupervised model (Semi-sup) is trained to maxi-\nmize the variational lower bounds and minimize the classification cross-entropy error of 10.", 
        "123": "L(x(s),x(t),y(t),x) = \u03b1 \u00b7 U(x) + Lu(x(s)|x(t)) + Ll(x(t),y(t)|x(s))\u2212D(x(t),y(t)) (14)\nThe weight \u03b1 controls the relative weight between the loss from unlabeled data and labeled data.", 
        "124": "We use Monte Carlo methods to estimate the expectation over the posterior distribution q(z|x) and q(y|x) inside the objective function 14.", 
        "125": "Specifically, we draw Gumbel noise and Gaussian noise one at a time to compute the latent variables y and z.", 
        "126": "The overall model architecture is shown in Fig.", 
        "127": "3.", 
        "128": "Each character and each label is associated with a continuous vector.", 
        "129": "We employ Gated Recurrent Units (GRUs) for the encoder and de-\ncoder.", 
        "130": "Let \u2212\u2192 ht and \u2190\u2212 ht denote the hidden state of the forward and backward encoder RNN at time step t. u is the hidden representation of x(s) concatenating the last hidden state from both directions i.e.", 
        "131": "[ \u2212\u2192 hT ; \u2190\u2212 hT ] where T is the word length.", 
        "132": "u is used as the input for the inference model on z.", 
        "133": "We represent \u00b5(u) and \u03c32(u) as MLPs and sample z fromN (\u00b5(u), diag(\u03c32(u))), using z = \u00b5+ \u03c3 \u25e6 , where \u223c N (0, I).", 
        "134": "Similarly, we can obtain the hidden representation of x(t) and use this as input to the inference model on each label y(t)i which is also an MLP following a softmax layer to generate the categorical probabilities of target labels.", 
        "135": "In decoding, we use 3 types of information in calculating the probability of the next character : (1) the current decoder state, (2) a tag context vector using attention (Bahdanau et al., 2015) over the tag embeddings, and (3) the latent variable z.", 
        "136": "The intuition behind this design is that we would like the model to constantly consider the lemma represented by z, and also reference the tag corresponding to the current morpheme being generated at this point.", 
        "137": "We do not marginalize over the latent variable z however, instead we use the mode \u00b5 of z as the latent representation for z.", 
        "138": "We use beam search with a beam size of 8 to perform search over the character vocabulary at each decoding time step.", 
        "139": "Other experimental setups: All hyperparameters are tuned on the validation set, and include the following: For KL cost annealing, \u03bbm is set to be 0.2 for all language settings.", 
        "140": "For character drop-out at the decoder, we empirically set \u03b2 to be 0.4 for all languages.", 
        "141": "We set the dimension of character embeddings to be 300, tag label embeddings to be 200, RNN hidden state to be 256, and\nlatent variable z to be 150.", 
        "142": "We set \u03b1 the weight for the unsupervised loss to be 0.8.", 
        "143": "We train the model with Adadelta (Zeiler, 2012) and use earlystop with a patience of 10.", 
        "144": "6 Experiments    6.1 Background: SIGMORPHON 2016  SIGMORPHON 2016 is a shared task on morphological inflection over 10 different morphologically rich languages.", 
        "145": "There are a total of three tasks, the most difficult of which is task 3, which requires the system to output the reinflection of an inflected word.6 The training data format in task 3 is in triples: (source word, target labels, target word).", 
        "146": "In the test phase, the system is asked to generate the target word given a source word and the target labels.", 
        "147": "There are a total of three tracks for each task, divided based the amount of supervised data that can be used to solve the problem, among which track 2 has the strictest limitation of only using data for the corresponding task.", 
        "148": "As this is an ideal testbed for our method, which can learn from unlabeled data, we choose track 2 and task 3 to test our our model\u2019s ability to exploit this data.", 
        "149": "As a baseline, we compare our results with the MED system (Kann and Schu\u0308tze, 2016a) which achieved state-of-the-art results in the shared task.", 
        "150": "This system used an encoder-decoder model with attention on the concatenated source word and target labels.", 
        "151": "Its best result is obtained from an ensemble of five RNN encoder-decoders (Ensemble).", 
        "152": "To make a fair comparison with our models, which don\u2019t use ensembling, we also calculated single model results (Single).", 
        "153": "All models are trained using the labeled training data provided for task 3.", 
        "154": "For our semi-supervised model (Semi-sup), we also leverage unlabeled data from the training and validation data for tasks 1 and 2 to train variational auto-encoders.", 
        "155": "6.2 Results and Analysis  From the results in Tab.", 
        "156": "1, we can glean a number of observations.", 
        "157": "First, comparing the results of our full Semi-sup model, we can see that for all languages except Spanish, it achieves accuracies better than the single MED system, often by a large margin.", 
        "158": "Even compared to the MED ensembled model, our single-model system is quite competitive, achieving higher accuracies for Hungarian,\n6Task 1 is inflection of a lemma word and task 2 is reinflection but also provides the source word labels.", 
        "159": "Navajo, Maltese, and Arabic, as well as achieving average accuracies that are state-of-the-art.", 
        "160": "Next, comparing the different varieties of our proposed models, we can see that the semisupervised model consistently outperforms the bidirectional model for all languages.", 
        "161": "And similarly, the bidirectional model consistently outperforms the single direction model.", 
        "162": "From these results, we can conclude that the unlabeled data is beneficial to learn useful latent variables that can be used to decode the corresponding word.", 
        "163": "Examining the linguistic characteristics of the models in which our model performs well provides even more interesting insights.", 
        "164": "Cotterell et al.", 
        "165": "(2016) estimate how often the inflection process involves prefix changes, stem-internal changes or suffix changes, the results of which are shown in Tab.", 
        "166": "2.", 
        "167": "Among the many languages, the inflection processes of Arabic, Maltese and Navajo are relatively diverse, and contain a large amount of all three forms of inflection.", 
        "168": "By examining the experimental results together with the morphological inflection process of different languages, we found that among all the languages, Navajo, Maltese and Arabic obtain the largest gains in performance compared with the ensem-\nbled MED system.", 
        "169": "To demonstrate this visually, in Fig.", 
        "170": "4, we compare the semi-supervised MSVED with the MED single model w.r.t.", 
        "171": "the percentage of suffixing inflection of each language, showing this clear trend.", 
        "172": "This strongly demonstrates that our model is agnostic to different morphological inflection forms whereas the conventional encoder-decoder with attention on the source input tends to perform better on suffixing-oriented morphological inflection.", 
        "173": "We hypothesize that for languages that the inflection mostly comes from suffixing, transduction is relatively easy because the source and target words share the same prefix and the decoder can copy the prefix of the source word via attention.", 
        "174": "However, for languages in which different inflections of a lemma go through different morphological processes, the inflected word and the target word may differ greatly and thus it is crucial to first analyze the lemma of the inflected word before generating the corresponding the reinflection form based on the target labels.", 
        "175": "This is precisely what our model does by extracting the lemma representation z learned by the variational inference model.", 
        "176": "6.3 Analysis on Tag Attention  To analyze how the decoder attends to the linguistic labels associated with the target word, we randomly pick two words from the Arabic and Navajo test set and plot the attention weight in Fig.", 
        "177": "5.", 
        "178": "The Arabic word \u201cal-\u2019ima\u0304ra\u0304tiyya\u0304tu\u201d is an adjective which means \u201cEmirati\u201d, and its source word in the test data is \u201c\u2019ima\u0304ra\u0304tiyyin\u201d 7.", 
        "179": "Both of these are declensions of \u201c\u2019ima\u0304ra\u0304tiyy\u201d.", 
        "180": "The source word is\n7https://en.wiktionary.org/wiki/%D8% A5%D9%85%D8%A7%D8%B1%D8%A7%D8%AA%D9%8A\nsingular, masculine, genitive and indefinite, while the required inflection is plural, feminine, nominative and definite.", 
        "181": "We can see from the left heat map that the attention weights are turned on at several positions of the word when generating corresponding inflections.", 
        "182": "For example, \u201cal-\u201d in Arabic is the definite article that marks definite nouns.", 
        "183": "The same phenomenon can also be observed in the Navajo example, as well as other languages, but due to space limitation, we don\u2019t provide detailed analysis here.", 
        "184": "6.4 Visualization of Latent Lemmas  To investigate the learned latent representations, in this section we visualize the z vectors, examining whether the latent space groups together words with the same lemma.", 
        "185": "Each sample in SIGMORPHON 2016 contains source word and target words which share the same lemma.", 
        "186": "We run a heuristic process to assign pairs of words to groups that likely share a lemma by grouping together word pairs for which at least one of the words in each pair shares a surface form.", 
        "187": "This process is not error free \u2013 errors may occur in the case where multiple lemmas share the same surface form \u2013 but in general the groupings will generally reflect lemmas except in these rare erroneous cases, so we dub each of these groups a pseudo-lemma.", 
        "188": "In Fig.", 
        "189": "6, we randomly pick 1500 words from Maltese and visualize the continuous latent vectors of these words.", 
        "190": "We compute the latent vectors as \u00b5\u03c6(x) in the variational posterior inference (Eq.", 
        "191": "6) without adding the variance.", 
        "192": "As expected, words that belong to the same pseudo-lemma (in the same color) are projected into adjacent points in the two-dimensional space.", 
        "193": "This demonstrates that the continuous latent variable captures the canonical form of a set of words and demonstrates the effectiveness of the proposed representation.", 
        "194": "6.5 Analyzing Effects of Size of Unlabeled Data  From Tab.", 
        "195": "1, we can see that semi-supervised learning always performs better than supervised learning without unlabeled data.", 
        "196": "In this section, we investigate to what extent the size of unlabeled data can help with performance.", 
        "197": "We process a German corpus from a 2017 Wikipedia dump and obtain more than 100,000 German words.", 
        "198": "These words are ranked in order of occurrence frequency in Wikipedia.", 
        "199": "The data contains a certain amount of noise since we did not apply any special processing.", 
        "200": "We shuffle all unlabeled data from both the Wikipedia and the data provided in the shared task used in previous experiments, and increase the number of unlabeled words used in learning by 10,000 each time, and finally use all the unlabeled data (more than 150,000 words) to train the model.", 
        "201": "Fig.", 
        "202": "7 shows that the performance on the test data improves as the amount of unlabeled data increases, which implies that the unsupervised learning continues to help improve the model\u2019s ability to model the latent lemma representation even as we scale to a noisy, real, and relatively large-scale dataset.", 
        "203": "Note that the growth rate of the performance grows slower as more data is added, because although the number of unlabeled data is increasing, the model has seen most word patterns in a relatively small vocabulary.", 
        "204": "6.6 Case Study on Reinflected Words  In Tab.", 
        "205": "3, we examine some model outputs on the test data from the MED system and our model respectively.", 
        "206": "It can be seen that most errors of MED and our models can be ascribed to either over-copy or under-copy of characters.", 
        "207": "In particular, from the complete outputs we observe that our model tends to be more aggressive in its changes, resulting in\nit performing more complicated transformations, both successfully (such as Maltese \u201cndammhomli\u201d to \u201ctindammhiex\u201d) and unsuccessfully (\u201ctqoz\u0307z\u0307x\u201d to \u201cqaz\u0307z\u0307ejtx\u201d).", 
        "208": "In contrast, the attentional encoderdecoder model is more conservative in its changes, likely because it is less effective in learning an abstracted representation for the lemma, and instead copies characters directly from the input.", 
        "209": "7 Conclusion and Future Work  In this work, we propose a multi-space variational encoder-decoder framework for labeled sequence transduction problem.", 
        "210": "The MSVED performs well in the task of morphological reinflection, outperforming the state of the art, and further improving with the addition of external unlabeled data.", 
        "211": "Future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answering, where continuous and discrete latent variables can be abstracted to guide sequence generation.", 
        "212": "Acknowledgments  The authors thank Jiatao Gu, Xuezhe Ma, Zihang Dai and Pengcheng Yin for their helpful discussions.", 
        "213": "This work has been supported in part by an Amazon Academic Research Award."
    }, 
    "document_id": "P17-1029.pdf.json"
}
