{
    "abstract_sentences": {
        "1": "Question classification is an important task with wide applications.", 
        "2": "However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data.", 
        "3": "In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set.", 
        "4": "We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs.", 
        "5": "The proposed model significantly outperform strong baselines on four datasets."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 335\u2013340 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2053  1 Introduction  Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years.", 
        "2": "Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task.", 
        "3": "We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification.", 
        "4": "First, different from the flat and coarse categories in most sentence classification tasks (i.e.", 
        "5": "sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig.", 
        "6": "1).", 
        "7": "Another unique aspect of question classification is the well prepared answers for each question or question category.", 
        "8": "These answer\n1Crawled from http://nysdmv.custhelp.com/app/home.", 
        "9": "This data and our code will be at http://github.com/cosmmb.", 
        "10": "sets generally cover a larger vocabulary (than the questions themselves) and provide richer information for each class.", 
        "11": "We believe there is a great potential to enhance question representation with extra information from corresponding answer sets.", 
        "12": "To exploit the hierarchical and overlapping structures in question categories and extra information from answer sets, we consider dictionary learning (Cande\u0300s and Wakin, 2008; Rubinstein et al., 2010) which is a common approach for representing samples from many correlated groups with external information.", 
        "13": "This learning procedure first builds a dictionary with a series of grouped bases.", 
        "14": "These bases can be initialized randomly or from external data (from the answer set in our case) and optimized during training through Sparse Group Lasso (SGL) (Simon et al., 2013).", 
        "15": "To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints.", 
        "16": "The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories.", 
        "17": "The bases in different groups can be either initialized randomly or by\n335\nthe sentences in corresponding answer categories.", 
        "18": "Each question sentence will be reconstructed by a few bases within a few groups.", 
        "19": "GSA can use either linear or nonlinear encoding or decoding while SGL is restricted to be linear.", 
        "20": "Eventually, to model questions with sparsity, we further propose novel Group Sparse Convolutional Neural Networks (GSCNNs) by implanting the GSA onto CNNs, essentially enforcing group sparsity between the convolutional and classification layers.", 
        "21": "This framework is a jointly trained neural model to learn question representation with group sparse constraints from both question and answer sets.", 
        "22": "2 Group Sparse Autoencoders    2.1 Sparse Autoencoders  Autoencoder (Bengio et al., 2007) is an unsupervised neural network which learns the hidden representations from data.", 
        "23": "When the number of hidden units is large (e.g., bigger than input dimension), we can still discover the underlying structure by imposing sparsity constraints, using sparse autoencoders (SAE) (Ng, 2011):\nJsparse(\u03c1) = J + \u03b1\ns\u2211\nj=1\nKL(\u03c1\u2016\u03c1\u0302j) (1)\nwhere J is the autoencoder reconstruction loss, \u03c1 is the desired sparsity level which is small, and thus Jsparse(\u03c1) is the sparsity-constrained version of loss J .", 
        "24": "Here \u03b1 is the weight of the sparsity penalty term defined below:\nKL(\u03c1\u2016\u03c1\u0302j) = \u03c1 log \u03c1 \u03c1\u0302j + (1\u2212 \u03c1) log 1\u2212 \u03c1 1\u2212 \u03c1\u0302j (2)\nwhere\n\u03c1\u0302j = 1\nm\nm\u2211\ni=1\nhij\nrepresents the average activation of hidden unit j over m examples (SAE assumes the input features are correlated).", 
        "25": "As described above, SAE has a similar objective to traditional sparse coding which tries to find sparse representations for input samples.", 
        "26": "Besides applying simple sparse constraints to the network, group sparse constraints is also desired when the class categories are structured and overlapped.", 
        "27": "Inspired by group sparse lasso (Yuan and Lin, 2006) and sparse group lasso (Simon et al., 2013), we propose a novel architecture below.", 
        "28": "2.2 Group Sparse Autoencoders  Group Sparse Autoencoder (GSA), unlike SAE, categorizes the weight matrix into different groups.", 
        "29": "For a given input, GSA reconstructs the input signal with the activations from only a few groups.", 
        "30": "Similar to the average activation \u03c1\u0302j for sparse autoencoders, GSA defines each grouped average activation for the hidden layer as follows:\n\u03b7\u0302p = 1\nmg\nm\u2211\ni=1\ng\u2211\nl=1\n\u2016hip,l\u20162 (3)\nwhere g represents the size of each group, and \u03b7\u0302j first sums up all the activations within pth group, then computes the average pth group respond across different samples\u2019 hidden activations.", 
        "31": "Similar to Eq.", 
        "32": "2, we also use KL divergence to measure the difference between estimated intragroup activation and global group sparsity:\nKL(\u03b7\u2016\u03b7\u0302p) = \u03b7 log \u03b7 \u03b7\u0302p + (1\u2212 \u03b7) log 1\u2212 \u03b7 1\u2212 \u03b7\u0302p (4)\nwhere G is the number of groups.", 
        "33": "Then the objective function of GSA is:\nJgroupsparse(\u03c1, \u03b7) = J + \u03b1\ns\u2211\nj=1\nKL(\u03c1\u2016\u03c1\u0302j)\n+ \u03b2 G\u2211\np=1\nKL(\u03b7\u2016\u03b7\u0302p) (5)\nwhere \u03c1 and \u03b7 are constant scalars which are our target sparsity and group-sparsity levels, resp.", 
        "34": "When \u03b1 is set to zero, GSA only considers the structure between difference groups.", 
        "35": "When \u03b2 is set to zero, GSA is reduced to SAE.", 
        "36": "2.3 Visualizing Group Sparse Autoencoders  In order to have a better understanding of GSA, we use the MNIST dataset to visualize GSA\u2019s internal parameters.", 
        "37": "Fig.", 
        "38": "2 and Fig.", 
        "39": "3 illustrate the projection matrix and the corresponding hidden activations.", 
        "40": "We use 10,000 training samples.", 
        "41": "We set the size of the hidden layer to 500 with 10 groups.", 
        "42": "Fig.", 
        "43": "2(a) visualizes the input image for hand written digit 0.", 
        "44": "In Fig.", 
        "45": "2(b), we find similar patterns within each group.", 
        "46": "For example, group 8 has different forms of digit 0, and group 9 includes different forms of digit 7.", 
        "47": "However, it is difficult to see any meaningful patterns from the projection matrix of basic autoencoders in Fig.", 
        "48": "2(c).", 
        "49": "Fig.", 
        "50": "3(a) shows the hidden activations with respect to the input image of digit 0.", 
        "51": "The patterns of the 10th row in Fig.", 
        "52": "2(b) are very similar to digit 1 which is very different from digit 0 in shape.", 
        "53": "Therefore, there is no activation in group 10 in Fig.", 
        "54": "3(a).", 
        "55": "The majority of hidden layer activations are in groups 1, 2, 6 and 8, with group 8 being the most significant.", 
        "56": "When compared to the projection matrix visualization in Fig.", 
        "57": "2(b), these results are reasonable since the 8th row has the most similar patterns of digit 0.", 
        "58": "However, we could not find any meaningful pattern from the hidden activations of basic autoencoder as shown in Fig.", 
        "59": "3(b).", 
        "60": "GSA could be directly applied to small image data (e.g.", 
        "61": "MINIST dataset) for pre-training.", 
        "62": "However, in tasks which prefer dense semantic representations (e.g.", 
        "63": "sentence classification), we still need CNNs to learn the sentence representation automatically.", 
        "64": "In order to combine advantages from GSA and CNNs, we propose Group Sparse\nConvolutional Neural Networks below.", 
        "65": "3 Group Sparse CNNs  CNNs were first proposed by (LeCun et al., 1995) in computer vision and adapted to NLP by (Collobert et al., 2011).", 
        "66": "Recently, many CNN-based techniques have achieved great successes in sentence modeling and classification (Kim, 2014; Kalchbrenner et al., 2014).", 
        "67": "Following sequential CNNs, one dimensional convolutions operate the convolution kernel in sequential order xi,j = xi \u2295 xi+1 \u2295 \u00b7 \u00b7 \u00b7 \u2295 xi+j , where xi \u2208 Re represents the e dimensional word representation for the i-th word in the sentence, and \u2295 is the concatenation operator.", 
        "68": "Therefore xi,j refers to concatenated word vector from the i-th word to the (i+ j)-th word in sentence.", 
        "69": "A convolution operates a filter w \u2208 Rn\u00d7e to a window of n words xi,i+n with bias term b\u2032 by ai = \u03c3(w \u00b7 xi,i+n + b\u2032) with non-linear activation\nfunction \u03c3 to produce a new feature.", 
        "70": "The filter w is applied to each word in the sentence, generating the feature map a = [a1, a2, \u00b7 \u00b7 \u00b7 , aL] where L is the sentence length.", 
        "71": "We then use a\u0302 = max{a} to represent the entire feature map after max-pooling.", 
        "72": "In order to capture different aspects of patterns, CNNs usually randomly initialize a set of filters with different sizes and values.", 
        "73": "Each filter will generate a feature as described above.", 
        "74": "To take all the features generated by N different filters into count, we use z = [a\u03021, \u00b7 \u00b7 \u00b7 , a\u0302N ] as the final representation.", 
        "75": "In conventional CNNs, this z will be directly fed into classifiers after the sentence representation is obtained, e.g.", 
        "76": "fully connected neural networks (Kim, 2014).", 
        "77": "There is no easy way for CNNs to explore the possible hidden representations with underlaying structures.", 
        "78": "In order to exploit these structures, we propose Group Sparse Convolutional Neural Networks (GSCNNs) by placing one extra layer between the convolutional and the classification layers.", 
        "79": "This extra layer mimics the functionality of GSA from Section 2.", 
        "80": "Shown in Fig.", 
        "81": "4, after the conventional convolutional layer, we get the feature map z for each sentence.", 
        "82": "In stead of directly feeding it into a fully connected neural network for classification, we enforce the group sparse constraint on z in a way similar to the group sparse constraints on hidden layer in GSA from Sec.", 
        "83": "2.", 
        "84": "Then, we use the sparse hidden representation h in Eq.", 
        "85": "5 as the new sentence representation, which is then fed into a fully connected neural network for classification.", 
        "86": "The parameters W in Eq.", 
        "87": "5 will\nalso be fine tunned during the last step.", 
        "88": "Different ways of initializing the projection matrix in Eq.", 
        "89": "5 can be summarized below:\n\u2022 Random Initialization: When there is no answer corpus available, we first randomly initializeN vectors to represent the group information from the answer set.", 
        "90": "Then we cluster these N vectors into G categories with g centroids for each category.", 
        "91": "These centroids from different categories will be the initialized bases for projection matrix W which will be learned during training.", 
        "92": "\u2022 Initialization from Questions: Instead of using random initialized vectors, we can also use question sentences for initializing the projection matrix when the answer set is not available.", 
        "93": "We need to pre-train the sentences with CNNs to get the sentence representation.", 
        "94": "We then select G largest categories in terms of number of question sentences.", 
        "95": "Then we get g centroids from each category by kmeans.", 
        "96": "We concatenate these G \u00d7 g vectors to form the projection matrix.", 
        "97": "\u2022 Initialization from Answers: This is the most ideal case.", 
        "98": "We follow the same procedure as above, with the only difference being using the answer sentences in place of question sentences to pre-train the CNNs.", 
        "99": "4 Experiments  Since there is little effort to use answer sets in question classification, we did not find any suit-\nable datasets which are publicly available.", 
        "100": "We collected two datasets ourselves and also used two other well-known ones.", 
        "101": "These datasets are summarized in Table 1.", 
        "102": "INSURANCE is a private dataset we collected from a car insurance company\u2019s website.", 
        "103": "Each question is classified into 319 classes with corresponding answer data.", 
        "104": "All questions which belong to the same category share the same answers.", 
        "105": "The DMV dataset is collected from New York State the DMV\u2019s FAQ website.", 
        "106": "The YAHOO Ans dataset is only a subset of the original publicly available YAHOO Answers dataset (Fleming et al., 2012; Shah and Pomerantz, 2010).", 
        "107": "Though not very suitable for our framework, we still included the frequently used TREC dataset (factoid question type classification) for comparison.", 
        "108": "We only compare our model\u2019s performance with CNNs for two following reasons: we consider our \u201cgroup sparsity\u201d as a modification to the general CNNs for grouped feature selection.", 
        "109": "This idea is orthogonal to any other CNN-based models and can be easily applied to them; in addition, as discussed in Sec.", 
        "110": "1, we did not find any other model in comparison with solving question classification tasks with answer sets.", 
        "111": "There is crucial difference between the INSURANCE and DMV datasets on one hand and the YAHOO set on the other.", 
        "112": "In INSURANCE and DMV, all questions in the same (sub)category share the same answers, whereas YAHOO provides individual answers to each question.", 
        "113": "For multi-label classification (INSURANCE and DMV), we replace the softmax layer in CNNs with a sigmoid layer which predicts each category independently while softmax is not.", 
        "114": "All experimental results are summarized in Table 2.", 
        "115": "The improvements are substantial for INSURANCE and DMV, but not as significant for YAHOO and TREC.", 
        "116": "One reason for this is the\nquestions in YAHOO/TREC are shorter, which makes the group information harder to encode.", 
        "117": "Another reason is that each question in YAHOO/TREC has a single label, and thus can not fully benefit from group sparse properties.", 
        "118": "Besides the conventional classification tasks, we also test our proposed model on an unseenlabel case.", 
        "119": "In these experiments, there are a few sub-category labels that are not included in the training data.", 
        "120": "However, we still hope that our model could still return the correct parent category for these unseen subcategories at test time.", 
        "121": "In the testing set of YAHOO dataset, we randomly add 100 questions whose subcategory labels are unseen in training set.", 
        "122": "The classification results of YAHOO-unseen in Table 2 are obtained by mapping the predicted subcategories back to top-level categories.", 
        "123": "The improvements are substantial due to the group information encoding.", 
        "124": "5 Conclusions  In order to better represent question sentences with answer sets and group structure, we first presented a novel GSA framework, a neural version of dictionary learning.", 
        "125": "We then proposed group sparse convolutional neural networks by embedding GSA into CNNs, which result in significantly better question classification over strong baselines.", 
        "126": "Acknowledgment  We thank the anonymous reviewers for their suggestions.", 
        "127": "This work is supported in part by NSF IIS-1656051, DARPA FA8750-13-2-0041 (DEFT), DARPA XAI, a Google Faculty Research Award, and an HP Gift."
    }, 
    "document_id": "P17-2053.pdf.json"
}
