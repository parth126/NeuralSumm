{
    "abstract_sentences": {
        "1": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-tosequence (seq2seq) models.", 
        "2": "By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)\u2013a well-known technique for correcting exposure bias\u2013we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value.", 
        "3": "In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training.", 
        "4": "Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 366\u2013371 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2058  1 Introduction  Sequence-to-Sequence (seq2seq) models have demonstrated excellent performance in several tasks including machine translation (Sutskever et al., 2014), summarization (Rush et al., 2015), dialogue generation (Serban et al., 2015), and image captioning (Xu et al., 2015).", 
        "2": "However, the standard cross-entropy training procedure for these models suffers from the well-known problem of exposure bias: because cross-entropy training always uses gold contexts, the states and contexts encountered during training do not match those encountered at test time.", 
        "3": "This issue has been addressed using several approaches that try to incorporate awareness of decoding choices into the training optimization.", 
        "4": "These include reinforcement learning (Ranzato et al., 2016; Bahdanau\net al., 2017), imitation learning (Daume\u0301 et al., 2009; Ross et al., 2011; Bengio et al., 2015), and beam-based approaches (Wiseman and Rush, 2016; Andor et al., 2016; Daume\u0301 III and Marcu, 2005).", 
        "5": "In this paper, we focus on one the simplest to implement and least computationally expensive approaches, scheduled sampling (Bengio et al., 2015), which stochastically incorporates contexts from previous decoding decisions into training.", 
        "6": "While scheduled sampling has been empirically successful, its training objective has a drawback: because the procedure directly incorporates greedy decisions at each time step, the objective is discontinuous at parameter settings where previous decisions change their value.", 
        "7": "As a result, gradients near these points are non-informative and scheduled sampling has difficulty assigning credit for errors.", 
        "8": "In particular, the gradient does not provide information useful in distinguishing between local errors without future consequences and cascading errors which are more serious.", 
        "9": "Here, we propose a novel approach based on scheduled sampling that uses a differentiable approximation of previous greedy decoding decisions inside the training objective by incorporating a continuous relaxation of argmax.", 
        "10": "As a result, our end-to-end relaxed greedy training objective is differentiable everywhere and fully continuous.", 
        "11": "By making the objective continuous at points where previous decisions change value, our approach provides gradients that can respond to cascading errors.", 
        "12": "In addition, we demonstrate a related approximation and reparametrization for sample-based training (another training scenario considered by scheduled sampling (Bengio et al., 2015)) that can yield stochastic gradients with lower variance than in standard scheduled sampling.", 
        "13": "In our experiments on two different tasks, machine translation (MT) and named entity recognition (NER), we show that our approach outperforms both cross-entropy training and standard\n366\nscheduled sampling procedures with greedy and sampled-based training.", 
        "14": "2 Discontinuity in Scheduled Sampling  While scheduled sampling (Bengio et al., 2015) is an effective way to rectify exposure bias, it cannot differentiate between cascading errors, which can lead to a sequence of bad decisions, and local errors, which have more benign effects.", 
        "15": "Specifically, scheduled sampling focuses on learning optimal behavior in the current step given the fixed decoding decision of the previous step.", 
        "16": "If a previous bad decision is largely responsible for the current error, the training procedure has difficulty adjusting the parameters accordingly.", 
        "17": "The following machine translation example highlights this credit assignment issue:\nRef: The cat purrs .", 
        "18": "Pred: The dog barks .", 
        "19": "At step 3, the model prefers the word \u2018barks\u2019 after incorrectly predicting \u2018dog\u2019 at step 2.", 
        "20": "To correct this error, the scheduled sampling procedure would increase the score of \u2018purrs\u2019 at step 3, conditioned on the fact that the model predicted (incorrectly) \u2018dog\u2019 at step 2, which is not the ideal learning behaviour.", 
        "21": "Ideally, the model should be able to backpropagate the error from step 3 to the source of the problem which occurred at step 2, where \u2018dog\u2019 was predicted instead of \u2018cat\u2019.", 
        "22": "The lack of credit assignment during training is a result of discontinuity in the objective function used by scheduled sampling, as illustrated in Figure 1.", 
        "23": "We denote the ground truth target symbol at step i by y\u2217i , the embedding representation of word y by e(y), and the hidden state of a seq2seq decoder at step i as hi.", 
        "24": "Standard crossentropy training defines the loss at each step to be log p(y\u2217i |hi(e(y\u2217i\u22121), hi\u22121)), while scheduled sampling uses loss log p(y\u2217i |hi(e(y\u0302i\u22121), hi\u22121)), where\ny\u0302i\u22121 refers the model\u2019s prediction at the previous step.1 Here, the model prediction y\u0302i\u22121 is obtained by argmaxing over the output softmax layer.", 
        "25": "Hence, in addition to the intermediate hidden states and final softmax scores, the previous model prediction, y\u0302i\u22121, itself depends on the model parameters, \u03b8, and ideally, should be backpropagated through, unlike the gold target symbol y\u2217i\u22121 which is independent of model parameters.", 
        "26": "However, the argmax operation is discontinuous, and thus the training objective (depicted in Figure 1 as the red line) exhibits discontinuities at parameter settings where the previous decoding decisions change value (depicted as changes from \u2018kitten\u2019 to \u2018dog\u2019 to \u2018cat\u2019).", 
        "27": "Because these change points represent discontinuities, their gradients are undefined and the effect of correcting an earlier mistake (for example \u2018dog\u2019 to \u2018cat\u2019) as the training procedure approaches such a point is essentially hidden.", 
        "28": "In our approach, described in detail in the next section, we attempt to fix this problem by incorporating a continuous relaxation of the argmax operation into the scheduled sampling procedure in order to form an approximate but fully continuous objective.", 
        "29": "Our relaxed approximate objective is depicted in Figure 1 as blue and purple lines, depending on temperature parameter \u03b1which tradesoff smoothness and quality of approximation.", 
        "30": "3 Credit Assignment via Relaxation  In this section we explain in detail the continuous relaxation of greedy decoding that we will use to build a fully continuous training objective.", 
        "31": "We also introduce a related approach for sample-based training.", 
        "32": "1For the sake of simplicity, the \u2018always sample\u2019 variant of scheduled sampling is described (Bengio et al., 2015).", 
        "33": "3.1 Soft Argmax  In scheduled sampling, the embedding for the best scoring word at the previous step is passed as an input to the current step.", 
        "34": "This operation2 can be expressed as\ne\u0302i\u22121 = \u2211\ny\ne(y)1[\u2200y\u2032 6= y si\u22121(y) > si\u22121(y\u2032)]\nwhere y is a word in the vocabulary, si\u22121(y) is the output score of that word at the previous step, and e\u0302i\u22121 is the embedding passed to the next step.", 
        "35": "This operation can be relaxed by replacing the indicator function with a peaked softmax function with hyperparameter \u03b1 to define a soft argmax procedure:\ne\u0304i\u22121 = \u2211\ny\ne(y) \u00b7 exp (\u03b1 si\u22121(y))\u2211 y\u2032 exp (\u03b1 si\u22121(y \u2032))\nAs \u03b1 \u2192 \u221e, the equation above approaches the true argmax embedding.", 
        "36": "Hence, with a finite and large \u03b1, we get a linear combination of all the words (and therefore a continuous function of the parameters) that is dominated heavily by the word with maximum score.", 
        "37": "3.2 Soft Reparametrized Sampling  Another variant of scheduled sampling is to pass a sampled embedding from the softmax distribution at the previous step to the current step instead of the argmax.", 
        "38": "This is expected to enable better exploration of the search space during optimization due to the added randomness and hence result in a more robust model.", 
        "39": "In this section, we discuss and review an approximation to the Gumbel reparametrization trick that we use as a module in our sample-based decoder.", 
        "40": "This approximation was proposed by Maddison et al.", 
        "41": "(2017) and Jang et al.", 
        "42": "(2016), who showed that the same soft argmax operation introduced above can be used for reducing variance of stochastic gradients when sampling from softmax distributions.", 
        "43": "Unlike soft argmax, this approach is not a fully continuous approximation to the sampling operation, but it does result in much more informative gradients compared to naive scheduled sampling procedure.", 
        "44": "The Gumbel reparametrization trick shows that sampling from a categorical distribution can be refactored into sampling from a simple distribution followed by a deterministic transformation\n2Assuming there are no ties for the sake of simplicity.", 
        "45": "as follows: (i) sampling an independent Gumbel noise G for each element in the categorical distribution, typically done by transforming a sample from the uniform distribution: U \u223c Uniform(0, 1) as G = \u2212log(\u2212log U), then (ii) adding it componentwise to the unnormalized score of each element, and finally (iii) taking an argmax over the vector.", 
        "46": "Using the same argmax softening procedure as above, they arrive at an approximation to the reparametrization trick which mitigates some of the gradient\u2019s variance introduced by sampling.", 
        "47": "The approximation is3:\ne\u0303i\u22121 = \u2211\ny\ne(y) \u00b7 exp (\u03b1 (si\u22121(y) +Gy))\u2211 y\u2032 exp (\u03b1 (si\u22121(y \u2032) +Gy\u2032))\nWe will use this \u2018concrete\u2019 approximation of softmax sampling in our relaxation of scheduled sampling with a sample-based decoder.", 
        "48": "We discuss details in the next section.", 
        "49": "Note that our original motivation based on removing discontinuity does not strictly apply to this sampling procedure, which still yields a stochastic gradient due to sampling from the Gumbel distribution.", 
        "50": "However, this approach is conceptually related to greedy relaxations since, here, the soft argmax reparametrization reduces gradient variance which may yield a more informative training signal.", 
        "51": "Intuitively, this approach results in the gradient of the loss to be more aware of the sampling procedure compared to naive scheduled sampling and hence carries forward information about decisions made at previous steps.", 
        "52": "The empirical results, discussed later, show similar gains to the greedy scenario.", 
        "53": "3.3 Differentiable Relaxed Decoders  With the argmax relaxation introduced above, we have a recipe for a fully differentiable greedy decoder designed to produce informative gradients near change points.", 
        "54": "Our final training network for scheduled sampling with relaxed greedy decoding is shown in Figure 2.", 
        "55": "Instead of conditioning the current hidden state, hi, on the argmax embedding from the previous step, e\u0302i\u22121, we use the \u03b1-soft argmax embedding, e\u0304i\u22121, defined in Section 3.1.", 
        "56": "This removes the discontinuity in the original greedy scheduled sampling objective by passing a linear combination of embeddings, dominated by the argmax, to the next step.", 
        "57": "Figure 1\n3This is different from using the expected softmax embedding because our approach approximates the actual sampling process instead of linearly weighting the embeddings by their softmax probabilities\nillustrates the effect of varying \u03b1.", 
        "58": "As \u03b1 increases, we more closely approximate the greedy decoder.", 
        "59": "As in standard scheduled sampling, here we minimize the cross-entropy based loss at each time step.", 
        "60": "Hence the computational complexity of our approach is comparable to standard seq2seq training.", 
        "61": "As we discuss in Section 5, mixing model predictions randomly with ground truth symbols during training (Bengio et al., 2015; Daume\u0301 et al., 2009; Ross et al., 2011), while annealing the probability of using the ground truth with each epoch, results in better models and more stable training.", 
        "62": "As a result, training is reliant on the annealing schedule of two important hyperparameters: i) ground truth mixing probability and ii) the \u03b1 parameter used for approximating the argmax function.", 
        "63": "For output prediction, at each time step, we can still output the hard argmax, depicted in Figure 2.", 
        "64": "For the case of scheduled sampling with sample-based training\u2013where decisions are sampled rather than chosen greedily (Bengio et al., 2015)\u2013we conduct experiments using a related training procedure.", 
        "65": "Instead of using soft argmax, we use the soft sample embedding, e\u0303i\u22121, defined in Section 3.2.", 
        "66": "Apart from this difference, training is carried out using the same procedure.", 
        "67": "4 Related Work  Gormley et al.", 
        "68": "(2015)\u2019s approximation-aware training is conceptually related, but focuses on variational decoding procedures.", 
        "69": "Hoang et al.", 
        "70": "(2017) also propose continuous relaxations of decoders, but are focused on developing better inference procedures.", 
        "71": "Grefenstette et al.", 
        "72": "(2015) successfully use a soft approximation to argmax in neural stack mechanisms.", 
        "73": "Finally, Ranzato et al.", 
        "74": "(2016) experiment with a similarly motivated objective that was not fully continuous, but found it performed worse than the standard training.", 
        "75": "5 Experimental Setup  We perform experiments with machine translation (MT) and named entity recognition (NER).", 
        "76": "Data: For MT, we use the same dataset (the German-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014)), preprocessing and data splits as Ranzato et al.", 
        "77": "(2016).", 
        "78": "For named entity recognition, we use the CONLL 2003 shared task data (Tjong\nKim Sang and De Meulder, 2003) for German language and use the provided data splits.", 
        "79": "We perform no preprocessing on the data.The output vocabulary length for MT is 32000 and 10 for NER.", 
        "80": "Implementation details: For MT, we use a seq2seq model with a simple attention mechanism (Bahdanau et al., 2015), a bidirectional LSTM encoder (1 layer, 256 units), and an LSTM decoder (1 layer, 256 units).", 
        "81": "For NER, we use a seq2seq model with an LSTM encoder (1 layer, 64 units) and an LSTM decoder (1 layer, 64 units) with a fixed attention mechanism that deterministically attends to the ith input token when decoding the ith output, and hence does not involve learning of attention parameters.", 
        "82": "4\nHyperparameter tuning: We start by training with actual ground truth sequences for the first epoch and decay the probability of selecting the ground truth token as an inverse sigmoid (Bengio et al., 2015) of epochs with a decay strength parameter k. We also tuned for different values of \u03b1 and explore the effect of varying \u03b1 exponentially (annealing) with the epochs.", 
        "83": "In table 1, we report results for the best performing configuration of decay parameter and the \u03b1 parameter on the validation set.", 
        "84": "To account for variance across randomly started runs, we ran multiple random restarts (RR) for all the systems evaluated and always used the RR with the best validation set score to calculate test performance.", 
        "85": "Comparison We report validation and test metrics for NER and MT tasks in Table 1, F1 and BLEU respectively.", 
        "86": "\u2018Greedy\u2019 in the table refers to scheduled sampling with soft argmax decisions (either soft or hard) and \u2018Sample\u2019 refers the corresponding reparametrized sample-based decoding scenario.", 
        "87": "We compare our approach with two baselines: standard cross-entropy loss minimization for seq2seq models (\u2018Baseline CE\u2019) and the standard scheduled sampling procedure (Bengio et al.", 
        "88": "(2015)).", 
        "89": "We report results for two variants of our approach: one with a fixed \u03b1 parameter throughout the training procedure (\u03b1-soft fixed), and the other in which we vary \u03b1 exponentially with the number of epochs (\u03b1-soft annealed).", 
        "90": "4Fixed attention refers to the scenario when we use the bidirectional LSTM encoder representation of the source sequence token at time step t while decoding at time step t instead of using a linear combination of all the input sequences weighted according to the attention parameters in the standard attention mechanism based models.", 
        "91": "6 Results  All three approaches improve over the standard cross-entropy based seq2seq training.", 
        "92": "Moreover, both approaches using continuous relaxations (greedy and sample-based) outperform standard scheduled sampling (Bengio et al., 2015).", 
        "93": "The best results for NER were obtained with the relaxed greedy decoder with annealed \u03b1 which yielded an F1 gain of +3.1 over the standard seq2seq baseline and a gain of +1.5 F1 over standard scheduled sampling.", 
        "94": "For MT, we obtain the best results with the relaxed sample-based decoder, which yielded a gain of +1.5 BLEU over standard seq2seq and a gain of +0.75 BLEU over standard scheduled sampling.", 
        "95": "We observe that the reparametrized samplebased method, although not fully continuous endto-end unlike the soft greedy approach, results in good performance on both the tasks, particularly MT.", 
        "96": "This might be an effect of stochastic exploration of the search space over the output sequences during training and hence we expect MT to benefit from sampling due to a much larger search space associated with it.", 
        "97": "We also observe that annealing \u03b1 results in good performance which suggests that a smoother approximation to the loss function in the initial stages of training is helpful in guiding the learning in the right direction.", 
        "98": "However, in our experiments we noticed that\nthe performance while annealing \u03b1 was sensitive to the hyperparameter associated with the annealing schedule of the mixing probability in scheduled sampling during training.", 
        "99": "The computational complexity of our approach is comparable to that of standard seq2seq training.", 
        "100": "However, instead of a vocabulary-sized max and lookup, our approach requires a matrix multiplication.", 
        "101": "Practically, we observed that on GPU hardware, all the models for both the tasks had similar speeds which suggests that our approach leads to accuracy gains without compromising run-time.", 
        "102": "Moreover, as shown in Table 2, we observe that a gradual decay of mixing probability consistently compared favorably to more aggressive decay schedules.", 
        "103": "We also observed that the \u2018always sample\u2019 case of relaxed greedy decoding, in which we never mix in ground truth inputs (see Bengio et al.", 
        "104": "(2015)), worked well for NER but resulted in unstable training for MT.", 
        "105": "We reckon that this is an effect of large difference between the search space associated with NER and MT.", 
        "106": "7 Conclusion  Our positive results indicate that mechanisms for credit assignment can be useful when added to the models that aim to ameliorate exposure bias.", 
        "107": "Further, our results suggest that continuous relaxations of the argmax operation can be used as effective approximations to hard decoding during training.", 
        "108": "Acknowledgements  We thank Graham Neubig for helpful discussions.", 
        "109": "We also thank the three anonymous reviewers for their valuable feedback."
    }, 
    "document_id": "P17-2058.pdf.json"
}
