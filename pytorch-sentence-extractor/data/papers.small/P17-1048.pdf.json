{
    "abstract_sentences": {
        "1": "End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and contextdependency trees, leading to a greatly simplified model-building process.", 
        "2": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming.", 
        "3": "This paper proposes a joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding.", 
        "4": "We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chinese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 518\u2013529 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1048  1 Introduction  Automatic speech recognition (ASR) is currently a mature set of technologies that have been widely deployed, resulting in great success in interface applications such as voice search.", 
        "2": "A typical ASR system is factorized into several modules including acoustic, lexicon, and language models based on a probabilistic noisy channel model (Jelinek, 1976).", 
        "3": "Over the last decade, dramatic improvements in acoustic and language models have been\ndriven by machine learning techniques known as deep learning (Hinton et al., 2012).", 
        "4": "However, current systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques.", 
        "5": "For example, when we build an acoustic model from scratch, we have to first build hidden Markov model (HMM) and Gaussian mixture model (GMM) followed by deep neural networks (DNN).", 
        "6": "In addition, the factorization of acoustic, lexicon, and language models is derived by conditional independence assumptions (especially Markov assumptions), although the data do not necessarily follow such assumptions leading to model misspecification.", 
        "7": "This factorization form also yields a local optimum since the above modules are optimized separately.", 
        "8": "Further, to well factorize acoustic and language models, the system requires linguistic knowledge based on a lexicon model, which is usually based on a hand-crafted pronunciation dictionary to map word to phoneme sequence.", 
        "9": "In addition to the pronunciation dictionary issue, some languages, which do not explicitly have a word boundary, need languagespecific tokenization modules (Kudo et al., 2004; Bird, 2006) for language modeling.", 
        "10": "Finally, inference/decoding has to be performed by integrating all modules resulting in complex decoding.", 
        "11": "Consequently, it is quite difficult for non-experts to use/develop ASR systems for new applications, especially for new languages.", 
        "12": "End-to-end ASR has the goal of simplifying the above module-based architecture into a singlenetwork architecture within a deep learning framework, in order to address the above issues.", 
        "13": "There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connectionist temporal classification (CTC), uses Markov\n518\nassumptions to efficiently solve sequential problems by dynamic programming (Chorowski et al., 2014; Graves and Jaitly, 2014).", 
        "14": "The attention-based end-to-end method solves the ASR problem as a sequence mapping from speech feature sequences to text by using encoderdecoder architecture.", 
        "15": "The decoder network uses an attention mechanism to find an alignment between each element of the output sequence and the hidden states generated by the acoustic encoder network for each frame of acoustic input (Chorowski et al., 2014, 2015; Chan et al., 2015; Lu et al., 2016).", 
        "16": "This basic temporal attention mechanism is too flexible in the sense that it allows extremely non-sequential alignments.", 
        "17": "This may be fine for applications such as machine translation where input and output word order are different (Bahdanau et al., 2014; Wu et al., 2016).", 
        "18": "However, in speech recognition, the feature inputs and corresponding letter outputs generally proceed in the same order.", 
        "19": "Another problem is that the input and output sequences in ASR can have very different lengths, and these vary greatly from case to case, depending on the speaking rate and writing system, making it more difficult to track the alignment.", 
        "20": "However, an advantage is that the attention mechanism does not require any conditional independence assumptions, and could address all the problems cited above.", 
        "21": "Although the alignment problems of attention-based mechanisms have been partially addressed in (Chorowski et al., 2014; Chorowski and Jaitly, 2016) using various mechanisms, here we propose more rigorous constraints by using CTC-based alignment to guide the decoding.", 
        "22": "CTC permits an efficient computation of a strictly monotonic alignment using dynamic programming (Graves et al., 2006; Graves and Jaitly, 2014) although it requires language models and graph-based decoding (Miao et al., 2015) except in the case of huge training data (Amodei et al., 2015; Soltau et al., 2016).", 
        "23": "We propose to take advantage of the constrained CTC alignment in a hybrid CTC/attention based system during decoding.", 
        "24": "The proposed method adopts a CTC/attention hybrid architecture, which was originally designed to regularize an attention-based encoder network by additionally using a CTC during training (Kim et al., 2017).", 
        "25": "The proposed method extends the architecture to perform one-pass/rescoring joint de-\ncoding, where hypotheses of attention-based ASR are boosted by scores obtained by using CTC outputs.", 
        "26": "This greatly reduces irregular alignments without any heuristic search techniques.", 
        "27": "The proposed method is applied to Japanese and Mandarin ASR tasks, which require extra linguistic resources including morphological analyzer (Kudo et al., 2004) or word segmentation (Xue et al., 2003) in addition to pronunciation dictionary to provide accurate lexicon and language models in conventional DNN/HMM ASR.", 
        "28": "Surprisingly, the method achieved performance comparable to, and in some cases superior to, several state-of-the-art DNN/HMM ASR systems, without using the above linguistic resources.", 
        "29": "2 From DNN/HMM to end-to-end ASR  This section briefly provides a formulation of conventional DNN/HMM ASR and CTC or attention based end-to-end ASR.", 
        "30": "2.1 Conventional DNN/HMM ASR  ASR deals with a sequence mapping from T - length speech feature sequence X = {xt \u2208 RD|t = 1, \u00b7 \u00b7 \u00b7 , T} to N -length word sequence W = {wn \u2208 V|n = 1, \u00b7 \u00b7 \u00b7 , N}.", 
        "31": "xt is a D dimensional speech feature vector (e.g., log Mel filterbanks) at frame t and wn is a word at position n in vocabulary V .", 
        "32": "ASR is mathematically formulated with the Bayes decision theory, where the most probable word sequence W\u0302 is estimated among all possible word sequences V\u2217 as follows:\nW\u0302 = arg max W\u2208V\u2217\np(W |X).", 
        "33": "(1)\nThe posterior distribution p(W |X) is factorized into the following three distributions by using the Bayes theorem and introducing HMM state sequence S = {st \u2208 {1, \u00b7 \u00b7 \u00b7 , J}|t = 1, \u00b7 \u00b7 \u00b7 , T}:\nEq.", 
        "34": "(1) \u2248 arg max W\n\u2211\nS\np(X|S)p(S|W )p(W ).", 
        "35": "The three factors, p(X|S), p(S|W ), and p(W ), are acoustic, lexicon, and language models, respectively.", 
        "36": "These are further factorized by using a probabilistic chain rule and conditional independence assumption as follows:    p(X|S) \u2248\u220ft p(st|xt) p(st) ,\np(S|W )\u2248\u220ft p(st|st\u22121,W ), p(W ) \u2248\u220fn p(wn|wn\u22121, .", 
        "37": ".", 
        "38": ".", 
        "39": ", wn\u2212m\u22121),\nwhere the acoustic model is replaced with the product of framewise posterior distributions p(st|xt) computed by powerful DNN classifiers by using so-called pseudo likelihood trick (Bourlard and Morgan, 1994).", 
        "40": "p(st|st\u22121,W ) is represented by an HMM state transition given W , and the conversion from W to HMM states is deterministically performed by using a pronunciation dictionary through a phoneme representation.", 
        "41": "p(wn|wn\u22121, .", 
        "42": ".", 
        "43": ".", 
        "44": ", wn\u2212m\u22121) is obtained based on an (m \u2212 1)th-order Markov assumption as a mgram model.", 
        "45": "These conditional independence assumptions are often regarded as too strong assumption, leading to model mis-specification.", 
        "46": "Also, to train the framewise posterior p(st|xt), we have to provide a framewise state alignment st as a target, which is often provided by a GMM/HMM system.", 
        "47": "Thus, conventional DNN/HMM systems make the ASR problem formulated with Eq.", 
        "48": "(1) feasible by using factorization and conditional independence assumptions, at the cost of the problems discussed in Section 1.", 
        "49": "2.2 Connectionist Temporal Classification (CTC)  The CTC formulation also follows from Bayes decision theory (Eq.", 
        "50": "(1)).", 
        "51": "Note that the CTC formulation uses L-length letter sequence C = {cl \u2208 U|l = 1, \u00b7 \u00b7 \u00b7 , L} with a set of distinct letters U .", 
        "52": "Similarly to Section 2.1, by introducing framewise letter sequence with an additional \u201dblank\u201d ( < b >) symbol Z = {zt \u2208 U \u222a < b >|t = 1, \u00b7 \u00b7 \u00b7 , T}, and by using the probabilistic chain rule and conditional independence assumption, the posterior distribution p(C|X) is factorized as follows:\np(C|X) \u2248 \u2211\nZ\n\u220f\nt p(zt|zt\u22121, C)p(zt|X) \ufe38 \ufe37\ufe37 \ufe38\n,pctc(C|X)\np(C)\np(Z)\n(2)\nAs a result, CTC has three distribution components similar to the DNN/HMM case, i.e., framewise posterior distribution p(zt|X), transition probability p(zt|zt\u22121, C)1, and prior distributions of letter and hidden-state sequences,\n1Note that in the implementation, the transition value is not normalized (i.e., not a probabilistic value) (Graves and Jaitly, 2014; Miao et al., 2015), similar to the HMM state transition implementation (Povey et al., 2011)\np(C) and p(Z), respectively.", 
        "53": "We also define the CTC objective function pctc(C|X) used in the later formulation.", 
        "54": "The framewise posterior distribution p(zt|X) is conditioned on all inputs X , and it is quite natural to be modeled by using bidirectional long short-term memory (BLSTM): p(zt|X) = Softmax(Lin(ht)) and ht = BLSTM(X).", 
        "55": "Softmax(\u00b7) is a sofmax activation function, and Lin(\u00b7) is a linear layer to convert hidden vector ht to a (|U|+ 1) dimensional vector (+1 means a blank symbol introduced in CTC).", 
        "56": "Although Eq.", 
        "57": "(2) has to deal with a summation over all possible Z, it is efficiently computed by using dynamic programming (Viterbi/forwardbackward algorithm) thanks to the Markov property.", 
        "58": "In summary, although CTC and DNN/HMM systems are similar to each other due to conditional independence assumptions, CTC does not require pronunciation dictionaries and omits an GMM/HMM construction step.", 
        "59": "2.3 Attention mechanism  Compared with hybrid and CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(C|X) based on a probabilistic chain rule, as follows:\np(C|X) = \u220f\nl p(cl|c1, \u00b7 \u00b7 \u00b7 , cl\u22121, X) \ufe38 \ufe37\ufe37 \ufe38\n,patt(C|X)\n, (3)\nwhere patt(C|X) is an attention-based objective function.", 
        "60": "p(cl|c1, \u00b7 \u00b7 \u00b7 , cl\u22121, X) is obtained by\np(cl|c1, \u00b7 \u00b7 \u00b7 , cl\u22121, X) = Decoder(rl,ql\u22121, cl\u22121) ht = Encoder(X) (4)\nalt = Attention({al\u22121}t,ql\u22121,ht) (5) rl = \u2211\nt\naltht.", 
        "61": "(6)\nEq.", 
        "62": "(4) converts input feature vectors X into a framewise hidden vector ht in an encoder network based on BLSTM, i.e., Encoder(X) , BLSTM(X).", 
        "63": "Attention(\u00b7) in Eq.", 
        "64": "(5) is based on a content-based attention mechanism with convolutional features, as described in (Chorowski et al., 2015) (see Appendix A).", 
        "65": "alt is an attention weight, and represents a soft alignment of hidden vector ht for each output cl based on the weighted summation of hidden vectors to form letter-wise hidden vector rl in Eq.", 
        "66": "(6).", 
        "67": "A decoder network is another\nrecurrent network conditioned on previous output cl\u22121 and hidden vector ql\u22121, similar to RNNLM, in addition to letter-wise hidden vector rl.", 
        "68": "We use Decoder(\u00b7) , Softmax(Lin(LSTM(\u00b7))).", 
        "69": "Attention-based ASR does not explicitly separate each module, and potentially handles the all issues pointed out in Section 1.", 
        "70": "It implicitly combines acoustic models, lexicon, and language models as encoder, attention, and decoder networks, which can be jointly trained as a single deep neural network.", 
        "71": "Compared with DNN/HMM and CTC, which are based on a transition form from t \u2212 1 to t due to the Markov assumption, the attention mechanism does not maintain this constraint, and often provides irregular alignments.", 
        "72": "A major focus of this paper is to address this problem by using joint CTC/attention decoding.", 
        "73": "3 Joint CTC/attention decoding  This section explains a hybrid CTC/attention network, which potentially utilizes both benefits of CTC and attention in ASR.", 
        "74": "3.1 Hybrid CTC/attention architecture  Kim et al.", 
        "75": "(2017) uses a CTC objective function as an auxiliary task to train the attention model encoder within the multitask learning (MTL) framework, and this paper also uses the same architecture.", 
        "76": "Figure 1 illustrates the overall architecture of the framework, where the same BLSTM is shared with CTC and attention encoder networks, respectively).", 
        "77": "Unlike the sole attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences during training.", 
        "78": "That is, rather than solely depending on data-driven attention methods to estimate the desired alignments in long sequences, the forward-backward algorithm in CTC helps to speed up the process of estimating the desired alignment.", 
        "79": "The objective to be maximized is a logarithmic linear combination of the CTC and attention objectives, i.e., pctc(C|X) in Eq.", 
        "80": "(2) and patt(C|X) in Eq.", 
        "81": "(3): LMTL = \u03bb log pctc(C|X) + (1\u2212 \u03bb) log patt(C|X),\n(7)\nwith a tunable parameter \u03bb : 0 \u2264 \u03bb \u2264 1.", 
        "82": "3.2 Decoding strategies  The inference step of our joint CTC/attentionbased end-to-end speech recognition is performed\nby label synchronous decoding with a beam search similar to conventional attention-based ASR.", 
        "83": "However, we take the CTC probabilities into account to find a hypothesis that is better aligned to the input speech, as shown in Figure 1.", 
        "84": "Hereafter, we describe the general attention-based decoding and conventional techniques to mitigate the alignment problem.", 
        "85": "Then, we propose joint decoding methods with a hybrid CTC/attention architecture.", 
        "86": "3.2.1 Attention-based decoding in general  End-to-end speech recognition inference is generally defined as a problem to find the most probable letter sequence C\u0302 given the speech input X , i.e.", 
        "87": "C\u0302 = arg max C\u2208U\u2217\nlog p(C|X).", 
        "88": "(8)\nIn attention-based ASR, p(C|X) is computed by Eq.", 
        "89": "(3), and C\u0302 is found by a beam search technique.", 
        "90": "Let \u2126l be a set of partial hypotheses of the length l. At the beginning of the beam search, \u21260 contains only one hypothesis with the starting symbol <sos> and the hypothesis score \u03b1(<sos>, X) is set to 0.", 
        "91": "For l = 1 to Lmax, each partial hypothesis in \u2126l\u22121 is expanded by appending possible single letters, and the new hypotheses are stored in \u2126l, where Lmax is the maximum\nlength of the hypotheses to be searched.", 
        "92": "The score of each new hypothesis is computed in the log domain as\n\u03b1(h,X) = \u03b1(g,X) + log p(c|g,X), (9)\nwhere g is a partial hypothesis in \u2126l\u22121, c is a letter appended to g, and h is the new hypothesis such that h = g \u00b7 c. If c is a special symbol that represents the end of a sequence, <eos>, h is added to \u2126\u0302 but not \u2126l, where \u2126\u0302 denotes a set of complete hypotheses.", 
        "93": "Finally, C\u0302 is obtained by\nC\u0302 = arg max h\u2208\u2126\u0302 \u03b1(h,X).", 
        "94": "(10)\nIn the beam search process, \u2126l is allowed to hold only a limited number of hypotheses with higher scores to improve the search efficiency.", 
        "95": "Attention-based ASR, however, may be prone to include deletion and insertion errors because of its flexible alignment property, which can attend to any portion of the encoder state sequence to predict the next label, as discussed in Section 2.3.", 
        "96": "Since attention is generated by the decoder network, it may prematurely predict the end-ofsequence label, even when it has not attended to all of the encoder frames, making the hypothesis too short.", 
        "97": "On the other hand, it may predict the next label with a high probability by attending to the same portions as those attended to before.", 
        "98": "In this case, the hypothesis becomes very long and includes repetitions of the same letter sequence.", 
        "99": "3.2.2 Conventional decoding techniques  To alleviate the alignment problem, a length penalty term is commonly used to control the hypothesis length to be selected (Chorowski et al., 2015; Bahdanau et al., 2016).", 
        "100": "With the length penalty, the decoding objective in Eq.", 
        "101": "(8) is changed to\nC\u0302 = arg max C\u2208U\u2217\n{log p(C|X) + \u03b3|C|} , (11)\nwhere |C| is the length of the sequence C, and \u03b3 is a tunable parameter.", 
        "102": "However, it is actually difficult to completely exclude hypotheses that are too long or too short even if \u03b3 is carefully tuned.", 
        "103": "It is also effective to control the hypothesis length by the minimum and maximum lengths to some extent, where the minimum and maximum are selected as fixed ratios to the length of the input speech.", 
        "104": "However, since there are exceptionally long or short transcripts compared to the input\nspeech, it is difficult to balance saving such exceptional transcripts and preventing hypotheses with irrelevant lengths.", 
        "105": "Another approach is the coverage term recently proposed in (Chorowski and Jaitly, 2016), which is incorporated in the decoding objective in Eq.", 
        "106": "(11) as\nC\u0302 = arg max C\u2208U\u2217\n{log p(C|X) + \u03b3|C|\n+\u03b7 \u00b7 coverage(C|X)} , (12)\nwhere the coverage term is computed by\ncoverage(C|X) = T\u2211\nt=1\n[ L\u2211\nl=1\nalt > \u03c4\n] .", 
        "107": "(13)\n\u03b7 and \u03c4 are tunable parameters.", 
        "108": "The coverage term represents the number of frames that have received a cumulative attention greater than \u03c4 .", 
        "109": "Accordingly, it increases when paying close attention to some frames for the first time, but does not increase when paying attention again to the same frames.", 
        "110": "This property is effective for avoiding looping of the same label sequence within a hypothesis.", 
        "111": "However, it is still difficult to obtain a common parameter setting for \u03b3, \u03b7, \u03c4 , and the optional min/max lengths so that they are appropriate for any speech data from different tasks.", 
        "112": "3.2.3 Joint decoding  Our joint CTC/attention approach combines the CTC and attention-based sequence probabilities in the inference step, as well as the training step.", 
        "113": "Suppose pctc(C|X) in Eq.", 
        "114": "(2) and patt(C|X) in Eq.", 
        "115": "(3) are the sequence probabilities given by CTC and the attention model.", 
        "116": "The decoding objective is defined similarly to Eq.", 
        "117": "(7) as\nC\u0302 = arg max C\u2208U\u2217\n{\u03bb log pctc(C|X)\n+(1\u2212 \u03bb) log patt(C|X)} .", 
        "118": "(14)\nThe CTC probability enforces a monotonic alignment that does not allow large jumps or looping of the same frames.", 
        "119": "Accordingly, it is possible to choose a hypothesis with a better alignment and exclude irrelevant hypotheses without relying on the coverage term, length penalty, or min/max lengths.", 
        "120": "In the beam search process, the decoder needs to compute a score for each partial hypothesis using Eq.", 
        "121": "(9).", 
        "122": "However, it is nontrivial to combine the CTC and attention-based scores in the beam\nsearch, because the attention decoder performs it output-label-synchronously while CTC performs it frame-synchronously.", 
        "123": "To incorporate the CTC probabilities in the hypothesis score, we propose two methods.", 
        "124": "Rescoring The first method is a two-pass approach, in which the first pass obtains a set of complete hypotheses using the beam search, where only the attentionbased sequence probabilities are considered.", 
        "125": "The second pass rescores the complete hypotheses using the CTC and attention probabilities, where the CTC probabilities are obtained by the forward algorithm for CTC (Graves et al., 2006).", 
        "126": "The rescoring pass obtains the final result according to\nC\u0302 = arg max h\u2208\u2126\u0302 {\u03bb\u03b1ctc(h,X) + (1\u2212 \u03bb)\u03b1att(h,X)} ,\n(15)\nwhere { \u03b1ctc(h,X) , log pctc(h|X) \u03b1att(h,X) , log patt(h|X) .", 
        "127": "(16)\nOne-pass decoding The second method is one-pass decoding, in which we compute the probability of each partial hypothesis using CTC and an attention model.", 
        "128": "Here, we utilize the CTC prefix probability (Graves, 2008) defined as the cumulative probability of all label sequences that have the partial hypothesis h as their prefix:\npctc(h, .", 
        "129": ".", 
        "130": ".", 
        "131": "|X) = \u2211\n\u03bd\u2208(U\u222a{<eos>})+ pctc(h \u00b7 \u03bd|X),\nand we define the CTC score as\n\u03b1ctc(h,X) , log pctc(h, .", 
        "132": ".", 
        "133": ".", 
        "134": "|X), (17)\nwhere \u03bd represents all possible label sequences except the empty string.", 
        "135": "The CTC score cannot be obtained recursively as in Eq.", 
        "136": "(9), but it can be computed efficiently by keeping the forward probabilities over the input frames for each partial hypothesis.", 
        "137": "Then it is combined with \u03b1att(h,X).", 
        "138": "The beam search algorithm for one-pass decoding is shown in Algorithm 1.", 
        "139": "\u2126l and \u2126\u0302 are initialized in lines 2 and 3 of the algorithm, which are implemented as queues that accept partial hypotheses of the length l and complete hypotheses, respectively.", 
        "140": "In lines 4\u201325, each partial hypothesis g in \u2126l\u22121 is extended by each label c\nAlgorithm 1 Joint CTC/attention one-pass decoding 1: procedure ONEPASSBEAMSEARCH(X ,Lmax) 2: \u21260 \u2190 {<sos>} 3: \u2126\u0302\u2190 \u2205 4: for l = 1 .", 
        "141": ".", 
        "142": ".", 
        "143": "Lmax do 5: \u2126l \u2190 \u2205 6: while \u2126l\u22121 6= \u2205 do 7: g \u2190 HEAD(\u2126l\u22121) 8: DEQUEUE(\u2126l\u22121) 9: for each c \u2208 U \u222a {<eos>} do 10: h\u2190 g \u00b7 c 11: \u03b1(h,X)\u2190\u03bb\u03b1ctc(h,X)+(1\u2212\u03bb)\u03b1att(h,X) 12: if c = <eos> then 13: ENQUEUE(\u2126\u0302, h) 14: else 15: ENQUEUE(\u2126l, h) 16: if |\u2126l| > beamWidth then 17: REMOVEWORST(\u2126l) 18: end if 19: end if 20: end for 21: end while 22: if ENDDETECT(\u2126\u0302, l) = true then 23: break .", 
        "144": "exit for loop 24: end if 25: end for 26: return arg maxh\u2208\u2126\u0302 \u03b1(h,X) 27: end procedure\nin the label set U .", 
        "145": "Each extended hypothesis h is scored in line 11, where CTC and attentionbased scores are obtained by \u03b1ctc() and \u03b1att().", 
        "146": "After that, if c = <eos>, the hypothesis h is assumed to be complete and stored in \u2126\u0302 in line 13.", 
        "147": "If c 6= <eos>, h is stored in \u2126l in line 15, where the number of hypotheses in \u2126l is checked in line 16.", 
        "148": "If the number exceeds the beam width, the hypothesis with the worst score in \u2126l is removed by REMOVEWORST() in line 17.", 
        "149": "In line 11, the CTC and attention model scores are computed for each partial hypothesis.", 
        "150": "The attention score is easily obtained in the same manner as Eq.", 
        "151": "(9), whereas the CTC score requires a modified forward algorithm that computes it label-synchronously.", 
        "152": "The algorithm to compute the CTC score is summarized in Appendix B.", 
        "153": "By considering the attention and CTC scores during the beam search, partial hypotheses with irregular alignments can be excluded, and the number of search errors is reduced.", 
        "154": "We can optionally apply an end detection technique to reduce the computation by stopping the beam search before l reaches Lmax.", 
        "155": "Function ENDDETECT(\u2126\u0302, l) in line 22 returns true if there is little chance of finding complete hypotheses with higher scores as l increases in the future.", 
        "156": "In our implementation, the function returns true if\nM\u22121\u2211\nm=0\n[ max\nh\u2208\u2126\u0302:|h|=l\u2212m \u03b1(h,X)\u2212max h\u2032\u2208\u2126\u0302 \u03b1(h\u2032, X)<Dend\n] =M,\n(18) where Dend and M are predetermined thresholds.", 
        "157": "This equation becomes true if complete hypotheses with smaller scores are generated M times consecutively.", 
        "158": "This technique is also available in attention-based decoding and rescoring methods described in Sections 3.2.1\u20133.2.3.", 
        "159": "4 Experiments  We used Japanese and Mandarin Chinese ASR benchmarks to show the effectiveness of the proposed joint CTC/attention decoding approach.", 
        "160": "The main reason for choosing these two languages is that those ideogram languages have relatively shorter lengths for letter sequences than those in alphabet languages, which reduces computational complexities greatly, and makes it easy to handle context information in a decoder network.", 
        "161": "Our preliminary investigation shows that Japanese and Mandarin Chinese end-to-end ASR can be easily scaled up, and shows state-of-the-art performance without using various tricks developed in English tasks.", 
        "162": "Also, we would like to emphasize that the system did not use language-specific processing (e.g., morphological analyzer, Pinyin dictionary), and simply used all appeared characters in their transcriptions including Japanese syllable and Kanji, Chinese, Arabic number, and alphabet characters, as they are.", 
        "163": "4.1 Corpus of Spontaneous Japanese (CSJ)  We demonstrated ASR experiments by using the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000).", 
        "164": "CSJ is a standard Japanese ASR task based on a collection of monologue speech data including academic lectures and simulated presentations.", 
        "165": "It has a total of 581 hours of training data and three types of evaluation data, where each evaluation task consists of 10 lectures (totally 5 hours).", 
        "166": "As input features, we used 40 mel-scale filterbank coefficients, with their first and second order temporal derivatives to obtain a total of 120- dimensional feature vector per frame.", 
        "167": "The encoder was a 4-layer BLSTM with 320 cells in each layer and direction, and linear projection layer is followed by each BLSTM layer.", 
        "168": "The 2nd and 3rd\nbottom layers of the encoder read every second hidden state in the network below, reducing the utterance length by the factor of 4.", 
        "169": "We used the content-based attention mechanism (Chorowski et al., 2015), where the 10 centered convolution filters of width 100 were used to extract the convolutional features.", 
        "170": "The decoder network was a 1-layer LSTM with 320 cells.", 
        "171": "The AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2012) was used for the optimization.", 
        "172": "Dend and M in Eq (18) were set as log 1e\u221210 and 3, respectively.", 
        "173": "The hybrid CTC/attention ASR was implemented by using the Chainer deep learning toolkit (Tokui et al., 2015).", 
        "174": "Table 1 first compares the character error rate (CER) for conventional attention and MTL based end-to-end ASR without the joint decoding.", 
        "175": "\u03bb in Eq.", 
        "176": "(7) was set to 0.1.", 
        "177": "When decoding, we manually set the minimum and maximum lengths of output sequences by 0.025 and 0.15 times input sequence lengths, respectively.", 
        "178": "The length penalty \u03b3 in Eq.", 
        "179": "(11) was set to 0.1.", 
        "180": "Multitask learning (MTL) significantly outperformed attention-based ASR in the all evaluation tasks, which confirms the effectiveness of a hybrid CTC/attention architecture.", 
        "181": "Table 1 also shows that joint decoding, described in Section 3.2, further improved the performance without setting any search parameters (maximum and minimum lengths, length penalty), but only setting a weight parameter \u03bb = 0.1 in Eq.", 
        "182": "(15) similar to the MTL case.", 
        "183": "Figure 2 also compares the dependency of \u03bb on the CER for the CSJ evaluation tasks, and showing that \u03bb was not so sensitive to the performance if we set \u03bb around the value we used at MTL (i.e., 0.1).", 
        "184": "We also compare the performance of the proposed MTL-large, which has a larger network (5-layer encoder network), with the conventional state-of-the-art techniques obtained by using linguistic resources.", 
        "185": "The state-of-the-art CERs of GMM discriminative training and DNNsMBR/HMM systems are obtained from the Kaldi recipe (Moriya et al., 2015) and a system based on syllable-based CTC with MAP decoding (Kanda et al., 2016).", 
        "186": "The Kaldi recipe systems use academic lectures (236h) for AM training and all training-data transcriptions for LM training.", 
        "187": "Unlike the proposed method, these methods use linguistic resources including a morphological analyzer, pronunciation dictionary, and language model.", 
        "188": "Note that since the amount of training\nTable 1: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end ASR.", 
        "189": "Corpus of Spontaneous Japanese speech recognition (CSJ) task.", 
        "190": "Model Hour Task1 Task2 Task3 Attention 581 11.4 7.9 9.0 MTL 581 10.5 7.6 8.3 MTL + joint decoding (rescoring) 581 10.1 7.1 7.8 MTL + joint decoding (one pass) 581 10.0 7.1 7.6 MTL-large + joint decoding (rescoring) 581 8.4 6.2 6.9 MTL-large + joint decoding (one pass) 581 8.4 6.1 6.9 GMM-discr.", 
        "191": "(Moriya et al., 2015) 236 for AM, 581 for LM 11.2 9.2 12.1 DNN/HMM (Moriya et al., 2015) 236 for AM, 581 for LM 9.0 7.2 9.6 CTC-syllable (Kanda et al., 2016) 581 9.4 7.3 7.5\ndata and experimental configurations of the proposed and reference methods are different, it is difficult to compare the performance listed in the table directly.", 
        "192": "However, since the CERs of the proposed method are superior to those of the best reference results, we can state that the proposed method achieves the state-of-the-art performance.", 
        "193": "4.2 Mandarin telephone speech  We demonstrated ASR experiments on HKUST Mandarin Chinese conversational telephone speech recognition (MTS) (Liu et al., 2006).", 
        "194": "It has 5 hours recording for evaluation, and we extracted 5 hours from training data as a development set, and used the rest (167 hours) as a training set.", 
        "195": "All experimental conditions were same as those in Section 4.1 except that we used the \u03bb = 0.5 in training and decoding instead of 0.1 based on our preliminary investigation and 80 mel-scale filterbank coefficients with pitch features as suggested in (Miao et al., 2016).", 
        "196": "In decoding, we also added a result of the coverage-term based decoding (Chorowski and Jaitly, 2016), as discussed in Section 3.2\n(\u03b7 = 1.5, \u03c4 = 0.5, \u03b3 = \u22120.6 for attention model and \u03b7 = 1.0, \u03c4 = 0.5, \u03b3 = \u22120.1 for MTL), since it was difficult to eliminate the irregular alignments during decoding by only tuning the maximum and minimum lengths and length penalty (we set the minimum and maximum lengths of output sequences by 0.0 and 0.1 times input sequence lengths, respectively and set \u03b3 = 0.6 in Table 2).", 
        "197": "Table 2 shows the effectiveness of MTL and joint decoding over the attention-based approach, especially showing the significant improvement of the joint CTC/attention decoding.", 
        "198": "Similar to the CSJ experiments in Section 4.1, we did not use the length-penalty term or the coverage term in joint decoding.", 
        "199": "This is an advantage of joint decoding over conventional approaches that require many tuning parameters.", 
        "200": "We also generated more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 (speed perturb.).", 
        "201": "The final model achieved 29.9% without using linguistic resources, which defeats moderate state-of-the-art systems including CTC-based methods2.", 
        "202": "4.3 Decoding speed  We evaluated the speed of the joint decoding methods described in Section 3.2.3.", 
        "203": "ASR decoding was performed with different beam widths of 1, 3, 5, 10, and 20, and the processing time and CER were measured using a computer with Intel(R) Xeon(R) processors, E5-2690 v3, 2.6 GHz.", 
        "204": "Although the processors were multicore CPUs and the computer had GPUs, we ran the decoding program as a\n2 Although the proposed method did not reach the performance obtained by a time delayed neural network (TDNN) with lattice-free sequence discriminative training (Povey et al., 2016), our recent work scored 28.0%, and outperformed the lattice-free MMI result with advanced network architectures.", 
        "205": "single-threaded process on a CPU to investigate its basic computational cost.", 
        "206": "Table 3 shows the relationships between the real-time factor (RTF) and the CER for the CSJ and HKUST tasks.", 
        "207": "We evaluated the rescoring and one-pass decoding methods when using the end detection in Eq.", 
        "208": "(18).", 
        "209": "In every beam width, we can see that the one-pass method runs faster with an equal or lower CER than the rescoring method.", 
        "210": "This result demonstrates that the one-pass decoding is effective for reducing search errors.", 
        "211": "Finally, we achieved 1xRT with one-pass decoding when using a beam width around 3 to 5, even though it was a single-threaded process on a CPU.", 
        "212": "However, the decoding process has not yet achieved realtime ASR since CTC and the attention mechanism need to access all of the frames of the input utterance even when predicting the first label.", 
        "213": "This is an essential problem of most end-to-end ASR approaches and will be solved in future work.", 
        "214": "5 Summary and discussion  This paper proposes end-to-end ASR by using joint CTC/attention decoding, which outperformed ordinary attention-based end-to-end ASR by solving the misalignment issues.", 
        "215": "The joint decoding methods actually reduced most of the irregular alignments, which can be confirmed from the examples of recognition errors and alignment plots shown in Appendix C.\nThe proposed end-to-end ASR does not require linguistic resources, such as morphological analyzer, pronunciation dictionary, and language model, which are essential components of conventional Japanese and Mandarin Chinese ASR systems.", 
        "216": "Nevertheless, the method achieved comparable/superior performance to the state-of-theart conventional systems for the CSJ and MTS tasks.", 
        "217": "In addition, the proposed method does not require GMM/HMM construction for initial alignments, DNN pre-training, lattice generation for sequence discriminative training, complex search in decoding (e.g., FST decoder or lexical tree search based decoder).", 
        "218": "Thus, the method greatly simplifies the ASR building process, reducing code size and complexity.", 
        "219": "Future work will apply this technique to the other languages including English, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it difficult to train a decoder network.", 
        "220": "Actually, neural machine translation handles this issue by using a sub word unit (concatenating several letters to form a new sub word unit) (Wu et al., 2016), which would be a promising direction for end-toend ASR.", 
        "221": "A Location-based attention mechanism  This section provides the equations of a locationbased attention mechanism Attention(\u00b7) in Eq.", 
        "222": "(5).", 
        "223": "alt = Attention({al\u22121}t,ql\u22121,ht),\nwhere {al\u22121}t = [al\u22121,1, \u00b7 \u00b7 \u00b7 , al\u22121,T ]>.", 
        "224": "To obtain alt, we use the following equations:\n{ft}t = K \u2217 al\u22121 (19) elt = g\n>tanh(Gqql\u22121 + Ghht + Gfft + b) (20)\nalt = exp(etl)\u2211 t exp(etl)\n(21)\nK, Gq, Gh, Gf are matrix parameters.", 
        "225": "b and g are vector parameters.", 
        "226": "\u2217 denotes convolution along input feature axis twith matrix K to produce feature {ft}t.\nAlgorithm 2 CTC hypothesis score 1: function \u03b1CTC(h,X) 2: g, c\u2190 h .", 
        "227": "split h into the last label c and the rest g 3: if c = <eos> then 4: return log{\u03b3(n)T (g) + \u03b3 (b) T (g)}\n5: else 6: \u03b3(n)1 (h)\u2190 { p(z1 = c|X) if g = <sos> 0 otherwise 7: \u03b3(b)1 (h)\u2190 0 8: \u03a8\u2190 \u03b3(n)1 (h) 9: for t = 2 .", 
        "228": ".", 
        "229": ".", 
        "230": "T do\n10: \u03a6\u2190 \u03b3(b)t\u22121(g) + { 0 if last(g)=c \u03b3\n(n) t\u22121(g) otherwise\n11: \u03b3(n)t (h)\u2190 ( \u03b3 (n) t\u22121(h) + \u03a6 ) p(zt = c|X) 12: \u03b3(b)t (h) \u2190 ( \u03b3 (b) t\u22121(h) + \u03b3 (n) t\u22121(h) ) p(zt =\n<b>|X) 13: \u03a8\u2190 \u03a8 + \u03a6 \u00b7 p(zt = c|X) 14: end for 15: return log(\u03a8) 16: end if 17: end function  B CTC-based hypothesis score  The CTC score \u03b1ctc(h,X) in Eq.", 
        "231": "(17) is computed as shown in Algorithm 2.", 
        "232": "Let \u03b3(n)t (h) and \u03b3 (b) t (h) be the forward probabilities of the hypothesis h over the time frames 1 .", 
        "233": ".", 
        "234": ".", 
        "235": "t, where the superscripts (n) and (b) denote different cases in which all CTC paths end with a nonblank or blank symbol, respectively.", 
        "236": "Before starting the beam search, \u03b3\n(n) t () and \u03b3 (b) t () are initialized for t = 1, .", 
        "237": ".", 
        "238": ".", 
        "239": ", T\nas\n\u03b3 (n) t (<sos>) = 0, (22)\n\u03b3 (b) t (<sos>)=\nt\u220f\n\u03c4=1\n\u03b3 (b) \u03c4\u22121(<sos>)p(z\u03c4 =<b>|X),\n(23)\nwhere we assume that \u03b3(b)0 (<sos>) = 1 and <b> is a blank symbol.", 
        "240": "Note that the time index t and input length T may differ from those of the input utterance X owing to the subsampling technique for the encoder (Povey et al., 2016; Chan et al., 2015).", 
        "241": "In Algorithm 2, the hypothesis h is first split into the last label c and the rest g in line 2.", 
        "242": "If c is <eos>, it returns the logarithm of the forward probability assuming that h is a complete hypothesis in line 4.", 
        "243": "The forward probability of h is given by\npctc(h|X) = \u03b3(n)T (g) + \u03b3 (b) T (g) (24)\naccording to the definition of \u03b3(n)t () and \u03b3 (b) t ().", 
        "244": "If c is not <eos>, it computes the forward proba-\nbilities \u03b3(n)t (h) and \u03b3 (b) t (h), and the prefix probability \u03a8 = pctc(h, .", 
        "245": ".", 
        "246": ".", 
        "247": "|X) assuming that h is not a complete hypothesis.", 
        "248": "The initialization and recursion steps for those probabilities are described in lines 6\u201314.", 
        "249": "In this function, we assume that whenever we compute the probabilities \u03b3(n)t (h), \u03b3\n(b) t (h) and \u03a8, the forward probabilities \u03b3 (n) t (g) and \u03b3(b)t (g) have already been obtained through the beam search process because g is a prefix of h such that |g| < |h|.", 
        "250": "C Examples of irregular alignments  We list examples of irregular alignments caused by attention-based ASR.", 
        "251": "Figure 3 shows an example of repetitions of word chunks.", 
        "252": "The first chunk of blue characters in attention-based ASR (MTL) is appeared again, and the whole second chunk part becomes insertion errors.", 
        "253": "Figure 4 shows an example of deletion errors.", 
        "254": "The latter half of the sentence in attention-based ASR (MTL) is broken, which causes deletion errors.", 
        "255": "The hybrid CTC/attention with both multitask learning and joint decoding avoids these issues.", 
        "256": "Figures 5 and 6 show alignment plots corresponding to Figs.", 
        "257": "3 and 4, respectively, where X-axis shows time frames and Y-axis shows the character sequence hypothesis.", 
        "258": "These visual plots also demonstrate that the proposed joint decoding approach can suppress irregular alignments."
    }, 
    "document_id": "P17-1048.pdf.json"
}
