{
    "abstract_sentences": {
        "1": "While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks.", 
        "2": "In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoderdecoder framework.", 
        "3": "We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1150\u20131159 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1106  1 Introduction  End-to-end neural machine translation (NMT), which leverages neural networks to directly map between natural languages, has gained increasing popularity recently (Sutskever et al., 2014; Bahdanau et al., 2015).", 
        "2": "NMT proves to outperform conventional statistical machine translation (SMT) significantly across a variety of language pairs (Junczys-Dowmunt et al., 2016) and becomes the new de facto method in practical MT systems (Wu et al., 2016).", 
        "3": "However, there still remains a severe challenge: it is hard to interpret the internal workings of NMT.", 
        "4": "In SMT (Koehn et al., 2003; Chiang, 2005), the translation process can be denoted as a derivation that comprises a sequence of translation rules (e.g., phrase pairs and synchronous CFG rules).", 
        "5": "Defined on language structures with varying granularities, these translation rules are interpretable from a linguistic perspective.", 
        "6": "In contrast, NMT takes an end-to-end approach: all internal information is represented as real-valued vectors or\n\u2217Corresponding author.", 
        "7": "matrices.", 
        "8": "It is challenging to associate hidden states in neural networks with interpretable language structures.", 
        "9": "As a result, the lack of interpretability makes it very difficult to understand translation process and debug NMT systems.", 
        "10": "Therefore, it is important to develop new methods for visualizing and understanding NMT.", 
        "11": "Existing work on visualizing and interpreting neural models has been extensively investigated in computer vision (Krizhevsky et al., 2012; Mahendran and Vedaldi, 2015; Szegedy et al., 2014; Simonyan et al., 2014; Nguyen et al., 2015; Girshick et al., 2014; Bach et al., 2015).", 
        "12": "Although visualizing and interpreting neural models for natural language processing has started to attract attention recently (Karpathy et al., 2016; Li et al., 2016), to the best of our knowledge, there is no existing work on visualizing NMT models.", 
        "13": "Note that the attention mechanism (Bahdanau et al., 2015) is restricted to demonstrate the connection between words in source and target languages and unable to offer more insights in interpreting how target words are generated (see Section 4.5).", 
        "14": "In this work, we propose to use layer-wise relevance propagation (LRP) (Bach et al., 2015) to visualize and interpret neural machine translation.", 
        "15": "Originally designed to compute the contributions of single pixels to predictions for image classifiers, LRP back-propagates relevance recursively from the output layer to the input layer.", 
        "16": "In contrast to visualization methods relying on derivatives, a major advantage of LRP is that it does not require neural activations to be differentiable or smooth (Bach et al., 2015).", 
        "17": "We adapt LRP to the attention-based encoder-decoder framework (Bahdanau et al., 2015) to calculate relevance that measures the association degree between two arbitrary neurons in neural networks.", 
        "18": "Case studies on Chinese-English translation show that visualization helps to interpret the internal workings of\n1150\n\u5728 \u7ebd\u7ea6 zai niuyue\n</s>\nin New </s>York\nsource words\nsource word embeddings\nsource forward hidden states\nsource backward hidden states\nsource hidden states\nsource contexts\ntarget hidden states\ntarget word embeddings\ntarget words\nattention\nFigure 1: The attention-based encoder-decoder architecture for neural machine translation (Bahdanau et al., 2015).", 
        "19": "NMT and analyze translation errors.", 
        "20": "2 Background  Given a source sentence x = x1, .", 
        "21": ".", 
        "22": ".", 
        "23": ", xi, .", 
        "24": ".", 
        "25": ".", 
        "26": ", xI with I source words and a target sentence y = y1, .", 
        "27": ".", 
        "28": ".", 
        "29": ", yj , .", 
        "30": ".", 
        "31": ".", 
        "32": ", yJ with J target words, neural machine translation (NMT) decomposes the sentence-level translation probability as a product of word-level translation probabilities:\nP (y|x;\u03b8) = J\u220f\nj=1\nP (yj |x,y<j ;\u03b8), (1)\nwhere y<j = y1, .", 
        "33": ".", 
        "34": ".", 
        "35": ", yj\u22121 is a partial translation.", 
        "36": "In this work, we focus on the attention-based encoder-decoder framework (Bahdanau et al., 2015).", 
        "37": "As shown in Figure 1, given a source sentence x, the encoder first uses source word embeddings to map each source word xi to a real-valued vector xi.1\nThen, a forward recurrent neural network (RNN) with GRU units (Cho et al., 2014) runs to calculate source forward hidden states:\n\u2212\u2192 h i = f( \u2212\u2192 h i\u22121,xi), (2)\nwhere f(\u00b7) is a non-linear function.", 
        "38": "Similarly, the source backward hidden states can be obtained using a backward RNN:\n\u2190\u2212 h i = f( \u2190\u2212 h i+1,xi).", 
        "39": "(3)\n1Note that we use x to denote a source sentence and x to denote the vector representation of a single source word.", 
        "40": "To capture global contexts, the forward and backward hidden states are concatenated as the hidden state for each source word:\nhi = [ \u2212\u2192 h i; \u2190\u2212 h i].", 
        "41": "(4)\nBahdanau et al.", 
        "42": "(2015) propose an attention mechanism to dynamically determine the relevant source context cj for each target word:\ncj =\nI+1\u2211\ni=1\n\u03b1j,ihi, (5)\nwhere \u03b1j,i is an attention weight that indicates how well the source word xi and the target word yj match.", 
        "43": "Note that an end-of-sentence token is appended to the source sentence.", 
        "44": "In the decoder, a target hidden state for the j-th target word is calculated as\nsj = g(sj\u22121,yj , cj), (6)\nwhere g(\u00b7) is a non-linear function, yj\u22121 denotes the vector representation of the (j \u2212 1)-th target word.", 
        "45": "Finally, the word-level translation probability is given by\nP (yj |x,y<j ;\u03b8) = \u03c1(yj\u22121, sj , cj), (7)\nwhere \u03c1(\u00b7) is a non-linear function.", 
        "46": "Although NMT proves to deliver state-of-theart translation performance with the capability to handle long-distance dependencies due to GRU and attention, it is hard to interpret the internal information such as \u2212\u2192 h i, \u2190\u2212 h i, hi, cj , and sj in the encoder-decoder framework.", 
        "47": "Though projecting word embedding space into two dimensions (Faruqui and Dyer, 2014) and the attention matrix (Bahdanau et al., 2015) shed partial light on how NMT works, how to interpret the entire network still remains a challenge.", 
        "48": "Therefore, it is important to develop new methods for understanding the translation process and analyzing translation errors for NMT.", 
        "49": "3 Approach    3.1 Problem Statement  Recent efforts on interpreting and visualizing neural models has focused on calculating the contribution of a unit at the input layer to the final decision at the output layer (Simonyan et al., 2014; Mahendran and Vedaldi, 2015; Nguyen et al., 2015;\nGirshick et al., 2014; Bach et al., 2015; Li et al., 2016).", 
        "50": "For example, in image classification, it is important to understand the contribution of a single pixel to the prediction of classifier (Bach et al., 2015).", 
        "51": "In this work, we are interested in calculating the contribution of source and target words to the following internal information in the attention-based encoder-decoder framework:\n1.", 
        "52": "\u2212\u2192 h i: the i-th source forward hidden state, 2.", 
        "53": "\u2190\u2212 h i: the i-th source backward hidden state,\n3. hi: the i-th source hidden state,\n4. cj : the j-th source context vector,\n5. sj : the j-th target hidden state,\n6. yj : the j-th target word embedding.", 
        "54": "For example, as shown in Figure 2, the generation of the third target word \u201cYork\u201d depends on both the source context (i.e., the source sentence \u201czai niuyue </s>\u201d) and the target context (i.e., the partial translation \u201cin New\u201d).", 
        "55": "Intuitively, the source word \u201cniuyue\u201d and the target word \u201cNew\u201d are more relevant to \u201cYork\u201d and should receive higher relevance than other words.", 
        "56": "The problem is how to quantify and visualize the relevance between hidden states and contextual word vectors.", 
        "57": "More formally, we introduce a number of definitions to facilitate the presentation.", 
        "58": "Definition 1 The contextual word set of a hidden state v \u2208 RM\u00d71 is denoted as C(v), which is a set of source and target contextual word vectors u \u2208 RN\u00d71 that influences the generation of v.\nFor example, the context word set for \u2212\u2192 h i is {x1, .", 
        "59": ".", 
        "60": ".", 
        "61": ",xi}, for \u2190\u2212h i is {xi, .", 
        "62": ".", 
        "63": ".", 
        "64": ",xI+1}, and for hi is {x1, .", 
        "65": ".", 
        "66": ".", 
        "67": ",xI+1}.", 
        "68": "The contextual word set for cj is {x1, .", 
        "69": ".", 
        "70": ".", 
        "71": ",xI+1}, for sj and yj is {x1, .", 
        "72": ".", 
        "73": ".", 
        "74": ",xI+1,y1, .", 
        "75": ".", 
        "76": ".", 
        "77": ",yj\u22121}.", 
        "78": "As both hidden states and contextual words are represented as real-valued vectors, we need to factorize vector-level relevance at the neuron level.", 
        "79": "Definition 2 The neuron-level relevance between the m-th neuron in a hidden state vm \u2208 R and the n-th neuron in a contextual word vector un \u2208 R is denoted as run\u2190vm \u2208 R, which satisfies the following constraint:\nvm = \u2211\nu\u2208C(v)\nN\u2211\nn=1\nrun\u2190vm (8)\nDefinition 3 The vector-level relevance between a hidden state v and one contextual word vector u \u2208 C(v) is denoted as Ru\u2190v \u2208 R, which quantifies the contribution of u to the generation of v. It is calculated as\nRu\u2190v = M\u2211\nm=1\nN\u2211\nn=1\nrun\u2190vm (9)\nDefinition 4 The relevance vector of a hidden state v is a sequence of vector-level relevance of its contextual words:\nRv = {Ru1\u2190v, .", 
        "80": ".", 
        "81": ".", 
        "82": ", Ru|C(v)|\u2190v} (10) Therefore, our goal is to compute relevance vectors for hidden states in a neural network, as shown in Figure 2.", 
        "83": "The key problem is how to compute neuron-level relevance.", 
        "84": "3.2 Layer-wise Relevance Propagation  We follow (Bach et al., 2015) to use layer-wise relevance propagation (LRP) to compute neuronlevel relevance.", 
        "85": "We use a simple feed-forward network shown in Figure 3 to illustrate the central idea of LRP.", 
        "86": "Input: A neural network G for a sentence pair and a set of hidden states to be visualized V .", 
        "87": "Output: Vector-level relevance setR.", 
        "88": "1 for u \u2208 G in a forward topological order do 2 for v \u2208 OUT(u) do 3 calculating weight ratios wu\u2192v; 4 end 5 end 6 for v \u2208 V do 7 for v \u2208 v do 8 rv\u2190v = v; // initializing neuron-level relevance 9 end\n10 for u \u2208 G in a backward topological order do 11 ru\u2190v = \u2211 z\u2208OUT(u)wu\u2192zrz\u2190v ; // calculating neuron-level relevance 12 end 13 for u \u2208 C(v) do 14 Ru\u2190v = \u2211 u\u2208u \u2211 v\u2208v ru\u2190v ; // calculating vector-level relevance 15 R = R\u222a {Ru\u2190v}; // Update vector-level relevance set 16 end 17 end\nAlgorithm 1: Layer-wise relevance propagation for neural machine translation.", 
        "89": "LRP first propagates the relevance from the output layer to the intermediate layer:\nrz1\u2190v1 = W\n(2) 1,1z1\nW (2) 1,1z1 +W (2) 2,1z2\nv1 (11)\nrz2\u2190v1 = W\n(2) 2,1z2\nW (2) 1,1z1 +W (2) 2,1z2\nv1 (12)\nNote that we ignore the non-linear activation function because Bach et al.", 
        "90": "(2015) indicate that LRP is invariant against the choice of non-linear function.", 
        "91": "Then, the relevance is further propagated to the input layer:\nru1\u2190v1 = W\n(1) 1,1u1\nW (1) 1,1u1 +W (1) 2,1u2\nrz1\u2190v1 +\nW (1) 1,2u1\nW (1) 1,2u1 +W (1) 2,2u2\nrz2\u2190v1 (13)\nru2\u2190v1 = W\n(1) 2,1u2\nW (1) 1,1u1 +W (1) 2,1u2\nrz1\u2190v1 +\nW (1) 2,2u2\nW (1) 1,2u1 +W (1) 2,2u2\nrz2\u2190v1 (14)\nNote that ru1\u2190v1 + ru2\u2190v1 = v1.", 
        "92": "More formally, we introduce the following definitions to ease exposition.", 
        "93": "Definition 5 Given a neuron u, its incoming neuron set IN(u) comprises all its direct connected preceding neurons in the network.", 
        "94": "For example, in Figure 3, the incoming neuron set of z1 is IN(z1) = {u1, u2}.", 
        "95": "Definition 6 Given a neuron u, its outcoming neuron set OUT(u) comprises all its direct connected descendant neurons in the network.", 
        "96": "For example, in Figure 3, the incoming neuron set of z1 is OUT(z1) = {v1, v2}.", 
        "97": "Definition 7 Given a neuron v and its incoming neurons u \u2208 IN(v), the weight ratio that measures the contribution of u to v is calculated as\nwu\u2192v = Wu,vu\u2211\nu\u2032\u2208IN(v)Wu\u2032,vu \u2032 (15)\nAlthough the NMT model usually involves multiple operators such as matrix multiplication, element-wise multiplication, and maximization, they only influence the way to calculate weight ratios in Eq.", 
        "98": "(15).", 
        "99": "For matrix multiplication such as v = Wu, its basic form that is calculated at the neuron level is given by v = \u2211 u\u2208IN(v)Wu,vu .", 
        "100": "We follow Bach et al.", 
        "101": "(2015) to calculate the weight ratio using Eq.", 
        "102": "(15).", 
        "103": "For element-wise multiplication such as v = u1\u25e6u2, its basic form is given by v = \u220f u\u2208IN(v) u.", 
        "104": "We use the following method to calculate its weight ratio:\nwu\u2192v = u\u2211\nu\u2032\u2208IN(v) u \u2032 (16)\nFor maximization such as v = max{u1, u2}, we calculate its weight ratio as follows:\nwu\u2192v = {\n1 if u = maxu\u2032\u2208IN(v){u\u2032} 0 otherwise\n(17)\nTherefore, the general local redistribution rule for LRP is given by\nru\u2190v = \u2211\nz\u2208OUT(u) wu\u2192zrz\u2190v (18)\nAlgorithm 1 gives the layer-wise relevance propagation algorithm for neural machine translation.", 
        "105": "The input is an attention-based encoderdecoder neural network for a sentence pair after decoding G and a set of hidden states to be visualized V .", 
        "106": "The output is a set of vector-level relevance between intended hidden states and their contextual words R. The algorithm first computes weight ratios for each neuron in a forward pass (lines 1-4).", 
        "107": "Then, for each hidden state to be visualized (line 6), the algorithm initializes the neuron-level relevance for itself (lines 7-9).", 
        "108": "After initialization, the neuron-level relevance is backpropagated through the network (lines 10-12).", 
        "109": "Finally, vector-level relevance is calculated based on neuron-level relevance (lines 13-16).", 
        "110": "The time complexity of Algorithm 1 isO(|G|\u00d7|V|\u00d7Omax),\nwhere |G| is the number of neuron units in the neural network G, |V| is the number of hidden states to be visualized and Omax is the maximum of outdegree for neurons in the network.", 
        "111": "Calculating relevance is more computationally expensive than computing attention as it involves all neurons in the network.", 
        "112": "Fortunately, it is possible to take advantage of parallel architectures of GPUs and relevance caching for speed-up.", 
        "113": "4 Analysis    4.1 Data Preparation  We evaluate our approach on Chinese-English translation.", 
        "114": "The training set consists of 1.25M pairs of sentences with 27.93M Chinese words and 34.51M English words.", 
        "115": "We use the NIST 2003 dataset as the development set for model selection and the NIST 2004 dataset as test set.", 
        "116": "The BLEU score on NIST 2003 is 32.73.", 
        "117": "We use the open-source toolkit GROUNDHOG (Bahdanau et al., 2015), which implements the attention-based encoder-decoder framework.", 
        "118": "After model training and selection on the training and development sets, we use the resulting NMT model to translate the test set.", 
        "119": "Therefore, the visualization examples in the following subsections are taken from the test set.", 
        "120": "4.2 Visualization of Hidden States    4.2.1 Source Side  Figure 4 visualizes the source hidden states for a source content word \u201cnian\u201d (years).", 
        "121": "For each word in the source string \u201cjin liang nian lai , meiguo\u201d (in recent two years, USA), we attach a number\nto denote the position of the word in the sentence.", 
        "122": "For example, \u201cnian\u201d (years) is the third word.", 
        "123": "We are interested in visualizing the relevance between the third source forward hidden state \u2212\u2192 h 3 and all its contextual words \u201cjin\u201d (recent) and \u201cliang\u201d (two).", 
        "124": "We observe that the direct preceding word \u201cliang\u201d (two) contributes more to forming the forward hidden state of \u201cnian\u201d (years).", 
        "125": "For the third source backward hidden state \u2190\u2212 h 3, the relevance of contextual words generally decreases with the increase of the distance to \u201cnian\u201d (years).", 
        "126": "Clearly, the concatenation of forward and backward hidden states h3 capture contexts in both directions.", 
        "127": "The situations for function words and punctuation marks are similar but the relevance is usually more concentrated on the word itself.", 
        "128": "We omit the visualization due to space limit.", 
        "129": "4.2.2 Target Side  Figure 5 visualizes the target-side hidden states for the second target word \u201cvisit\u201d.", 
        "130": "For comparison, we also give the attention weights \u03b12, which correctly identifies the second source word \u201ccanbai\u201d (\u201cvisit\u201d) is most relevant to \u201cvisit\u201d.", 
        "131": "The relevance vector of the source context c2 is generally consistent with the attention but reveals that the third word \u201cshi\u201d (is) also contributes to the generation of \u201cvisit\u201d.", 
        "132": "For the target hidden state s2, the contextual word set includes the first target word \u201cmy\u201d.", 
        "133": "We find that most contextual words receive high values of relevance.", 
        "134": "This phenomenon has been frequently observed for most target words in other sentences.", 
        "135": "Note that relevance vector is not normalized.", 
        "136": "This is an essential difference between\nattention and relevance.", 
        "137": "While attention is defined to be normalized, the only constraint on relevance is that the sum of relevance of contextual words is identical to the value of intended hidden state neuron.", 
        "138": "For the target word embedding y2, the relevance is generally consistent with the attention by identifying that the second source word contributes more to the generation of \u201cvisit\u201d.", 
        "139": "But Ry2 further indicates that the target word \u201cmy\u201d is also very important for generating \u201cvisit\u201d.", 
        "140": "Figure 6 shows the hidden states of a target UNK word, which is very common to see in NMT because of limited vocabulary.", 
        "141": "It is interesting to investigate whether the attention mechanism could put a UNK in the right place in the translation.", 
        "142": "In this example, the 6-th source word \u201czhaiwuguo\u201d is a UNK.", 
        "143": "We find that the model successfully predicts the correct position of UNK by exploiting surrounding source and target contexts.", 
        "144": "But the ordering of UNK usually becomes worse if multiple UNK words exist on the source side.", 
        "145": "4.3 Translation Error Analysis  Given the visualization of hidden states, it is possible to offer useful information for analyzing translation errors commonly observed in NMT such as word omission, word repetition, unrelated words and negation reversion.", 
        "146": "4.3.1 Word Omission  Given a source sentence \u201cbajisitan zongtong muxialafu yingde can zhong liang yuan xinren toupiao\u201d (pakistani president musharraf wins votes of confidence in senate and house), the NMT model pro-\nduces a wrong translation \u201cpakistani president win over democratic vote of confidence in the senate\u201d.", 
        "147": "One translation error is that the 6-th source word \u201czhong\u201d (house) is incorrectly omitted for translation.", 
        "148": "As the end-of-sentence token \u201c</s>\u201d occurs early than expected, we choose to visualize its corresponding target hidden states.", 
        "149": "Although the attention correctly identifies the 6-th source word \u201czhong\u201d (house) to be important for generating the next target word, the relevance of source context Rc12 attaches more importance to the end-ofsentence token.", 
        "150": "Finally, the relevance of target word Ry12 reveals that the end-of-sentence token and the 11-th target word \u201csenate\u201d become dominant in the softmax layer for generating the target word.", 
        "151": "This example demonstrates that only using attention matrices does not suffice to analyze the internal workings of NMT.", 
        "152": "The values of relevance of contextual words might vary significantly across different layers.", 
        "153": "4.3.2 Word Repetition  Given a source sentence \u201cmeiguoren lishi shang you jiang chengxi de chuantong , you fancuo rencuo de chuantong\u201d (in history , the people of america have the tradition of honesty and would not hesitate to admit their mistakes), the NMT model produces a wrong translation \u201cin the history of the history of the history of the americans , there is a tradition of faith in the history of mistakes\u201d.", 
        "154": "The\ntranslation error is that \u201chistory\u201d repeats four times in the translation.", 
        "155": "Figure 8 visualizes the target hidden states of the 6-th target word \u201chistory\u201d.", 
        "156": "According to the relevance of the target word embedding Ry6 , the first source word \u201cmeiguoren\u201d (american), the second source word \u201clishi\u201d (history) and the 5-th target word \u201cthe\u201d are most relevant to the generation of \u201chistory\u201d.", 
        "157": "Therefore, word repetition not only results from wrong attention but also is significantly influenced by target side context.", 
        "158": "This finding confirms the importance of controlling source and target contexts to improve fluency and adequacy (Tu et al., 2017).", 
        "159": "4.3.3 Unrelated Words  Given a source sentence \u201cci ci huiyi de yi ge zhongyao yiti shi kuadaxiyang guanxi\u201d (one the the top agendas of the meeting is to discuss the cross-atlantic relations), the model prediction is \u201ca key topic of the meeting is to forge ahead\u201d.", 
        "160": "One translation error is that the 9-th English word \u201cforge\u201d is totally unrelated to the source sentence.", 
        "161": "Figure 9 visualizes the hidden states of the 9-th target word \u201cforge\u201d.", 
        "162": "We find that while the attention identifies the 10-th source word \u201ckuadaxiyang\u201d (cross-atlantic) to be most relevant, the relevance vector of the target word Ry9 finds that multiple source and target words should contribute to the generation of the next target word.", 
        "163": "We observe that unrelated words are more likely to occur if multiple contextual words have high\nvalues in the relevance vector of the target word being generated.", 
        "164": "4.3.4 Negation Reversion  Given a source sentence \u201cbu jiejue shengcun wenti , jiu tan bu shang fa zhan , geng tan bu shang ke chixu fazhan\u201d (without solution to the issue of subsistence , there will be no development to speak of , let alone sustainable development), the model prediction is \u201cif we do not solve the problem of living , we will talk about development and still less can we talk about sustainable development\u201d.", 
        "165": "The translation error is that the 8-th negation source word \u201cbu\u201d (not) is untranslated.", 
        "166": "The omission of negation is a severe translation error it reverses the meaning of the source sentence.", 
        "167": "As shown in Figure 10, while both attention and relevance correctly identify the 8-th negation word \u201cbu\u201d (not) to be most relevant, the model still generates \u201cabout\u201d instead of a negation target word.", 
        "168": "One possible reason is that target context words \u201cwill talk\u201d take the lead in determining the next target word.", 
        "169": "4.4 Extra Words  Given a source sentence \u201cbajisitan zongtong muxialafu yingde can zhong liang yuan xinren toupiao\u201d(pakistani president musharraf wins votes of confidence in senate and house), the model prediction is \u201cpakistani president win over democratic vote of confidence in the senate\u201d The translation error is that the 5-th target word \u201cdemocratic\u201d is extra generated.", 
        "170": "Figure 11 visualizes the hidden states of the 9-th target word \u201cforge\u201d.", 
        "171": "We find that while the attention identifies the 9-th source word \u201cxinren\u201d(confidence) to be most relevant, the relevance vector of the target word Ry9 indicates that the end-of-sentence token and target words contribute more to the generation of \u201cdemocratic\u201d.", 
        "172": "4.5 Summary of Findings  We summarize the findings of visualizing and analyzing the decoding process of NMT as follows:\n1.", 
        "173": "Although attention is very useful for understanding the connection between source and target words, only using attention is not sufficient for deep interpretation of target word generation (Figure 9);\n2.", 
        "174": "The relevance of contextual words might vary significantly across different layers of hidden states (Figure 9);\n3.", 
        "175": "Target-side context also plays a critical role in determining the next target word being generated.", 
        "176": "It is important to control both source and target contexts to produce correct translations (Figure 10);\n4.", 
        "177": "Generating the end-of-sentence token too early might lead to many problems such as word omission, unrelated word generation, and truncated translation (Figures 7 and 9).", 
        "178": "5 Related Work  Our work is closely related to previous visualization approaches that compute the contribution of a unit at the input layer to the final decision at the output layer (Simonyan et al., 2014; Mahendran and Vedaldi, 2015; Nguyen et al., 2015; Girshick et al., 2014; Bach et al., 2015; Li et al., 2016).", 
        "179": "Among them, our approach bears most resemblance to (Bach et al., 2015) since we adapt layer-wise relevance propagation to neural machine translation.", 
        "180": "The major difference is that word vectors rather than single pixels are the basic units in NMT.", 
        "181": "Therefore, we propose vectorlevel relevance based on neuron-level relevance for NMT.", 
        "182": "Calculating weight ratios has also been carefully designed for the operators in NMT.", 
        "183": "The proposed approach also differs from (Li et al., 2016) in that we use relevance rather than partial derivative to quantify the contributions of contextual words.", 
        "184": "A major advantage of using relevance is that it does not require neural activations to be differentiable or smooth (Bach et al., 2015).", 
        "185": "The relevance vector we used is significantly different from the attention matrix (Bahdanau et al., 2015).", 
        "186": "While attention only demonstrates the association degree between source and target words, relevance can be used to calculate the association degree between two arbitrary neurons in neural networks.", 
        "187": "In addition, relevance is effective in analyzing the effect of source and target contexts on generating target words.", 
        "188": "6 Conclusion  In this work, we propose to use layer-wise relevance propagation to visualize and interpret neural machine translation.", 
        "189": "Our approach is capable of calculating the relevance between arbitrary hidden states and contextual words by back-propagating relevance along the network recursively.", 
        "190": "Analyses of the state-of-art attention-based encoder-decoder framework on Chinese-English translation show that our approach is able to offer more insights than the attention mechanism for interpreting neural machine translation.", 
        "191": "In the future, we plan to apply our approach to more NMT approaches (Sutskever et al., 2014; Shen et al., 2016; Tu et al., 2016; Wu et al., 2016) on more language pairs to further verify its effectiveness.", 
        "192": "It is also interesting to develop relevancebased neural translation models to explicitly control relevance to produce better translations.", 
        "193": "Acknowledgements  This work is supported by the National Natural Science Foundation of China (No.61522204), the 863 Program (2015AA015407), and the National Natural Science Foundation of China (No.61432013).", 
        "194": "This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme."
    }, 
    "document_id": "P17-1106.pdf.json"
}
