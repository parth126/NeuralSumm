{
    "abstract_sentences": {
        "1": "Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation.", 
        "2": "However, both training and working procedures of the current neural models are computationally inefficient.", 
        "3": "This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks.", 
        "4": "Our segmenter is truly end-toend, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 608\u2013615 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2096  1 Introduction  Word segmentation is a fundamental task for processing most east Asian languages, typically Chinese.", 
        "2": "Almost all practical Chinese processing applications essentially rely on Chinese word segmentation (CWS), e.g., (Zhao et al., 2017).", 
        "3": "Since (Xue, 2003), most methods formalize this task as a sequence labeling problem.", 
        "4": "In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) (Low et al., 2005) and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004).", 
        "5": "However, these models rely heavily on hand-crafted features.", 
        "6": "\u2217Corresponding author.", 
        "7": "This paper was partially supported by Cai Yuanpei Program (CSC No.", 
        "8": "201304490199 and No.", 
        "9": "201304490171), National Natural Science Foundation of China (No.", 
        "10": "61170114, No.", 
        "11": "61672343 and No.", 
        "12": "61272248), National Basic Research Program of China (No.", 
        "13": "2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No.", 
        "14": "15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No.", 
        "15": "14JCRZ04), Key Project of National Society Science Foundation of China (No.", 
        "16": "15-ZDA041), and the joint research project with Youtu Lab of Tencent.", 
        "17": "To minimize the efforts in feature engineering, neural word segmentation has been actively studied recently.", 
        "18": "Zheng et al.", 
        "19": "(2013) first adapted the sliding-window based sequence labeling (Collobert et al., 2011) with character embeddings as input.", 
        "20": "A number of other researchers have attempted to improve the segmenter of (Zheng et al., 2013) by augmenting it with additional complexity.", 
        "21": "Pei et al.", 
        "22": "(2014) introduced tag embeddings.", 
        "23": "Chen et al.", 
        "24": "(2015a) proposed to model ngram features via a gated recursive neural network (GRNN).", 
        "25": "Chen et al.", 
        "26": "(2015b) used a Long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997) to capture long-distance context.", 
        "27": "Xu and Sun (2016) integrated both GRNN and LSTM for deeper feature extraction.", 
        "28": "Besides sequence labeling schemes, Zhang et al.", 
        "29": "(2016) proposed a transition-based framework.", 
        "30": "Liu et al.", 
        "31": "(2016) used a zero-order semiCRF based model.", 
        "32": "However, these two models rely on either traditional discrete features or nonneural-network components for performance enhancement, their performance drops rapidly when solely depending on neural models.", 
        "33": "Most closely related to this work, Cai and Zhao (2016) proposed to score candidate segmented outputs directly, employing a gated combination neural network over characters for word representation generation and an LSTM scoring model for segmentation result evaluation.", 
        "34": "Despite the active progress of most existing works in terms of accuracy, their computational needs have been significantly increased to the extent that training a neural segmenter usually takes days even using cutting-edge hardwares.", 
        "35": "Meanwhile, different applications often require diverse segmenters and offer large-scale incoming data.", 
        "36": "The efficiency of a word segmenter either for training and decoding is crucial in practice.", 
        "37": "In this paper, we propose a simple yet accurate neu-\n608\nral word segmenter who searches greedily during both training and working to overcome the existing efficiency obstacle.", 
        "38": "Our evaluation will be performed on Chinese benchmark datasets.", 
        "39": "2 Related Work  Statistical Chinese word segmentation has been studied for decades (Huang and Zhao, 2007).", 
        "40": "(Xue, 2003) was the first to cast it as a characterbased tagging problem.", 
        "41": "Peng et al.", 
        "42": "(2004) showed CRF based model is particularly effective to solve CWS in the sequence labeling fashion.", 
        "43": "This method has been followed by most later segmenters (Tseng et al., 2005; Zhao et al., 2006; Zhao and Kit, 2008c; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013).", 
        "44": "The same spirit has also be followed by most neural models (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015a,b; Ma and Hinrichs, 2015; Xu and Sun, 2016).", 
        "45": "Word based CWS to conveniently incorporate complete word features has also be explored.", 
        "46": "Andrew (2006) proposed a semi-CRF model.", 
        "47": "Zhang and Clark (2007, 2011) used a perceptron algorithm with inexact search.", 
        "48": "Both of them have been followed by neural model versions (Liu et al., 2016) and (Zhang et al., 2016) respectively.", 
        "49": "There are also works integrating both character-based and word-based segmenters (Huang and Zhao, 2006; Sun, 2010; Wang et al., 2014) and semisupervised learning (Zhao and Kit, 2008b, 2011; Zeng et al., 2013; Zhang et al., 2013).", 
        "50": "Unlike most previous works, which extract features within a fixed sized sliding window, Cai and Zhao (2016) proposed a direct segmentation framework that extends the feature window to cover complete input and segmentation history and uses beam search for decoding.", 
        "51": "In this work, we will make a series of significant improvement over the basic framework and especially adopt greedy search instead.", 
        "52": "Another notable exception of embedding based methods is (Ma and Hinrichs, 2015), which used character-specified tags matching for fast decoding and resulted in a character-based greedy segmenter.", 
        "53": "3 Models  To segment a character sequence, we employ neural networks to score the likelihood of a candidate segmented sequence being a true sentence, and the\none with the highest score will be picked as output.", 
        "54": "3.1 Neural Scorer  Our neural architecture to score a segmented sequence (word sequence) can be described in the following three steps (illustrated in Figure 1).", 
        "55": "Encoding To make use of neural networks, symbolic data needs to be transformed into distributed representations.", 
        "56": "The most straightforward solution is to use a lookup table for word vectors (Bengio et al., 2003).", 
        "57": "However, in the context of neural word segmentation, it will generalize poorly due to the severe word sparsity in Chinese.", 
        "58": "An alternative is employing neural networks to compose word representations from character embedding inputs.", 
        "59": "However, it is empirically hard to learn a satisfactory composition function.", 
        "60": "In fact, quite a lot of Chinese words, like \u201c\u6c99(sand)\u53d1(issue)\u201d (sofa) , are not semantically character-level compositional at all.", 
        "61": "For the dilemma that composing word representations from character may be insufficient while the direct use of word embedding may lose generalization ability, we propose a hybrid mechanism to alleviate the problem.", 
        "62": "Concretely, we keep a short list H of the most frequent words w = c1..cl to balance character composition.", 
        "63": "If w inH, the immediate word embedding w \u2208 Rdw is attached via average pooling1, otherwise, the character composition is used alone.", 
        "64": "WORD(c1..cl) =\n{ COMP(c1..cl)+w[w]\n2 if c1..cl \u2208 H COMP(c1..cl) otherwise\nOur character composition function COMP(\u00b7) for 1We tried other two integration functions, concatenation and adaptive gating mechanism, but it finally shows that the simplest averaging works best.", 
        "65": "l-length word is\nCOMP(c1..cl) = tanh(Wcl [r1 c1; .", 
        "66": ".", 
        "67": ".", 
        "68": "; rl cl]+bcl )\nwhere denotes the element-wise multiplication.", 
        "69": "ri \u2208 Rdc is the gate that controls the information flow from character embedding ci \u2208 Rdc to word.", 
        "70": "Intuitively, the gating mechanism is used to determine which part of the character vectors should be retrieved when composing a certain word.", 
        "71": "This is indeed important due to the ambiguity of individual Chinese characters.", 
        "72": "[r1; .", 
        "73": ".", 
        "74": ".", 
        "75": "; rl] = sigmoid(Wrl [c1; .", 
        "76": ".", 
        "77": ".", 
        "78": "; cl] + b r l )\nIn contrast, the model in (Cai and Zhao, 2016) further combined COMP(\u00b7) and character embeddings ci via an update gate z (As in Figure 2), which has been shown helpless to the performance but requires huge computational cost according to our empirical study.", 
        "79": "Linking To capture word interactions within a word sequence, the resulted word vectors are then linked sequentially via an LSTM (Sundermeyer et al., 2012).", 
        "80": "At each time step i, a prediction about next word is made according to the current hidden state hi \u2208 RH of LSTM.", 
        "81": "The procedure can be described as the following equation.", 
        "82": "pi+1 = tanh(W phi + b p)\nThe predictions p \u2208 Rdw will then be used to evaluate how reasonable the transition is between next word and the preceding word sequence.", 
        "83": "Scoring The segmented sequence is evaluated from two perspectives, (i) the legality of individual words, (ii) the smoothness or coherence of the word sequence.", 
        "84": "The former is judged by a trainable parameter vector u \u2208 Rdw , which is supposed to work like a hyperplane separating legal and illegal words.", 
        "85": "For the latter, the prediction p made for each position can be used to score the fitness of the\nactual word.", 
        "86": "Both scoring operations are implemented via dot product in our settings.", 
        "87": "Summing up all scores, the segmented sequence (sentence) is scored as follow.", 
        "88": "s([w1, w2, .", 
        "89": ".", 
        "90": ".", 
        "91": ", wn]) = n\u2211\ni=1\n(u+ pi) TWORD(wi)  3.2 Search  The number of possible segmented sentences grows exponentially with the length of the input character sequence.", 
        "92": "Most existing methods made Markov assumptions to keep the exact search tractable.2 However, such assumptions cannot be made in our model as the LSTM component takes advantage of the full segmentation history.", 
        "93": "We then adopt a beam search scheme, which works iteratively on every prefix of the input character sequence, approximating the k highest-scored word sequences of each prefix (i.e., k is the beam size).", 
        "94": "The time complexity of our beam search is O(wkn), where w is the maximum word length and n is the input sequence length.", 
        "95": "3.3 Training Criteria  Our segmenter is trained using max-margin methods (Taskar et al., 2005) where the structured margin loss is defined as \u00b5 times the number of incorrectly segmented characters (Cai and Zhao, 2016).", 
        "96": "However, according to (Huang et al., 2012), standard parameter update cannot guarantee convergence in the case of inexact search.", 
        "97": "We thus additionally examine two strategies as follows.", 
        "98": "Early update This strategy proposed in (Collins and Roark, 2004) can be simplified into \u201cupdate once the golden answer is unreachable\u201d.", 
        "99": "In our case, when the considering character prefix can be correctly segmented but the correct one falls off the beam, an update operation will be conducted and the rest part will be ignored.", 
        "100": "LaSO update One drawback of early update is that the search may never reach the end of a training instance, which means the rest part of the instance is \u201cwasted\u201d.", 
        "101": "Differently, LaSO method of (Daume\u0301 III and Marcu, 2005) continues on the same instance with correct hypothesis after each update.", 
        "102": "In our case, the beam will be emptied and the corresponding prefix of the correct word sequence will be inserted into the beam.", 
        "103": "2By assuming that tag interactions or word interactions only exist in adjacent positions.", 
        "104": "4 Experiments    4.1 Datasets and Settings  We conduct experiments on two popular benchmark datasets, namely PKU and MSR, from the second international Chinese word segmentation bakeoff (Emerson, 2005) (Bakeoff-2005).", 
        "105": "Data statistics are in Table 1.", 
        "106": "Throughout this paper, we use the same model setting as shown in Table 2.", 
        "107": "These numbers are tuned on development sets.3 We follow (Dyer et al., 2015) to train model parameters.", 
        "108": "The learning rate at epoch t is set as \u03b7t = 0.2/(1 + \u03b3t), where \u03b3 = 0.1 for PKU dataset and \u03b3 = 0.2 for MSR dataset.", 
        "109": "The character embeddings are either randomly initialized or pre-trained by word2vec (Mikolov et al., 2013) toolkit on Chinese Wikipedia corpus (which will be indicated by +pre-train in tables.", 
        "110": "), while the word embeddings are always randomly initialized.", 
        "111": "The beam size is kept the same during training and working.", 
        "112": "By default, early update strategy is adopted and the word table H is top half of in-vocabulary (IV) words by frequency.", 
        "113": "4.2 Model Analysis  Beam search collapses into greedy search Figure 3 demonstrates the effect of beam size.", 
        "114": "To our surprise, beam size change has little influence on the performance.", 
        "115": "Namely, simple stepwise greedy search nearly achieves the best performance, which suggests that word segmentation can be greedily solvable at word-level.", 
        "116": "It may be due to that right now the model is optimal\n3Following conventions, the last 10% sentences of training corpus are used as development set.", 
        "117": "enough to make correct decisions at the first position.", 
        "118": "In fact, similar phenomenon was observed at character-level (Ma and Hinrichs, 2015).", 
        "119": "The rest experiments will thus only report the results of our greedy segmenter.", 
        "120": "Comparing different update methods Table 3 compares the concerned three training strategies.", 
        "121": "We find that early update leads to faster convergence and a bit better performance compared to both standard and LaSO update.", 
        "122": "Character composition versus word embedding Following Section 3.1, direct use of word embedding may bring efficiency and effectiveness for identifying IV words, but weaken the ability to recognize out-of-vocabulary (OOV) words.", 
        "123": "We accordingly conduct experiments on different sizes of word table H. Concretely, sorting all IV words by frequency, the first {0, 25%, 50%, 75%, 100%} fraction of them respectively forms the word table.", 
        "124": "The corresponding results on PKU in Figure 4 demonstrate that by the use of direct word embedding, F1 score increases first but then drops rapidly.", 
        "125": "In contrast, OOV recall, which partially reflects the model generalization ability, decreases consistently.", 
        "126": "In addition, we also found the number of training epochs to convergence decreases continually.", 
        "127": "Overall, the results indicate that word-aware segmenter learns faster and fits better on training set, but generalizes relatively poorly.", 
        "128": "4.3 Main Results  Table 4 compares our final results (greedy search is adopted by setting k=1) to prior neural models.", 
        "129": "Pre-training character embeddings on largescale unlabeled corpus (not limited to the training corpus) has been shown helpful for extra performance improvement.", 
        "130": "The results with or without pre-trained character embeddings are listed separately for following the strict closed test setting of SIGHAN Bakeoff in which no linguistic resource other than training corpus is allowed.", 
        "131": "We also show the state-of-the-art results in (Zhao and Kit, 2008b) of traditional methods.", 
        "132": "The comparison shows our neural word segmenter outperforms all state-of-the-art neural systems with much less computational cost.", 
        "133": "Finally, we present the results on all four Bakeoff-2005 datasets compared to (Zhao and Kit, 2008c) in Table 5.", 
        "134": "Note (Zhao and Kit, 2008c) used AV features, which are derived from global\n4To distinguish the performance improvement from model optimization, we especially list the results of standalone neural models in (Zhang et al., 2016) and (Liu et al., 2016).", 
        "135": "All the running time results are from our runs of released implementations on a single CPU (Intel i7-5960X) with two threads only, except for those of (Zhang et al., 2016) which are from personal communication.", 
        "136": "The results of (Xu and Sun, 2016) are not listed due to their use of external Chinese idiom dictionary.", 
        "137": "statistics over entire training corpus in a similar way of unsupervised segmentation (Zhao and Kit, 2008a), for performance enhancement.5 The comparison to their results without AV features show that our neural models for the first time present comparable performance against state-of-the-art traditional ones under strict closed test setting.6  5 Conclusion  In this paper, we presented a fast and accurate word segmenter using neural networks.", 
        "138": "Our experiments show a significant improvement over existing state-of-the-art neural models by adopting the following key model refinements.", 
        "139": "(1) A novel character-word balanced mechanism for word representation generation.", 
        "140": "(2) A more efficient model for character composition by dropping unnecessary designs.", 
        "141": "(3) Early update strategy during max-margin training.", 
        "142": "(4) With the above modifications, we discover that beam size has little influence on the performance.", 
        "143": "Actually, greedy search achieves very high accuracy.", 
        "144": "Through these improvement from both neural models and linguistic motivation, our model becomes simpler, faster and more accurate.7\n5In fact, this kind of features may also be incorporated to our model.", 
        "145": "We leave it as future work.", 
        "146": "6To our knowledge, none of previous neural models has ever performed a complete evaluation over all four segmentation corpora of Bakeoff-2005, in which only two, PKU and MSR, are used since (Pei et al., 2014).", 
        "147": "7Our code based on Dynet (Neubig et al., 2017) is released at https://github.com/jcyk/greedyCWS."
    }, 
    "document_id": "P17-2096.pdf.json"
}
