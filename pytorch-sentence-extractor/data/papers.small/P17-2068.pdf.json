{
    "abstract_sentences": {
        "1": "Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system.", 
        "2": "In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities.", 
        "3": "Further, we explore models that predict both MWEspans and an MWE-aware dependency structure.", 
        "4": "Experimental results show that our joint model using additional MWEspan features achieves an MWE recognition improvement of 1.35 points over a pipeline model."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 427\u2013432 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2068\nBecause syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system.", 
        "2": "In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities.", 
        "3": "Further, we explore models that predict both MWEspans and an MWE-aware dependency structure.", 
        "4": "Experimental results show that our joint model using additional MWEspan features achieves an MWE recognition improvement of 1.35 points over a pipeline model.", 
        "5": "1 Introduction  To solve complex Natural Language Processing (NLP) tasks that require deep syntactic analysis, various levels of annotation such as parse trees and named entities (NEs) must be consistent with one another (Finkel and Manning, 2009).", 
        "6": "Otherwise, it is usually impossible to combine these pieces of information effectively.", 
        "7": "However, the standard syntactic corpus of English, Penn Treebank, is not concerned with consistency between syntactic trees and spans of multiword expressions (MWEs).", 
        "8": "In Penn Treebank, that is, an MWE-span does not always correspond to a span dominated by a single non-terminal node.", 
        "9": "Therefore, word-based dependency structures converted from Penn Treebank are generally inconsistent with MWE-spans (Figure 1a).", 
        "10": "To mitigate this inconsistency, Kato et al.", 
        "11": "(2016) estab-\nlishes each span of functional MWEs 1 as a subtree of a phrase structure in the Wall Street Journal portion of Ontonotes (Pradhan et al., 2007).", 
        "12": "To pursue this direction further, we construct a corpus such that dependency structures are consistent with MWEs, by extending Kato et al.", 
        "13": "(2016)\u2019s corpus 2.", 
        "14": "As is the case with their corpus, each MME is a syntactic unit in an MWE-aware dependency structure from our corpus (Figure 1b).", 
        "15": "Moreover, our corpus includes not only functional MWEs but also NEs.", 
        "16": "Because NEs are highly productive and occur more frequently than functional MWEs, they are difficult to cover in a dictionary.", 
        "17": "Consistency between NE-spans and phrase structures is not guaranteed because they are independently annotated in most syntactic corpora.", 
        "18": "1By functional MWEs, we mean MWEs that function either as prepositions, conjunctions, determiners, pronouns, or adverbs.", 
        "19": "2We release our dependency corpus at https: //github.com/naist-cl-parsing/ mwe-aware-dependency.", 
        "20": "MWE-aware phrase structures will be distributed from LDC as a part of LDC2017T01.", 
        "21": "427\nFor instance, in Figure 2, an NE-span is \u201cBoard of Investment,\u201d which is inconsistent with the syntactic tree.", 
        "22": "Therefore, we resolve this inconsistency by modifying phrase structures locally and establishing each NE as a subtree.", 
        "23": "Furthermore, to evaluate the constructed corpus, we explore pipeline and joint models that predict both MWE-spans and an MWE-aware dependency tree 3.", 
        "24": "Our experimental results show that the proposed joint model with additional MWEspan features achieves an MWE recognition improvement of 1.35 points over the pipeline model.", 
        "25": "2 MWE-aware Dependency Corpus  To ensure consistency between MWE annotations and dependency structures, we first integrate NE\n3Although Kato et al.", 
        "26": "(2016) conducts experiments regarding MWE-aware dependency parsing, they use gold MWE-spans.", 
        "27": "This is not a realistic scenario.", 
        "28": "By contrast, our parsing models do not use gold MWE-spans.", 
        "29": "annotations on Ontonotes 4 into phrase structures such that functional MWEs are established as subtrees.", 
        "30": "Subsequently, we convert phrase structures to dependency structures.", 
        "31": "We construct our corpus by extending Kato et al.", 
        "32": "(2016)\u2019s corpus 5, which is itself built on a corpus by Shigeto et al.", 
        "33": "(2013).", 
        "34": "RegardingMWE annotations, Shigeto et al.", 
        "35": "(2013) first constructed an MWE dictionary by extracting functional MWEs from the English-language Wiktionary 6, and classified their occurrences in Ontonotes into either MWE or literal usage.", 
        "36": "Kato et al.", 
        "37": "(2016) integrated these MWE annotations into phrase structures and established functional MWEs as subtrees.", 
        "38": "Next, we describe the establishment of each NE as a subtree.", 
        "39": "If an NE-span does not correspond to any non-terminal in a phrase structure, there are two possibilities: (A) the NE-span corresponds to multiple contiguous children of a subtree, or (B) the NE-span has crossing brackets with the spans in the parse tree (Finkel and Manning, 2009; Kato et al., 2016).", 
        "40": "In Case (A), we insert a new non-terminal (\u201cMWE NNP\u201d) that governs the NE-span 7.", 
        "41": "In Case (B), many instances correspond to a noun phrase (NP) comprised of a nested NP and a prepositional phrase (Figure 2).", 
        "42": "In the main NP, a modifier, such as a determiner, an adjective, or a possessive NP, precedes an NE.", 
        "43": "For these instances, according to Finkel and Manning (2009), we reduce Case (B) to Case (A) by moving the modifier from the nested NP to the main NP.", 
        "44": "Then, we establish each NE as a subtree by inserting an MWE-specific non-terminal.", 
        "45": "Furthermore, in some instances it is more reasonable to enlarge NE-spans than to modify phrase structures.", 
        "46": "As a typical example, there is an NE annotation that covers only part of a coordination structure, such as \u201cPeter and Edward Bronfman,\u201d where \u201cEdward Bronfman\u201d is annotated as an NE.", 
        "47": "In this case, we extend an original NE-span to the whole coordination structure.", 
        "48": "We show the statistics for the corpus in Table 1 8.", 
        "49": "This corpus has 27,949 MWE instances in 37,015 sentences.", 
        "50": "A histogram\n4We exploit NE annotations on Ontonotes Release 5.0 (LDC2013T19).", 
        "51": "We address traditional NEs, such as persons, locations, and organizations, while omitting the following: DATE, TIME, PERCENT, MONEY, QUANTITY, ORDINAL, and CARDINAL.", 
        "52": "Note that we only focus on multiword NEs.", 
        "53": "5https://catalog.ldc.upenn.edu/LDC2017T01 6https://en.wiktionary.org 7We do not require manual annotations for Case (A).", 
        "54": "8NEs have NNP as an MWE-level POS tag.", 
        "55": "tabling the consistency between MWE-spans and phrase structures is shown in Table 2.", 
        "56": "For treeto-dependency conversion, we first replace a subtree corresponding to an MWE by a preterminal node and its child node.", 
        "57": "The preterminal node has an MWE-level POS (MWE POS) tag.", 
        "58": "The child node is generated by joining all components of the MWE with underscores.", 
        "59": "We then convert a phrase structure into a Stanford-style dependency structure (Marneffe and Manning, 2008) (Figure 1b).", 
        "60": "3 Models for MWE identification and MWE-aware dependency parsing  In this section, we explore models that predict both MWE-spans and an MWE-aware dependency structure (Figure 1b).", 
        "61": "3.1 Pipeline Model  The pipeline model involves the following three steps.", 
        "62": "First, BIO tags encoding MWE-spans and MWE POS tags, such as \u201cB NNP\u201d and \u201cI DT\u201d are predicted by a sequential labeler based on Conditional Random Fields (CRFs) (Lafferty et al., 2001).", 
        "63": "Second, tokens belonging to each predicted MWE-span are concatenated into a single node.", 
        "64": "Finally, an MWE-based dependency structure (Figure 1b) is predicted by an arc-eager transition-based parser.", 
        "65": "For the CRFs, in addition to word-form and character-based features, we use 1- to 3-gram features based on dictionaries of functional MWEs and NEs within 5-word windows from a target token.", 
        "66": "For a dictionary of functional MWEs, we use the dictionary by Shigeto et al.", 
        "67": "(2013) (Section 2).", 
        "68": "Meanwhile, we create a dictionary of NEs from a title list of English Wikipedia articles, excepting stop words, provided by UniNE 9.", 
        "69": "Regarding parsing features, we use\n9http://members.unine.ch/jacques.savoy/clef/englishST.txt\nbaseline features and rich non-local features proposed by Zhang and Nivre (2011).", 
        "70": "3.2 Joint Model  In the proposed joint model, MWE-spans and MWE POS tags are encoded as dependency labels, and conventional word-based dependency parsing is performed by an arc-eager transitionbased parser.", 
        "71": "We use the same parsing features used in the pipeline model.", 
        "72": "We convert MWEs in MWE-aware dependency structures (Figure 1b) to head-initial structures (Figure 3) that encode MWE-spans and MWE POS tags.", 
        "73": "Note that this representation is similar to Universal Dependency (McDonald et al., 2013).", 
        "74": "When parsing, we use constraints based on a history of transitions and the dictionary of functional MWEs.", 
        "75": "This is done to avoid invalid dependency trees.", 
        "76": "Because NEs are highly productive, we do not use a constraint regarding NEs.", 
        "77": "Joint(+dict) We designed additional features based on matches with dictionaries of NEs and functional MWEs.", 
        "78": "Hereafter, we refer to the joint model coupled with these additional features as joint(+dict).", 
        "79": "For instance, given a sentence that starts with \u201ca number of cities,\u201d the additional features are as follows: a / B DT, number / I DT, of / I DT, cities / O.", 
        "80": "Based on these additional features, we extend the baseline features proposed by Zhang and Nivre (2011) to develop MWE-specific features whose atomic features include not only words and word-level POS tags, but also BIO tags encoding MWE-spans and MWE POS tags.", 
        "81": "Joint(+pred span) Because dictionary matching is not concerned with context, in this setting, we use MWE-spans and MWE POS tags predicted by CRF, rather than dictionary matching.", 
        "82": "Hereafter, we refer to this as joint(+pred span).", 
        "83": "By using features extracted from CRF predictions, we can mitigate error propagation from sequential labeling and consider information from a full sentence.", 
        "84": "Moreover, we can alleviate difficulties in predicting MWE-spans and MWE POS tags encoded as head-initial structures (Figure 3) by the parser.", 
        "85": "4 Experimental Setting  We split the Wall Street Journal (WSJ) portion of Ontonotes, using sections 2-21 for training, and section 23 for testing.", 
        "86": "For all models, we used\nthe POS tags predicted by the Stanford POS tagger (Toutanova et al., 2003) 10.", 
        "87": "For the pipeline model and joint(+pred span), we used MWEspans and MWE POS tags predicted by CRF 11.", 
        "88": "For dependency parsing, we used Redshift (Honnibal et al., 2013) for all models, with a beam size of 16 for decoding.", 
        "89": "For training, we removed non-projective dependency trees.", 
        "90": "For testing, we parsed all sentences.", 
        "91": "To evaluate parsing, we used unlabeled and labeled attachment scores (UAS/LAS) 12.", 
        "92": "For the pipeline model, we converted each concatenated token corresponding to an MWE into a head-initial structure and compared this with the gold tree.", 
        "93": "For the joint model, we directly compared a predicted tree with the gold tree.", 
        "94": "To evaluate MWE recognition, we used the F-measure for untagged / tagged MWEs (FUM/FTM) 13.", 
        "95": "For the pipeline model, we compared the gold MWEs with predictions by CRF.", 
        "96": "For the proposed joint model, we compared the gold MWEs with predicted MWE-spans and\n10We used 20-way jackknifing for the training split.", 
        "97": "The test split was automatically tagged by the POS tagger trained on the training split.", 
        "98": "11We used 20-way jackknifing for the training split.", 
        "99": "The test split was automatically tagged by the sequential labeler trained on the training split.", 
        "100": "12When calculating UAS/LAS, we removed punctuation.", 
        "101": "13FUM only focuses on MWE-spans, whereas FTM fo-\ncuses on both MWE-spans and MWE POS tags.", 
        "102": "MWE POS tags represented as dependency labels.", 
        "103": "5 Experimental Results and Discussion  We present the experimental results in Table 3.", 
        "104": "Comparing the joint model with the pipeline model, there is not much difference between these models regarding UAS / LAS for all sentences.", 
        "105": "However, the former is 2.13 / 0.48 points worse than the latter in terms of UAS / LAS regarding the first tokens of MWEs (1269 in 34,526 tokens), and 2.37 / 2.53 points worse than the latter regarding FUM / FTM.", 
        "106": "These results suggest that the performance of the joint model with no additional features at predicting dependencies inside and around MWEs is worse than the pipeline model.", 
        "107": "One of the reasons for this is that the exploitation of headinitial structures in the joint model (Figure 3) involves the addition of MWE-specific labels.", 
        "108": "This results in an increase in the total number of dependency labels from 41 to 50.", 
        "109": "Because of this broader output space, more search errors can occur in the joint model compared with the pipeline model.", 
        "110": "Moreover, a breakdown by type of MWE (Table 4) shows that most differences in performance between these two models are related to functional MWEs.", 
        "111": "These results suggest that constraints regarding functional MWEs during parsing (3.2) are harmful to the joint model with no additional features in terms of its performance with\nrespect to functional MWEs.", 
        "112": "By adding MWE-specific features to the joint model, however, we observe at least a 2.52 / 3.00 point improvement in terms of UAS / LAS regarding the first tokens of MWEs, and a 2.90 / 2.99 point improvement regarding FUM / FTM.", 
        "113": "As a result, we obtain a 1.35 / 1.28 point improvement with joint(+pred span) compared with the pipeline model in terms of FUM / FTM.", 
        "114": "A breakdown by type of MWE shows that the addition of MWEspecific features leads to a performance improvement, especially for functional MWEs (Table 4).", 
        "115": "These results suggest that MWE-specific features are effective at both MWE recognition through dependency parsing and the prediction of dependencies connecting inside and outside of MWEs.", 
        "116": "Comparing the joint(+pred span) with the joint(+dict), the former is 0.40 / 0.55 points better than the latter in terms of UAS / LAS regarding the first tokens of MWEs, and 0.82 / 0.82 points better than the latter regarding FUM / FTM.", 
        "117": "We can attribute this gain in performance to the additional features extracted from more accurate predictions of MWE-spans and MWE POS tags by CRF than those by dictionary matching.", 
        "118": "6 Related Work  Whereas French Treebank is available for French MWEs (Abeille\u0301 et al., 2003), there have been only limited corpora for English MWE-aware dependency parsing.", 
        "119": "Schneider et al.", 
        "120": "(2014) constructs an MWE-annotated corpus on English Web Treebank (Bies et al., 2012).", 
        "121": "However, this corpus is relatively small as training data for a parser, and its MWE annotations are not consistent with syntactic trees.", 
        "122": "By contrast, our corpus covers the whole of the WSJ portion of Ontonotes and ensures consistency between MWE annotations and parse trees.", 
        "123": "Korkontzelos and Manandhar (2010) reports an improvement in base-phrase chunking by pregrouping MWEs as words-with-spaces.", 
        "124": "They focus on compound nouns, adjective-noun constructions, and named entities.", 
        "125": "However, they use gold MWE-spans, and this is not a realistic setting.", 
        "126": "By contrast, we use predicted MWE-spans.", 
        "127": "Three works concerned with a French MWEaware syntactic parsing are relevant.", 
        "128": "First, Green et al.", 
        "129": "(2013) proposes a method for recognizing contiguous MWEs as a part of constituency parsing by using MWE-specific non-terminals.", 
        "130": "They\ninvestigate a CFG-based model and a model based on tree-substitution grammars.", 
        "131": "Second, Candito and Constant (2014) compares several architectures for graph-based dependency parsing and MWE recognition, in which MWE recognition is conducted before, during, and after parsing.", 
        "132": "Finally, Nasr et al.", 
        "133": "(2015) explores a joint model of MWE recognition and dependency parsing.", 
        "134": "They focus on complex function words.", 
        "135": "In terms of data representation, they adopt one similar to ours, insofar as the components of an MWE are linked by dependency edges whose labels are MWEspecific.", 
        "136": "7 Conclusion  We constructed a corpus that ensures consistency in Ontonotes between dependency structures and English MWEs, including named entities.", 
        "137": "Furthermore, we explored models that can predict both MWE-spans and an MWE-aware dependency structure.", 
        "138": "Our experiments show that by using additional MWE-span features, our joint model achieves an MWE recognition improvement of 1.35 points over the pipeline model.", 
        "139": "Acknowledgments  This work was partially supported by JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number 15K16053.", 
        "140": "We are grateful to members of the Computational Linguistics Laboratory at NAIST, and to the anonymous reviewers for their valuable feedback.", 
        "141": "Regarding the preparation of a title list from English-language Wikipedia articles, we are particularly grateful for the assistance given by Motoki Sato."
    }, 
    "document_id": "P17-2068.pdf.json"
}
