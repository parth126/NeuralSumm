{
    "abstract_sentences": {
        "1": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user\u2019s goal at every step of the dialogue.", 
        "2": "However, most current approaches have difficulty scaling to larger, more complex dialogue domains.", 
        "3": "This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users\u2019 language.", 
        "4": "We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning.", 
        "5": "NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context.", 
        "6": "Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1777\u20131788 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1163  1 Introduction  Spoken dialogue systems (SDS) allow users to interact with computer applications through conversation.", 
        "2": "Task-based systems help users achieve goals such as finding restaurants or booking flights.", 
        "3": "The dialogue state tracking (DST) component of an SDS serves to interpret user input and update the belief state, which is the system\u2019s internal representation of the state of the conversation (Young et al., 2010).", 
        "4": "This is a probability distribution over dialogue states used by the downstream dialogue manager to decide which action the system should\nperform next (Su et al., 2016a,b); the system action is then verbalised by the natural language generator (Wen et al., 2015a,b; Dus\u030cek and Jurc\u030c\u0131\u0301c\u030cek, 2015).", 
        "5": "The Dialogue State Tracking Challenge (DSTC) series of shared tasks has provided a common evaluation framework accompanied by labelled datasets (Williams et al., 2016).", 
        "6": "In this framework, the dialogue system is supported by a domain ontology which describes the range of user intents the system can process.", 
        "7": "The ontology defines a collection of slots and the values that each slot can take.", 
        "8": "The system must track the search constraints expressed by users (goals or informable slots) and questions the users ask about search results (requests), taking into account each user utterance (input via a speech recogniser) and the dialogue context (e.g., what the system just said).", 
        "9": "The example in Figure 1 shows the true state after each user utterance in a three-turn conversation.", 
        "10": "As can be seen in this example, DST models depend on identifying mentions of ontology items in user utterances.", 
        "11": "This becomes a non-trivial task when confronted with lexical variation, the dynamics of context and noisy automated speech recognition (ASR) output.", 
        "12": "1777\nTraditional statistical approaches use separate Spoken Language Understanding (SLU) modules to address lexical variability within a single dialogue turn.", 
        "13": "However, training such models requires substantial amounts of domain-specific annotation.", 
        "14": "Alternatively, turn-level SLU and cross-turn DST can be coalesced into a single model to achieve superior belief tracking performance, as shown by Henderson et al.", 
        "15": "(2014d).", 
        "16": "Such coupled models typically rely on manually constructed semantic dictionaries to identify alternative mentions of ontology items that vary lexically or morphologically.", 
        "17": "Figure 2 gives an example of such a dictionary for three slot-value pairs.", 
        "18": "This approach, which we term delexicalisation, is clearly not scalable to larger, more complex dialogue domains.", 
        "19": "Importantly, the focus on English in DST research understates the considerable challenges that morphology poses to systems based on exact matching in morphologically richer languages such as Italian or German (see Vulic\u0301 et al.", 
        "20": "(2017)).", 
        "21": "In this paper, we present two new models, collectively called the Neural Belief Tracker (NBT) family.", 
        "22": "The proposed models couple SLU and DST, efficiently learning to handle variation without requiring any hand-crafted resources.", 
        "23": "To do that, NBT models move away from exact matching and instead reason entirely over pre-trained word vectors.", 
        "24": "The vectors making up the user utterance and preceding system output are first composed into intermediate representations.", 
        "25": "These representations are then used to decide which of the ontologydefined intents have been expressed by the user up to that point in the conversation.", 
        "26": "To the best of our knowledge, NBT models are the first to successfully use pre-trained word vector spaces to improve the language understanding capability of belief tracking models.", 
        "27": "In evaluation on two datasets, we show that: a) NBT models match the performance of delexicalisation-based models which make use of hand-crafted semantic lexicons;\nand b) the NBT models significantly outperform those models when such resources are not available.", 
        "28": "Consequently, we believe this work proposes a framework better-suited to scaling belief tracking models for deployment in real-world dialogue systems operating over sophisticated application domains where the creation of such domain-specific lexicons would be infeasible.", 
        "29": "2 Background  Models for probabilistic dialogue state tracking, or belief tracking, were introduced as components of spoken dialogue systems in order to better handle noisy speech recognition and other sources of uncertainty in understanding a user\u2019s goals (Bohus and Rudnicky, 2006; Williams and Young, 2007; Young et al., 2010).", 
        "30": "Modern dialogue management policies can learn to use a tracker\u2019s distribution over intents to decide whether to execute an action or request clarification from the user.", 
        "31": "As mentioned above, the DSTC shared tasks have spurred research on this problem and established a standard evaluation paradigm (Williams et al., 2013; Henderson et al., 2014b,a).", 
        "32": "In this setting, the task is defined by an ontology that enumerates the goals a user can specify and the attributes of entities that the user can request information about.", 
        "33": "Many different belief tracking models have been proposed in the literature, from generative (Thomson and Young, 2010) and discriminative (Henderson et al., 2014d) statistical models to rule-based systems (Wang and Lemon, 2013).", 
        "34": "To motivate the work presented here, we categorise prior research according to their reliance (or otherwise) on a separate SLU module for interpreting user utterances:1\nSeparate SLU Traditional SDS pipelines use Spoken Language Understanding (SLU) decoders to detect slot-value pairs expressed in the Automatic Speech Recognition (ASR) output.", 
        "35": "The downstream DST model then combines this information with the past dialogue context to update the belief state (Thomson and Young, 2010; Wang and Lemon, 2013; Lee and Kim, 2016; Perez, 2016; Perez and Liu, 2017; Sun et al., 2016; Jang et al., 2016; Shi et al., 2016; Dernoncourt et al., 2016; Liu and Perez, 2017; Vodola\u0301n et al., 2017).", 
        "36": "1The best-performing models in DSTC2 all used both raw ASR output and the output of (potentially more than one) SLU decoders (Williams, 2014; Williams et al., 2016).", 
        "37": "This does not mean that those models are immune to the drawbacks identified here for the two model categories; in fact, they share the drawbacks of both.", 
        "38": "In the DSTC challenges, some systems used the output of template-based matching systems such as Phoenix (Wang, 1994).", 
        "39": "However, more robust and accurate statistical SLU systems are available.", 
        "40": "Many discriminative approaches to spoken dialogue SLU train independent binary models that decide whether each slot-value pair was expressed in the user utterance.", 
        "41": "Given enough data, these models can learn which lexical features are good indicators for a given value and can capture elements of paraphrasing (Mairesse et al., 2009).", 
        "42": "This line of work later shifted focus to robust handling of rich ASR output (Henderson et al., 2012; Tur et al., 2013).", 
        "43": "SLU has also been treated as a sequence labelling problem, where each word in an utterance is labelled according to its role in the user\u2019s intent; standard labelling models such as CRFs or Recurrent Neural Networks can then be used (Raymond and Ricardi, 2007; Yao et al., 2014; Celikyilmaz and Hakkani-Tur, 2015; Mesnil et al., 2015; Peng et al., 2015; Zhang and Wang, 2016; Liu and Lane, 2016b; Vu et al., 2016; Liu and Lane, 2016a, i.a.).", 
        "44": "Other approaches adopt a more complex modelling structure inspired by semantic parsing (Saleh et al., 2014; Vlachos and Clark, 2014).", 
        "45": "One drawback shared by these methods is their resource requirements, either because they need to learn independent parameters for each slot and value or because they need fine-grained manual annotation at the word level.", 
        "46": "This hinders scaling to larger, more realistic application domains.", 
        "47": "Joint SLU/DST Research on belief tracking has found it advantageous to reason about SLU and DST jointly, taking ASR predictions as input and generating belief states as output (Henderson et al., 2014d; Sun et al., 2014; Zilka and Jurcicek, 2015; Mrks\u030cic\u0301 et al., 2015).", 
        "48": "In DSTC2, systems which used no external SLU module outperformed all systems that only used external SLU features.", 
        "49": "Joint models typically rely on a strategy known as delexicalisation whereby slots and values mentioned in the text are replaced with generic labels.", 
        "50": "Once the dataset is transformed in this manner, one can extract a collection of template-like n-gram features such as [want tagged-value food].", 
        "51": "To perform belief tracking, the shared model iterates over all slot-value pairs, extracting delexicalised feature vectors and making a separate binary decision regarding each pair.", 
        "52": "Delexicalisation introduces a hidden dependency that is rarely discussed: how do we identify slot/value mentions in text?", 
        "53": "For toy domains, one can manually construct semantic dictionaries which list the potential rephrasings for all slot values.", 
        "54": "As shown by Mrks\u030cic\u0301 et al.", 
        "55": "(2016), the use of such dictionaries is essential for the performance of current delexicalisation-based models.", 
        "56": "Again though, this will not scale to the rich variety of user language or to general domains.", 
        "57": "The primary motivation for the work presented in this paper is to overcome the limitations that affect previous belief tracking models.", 
        "58": "The NBT model efficiently learns from the avail-\nable data by: 1) leveraging semantic information from pre-trained word vectors to resolve lexical/morphological ambiguity; 2) maximising the number of parameters shared across ontology values; and 3) having the flexibility to learn domainspecific paraphrasings and other kinds of variation that make it infeasible to rely on exact matching and delexicalisation as a robust strategy.", 
        "59": "3 Neural Belief Tracker  The Neural Belief Tracker (NBT) is a model designed to detect the slot-value pairs that make up the user\u2019s goal at a given turn during the flow of dialogue.", 
        "60": "Its input consists of the system dialogue acts preceding the user input, the user utterance itself, and a single candidate slot-value pair that it needs to make a decision about.", 
        "61": "For instance, the model might have to decide whether the goal FOOD=ITALIAN has been expressed in \u2018I\u2019m looking for good pizza\u2019.", 
        "62": "To perform belief tracking, the NBT model iterates over all candidate slot-value pairs (defined by the ontology), and decides which ones have just been expressed by the user.", 
        "63": "Figure 3 presents the flow of information in the model.", 
        "64": "The first layer in the NBT hierarchy performs representation learning given the three model inputs, producing vector representations for the user utterance (r), the current candidate slot-value pair (c) and the system dialogue acts (tq, ts, tv).", 
        "65": "Subsequently, the learned vector representations interact through the context modelling and semantic decoding submodules to obtain the intermediate interaction summary vectors dr,dc and d. These are used as input to the final decision-making module which decides whether the user expressed the intent represented by the candidate slot-value pair.", 
        "66": "3.1 Representation Learning  For any given user utterance, system act(s) and candidate slot-value pair, the representation learning submodules produce vector representations which act as input for the downstream components of the model.", 
        "67": "All representation learning subcomponents make use of pre-trained collections of word vectors.", 
        "68": "As shown by Mrks\u030cic\u0301 et al.", 
        "69": "(2016), specialising word vectors to express semantic similarity rather than relatedness is essential for improving belief tracking performance.", 
        "70": "For this reason, we use the semantically-specialised Paragram-SL999 word vectors (Wieting et al., 2015) throughout this work.", 
        "71": "The NBT training procedure keeps these\nvectors fixed: that way, at test time, unseen words semantically related to familiar slot values (i.e.", 
        "72": "inexpensive to cheap) will be recognised purely by their position in the original vector space (see also Rockta\u0308schel et al.", 
        "73": "(2016)).", 
        "74": "This means that the NBT model parameters can be shared across all values of the given slot, or even across all slots.", 
        "75": "Let u represent a user utterance consisting of ku words u1, u2, .", 
        "76": ".", 
        "77": ".", 
        "78": ", uku .", 
        "79": "Each word has an associated word vector u1, .", 
        "80": ".", 
        "81": ".", 
        "82": ",uku .", 
        "83": "We propose two model variants which differ in the method used to produce vector representations of u: NBT-DNN and NBT-CNN.", 
        "84": "Both act over the constituent ngrams of the utterance.", 
        "85": "Let vni be the concatenation of the n word vectors starting at index i, so that:\nvni = ui \u2295 .", 
        "86": ".", 
        "87": ".\u2295 ui+n\u22121 (1)\nwhere\u2295 denotes vector concatenation.", 
        "88": "The simpler of our two models, which we term NBT-DNN, is shown in Figure 4.", 
        "89": "This model computes cumulative n-gram representation vectors r1, r2 and r3, which are the n-gram \u2018summaries\u2019 of the unigrams, bigrams and trigrams in the user utterance:\nrn =\nku\u2212n+1\u2211\ni=1\nvni (2)\nEach of these vectors is then non-linearly mapped to intermediate representations of the same size:\nr\u2032n = \u03c3(W s nrn + b s n) (3)\nwhere the weight matrices and bias terms map the cumulative n-grams to vectors of the same dimensionality and \u03c3 denotes the sigmoid activation function.", 
        "90": "We maintain a separate set of parameters for each slot (indicated by superscript s).", 
        "91": "The three vectors are then summed to obtain a single representation for the user utterance:\nr = r\u20321 + r \u2032 2 + r \u2032 3 (4)\nThe cumulative n-gram representations used by this model are just unweighted sums of all word vectors in the utterance.", 
        "92": "Ideally, the model should learn to recognise which parts of the utterance are more relevant for the subsequent classification task.", 
        "93": "For instance, it could learn to ignore verbs or stop words and pay more attention to adjectives and nouns which are more likely to express slot values.", 
        "94": "NBT-CNN Our second model draws inspiration from successful applications of Convolutional Neural Networks (CNNs) for language understanding (Collobert et al., 2011; Kalchbrenner et al., 2014; Kim, 2014).", 
        "95": "These models typically apply a number of convolutional filters to n-grams in the input sentence, followed by non-linear activation functions and max-pooling.", 
        "96": "Following this approach, the NBT-CNN model applies L = 300 different filters for n-gram lengths of 1, 2 and 3 (Figure 5).", 
        "97": "Let F sn \u2208 RL\u00d7nD denote the collection of filters for each value of n, where D = 300 is the word vector dimensionality.", 
        "98": "If vni denotes the concatenation of n word vectors starting at index i, let mn = [vn1 ;v n 2 ; .", 
        "99": ".", 
        "100": ".", 
        "101": ";v n ku\u2212n+1] be the list of n-grams that convolutional filters of length n run over.", 
        "102": "The three intermediate representations are then given by:\nRn = F s n mn (5)\nEach column of the intermediate matrices Rn is produced by a single convolutional filter of length n. We obtain summary n-gram representations by pushing these representations through a recti-\nfied linear unit (ReLU) activation function (Nair and Hinton, 2010) and max-pooling over time (i.e.", 
        "103": "columns of the matrix) to get a single feature for each of the L filters applied to the utterance:\nr\u2032n = maxpool (ReLU (Rn + b s n)) (6)\nwhere bsn is a bias term broadcast across all filters.", 
        "104": "Finally, the three summary n-gram representations are summed to obtain the final utterance representation vector r (as in Equation 4).", 
        "105": "The NBT-CNN model is (by design) better suited to longer utterances, as its convolutional filters interact directly with subsequences of the utterance, and not just their noisy summaries given by the NBT-DNN\u2019s cumulative n-grams.", 
        "106": "3.2 Semantic Decoding  The NBT diagram in Figure 3 shows that the utterance representation r and the candidate slotvalue pair representation c directly interact through the semantic decoding module.", 
        "107": "This component decides whether the user explicitly expressed an intent matching the current candidate pair\n(i.e.", 
        "108": "without taking the dialogue context into account).", 
        "109": "Examples of such matches would be \u2018I want Thai food\u2019 with food=Thai or more demanding ones such as \u2018a pricey restaurant\u2019 with price=expensive.", 
        "110": "This is where the use of high-quality pre-trained word vectors comes into play: a delexicalisation-based model could deal with the former example but would be helpless in the latter case, unless a human expert had provided a semantic dictionary listing all potential rephrasings for each value in the domain ontology.", 
        "111": "Let the vector space representations of a candidate pair\u2019s slot name and value be given by cs and cv (with vectors of multi-word slot names/values summed together).", 
        "112": "The NBT model learns to map this tuple into a single vector c of the same dimensionality as the utterance representation r. These two representations are then forced to interact in order to learn a similarity metric which discriminates between interactions of utterances with slot-value pairs that they either do or do not express:\nc = \u03c3 ( W sc (cs + cv) + b s c ) (7) d = r\u2297 c (8)\nwhere \u2297 denotes element-wise vector multiplication.", 
        "113": "The dot product, which may seem like the more intuitive similarity metric, would reduce the rich set of features in d to a single scalar.", 
        "114": "The element-wise multiplication allows the downstream network to make better use of its parameters by learning non-linear interactions between sets of features in r and c.2  3.3 Context Modelling  This \u2018decoder\u2019 does not yet suffice to extract intents from utterances in human-machine dialogue.", 
        "115": "To understand some queries, the belief tracker must be aware of context, i.e.", 
        "116": "the flow of dialogue leading up to the latest user utterance.", 
        "117": "While all previous system and user utterances are important, the most relevant one is the last system utterance, in which the dialogue system could have performed (among others) one of the following two system acts:\n1.", 
        "118": "System Request: The system asks the user about the value of a specific slot Tq.", 
        "119": "If the system utterance is: \u2018what price range would\n2We also tried to concatenate r and c and pass that vector to the downstream decision-making neural network.", 
        "120": "However, this set-up led to very weak performance since our relatively small datasets did not suffice for the network to learn to model the interaction between the two feature vectors.", 
        "121": "you like?\u2019 and the user answers with any, the model must infer the reference to price range, and not to other slots such as area or food.", 
        "122": "2.", 
        "123": "System Confirm: The system asks the user to confirm whether a specific slot-value pair (Ts, Tv) is part of their desired constraints.", 
        "124": "For example, if the user responds to \u2018how about Turkish food?\u2019 with \u2018yes\u2019, the model must be aware of the system act in order to correctly update the belief state.", 
        "125": "If we make the Markovian decision to only consider the last set of system acts, we can incorporate context modelling into the NBT.", 
        "126": "Let tq and (ts, tv) be the word vectors of the arguments for the system request and confirm acts (zero vectors if none).", 
        "127": "The model computes the following measures of similarity between the system acts, candidate pair (cs, cv) and utterance representation r:\nmr = (cs \u00b7 tq)r (9) mc = (cs \u00b7 ts)(cv \u00b7 tv)r (10)\nwhere \u00b7 denotes dot product.", 
        "128": "The computed similarity terms act as gating mechanisms which only pass the utterance representation through if the system asked about the current candidate slot or slot-value pair.", 
        "129": "This type of interaction is particularly useful for the confirm system act: if the system asks the user to confirm, the user is likely not to mention any slot values, but to just respond affirmatively or negatively.", 
        "130": "This means that the model must consider the three-way interaction between the utterance, candidate slot-value pair and the slot value pair offered by the system.", 
        "131": "If (and only if) the latter two are the same should the model consider the affirmative or negative polarity of the user utterance when making the subsequent binary decision.", 
        "132": "Binary Decision Maker The intermediate representations are passed through another hidden layer and then combined.", 
        "133": "If \u03c6dim(x) = \u03c3(Wx+ b) is a layer which maps input vector x to a vector of size dim, the input to the final binary softmax (which represents the decision) is given by:\ny = \u03c62 ( \u03c6100(d) + \u03c6100(mr) + \u03c6100(mc) )  4 Belief State Update Mechanism  In spoken dialogue systems, belief tracking models operate over the output of automatic speech recognition (ASR).", 
        "134": "Despite improvements to speech\nrecognition, the need to make the most out of imperfect ASR will persist as dialogue systems are used in increasingly noisy environments.", 
        "135": "In this work, we define a simple rule-based belief state update mechanism which can be applied to ASR N -best lists.", 
        "136": "For dialogue turn t, let syst\u22121 denote the preceding system output, and let ht denote the list of N ASR hypotheses hti with posterior probabilities pti.", 
        "137": "For any hypothesis h t i, slot s and slot value v \u2208 Vs, NBT models estimate P(s, v | hti, syst\u22121), which is the (turn-level) probability that (s, v) was expressed in the given hypothesis.", 
        "138": "The predictions for N such hypotheses are then combined as:\nP(s, v | ht, syst\u22121) = N\u2211\ni=1\npti P ( s, v | hti, syst )\nThis turn-level belief state estimate is then combined with the (cumulative) belief state up to time (t\u2212 1) to get the updated belief state estimate:\nP(s, v | h1:t, sys1:t\u22121) = \u03bb P ( s, v | ht, syst\u22121 )\n+ (1\u2212 \u03bb) P ( s, v | h1:t\u22121, sys1:t\u22122 )\nwhere \u03bb is the coefficient which determines the relative weight of the turn-level and previous turns\u2019 belief state estimates.3 For slot s, the set of its detected values at turn t is then given by:\nV ts = {v \u2208 Vs | P ( s, v | h1:t, sys1:t\u22121 ) \u2265 0.5}\nFor informable (i.e.", 
        "139": "goal-tracking) slots, the value in V ts with the highest probability is chosen as the current goal (if V ts 6= {\u2205}).", 
        "140": "For requests, all slots in V treq are deemed to have been requested.", 
        "141": "As requestable slots serve to model single-turn user queries, they require no belief tracking across turns.", 
        "142": "5 Experiments    5.1 Datasets  Two datasets were used for training and evaluation.", 
        "143": "Both consist of user conversations with taskoriented dialogue systems designed to help users find suitable restaurants around Cambridge, UK.", 
        "144": "The two corpora share the same domain ontology, which contains three informable (i.e.", 
        "145": "goal-tracking) slots: FOOD, AREA and PRICE.", 
        "146": "The users can specify values for these slots in order to find restaurants\n3This coefficient was tuned on the DSTC2 development set.", 
        "147": "The best performance was achieved with \u03bb = 0.55.\nwhich best meet their criteria.", 
        "148": "Once the system suggests a restaurant, the users can ask about the values of up to eight requestable slots (PHONE NUMBER, ADDRESS, etc.).", 
        "149": "The two datasets are:\n1.", 
        "150": "DSTC2: We use the transcriptions, ASR hypotheses and turn-level semantic labels provided for the Dialogue State Tracking Challenge 2 (Henderson et al., 2014a).", 
        "151": "The official transcriptions contain various spelling errors which we corrected manually; the cleaned version of the dataset is available at mi.eng.cam.ac.uk/\u02dcnm480/ dstc2-clean.zip.", 
        "152": "The training data contains 2207 dialogues and the test set consists of 1117 dialogues.", 
        "153": "We train NBT models on transcriptions but report belief tracking performance on test set ASR hypotheses provided in the original challenge.", 
        "154": "2.", 
        "155": "WOZ 2.0: Wen et al.", 
        "156": "(2017) performed a Wizard of Oz style experiment in which Amazon Mechanical Turk users assumed the role of the system or the user of a task-oriented dialogue system based on the DSTC2 ontology.", 
        "157": "Users typed instead of using speech, which means performance in the WOZ experiments is more indicative of the model\u2019s capacity for semantic understanding than its robustness to ASR errors.", 
        "158": "Whereas in the DSTC2 dialogues users would quickly adapt to the system\u2019s (lack of) language understanding capability, the WOZ experimental design gave them freedom to use more sophisticated language.", 
        "159": "We expanded the original WOZ dataset from Wen et al.", 
        "160": "(2017) using the same data collection procedure, yielding a total of 1200 dialogues.", 
        "161": "We divided these into 600 training, 200 validation and 400 test set dialogues.", 
        "162": "The WOZ 2.0 dataset is available at mi.eng.cam.ac.", 
        "163": "uk/\u02dcnm480/woz_2.0.zip.", 
        "164": "Training Examples The two corpora are used to create training data for two separate experiments.", 
        "165": "For each dataset, we iterate over all train set utterances, generating one example for each of the slotvalue pairs in the ontology.", 
        "166": "An example consists of a transcription, its context (i.e.", 
        "167": "list of preceding system acts) and a candidate slot-value pair.", 
        "168": "The binary label for each example indicates whether or not its utterance and context express the example\u2019s candidate pair.", 
        "169": "For instance, \u2018I would like Irish\nfood\u2019 would generate a positive example for candidate pair FOOD=IRISH, and a negative example for every other slot-value pair in the ontology.", 
        "170": "Evaluation We focus on two key evaluation metrics introduced in (Henderson et al., 2014a):\n1.", 
        "171": "Goals (\u2018joint goal accuracy\u2019): the proportion of dialogue turns where all the user\u2019s search goal constraints were correctly identified;\n2.", 
        "172": "Requests: similarly, the proportion of dialogue turns where user\u2019s requests for information were identified correctly.", 
        "173": "5.2 Models  We evaluate two NBT model variants: NBT-DNN and NBT-CNN.", 
        "174": "To train the models, we use the Adam optimizer (Kingma and Ba, 2015) with crossentropy loss, backpropagating through all the NBT subcomponents while keeping the pre-trained word vectors fixed (in order to allow the model to deal with unseen words at test time).", 
        "175": "The model is trained separately for each slot.", 
        "176": "Due to the high class bias (most of the constructed examples are negative), we incorporate a fixed number of positive examples in each mini-batch.4\nBaseline Models For each of the two datasets, we compare the NBT models to:\n1.", 
        "177": "A baseline system that implements a wellknown competitive delexicalisation-based model for that dataset.", 
        "178": "For DSTC2, the model is that of Henderson et al.", 
        "179": "(2014c; 2014d).", 
        "180": "This model is an n-gram based neural network model with recurrent connections between turns (but not inside utterances) which replaces occurrences of slot names and values with generic delexicalised features.", 
        "181": "For WOZ 2.0, we compare the NBT models to a more sophisticated belief tracking model presented in (Wen et al., 2017).", 
        "182": "This model uses an RNN for belief state updates and a CNN for turn-level feature extraction.", 
        "183": "Unlike NBTCNN, their CNN operates not over vectors,\n4Model hyperparameters were tuned on the respective validation sets.", 
        "184": "For both datasets, the initial Adam learning rate was set to 0.001, and 1\n8 th of positive examples were included\nin each mini-batch.", 
        "185": "The batch size did not affect performance: it was set to 256 in all experiments.", 
        "186": "Gradient clipping (to [\u22122.0, 2.0]) was used to handle exploding gradients.", 
        "187": "Dropout (Srivastava et al., 2014) was used for regularisation (with 50% dropout rate on all intermediate representations).", 
        "188": "Both NBT models were implemented in TensorFlow (Abadi et al., 2015).", 
        "189": "but over delexicalised features akin to those used by Henderson et al.", 
        "190": "(2014c).", 
        "191": "2.", 
        "192": "The same baseline model supplemented with a task-specific semantic dictionary (produced by the baseline system creators).", 
        "193": "The two dictionaries are available at mi.eng.cam.", 
        "194": "ac.uk/\u02dcnm480/sem-dict.zip.", 
        "195": "The DSTC2 dictionary contains only three rephrasings.", 
        "196": "Nonetheless, the use of these rephrasings translates to substantial gains in DST performance (see Sect.", 
        "197": "6.1).", 
        "198": "We believe this result supports our claim that the vocabulary used by Mechanical Turkers in DSTC2 was constrained by the system\u2019s inability to cope with lexical variation and ASR noise.", 
        "199": "The WOZ dictionary includes 38 rephrasings, showing that the unconstrained language used by Mechanical Turkers in the Wizard-of-Oz setup requires more elaborate lexicons.", 
        "200": "Both baseline models map exact matches of ontology-defined intents (and their lexiconspecified rephrasings) to one-hot delexicalised ngram features.", 
        "201": "This means that pre-trained vectors cannot be incorporated directly into these models.", 
        "202": "6 Results    6.1 Belief Tracking Performance  Table 1 shows the performance of NBT models trained and evaluated on DSTC2 and WOZ 2.0 datasets.", 
        "203": "The NBT models outperformed the baseline models in terms of both joint goal and request accuracies.", 
        "204": "For goals, the gains are always statistically significant (paired t-test, p < 0.05).", 
        "205": "Moreover, there was no statistically significant variation between the NBT and the lexicon-supplemented models, showing that the NBT can handle semantic relations which otherwise had to be explicitly encoded in semantic dictionaries.", 
        "206": "While the NBT performs well across the board, we can compare its performance on the two datasets to understand its strengths.", 
        "207": "The improvement over the baseline is greater on WOZ 2.0, which corroborates our intuition that the NBT\u2019s ability to learn linguistic variation is vital for this dataset containing longer sentences, richer vocabulary and no ASR errors.", 
        "208": "By comparison, the language of the subjects in the DSTC2 dataset is less rich, and compensating for ASR errors is the main hurdle: given access to the DSTC2 test set transcriptions, the NBT models\u2019 goal accuracy rises to 0.96.", 
        "209": "This\nindicates that future work should focus on better ASR compensation if the model is to be deployed in environments with challenging acoustics.", 
        "210": "6.2 The Importance of Word Vector Spaces  The NBT models use the semantic relations embedded in the pre-trained word vectors to handle semantic variation and produce high-quality intermediate representations.", 
        "211": "Table 2 shows the performance of NBT-CNN5 models making use of three different word vector collections: 1) \u2018random\u2019 word vectors initialised using the XAVIER initialisation (Glorot and Bengio, 2010); 2) distributional GloVe vectors (Pennington et al., 2014), trained using co-occurrence information in large textual corpora; and 3) semantically specialised ParagramSL999 vectors (Wieting et al., 2015), which are obtained by injecting semantic similarity constraints from the Paraphrase Database (Ganitkevitch et al., 2013) into the distributional GloVe vectors in order to improve their semantic content.", 
        "212": "The results in Table 2 show that the use of semantically specialised word vectors leads to considerable performance gains: Paragram-SL999 vectors (significantly) outperformed GloVe and XAVIER vectors for goal tracking on both datasets.", 
        "213": "The gains are particularly robust for noisy DSTC2 data, where both collections of pre-trained vectors consistently outperformed random initialisation.", 
        "214": "The gains are weaker for the noise-free WOZ 2.0 dataset, which seems to be large (and clean) enough for the NBT model to learn task-specific rephrasings and compensate for the lack of semantic content in the word vectors.", 
        "215": "For this dataset, GloVe vectors do not improve over the randomly initialised ones.", 
        "216": "We believe this happens because distributional models keep related, yet antonymous words close together (e.g.", 
        "217": "north and south, expensive and inexpensive), offsetting the useful semantic content embedded in this vector spaces.", 
        "218": "5The NBT-DNN model showed the same trends.", 
        "219": "For brevity, Table 2 presents only the NBT-CNN figures.", 
        "220": "7 Conclusion  In this paper, we have proposed a novel neural belief tracking (NBT) framework designed to overcome current obstacles to deploying dialogue systems in real-world dialogue domains.", 
        "221": "The NBT models offer the known advantages of coupling Spoken Language Understanding and Dialogue State Tracking, without relying on hand-crafted semantic lexicons to achieve state-of-the-art performance.", 
        "222": "Our evaluation demonstrated these benefits: the NBT models match the performance of models which make use of such lexicons and vastly outperform them when these are not available.", 
        "223": "Finally, we have shown that the performance of NBT models improves with the semantic quality of the underlying word vectors.", 
        "224": "To the best of our knowledge, we are the first to move past intrinsic evaluation and show that semantic specialisation boosts performance in downstream tasks.", 
        "225": "In future work, we intend to explore applications of the NBT for multi-domain dialogue systems, as well as in languages other than English that require handling of complex morphological variation.", 
        "226": "Acknowledgements  The authors would like to thank Ivan Vulic\u0301, Ulrich Paquet, the Cambridge Dialogue Systems Group and the anonymous ACL reviewers for their constructive feedback and helpful discussions."
    }, 
    "document_id": "P17-1163.pdf.json"
}
