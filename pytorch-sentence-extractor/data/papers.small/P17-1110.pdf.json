{
    "abstract_sentences": {
        "1": "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS).", 
        "2": "Most existingmethods focus on improve the performance for each single criterion.", 
        "3": "However, it is interesting to exploit these different criteria and mining their common underlying knowledge.", 
        "4": "In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge frommultiple heterogeneous segmentation criteria.", 
        "5": "Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning.", 
        "6": "Source codes of this paper are available on Github1."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1193\u20131203 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1110  1 Introduction  Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP).", 
        "2": "Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms, and rely on a large-scale annotated corpus whose cost is extremely expensive.", 
        "3": "Although there have been great achievements in building CWS corpora, they are somewhat incompatible due to different segmentation criteria.", 
        "4": "As shown in Table 1, given a sentence \u201c\u59da\u660e\u8fdb \u5165\u603b\u51b3\u8d5b (YaoMing reaches the final)\u201d, the two commonly-used corpora, PKU\u2019s People\u2019s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Fei, 2000), use different segmentation criteria.", 
        "5": "In a sense, it is a waste of resources if we fail to fully exploit these corpora.", 
        "6": "\u2217Corresponding author.", 
        "7": "1https://github.com/FudanNLP\nRecently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging (Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015, 2016).", 
        "8": "These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other.", 
        "9": "However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model.", 
        "10": "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016).", 
        "11": "In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria.", 
        "12": "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features.", 
        "13": "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.", 
        "14": "Finally, we exploit the eight segmentation criteria on the five simplified Chi-\n1193\nnese and three traditional Chinese corpora.", 
        "15": "Experiments show that our models are effective to improve the performance for CWS.", 
        "16": "We also observe that traditional Chinese could benefit from incorporating knowledge from simplified Chinese.", 
        "17": "The contributions of this paper could be summarized as follows.", 
        "18": "\u2022 Multi-criteria learning is first introduced for CWS, in which we propose three sharedprivate models to integrate multiple segmentation criteria.", 
        "19": "\u2022 An adversarial strategy is used to force the shared layer to learn criteria-invariant features, in which an new objective function is also proposed instead of the original cross-entropy loss.", 
        "20": "\u2022 We conduct extensive experiments on eight CWS corpora with different segmentation criteria, which is by far the largest number of datasets used simultaneously.", 
        "21": "2 General Neural Model for Chinese Word Segmentation  Chinese word segmentation task is usually regarded as a character based sequence labeling problem.", 
        "22": "Specifically, each character in a sentence is labeled as one of L = {B,M,E, S}, indicating the begin, middle, end of a word, or a word with single character.", 
        "23": "There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc.", 
        "24": "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015a,b).", 
        "25": "Specifically, given a sequence with n characters X = {x1, .", 
        "26": ".", 
        "27": ".", 
        "28": ", xn}, the aim of CWS task is to figure out the ground truth of labels Y \u2217 = {y\u22171, .", 
        "29": ".", 
        "30": ".", 
        "31": ", y\u2217n}:\nY \u2217 = argmax Y \u2208Ln p(Y |X), (1)\nwhere L = {B,M,E, S}.", 
        "32": "The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer.", 
        "33": "The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network.", 
        "34": "In this\npaper, we adopt the bi-directional long short-term memory neural networks followed by CRF as the tag inference layer.", 
        "35": "Figure 1 illustrates the general architecture of CWS.", 
        "36": "2.1 Embedding layer  In neural models, the first step usually is to map discrete language symbols to distributed embedding vectors.", 
        "37": "Formally, we lookup embedding vector from embedding matrix for each character xi as exi \u2208 Rde , where de is a hyper-parameter indicating the size of character embedding.", 
        "38": "2.2 Feature layers  We adopt bi-directional long short-term memory (Bi-LSTM) as feature layers.", 
        "39": "While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.", 
        "40": "LSTM LSTM introduces gate mechanism and memory cell to maintain long dependency information and avoid gradient vanishing.", 
        "41": "Formally, LSTM, with input gate i, output gate o, forget gate f and memory cell c, could be expressed as:  \nii oi fi c\u0303i\n  =  \n\u03c3 \u03c3 \u03c3 \u03d5\n  ( Wg\u1d40 [ exi hi\u22121 ] + bg ) , (2)\nci = ci\u22121 \u2299 fi + c\u0303i \u2299 ii, (3) hi = oi \u2299 \u03d5(ci), (4)\nwhere Wg \u2208 R(de+dh)\u00d74dh and bg \u2208 R4dh are trainable parameters.", 
        "42": "dh is a hyper-parameter, in-\ndicating the hidden state size.", 
        "43": "Function \u03c3(\u00b7) and \u03d5(\u00b7) are sigmoid and tanh functions respectively.", 
        "44": "Bi-LSTM In order to incorporate information from both sides of sequence, we use bi-directional LSTM (Bi-LSTM) with forward and backward directions.", 
        "45": "The update of each Bi-LSTM unit can be written precisely as follows:\nhi = \u2212\u2192h i \u2295 \u2190\u2212h i, (5) = Bi-LSTM(exi , \u2212\u2192h i\u22121, \u2190\u2212h i+1, \u03b8), (6)\nwhere \u2212\u2192h i and \u2190\u2212h i are the hidden states at position i of the forward and backward LSTMs respectively; \u2295 is concatenation operation; \u03b8 denotes all parameters in Bi-LSTM model.", 
        "46": "2.3 Inference Layer  After extracting features, we employ conditional random fields (CRF) (Lafferty et al., 2001) layer to inference tags.", 
        "47": "In CRF layer, p(Y |X) in Eq (1) could be formalized as:\np(Y |X) = \u03a8(Y |X)\u2211 Y \u2032\u2208Ln \u03a8(Y \u2032|X) .", 
        "48": "(7)\nHere, \u03a8(Y |X) is the potential function, and we only consider interactions between two successive labels (first order linear chain CRFs):\n\u03a8(Y |X) = n\u220f\ni=2\n\u03c8(X, i, yi\u22121, yi), (8)\n\u03c8(x, i, y\u2032, y) = exp(s(X, i)y + by\u2032y), (9)\nwhere by\u2032y \u2208 R is trainable parameters respective to label pair (y\u2032, y).", 
        "49": "Score function s(X, i) \u2208 R|L| assigns score for each label on tagging the i-th character:\ns(X, i) =W\u22a4s hi + bs, (10)\nwhere hi is the hidden state of Bi-LSTM at position i; Ws \u2208 Rdh\u00d7|L| and bs \u2208 R|L| are trainable parameters.", 
        "50": "3 Multi-Criteria Learning for Chinese Word Segmentation  Although neural models are widely used on CWS, most of them cannot deal with incompatible criteria with heterogonous segmentation criteria simultaneously.", 
        "51": "Inspired by the success of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003; Liu et al., 2016a,b), we regard the heterogenous\ncriteria as multiple \u201crelated\u201d tasks, which could improve the performance of each other simultaneously with shared information.", 
        "52": "Formally, assume that there areM corpora with heterogeneous segmentation criteria.", 
        "53": "We referDm as corpusm with Nm samples:\nDm = {(X(m)i , Y (m) i )}Nmi=1, (11)\nwhere Xmi and Y mi denote the i-th sentence and the corresponding label in corpusm.", 
        "54": "To exploit the shared information between these different criteria, we propose three sharing models for CWS task as shown in Figure 2.", 
        "55": "The feature layers of these three models consist of a private (criterion-specific) layer and a shared (criterioninvariant) layer.", 
        "56": "The difference between three models is the information flow between the task layer and the shared layer.", 
        "57": "Besides, all of these three models also share the embedding layer.", 
        "58": "3.1 Model-I: Parallel Shared-Private Model  In the feature layer of Model-I, we regard the private layer and shared layer as two parallel layers.", 
        "59": "For corpusm, the hidden states of shared layer and private layer are:\nh(s)i =Bi-LSTM(exi , \u2212\u2192h (s)i\u22121, \u2190\u2212h (s)i+1, \u03b8s), (12) h(m)i =Bi-LSTM(exi , \u2212\u2192h (m)i\u22121, \u2190\u2212h (m)i+1, \u03b8m), (13)\nand the score function in the CRF layer is computed as:\ns(m)(X, i) =W(m)s \u22a4 [ h(s)i h(m)i ] + b(m)s , (14)\nwhere W(m)s \u2208 R2dh\u00d7|L| and b(m)s \u2208 R|L| are criterion-specific parameters for corpusm.", 
        "60": "3.2 Model-II: Stacked Shared-Private Model  In the feature layer of Model-II, we arrange the shared layer and private layer in stacked manner.", 
        "61": "The private layer takes output of shared layer as input.", 
        "62": "For corpus m, the hidden states of shared layer and private layer are:\nh(s)i = Bi-LSTM(exi , \u2212\u2192h (s)i\u22121, \u2190\u2212h (s)i+1, \u03b8s), (15)\nh(m)i = Bi-LSTM( [ exi h(s)i ] , \u2212\u2192h (m)i\u22121, \u2190\u2212h (m)i+1 , \u03b8m) (16)\nand the score function in the CRF layer is computed as:\ns(m)(X, i) =W(m)s \u22a4 h(m)i + b (m) s , (17)\nwhere W(m)s \u2208 R2dh\u00d7|L| and b(m)s \u2208 R|L| are criterion-specific parameters for corpusm.", 
        "63": "3.3 Model-III: Skip-Layer Shared-Private Model  In the feature layer of Model-III, the shared layer and private layer are in stacked manner as ModelII.", 
        "64": "Additionally, we send the outputs of shared layer to CRF layer directly.", 
        "65": "The Model III can be regarded as a combination ofModel-I andModel-II.", 
        "66": "For corpusm, the hidden states of shared layer and private layer are the same with Eq (15) and (16), and the score function in CRF layer is computed as the same as Eq (14).", 
        "67": "3.4 Objective function  The parameters of the network are trained to maximize the log conditional likelihood of true labels on all the corpora.", 
        "68": "The objective functionJseg can be computed as:\nJseg(\u0398m,\u0398s) = M\u2211\nm=1\nNm\u2211\ni=1\nlog p(Y (m)i |X (m) i ; \u0398 m,\u0398s),\n(18) where\u0398m and\u0398s denote all the parameters in private and shared layers respectively.", 
        "69": "4 Incorporating Adversarial Training for Shared Layer  Although the shared-private model separates the feature space into shared and private spaces, there is no guarantee that sharable features do not exist in private feature space, or vice versa.", 
        "70": "Inspired by the work on domain adaptation (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria.", 
        "71": "Therefore, we jointly optimize the shared\nlayer via adversarial training (Goodfellow et al., 2014).", 
        "72": "Therefore, besides the task loss for CWS, we additionally introduce an adversarial loss to prevent criterion-specific feature from creeping into shared space as shown in Figure 3.", 
        "73": "We use a criterion discriminator which aims to recognizewhich criterion the sentence is annotated by using the shared features.", 
        "74": "Specifically, given a sentence X with length n, we refer to h(s)X as shared features for X in one of the sharing models.", 
        "75": "Here, we compute h(s)X by simply averaging the hidden states of shared layer h(s)X = 1 n \u2211n i h (s) xi .", 
        "76": "The criterion discriminator computes the probability p(\u00b7|X) over all criteria as:\np(\u00b7|X; \u0398d,\u0398s) = softmax(W\u22a4d h(s)X + bd), (19)\nwhere\u0398d indicates the parameters of criterion discriminatorWd \u2208 Rdh\u00d7M and bd \u2208 RM ; \u0398s denotes the parameters of shared layers.", 
        "77": "4.1 Adversarial loss function  The criterion discriminator maximizes the cross entropy of predicted criterion distribution p(\u00b7|X) and true criterion.", 
        "78": "max \u0398d J 1adv(\u0398d) =\nM\u2211\nm=1\nNm\u2211\ni=1\nlog p(m|X(m)i ; \u0398d,\u0398s).", 
        "79": "(20)\nAn adversarial loss aims to produce shared features, such that a criterion discriminator cannot reliably predict the criterion by using these shared features.", 
        "80": "Therefore, wemaximize the entropy of predicted criterion distribution when training shared parameters.", 
        "81": "max \u0398s J 2adv(\u0398s) =\nM\u2211\nm=1\nNm\u2211\ni=1\nH ( p(m|X(m)i ; \u0398d,\u0398s) ) ,\n(21) where H(p) = \u2212\u2211i pi log pi is an entropy of distribution p. Unlike (Ganin et al., 2016), we use entropy term instead of negative cross-entropy.", 
        "82": "5 Training  Finally, we combine the task and adversarial objective functions.", 
        "83": "J (\u0398;D) = Jseg(\u0398m,\u0398s) + J 1adv(\u0398d) + \u03bbJ 2adv(\u0398s), (22) where \u03bb is the weight that controls the interaction of the loss terms and D is the training corpora.", 
        "84": "The training procedure is to optimize two discriminative classifiers alternately as shown in Algorithm 1.", 
        "85": "We use Adam (Kingma and Ba, 2014) with minibatchs to maximize the objectives.", 
        "86": "Notably, when using adversarial strategy, we firstly train 2400 epochs (each epoch only trains on eight batches from different corpora), then we only optimize Jseg(\u0398m,\u0398s) with \u0398s fixed until convergence (early stop strategy).", 
        "87": "6 Experiments    6.1 Datasets  To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008).", 
        "88": "Table 2 gives the details of the eight datasets.", 
        "89": "Among these datasets, AS, CITYU and CKIP are traditional Chinese, while the\nAlgorithm 1 Adversarial multi-criteria learning for CWS task.", 
        "90": "1: for i = 1; i <= n_epoch; i++ do 2: # Train tag predictor for CWS 3: form = 1;m <=M ;m++ do 4: # Randomly pick data from corpusm 5: B = {X,Y }bm1 \u2208 Dm 6: \u0398s += \u03b1\u2207\u0398sJ (\u0398;B) 7: \u0398m += \u03b1\u2207\u0398mJ (\u0398;B) 8: end for 9: # Train criterion discriminator 10: form = 1;m <=M ;m++ do 11: B = {X,Y }bm1 \u2208 Dm 12: \u0398d += \u03b1\u2207\u0398dJ (\u0398;B) 13: end for 14: end for\nremains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese.", 
        "91": "We use 10% data of shuffled train set as development set for all datasets.", 
        "92": "6.2 Experimental Configurations  For hyper-parameter configurations, we set both the character embedding size de and the dimensionality of LSTM hidden states dh to 100.", 
        "93": "The initial learning rate \u03b1 is set to 0.01.", 
        "94": "The loss weight coefficient \u03bb is set to 0.05.", 
        "95": "Since the scale of each dataset varies, we use different training batch sizes for datasets.", 
        "96": "Specifically, we set batch sizes of AS andMSR datasets as 512 and 256 respectively, and 128 for remains.", 
        "97": "We employ dropout strategy on embedding layer, keeping 80% inputs (20% dropout rate).", 
        "98": "For initialization, we randomize all parameters following uniform distribution at (\u22120.05, 0.05).", 
        "99": "We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013).", 
        "100": "Following previous work (Chen et al., 2015b; Pei et al., 2014), all experiments including baseline results are using pre-trained character embedding with bigram feature.", 
        "101": "6.3 Overall Results  Table 3 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks.", 
        "102": "(1) In the first block, we can see that the performance is boosted by using Bi-LSTM, and the\nperformance of Bi-LSTM cannot be improved by merely increasing the depth of networks.", 
        "103": "In addition, although the F value of LSTMmodel in (Chen et al., 2015b) is 97.4%, they additionally incorporate an external idiom dictionary.", 
        "104": "(2) In the second block, our proposed three models based on multi-criteria learning boost performance.", 
        "105": "Model-I gains 0.75% improvement on averaging F-measure score compared with BiLSTM result (94.14%).", 
        "106": "Only the performance on MSRA drops slightly.", 
        "107": "Compared to the baseline results (Bi-LSTM and stacked Bi-LSTM), the proposed models boost the performance with the help of exploiting information across these heterogeneous segmentation criteria.", 
        "108": "Although various criteria have different segmentation granularities, there are still some underlying information shared.", 
        "109": "For instance, MSRA and CTB treat family name and last name as one token \u201c\u5b81\u6cfd\u6d9b (NingZeTao)\u201d, whereas some other datasets, like PKU, regard them as two tokens, \u201c\u5b81 (Ning)\u201d and \u201c\u6cfd\u6d9b (ZeTao)\u201d.", 
        "110": "The partial boundaries (before \u201c\u5b81 (Ning)\u201d or after \u201c\u6d9b (Tao)\u201d) can be shared.", 
        "111": "(3) In the third block, we introduce adversarial training.", 
        "112": "By introducing adversarial training, the performances are further boosted, and Model-I is slightly better than Model-II and Model-III.", 
        "113": "The adversarial training tries to make shared layer keep criteria-invariant features.", 
        "114": "For instance, as shown in Table 3, when we use shared information, the performance onMSRA drops (worse than baseline result).", 
        "115": "The reason may be that the shared parameters bias to other segmentation criteria and introduce noisy features into shared parameters.", 
        "116": "When we additionally incorporate the adversarial stra-\ntegy, we observe that the performance on MSRA is improved and outperforms the baseline results.", 
        "117": "We could also observe the improvements on other datasets.", 
        "118": "However, the boost from the adversarial strategy is not significant.", 
        "119": "The main reason might be that the proposed three sharing models implicitly attempt to keep invariant features by shared parameters and learn discrepancies by the task layer.", 
        "120": "6.4 Speed  To further explore the convergence speed, we plot the results on development sets through epochs.", 
        "121": "Figure 4 shows the learning curve of Model-I without incorporating adversarial strategy.", 
        "122": "As shown in Figure 4, the proposed model makes progress gradually on all datasets.", 
        "123": "After about 1000 epochs, the performance becomes stable and convergent.", 
        "124": "We also test the decoding speed, and our models process 441.38 sentences per second averagely.", 
        "125": "As the proposed models and the baseline models (Bi-LSTM and stacked Bi-LSTM) are nearly in the same complexity, all models are nearly the same efficient.", 
        "126": "However, the time consumption of training process varies from model to model.", 
        "127": "For the models without adversarial training, it costs about 10 hours for training (the same for stacked Bi-LSTM to train eight datasets), whereas it takes about 16 hours for the models with adversarial training.", 
        "128": "All the experiments are conducted on the hardware with Intel(R) Xeon(R) CPU E52643 v3 @ 3.40GHz and NVIDIA GeForce GTX TITAN X.", 
        "129": "6.5 Error Analysis  We further investigate the benefits of the proposed models by comparing the error distributions between the single-criterion learning (baselinemodel Bi-LSTM) andmulti-criteria learning (Model-I and Model-I with adversarial training) as shown in Figure 5.", 
        "130": "According to the results, we could observe that a large proportion of points lie above diagonal lines in Figure 5a and Figure 5b, which implies that performance benefit from integrating knowledge and complementary information from other corpora.", 
        "131": "As shown in Table 3, on the test set of CITYU, the performance of Model-I and its adversarial version (Model-I+ADV) boost from\n92.17% to 95.59% and 95.42% respectively.", 
        "132": "In addition, we observe that adversarial strategy is effective to prevent criterion specific features from creeping into shared space.", 
        "133": "For instance, the segmentation granularity of personal name is often different according to heterogenous criteria.", 
        "134": "With the help of adversarial strategy, our models could correct a large proportion of mistakes on personal name.", 
        "135": "Table 4 lists the examples from 2333-th and 89-th sentences in test sets of PKU and MSRA datasets respectively.", 
        "136": "7 Knowledge Transfer  We also conduct experiments of whether the shared layers can be transferred to the other related tasks or domains.", 
        "137": "In this section, we investigate the ability of knowledge transfer on two experiments: (1) simplified Chinese to traditional Chinese and (2) formal texts to informal texts.", 
        "138": "7.1 Simplified Chinese to Traditional Chinese  Traditional Chinese and simplified Chinese are two similar languages with slightly difference on character forms (e.g.", 
        "139": "multiple traditional characters might map to one simplified character).", 
        "140": "We investigate that if datasets in traditional Chinese and simplified Chinese could help each other.", 
        "141": "Table 5 gives the results of Model-I on 3 traditio-\nnal Chinese datasets under the help of 5 simplified Chinese datasets.", 
        "142": "Specifically, we firstly train the model on simplified Chinese datasets, then we train traditional Chinese datasets independently with shared parameters fixed.", 
        "143": "As we can see, the average performance is boosted by 0.41% on F-measure score (from 93.78% to 94.19%), which indicates that shared features learned from simplified Chinese segmentation criteria can help to improve performance on traditional Chinese.", 
        "144": "Like MSRA, as AS dataset is relatively large (train set of 5.4M tokens), the features learned by shared parameters might bias to other datasets and thus hurt performance on such large dataset AS.", 
        "145": "7.2 Formal Texts to Informal Texts    7.2.1 Dataset  We use the NLPCC 2016 dataset2 (Qiu et al., 2016) to evaluate our model on micro-blog texts.", 
        "146": "The NLPCC 2016 data are provided by the shared task in the 5th CCF Conference on Natural Language Processing & Chinese Computing (NLPCC 2016): Chinese Word Segmentation and POS Tagging for micro-blog Text.", 
        "147": "Unlike the popular used newswire dataset, the NLPCC 2016 dataset is collected from Sina Weibo3, which consists of the informal texts frommicro-blog with the various topics, such as finance, sports, entertainment, and so on.", 
        "148": "The information of the dataset is shown in Table 6.", 
        "149": "2https://github.com/FudanNLP/ NLPCC-WordSeg-Weibo\n3http://www.weibo.com/  7.2.2 Results  Formal documents (like the eight datasets in Table 2) and micro-blog texts are dissimilar in many aspects.", 
        "150": "Thus, we further investigate that if the formal texts could help to improve the performance of micro-blog texts.", 
        "151": "Table 7 gives the results of Model-I on the NLPCC 2016 dataset under the help of the eight datasets in Table 2.", 
        "152": "Specifically, we firstly train themodel on the eight datasets, then we train on the NLPCC 2016 dataset alone with shared parameters fixed.", 
        "153": "The baseline model is BiLSTM which is trained on the NLPCC 2016 dataset alone.", 
        "154": "As we can see, the performance is boosted by 0.30% on F-measure score (from 93.94% to 94.24%), and we could also observe that the OOV recall rate is boosted by 3.97%.", 
        "155": "It shows that the shared features learned from formal texts can help to improve the performance on ofmicro-blog texts.", 
        "156": "8 Related Works  There are many works on exploiting heterogeneous annotation data to improve various NLP tasks.", 
        "157": "Jiang et al.", 
        "158": "(2009) proposed a stacking-based model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations.", 
        "159": "Sun and Wan (2012) proposed a structurebased stacking model to reduce the approximation error, which makes use of structured features such as sub-words.", 
        "160": "These models are unidirectional aid and also suffer from error propagation problem.", 
        "161": "Qiu et al.", 
        "162": "(2013) used multi-tasks learning framework to improve the performance of POS tag-\nging on two heterogeneous datasets.", 
        "163": "Li et al.", 
        "164": "(2015) proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations.", 
        "165": "Chao et al.", 
        "166": "(2015) also utilize multiple corpora using coupled sequence labeling model.", 
        "167": "These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features.", 
        "168": "Our proposed models use deep neural networks, which can easily share information with hidden shared layers.", 
        "169": "Chen et al.", 
        "170": "(2016) also adopted neural network models for exploiting heterogeneous annotations based on neural multi-viewmodel, which can be regarded as a simplified version of our proposed models by removing private hidden layers.", 
        "171": "Unlike the above models, we design three sharing-private architectures and keep shared layer to extract criterion-invariance features by introducing adversarial training.", 
        "172": "Moreover, we fully exploit eight corpora with heterogeneous segmentation criteria to model the underlying shared information.", 
        "173": "9 Conclusions & Future Works  In this paper, we propose adversarial multi-criteria learning for CWS by fully exploiting the underlying shared knowledge across multiple heterogeneous criteria.", 
        "174": "Experiments show that our proposed three shared-private models are effective to extract the shared information, and achieve significant improvements over the single-criterion methods.", 
        "175": "Acknowledgments  We appreciate the contribution from Jingjing Gong and Jiacheng Xu.", 
        "176": "Besides, we would like to thank the anonymous reviewers for their valuable comments.", 
        "177": "This work is partially funded by National Natural Science Foundation of China (No.", 
        "178": "61532011 and 61672162), Shanghai Municipal Science and Technology Commission on (No.", 
        "179": "16JC1420401)."
    }, 
    "document_id": "P17-1110.pdf.json"
}
