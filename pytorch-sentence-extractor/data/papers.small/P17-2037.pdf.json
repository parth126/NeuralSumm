{
    "abstract_sentences": {
        "1": "Discourse segmentation is a crucial step in building end-to-end discourse parsers.", 
        "2": "However, discourse segmenters only exist for a few languages and domains.", 
        "3": "Typically they only detect intra-sentential segment boundaries, assuming gold standard sentence and token segmentation, and relying on high-quality syntactic parses and rich heuristics that are not generally available across languages and domains.", 
        "4": "In this paper, we propose statistical discourse segmenters for five languages and three domains that do not rely on gold preannotations.", 
        "5": "We also consider the problem of learning discourse segmenters when no labeled data is available for a language.", 
        "6": "Our fully supervised system obtains 89.5% F1 for English newswire, with slight drops in performance on other domains, and we report supervised and unsupervised (cross-lingual) results for five languages in total."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 237\u2013243 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2037  1 Introduction  Discourse segmentation is the first step in building a discourse parser.", 
        "2": "The goal is to identify the minimal units \u2014 called Elementary Discourse Units (EDU) \u2014 in the documents that will then be linked by discourse relations.", 
        "3": "For example, the sentences (1a) and (1b)1 are each segmented into two EDUs, then respectively linked by a CONTRAST and an ATTRIBUTION relation.", 
        "4": "The EDUs are mostly clauses and may cover a full sentence.", 
        "5": "This step is crucial: making a segmentation error leads to an error in the final analysis.", 
        "6": "Discourse segmentation can also inform other tasks, such as argumentation\n1The examples come from the RST Discourse Treebank.", 
        "7": "mining, anaphora resolution, or speech act assignment (Sidarenka et al., 2015).", 
        "8": "(1) a.", 
        "9": "[Such trappings suggest a glorious past] [but give no hint of a troubled present.]", 
        "10": "b.", 
        "11": "[He said] [the thrift will to get regulators to reverse the decision.]", 
        "12": "We focus on the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) \u2013 and resources such as the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) \u2013 in which discourse structures are trees covering the documents.", 
        "13": "Most recent works on RST discourse parsing focuses on the task of tree building, relying on a gold discourse segmentation (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Li et al., 2014; Joty et al., 2013).", 
        "14": "However, discourse parsers\u2019 performance drops by 12-14% when relying on predicted segmentation (Joty et al., 2015), underscoring the importance of discourse segmentation.", 
        "15": "State-of-the-art performance for discourse segmentation on the RST-DT is about 91% in F1 with predicted parses (Xuan Bach et al., 2012), but these systems rely on a gold segmentation of sentences and words, therefore probably overestimating performance in the wild.", 
        "16": "We propose to build discourse segmenters without making any data assumptions.", 
        "17": "Specifically, rather than segmenting sentences, our systems segment documents directly.", 
        "18": "Furthermore, only a few systems have been developed for languages other than English and domains other than the Wall Street Journal texts from the RST-DT.", 
        "19": "We are the first to perform experiments across 5 languages, and 3 non-newswire English domains.", 
        "20": "Since our goal is to provide a system usable for low-resource languages, we only use language-independent resources: here, the Universal Dependencies (UD) (Nivre et al.,\n237\n2016) Part-of-Speech (POS) tags, for which annotations exist for about 50 languages.", 
        "21": "For the cross-lingual experiments, we also rely on crosslingual word embeddings induced from parallel data.", 
        "22": "With a shared representation, we can transfer model parameters across languages, or learn models jointly through multi-task learning.", 
        "23": "Contributions: We (i) propose a general statistical discourse segmenter (ii) that does not assume gold sentences and tokens, and (iii) evaluate it across 5 languages and 3 domains.", 
        "24": "We make our code available at https://bitbucket.", 
        "25": "org/chloebt/discourse.", 
        "26": "2 Related work  For English RST-DT, the best discourse segmentation results were presented in Xuan Bach et al.", 
        "27": "(2012) (F1 91.0% with automatic parse, 93.7 with gold parse) \u2013 and in Joty et al.", 
        "28": "(2015) for the Instructional corpus (Subba and Di Eugenio, 2009) (F1 80.9% on 10-fold).", 
        "29": "Segmenters based on handwritten rules have been developed for Brazilian Portuguese (Pardo and Nunes, 2008) (51.3% to 56.8%, depending on the genre), Spanish (da Cunha et al., 2010, 2012) (80%) and Dutch (van der Vliet, 2010) (73% with automatic parse, 82% with gold parse).2\nMost statistical discourse segmenters are based on classifiers (Fisher and Roark, 2007; Joty et al., 2015).", 
        "30": "Subba and Di Eugenio (2007) were the first to use a neural network, and Sporleder and Lapata (2005) to model the task as a sequence prediction problem.", 
        "31": "In this work, we do sequence prediction using a neural network.", 
        "32": "All these systems rely on a quite large range of lexical and syntactic features (e.g.", 
        "33": "token, POS tags, lexicalized production rules).", 
        "34": "Sporleder and Lapata (2005) present arguments for a knowledgelean system that can be used for low-resourced languages.", 
        "35": "Their system, however, still relies on several tools and gold annotations (e.g.", 
        "36": "POS tagger, chunker, list of connectives, gold sentences).", 
        "37": "In contrast, we present what is to the best of our knowledge the first work on discourse segmentation that is directly applicable to low-resource languages, presenting results for scenarios where no labeled data is available for the target language.", 
        "38": "Previous work, relying on gold sentence boundaries, also only considers intra-sentential segment\n2 For German (Sidarenka et al., 2015) propose a segmenter in clauses (that may be EDU or not).", 
        "39": "boundaries.", 
        "40": "We move to processing entire documents, motivated by the fact that sentence boundaries are not easily detected across all languages.", 
        "41": "3 Discourse segmentation  Nature of the EDUs Discourse segmentation is the first step in annotating a discourse corpus.", 
        "42": "The annotation guidelines define what is the nature of the EDUs, broadly relying on lexical and syntactic clues.", 
        "43": "If sentences and independent clauses are always minimal units, some fine distinctions make the task difficult.", 
        "44": "In the English RST-DT (Carlson and Marcu, 2001), lexical information is crucial: for instance, the presence of the discourse connective \u201cbut\u201d in example (1a)3 indicates the beginning of an EDU.", 
        "45": "In addition, clausal complements of verbs are generally not treated as EDUs.", 
        "46": "Exceptions are the complements of attribution verbs, as in (1b), and the infinitival clauses marking a PURPOSE relation as the second EDU in (2a).", 
        "47": "Note that, in this latter example, the first infinitival clause (\u201cto cover up .", 
        "48": ".", 
        "49": ".\u201d) is, however, not considered as an EDU.", 
        "50": "This fine distinction corresponds to one of the main difficulties of the task.", 
        "51": "Another one is linked to coordination: coordinated clauses are generally segmented as in (2b), but not coordinated verb phrases as in (2c).", 
        "52": "(2) a.", 
        "53": "[A grand jury has been investigating whether officials at Southern Co. accounting conspired to cover up their accounting for spare parts] [to evade federal income taxes.]", 
        "54": "b.", 
        "55": "[they parcel out money] [so that their clients can find temporary living quarters,] [buy food] (.", 
        "56": ".", 
        "57": ".)", 
        "58": "[and replaster walls.]", 
        "59": "c. [Under Superfund, those] [who owned, generated or transported hazardous waste] [are liable for its cleanup, (.", 
        "60": ".", 
        "61": ".)]", 
        "62": "Finally, in a multi-lingual and multi-domain setting, note that all the corpora do not follow the same rules: for example, the relation ATTRIBUTION is only annotated in the English RST-DT and the corpora for Brazilian Portuguese, consequently, complements of attribution verbs are not segmented in the other corpora.", 
        "63": "3All the examples given come from (Carlson et al., 2001).", 
        "64": "Binary task As in previous studies, we view segmentation as a binary task at the word level: a word is either an EDU boundary (label B, beginning an EDU) or not (label I, inside an EDU).", 
        "65": "This design choice is motivated by the fact that, in RST corpora, the EDUs cover the documents entirely, and that EDUs mostly are adjacent spans of text.", 
        "66": "An exception is when embedded EDUs break up another EDU, as in Example (3).", 
        "67": "The units 1 and 3 form in fact one EDU.", 
        "68": "We follow previous work on treating this as three segments, but note that this may not be the optimal solution.", 
        "69": "(3) [But maintaining the key components (.", 
        "70": ".", 
        "71": ".", 
        "72": ")]1 [\u2013 a stable exchange rate and high levels of imports \u2013]2 [will consume enormous amounts (.", 
        "73": ".", 
        "74": ".).", 
        "75": "]3\nDocument-level segmentation Contrary to previous studies, we do not assume gold sentences: Since sentence boundaries are EDU boundaries, our system jointly predicts sentence and intrasentential EDU boundaries.", 
        "76": "4 Cross-lingual/-domain segmentation  Data is scarce for discourse.", 
        "77": "In order to build statistical segmenters for new, low-resourced languages and domains, we propose to combine corpora within a multi-task learning setting (Section 5) leveraging data from well-resourced languages or domains.", 
        "78": "Models are trained on several (source) languages (resp.", 
        "79": "domains) \u2013 each viewed as an auxiliary task \u2013 for building a system for a (target) language (resp.", 
        "80": "domain).", 
        "81": "Cross-domain For cross-domain experiments, the models are trained on all the other (source) domains and parameters are tuned on data for the target domain.", 
        "82": "This allows us to improve performance when only few data points (i.e.", 
        "83": "development set) are annotated for a specific domain (semi-supervised setting).", 
        "84": "Cross-lingual For cross-lingual experiments, we tune our system\u2019s parameters by training a system on the data for three languages with sufficient amounts of data (namely, German, Spanish and Brazilian Portuguese), and using English data as a development set.", 
        "85": "We then train a new model also using multi-task learning (with these tuned parameters) using only source training data, and report performance on the target test set.", 
        "86": "This allows us to estimate performance when no data is available for the language of interest (unsupervised adaptation).", 
        "87": "5 Multi-task learning  Our models perform sequence labeling based on a stacked k-layer bi-directional LSTM, a variant of LSTMs (Hochreiter and Schmidhuber, 1997) that reads the input in both regular and reversed order, allowing to take into account both left and right contexts (Graves and Schmidhuber, 2005).", 
        "88": "For our task, this enables us, for example, to distinguish between coordinated nouns and clauses.", 
        "89": "This model takes as input a sequence of words (and, here, POS tags) represented by vectors (initialized randomly or, for words, using pre-trained embedding vectors).", 
        "90": "The sequence goes through an embedding layer, and we compute the predictions of the forward and backward states for the k stacked layers.", 
        "91": "At the upper level, we compute the softmax predictions for each word based on a linear transformation.", 
        "92": "We use a logistic loss.", 
        "93": "We also investigate joint training of multiple languages and domains for discourse segmentation.", 
        "94": "We thus try to leverage languages and domains regularities by sharing the architecture and parameters through multi-task training, where an auxiliary task is a source language (resp.", 
        "95": "domain) different from the target language (resp.", 
        "96": "domain) of interest.", 
        "97": "Specifically, we train models based on hard parameters sharing (Caruana, 1993; Collobert et al., 2011; Klerke et al., 2016; Plank et al., 2016):4 each task is associated with a specific output layer, whereas the inner layers \u2013 the stacked LSTMs \u2013 are shared across the tasks.", 
        "98": "At training time, we randomly sample data points from one task and do forward predictions.", 
        "99": "During backpropagation, we modify the weights of the shared layers and the task-specific outer layer.", 
        "100": "The model is optimized for one target task (corresponding to the development data used).", 
        "101": "Except for the outer layer, the target task model is thus regularized by the induction of auxiliary models.", 
        "102": "6 Corpora  Table 1 summarizes statistics about the data.", 
        "103": "For English, we use four corpora, allowing us to evaluate cross-domain performance: the RST-DT (EnDT) composed of Wall Street Journal articles; the SFU review corpus5 (En-SFU-DT) containing product reviews; the instructional corpus (EnInstr-DT) (Subba and Di Eugenio, 2009) built\n4We used a modified version of (Plank et al., 2016) fixing the random seed and using standard SGD.", 
        "104": "5https://www.sfu.ca/\u223cmtaboada\non instruction manuals; and the GUM corpus6 (En-Gum-DT) containing interviews, news, travel guides and how-tos.", 
        "105": "For cross-lingual experiments, we use annotated corpora for Spanish (Es-DT) (da Cunha et al., 2011),7 German (De-DT) (Stede, 2004; Stede and Neumann, 2014), Dutch (Nl-DT) (Vliet et al., 2011; Redeker et al., 2012) and, for Brazilian Portuguese, we merged four corpora (PtDT) (Cardoso et al., 2011; Collovini et al., 2007; Pardo and Seno, 2005; Pardo and Nunes, 2003, 2004) as done in (Maziero et al., 2015).", 
        "106": "Three other RST corpora exist, but we were not able to obtain cross-lingual word embeddings for Basque (Iruskieta et al., 2013) and Chinese (Wu et al., 2016), and could not obtain the data for Tamil (Subalalitha and Parthasarathi, 2012).", 
        "107": "7 Experiments  Data We use the official test sets for the En-DT (38 documents) and the Es-DT (84).", 
        "108": "For the others, we randomly choose 38 documents as test set, and either keep the rest as development set (NlDT) or split it into a train and a development set.", 
        "109": "Baselines As baselines at the document level, we report the scores obtained (a) when only considering the sentence boundaries predicted using UDPipe (Straka et al., 2016) (UDP-S),8 and (b) when EDU boundaries are added after each token PoS-tagged with \u201cPUNCT\u201d (UDP-P), marking either an inter- or an intra-sentential boundary.", 
        "110": "Systems As described in Section 3, our systems are either mono-lingual or mono-domain (mono), or based on a joint training across languages or domains (cross).", 
        "111": "The \u201cmono\u201d systems are built for\n6https://corpling.uis.georgetown.edu/gum/ 7We only use the test set from the annotator A.", 
        "112": "8http://ufal.mff.cuni.cz/udpipe\nthe languages and domains represented by enough data (upper part of Table 1).", 
        "113": "The \u201ccross\u201d models are trained using multi-task learning.", 
        "114": "Parameters The hyper-parameters are tuned on the development set: number of iterations i \u2208 {10, 20, 30}, Gaussian noise \u03c3 \u2208 {0.1, 0.2}, and number of dimensions d \u2208 {50, 500}.", 
        "115": "We fix the number n of stacked hidden layers to 2 and the size of the hidden layers h to 100 after experimenting on the En-DT.9 Our final models use \u03c3 = 0.2 and d = 500.", 
        "116": "Representation We use tokens and POS tags as input data.10 The aim is to build a representation considering the current word and its context, i.e.", 
        "117": "its POS and the surrounding words/POS.", 
        "118": "We use the pre-trained UDPipe models to postag the documents for all languages.", 
        "119": "We experiment with randomly initialized and pre-trained cross-lingual word embeddings built on Europarl (Levy et al., 2017), keeping either the full 500 dimensions, or the first 50 ones.", 
        "120": "Results Our systems are evaluated using F1 over the boundaries (B labels), disregarding the first word of each document.", 
        "121": "Our scores are summarized in Table 2.", 
        "122": "Our supervised, monolingual systems unsurprisingly give the best performance, with F1 above 80%.", 
        "123": "The results are generally linked to the size of the corpora, the larger the better.", 
        "124": "Only exception is the En-SFU-DT, which, however, include more varied annotation (the authors stated that the annotations \u201chave not been checked for reliability\u201d).", 
        "125": "The (semi-supervised) cross-domain setting allows us to present the scores one can expect when\n9With n \u2208 {1, 2, 3} and h \u2208 {100, 200, 400}).", 
        "126": "10A document is a sequence alternating words and POS.", 
        "127": "The tokens are labeled with a B or an I, the POS, always labeled with an I, are inserted after each token they refer to.", 
        "128": "only 25 documents are annotated for a new domain (i.e.", 
        "129": "the development set for the target domain), and to give the first results on the En-Gum-DT, but here, our model is actually outperformed by the sentence-based baseline (UDP-S).", 
        "130": "The (unsupervised) cross-lingual models are generally largely better than UDPipe.", 
        "131": "These are scores that one can expect when doing crosslingual transfer to build a discourse segmenter for a new language for which no annotated data are available.", 
        "132": "The performance is still quite high, demonstrating the coherence between the annotation schemes, and the potential of cross-lingual transfer.", 
        "133": "We acknowledge that this is a small set of relatively similar Indo-European languages, however.", 
        "134": "Note that the sentence-based baseline has a high precision (e.g.", 
        "135": "96.6 on Es-DT against 59.8 for the cross-lingual system), but a much lower recall, since it mainly predicts the sentence boundaries.", 
        "136": "On corpora that mostly contain sentential EDUs (e.g.", 
        "137": "Nl-DT, see Table 1), this is a good strategy.", 
        "138": "Using the punctuation (UDP-P) could be a better approximation for corpora with more varied EDUs, see the large gain for the Pt-DT and the En-Instr-DT.", 
        "139": "Our scores are not directly comparable with sentence-level state-of-the-art systems (see Section 2).", 
        "140": "However, for En-DT, our best system correctly identifies 950 sentence boundaries out of 991, but gets only 84.5% in F1 for intra-sentential boundaries,11 thus lower than the state-of-the-art (91.0%).", 
        "141": "This is because we consider much less information, and because the system was not optimized for this task.", 
        "142": "Interestingly, our simple system beats HILDA (Hernault et al., 2010) (74.1% in F1), is as good as the other neural network based system (Subba and Di Eugenio, 2007), and is close to SPADE (Soricut and Marcu, 2003) (85.2% in F1) (Joty et al., 2015), while all of these systems use parse tree information.", 
        "143": "Finally, looking at the errors of our system on the En-DT, we found that most of them are on the tokens \u201cto\u201d (30 out of 94 not predicted as \u2019B\u2019) and \u201cand\u201d (24 out of 103), as expected given the annotation guidelines (see Section 3).", 
        "144": "These words are highly ambiguous regarding discourse segmentation (e.g.", 
        "145": "in the test set, 42.3% of \u201cand\u201d indicates a boundary).", 
        "146": "We also found errors with coordinated\n11This score ignores the sentences containing only one EDU (Sporleder and Lapata, 2005).", 
        "147": "verb phrases \u2013 e.g.", 
        "148": "\u201c[when rates are rising] [and shift out at times]\u201d \u2013 that should be split (Carlson et al., 2001), a distinction hard to make without syntactic trees.", 
        "149": "Finally, since we use predicted POS tags, our system learns from noisy data and makes errors due to postagging and tokenisation errors.", 
        "150": "8 Conclusion  We proposed new discourse segmenters with good performance for many languages and domains, at the document level, within a fully predicted setting and using only language independent tools.", 
        "151": "Acknowledgements  We would like to thank the anonymous reviewers for their comments.", 
        "152": "This research is funded by the ERC Starting Grant LOWLANDS No.", 
        "153": "313695."
    }, 
    "document_id": "P17-2037.pdf.json"
}
