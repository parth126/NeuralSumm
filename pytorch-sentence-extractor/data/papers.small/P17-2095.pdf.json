{
    "abstract_sentences": {
        "1": "Word segmentation plays a pivotal role in improving any Arabic NLP application.", 
        "2": "Therefore, a lot of research has been spent in improving its accuracy.", 
        "3": "Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent.", 
        "4": "We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network).", 
        "5": "On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance.", 
        "6": "In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601\u2013607 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2095  1 Introduction  Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002).", 
        "2": "A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016).", 
        "3": "Morphological segmentation splits words into morphemes.", 
        "4": "For example, \u2018\u2018wktAbnA\u201d \u201c A JK.", 
        "5": "A J\u00bb\u00f0\u201d (gloss: and our book) is decomposed into its stem and affixes as: \u201cw+ ktAb +nA\u201d \u201c A K+ H. A J\u00bb + \u00f0\u201d.", 
        "6": "Despite the gains obtained from using morphological segmentation, there are several caveats to using these tools.", 
        "7": "Firstly, they make the training pipeline cumbersome, as they come with complicated pre-processing (and additional postprocessing in the case of English-to-Arabic translation (El Kholy and Habash, 2012)).", 
        "8": "More importantly, these tools are dialect- and domain-specific.", 
        "9": "A segmenter trained for modern standard Arabic (MSA) performs significantly worse on dialectal Arabic (Habash et al., 2013), or when it is applied to a new domain.", 
        "10": "In this work, we explore whether we can avoid the language-dependent pre/post-processing components and learn segmentation directly from the training data being used for a given task.", 
        "11": "We investigate data-driven alternatives to morphological segmentation using i) unsupervised sub-word units obtained using byte-pair encoding (Sennrich et al., 2016), ii) purely character-based segmentation (Ling et al., 2015), and iii) a convolutional neural network over characters (Kim et al., 2016).", 
        "12": "We evaluate these techniques on the tasks of machine translation (MT) and part-of-speech (POS) tagging and compare them against morphological segmenters MADAMIRA (Pasha et al., 2014) and Farasa (Abdelali et al., 2016).", 
        "13": "On the MT task, byte-pair encoding (BPE) performs the best among the three methods, achieving very similar performance to morphological segmentation in the Arabic-to-English direction and slightly worse in the other direction.", 
        "14": "Character-based methods, in comparison, perform better on the task of POS tagging, reaching an accuracy of 95.9%, only 1.3% worse than morphological segmentation.", 
        "15": "We also analyze the effect of segmentation granularity of Arabic on the quality of MT.", 
        "16": "We observed that a neural MT (NMT) system is sensitive to source/target token ratio and performs best when this ratio is close to or greater than 1.", 
        "17": "601  2 Segmentation Approaches  We experimented with three data-driven segmentation schemes: i) morphological segmentation, ii) sub-word segmentation based on BPE, and iii) two variants of character-based segmentation.", 
        "18": "We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model.", 
        "19": "See Figure 1 for illustration.", 
        "20": "2.1 Morphological Segmentation  There is a vast amount of work on statistical segmentation for Arabic.", 
        "21": "Here we use the stateof-the-art Arabic segmenter MADAMIRA and Farasa as our baselines.", 
        "22": "MADAMIRA involves a morphological analyzer that generates a list of possible word-level analyses (independent of context).", 
        "23": "The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component.", 
        "24": "Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation.", 
        "25": "2.2 Data Driven Sub-word Units  A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012).", 
        "26": "Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016).", 
        "27": "In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic.", 
        "28": "BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants.", 
        "29": "In essence, frequent character n-gram sequences will be merged to form one symbol.", 
        "30": "The number of merge operations is controlled by a hyper-parameter OP which directly affects the granularity of segmentation: a high value of OP means coarse segmentation and a low value means fine-grained segmentation.", 
        "31": "2.3 Character-level Encoding  Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014).", 
        "32": "Ling et al.", 
        "33": "(2016) used character embeddings to address the OOV word problem.", 
        "34": "We explored them as an alternative to morphological segmentation.", 
        "35": "Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters.", 
        "36": "The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1).", 
        "37": "The decoding may follow identical settings.", 
        "38": "We restricted the character-level representation to the Arabic side of the parallel corpus and use words for the English side.", 
        "39": "Character-CNN Kim et al.", 
        "40": "(2016) presented a neural language model that takes character-level input and learns word embeddings using a CNN over characters.", 
        "41": "The embedding are then provided to the encoder as input.", 
        "42": "The intuition is that the character-based word embedding should be able to learn the morphological phenomena a word inherits.", 
        "43": "Compared to fully characterlevel encoding, the encoder gets word-level embeddings as in the case of unsegmented words (see Figure 1).", 
        "44": "However, the word embedding is intuitively richer than the embedding learned over unsegmented words because of the convolution over characters.", 
        "45": "The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-jussa\u0300 and Fonollosa, 2016).", 
        "46": "Belinkov et al.", 
        "47": "(2017) also showed character-based representations learned using a CNN to be superior, at learning word morphology, than their word-based counter-parts.", 
        "48": "However, they did not compare these against BPE-based segmentation.", 
        "49": "We use character-CNN to aid Arabic word segmentation.", 
        "50": "3 Experiments  In the following, we describe the data and system settings and later present the results of machine translation and POS tagging.", 
        "51": "3.1 Settings  Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzma\u0301n et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing.", 
        "52": "For EnglishArabic, outputs were detokenized using MADA detokenizer.", 
        "53": "Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013).", 
        "54": "POS tagging We used parts 2-3 (v3.1-2) of the Arabic Treebank (Mohamed Maamouri, 2010).", 
        "55": "The data consists of 18268 sentences (483,909 words).", 
        "56": "We used 80% for training, 5% for development and the remaining for test.", 
        "57": "Segmentation MADAMIRA and Farasa normalize the data before segmentation.", 
        "58": "In order to have consistent data, we normalize it for all segmentation approaches.", 
        "59": "For BPE, we tuned the value of merge operations OP and found 30k and 90k to be optimal for Ar-to-En and En-to-Ar respectively.", 
        "60": "In case of no segmentation (UNSEG) and character-CNN (cCNN), we tokenized the Arabic with the standard Moses tokenizer, which separates punctuation marks.", 
        "61": "For character-level encoding (CHAR), we preserved word boundaries by replacing space with a special symbol and then separated every character with a space.", 
        "62": "Englishside is tokenized/truecased using Moses scripts.", 
        "63": "Neural MT Settings We used the seq2seqattn (Kim, 2016) implementation, with 2 layers of\n1We used 3.75% as reported to be optimal filtering threshold in (Durrani et al., 2016).", 
        "64": "LSTM in the (bidirectional) encoder and the decoder, with a size of 500.", 
        "65": "We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments.", 
        "66": "The source and target vocabularies are limited to 50k each.", 
        "67": "3.2 Machine Translation Results  Table 1 presents MT results using various segmentation strategies.", 
        "68": "Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively.", 
        "69": "The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Ar-to-En direction.", 
        "70": "However, the performance is lower by at least 0.6 BLEU points compared to the MORPH system.", 
        "71": "In the En-to-Ar direction, where cCNN and CHAR are applied on the target side, the performance dropped significantly.", 
        "72": "In the case of CHAR, mapping one source word to many target characters makes it harder for NMT to learn a good model.", 
        "73": "This is in line with our finding on using a lower value of OP for BPE segmentation (see paragraph Analyzing the effect of OP).", 
        "74": "Surprisingly, the cCNN system results were inferior to the UNSEG system for En-to-Ar.", 
        "75": "A possible explanation is that the decoder\u2019s predictions are still done at word level even when using the cCNN model (which encodes the target input during training but not the output).", 
        "76": "In practice, this can lead to generating unknown words.", 
        "77": "Indeed, in the Ar-toEn case cCNN significantly reduces the unknown words in the test sets, while in the En-to-Ar case the number of unknown words remains roughly the same between UNSEG and cCNN.", 
        "78": "The BPE system outperformed all other systems in the Ar-to-En direction and is lower than MORPH by only 0.2 BLEU points in the opposite direction.", 
        "79": "This shows that machine translation involving the\n2Farasa performed better in the Ar-to-En experiments and MADAMIRA performed better in the En-to-Ar direction.", 
        "80": "We used best results as our baselines for comparison and call them MORPH.", 
        "81": "Arabic language can achieve competitive results with data-driven segmentation.", 
        "82": "This comes with an additional benefit of language-independent preprocessing and post-processing pipeline.", 
        "83": "In an attempt to find, whether the gains obtained from data-driven segmentation techniques and morphological segmentation are additive, we applied BPE to morphological segmented data.", 
        "84": "We saw further improvement of up to 1 BLEU point by using the two segmentations in tandem.", 
        "85": "Analyzing the effect of OP: The unsegmented training data consists of 23M Arabic tokens and 28M English tokens.", 
        "86": "The parameter OP decides the granularity of segmentation: a higher value of OP means fewer segments.", 
        "87": "For example, at OP=50k, the number of Arabic tokens is greater by 7% compared to OP=90k.", 
        "88": "We tested four different values of OP (15k, 30k, 50k, and 90k).", 
        "89": "Figure 2 summarizes our findings on test-2011 dataset, where x-axis presents the ratio of source to target language tokens and y-axis shows the BLEU score.", 
        "90": "The boundary values for segmentation are character-level segmentation (OP=0) and unsegmented text (OP=N ).3 For both language directions, we observed that a source to target token ratio close to 1 and greater works best provided that the boundary conditions (unsegmented Arabic and character-level segmentation) are avoided.", 
        "91": "In the En-to-Ar direction, the system improves for coarse segmentation whereas in the Ar-to-En direction, a much finer-grained segmentation of Arabic performed better.", 
        "92": "This is in line with the ratio of tokens generated using the MORPH systems (Ar-toEn ratio = 1.02).", 
        "93": "Generalizing from the perspective of neural MT, the system learns better when total numbers of source and target tokens are close to each other.", 
        "94": "The system shows better tolerance towards modeling many source words to a few target words compared to the other way around.", 
        "95": "Discussion: Though BPE performed well for machine translation, there are a few reservations that we would like to discuss here.", 
        "96": "Since the main goal of the algorithm is to compress data and segmentation comes as a by-product, it often produces different segmentations of a root word when occurred in different morphological forms.", 
        "97": "For example, the words driven and driving are segmented as driv en and drivi ng respectively.", 
        "98": "This adds ambiguity to the data and may result in un-\n3N is the number of types in the unsegmented corpus.", 
        "99": "expected translation errors.", 
        "100": "Another limitation of BPE is that at test time, it may divide the unknown words to semantically different known sub-word units which can result in a semantically wrong translation.", 
        "101": "For example, the word \u201cQ\u00a2\u0304\u201d is unknown to our vocabulary.", 
        "102": "BPE segmented it into known units which ended up being translated to courage.", 
        "103": "One possible solution to this problem is; at test time, BPE is applied to those words only which were known to the full vocabulary of the training corpus.", 
        "104": "In this way, the sub-word units created by BPE for the word are already seen in a similar context during training and the model has learned to translate them correctly.", 
        "105": "The downside of this method is that it limits BPE\u2019s power to segment unknown words to their correct sub-word units and outputs them as UNK in translation.", 
        "106": "3.3 Part of Speech Tagging  We also experimented with the aforementioned segmentation strategies for the task of Arabic POS tagging.", 
        "107": "Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al., 2001) consider previous words and/or tags to predict the tag of the current word.", 
        "108": "We mimic a similar setting but in a sequence-to-sequence learning framework.", 
        "109": "Figure 3 describes a step by step procedure to train a neural encoder-decoder tagger.", 
        "110": "Consider an Arabic phrase \u201cklm >SdqA}k b$rhm\u201d \u201c\u00d1\u00ebQ\u00e5 .", 
        "111": "\u00bd KA\u0304Y @ \u00d5\u00ce\u00bf\u201d (gloss: call your friends give them the good news), we want to learn the tag\nof the word \u201c\u00d1\u00ebQ\u00e5 .\u201d using the context of the previous two words and their tags.", 
        "112": "First, we segment the phrase using a segmentation approach (step 1) and then add POS tags to context words (step 2).", 
        "113": "The entire sequence with the words and tags is fed to the sequence-to-sequence framework.", 
        "114": "The embeddings (for both words and tags) are learned jointly with other parameters in an end-to-end fashion, and optimized on the target tag sequence; for example, \u201cNOUN PRON\u201d in this case.", 
        "115": "For a given word wi in a sentence s = {w1, w2, ..., wM} and its POS tag ti, We formulate the neural TAGGER as follows:\nSEGMENTER(\u03c4) : \u2200wi 7\u2192 Si TAGGER : Si\u22122 Si\u22121 Si 7\u2192 ti\nwhere Si is the segmentation of word wi.", 
        "116": "In case of UNSEG and cCNN, Si would be same as wi.", 
        "117": "SEGMENTER here is identical to the one described in Figure 1.", 
        "118": "TAGGER is a NMT architecture that learns to predict a POS tag of a segmented/unsegmented word given previous two words.4\nTable 2 summarizes the results.", 
        "119": "The MORPH system performed best with an improvement of 5.3% over UNSEG.", 
        "120": "Among the data-driven methods, CHAR model performed best and was behind MORPH by only 0.3%.", 
        "121": "Even though BPE was inferior compared to other methods, it was still better than UNSEG by 4%.5\nAnalysis of POS outputs We performed a comparative error analysis of predictions made\n4We also tried using previous words with their POS tags as context but did not see any significant difference in the end result.", 
        "122": "5Optimizing the parameter OP did not yield any difference in accuracy.", 
        "123": "We used 10k operations.", 
        "124": "through MORPH, CHAR and BPE based segmentations.", 
        "125": "MORPH and CHAR observed very similar error patterns, with most confusion between Foreign and Particle tags.", 
        "126": "In addition to this confusion, BPE had relatively scattered errors.", 
        "127": "It had lower precision in predicting nouns and had confused them with adverbs, foreign words and adjectives.", 
        "128": "This is expected, since most nouns are outof-vocabulary terms, and therefore get segmented by BPE into smaller, possibly known fragments, which then get confused with other tags.", 
        "129": "However, since the accuracies are quite close, the overall errors are very few and similar between the various systems.", 
        "130": "We also analyzed the number of tags that are output by the sequence-to-sequence model using various segmentation schemes.", 
        "131": "In 99.95% of the cases, the system learned to output the correct number of tags, regardless of the number of source segments.", 
        "132": "4 Conclusion  We explored several alternatives to languagedependent segmentation of Arabic and evaluated them on the tasks of machine translation and POS tagging.", 
        "133": "On the machine translation task, BPE segmentation produced the best results and even outperformed the state-of-the-art morphological segmentation in the Arabic-to-English direction.", 
        "134": "On the POS tagging task, character-based models got closest to using the state-of-the-art segmentation.", 
        "135": "Our results showed that data-driven segmentation schemes can serve as an alternative to heavily engineered language-dependent tools and achieve very competitive results.", 
        "136": "In our analysis we showed that NMT performs better when the source to target token ratio is close to one or greater.", 
        "137": "Acknowledgments  We would like to thank the three anonymous reviewers for their useful suggestions.", 
        "138": "This research was carried out in collaboration between the HBKU Qatar Computing Research Institute (QCRI) and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL)."
    }, 
    "document_id": "P17-2095.pdf.json"
}
