{
    "abstract_sentences": {
        "1": "We present the first attempt at using sequence to sequence neural networks to model text simplification (TS).", 
        "2": "Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction.", 
        "3": "An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 85\u201391 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2014  1 Introduction  Neural sequence to sequence models have been successfully used in many applications (Graves, 2012), from speech and signal processing to text processing or dialogue systems (Serban et al., 2015).", 
        "2": "Neural machine translation (Cho et al., 2014; Bahdanau et al., 2014) is a particular type of sequence to sequence model that recently attracted a lot of attention from industry (Wu et al., 2016) and academia, especially due to the capability to obtain state-of-the-art results for various translation tasks (Bojar et al., 2016).", 
        "3": "Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables.", 
        "4": "The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016).", 
        "5": "Automated text simplification (ATS) systems are meant to transform original texts into differ-\n\u2217Both authors have contributed equally to this work\nent (simpler) variants which would be understood by wider audiences and more successfully processed by various NLP tools.", 
        "6": "In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from \u2018original\u2019 to \u2018simple\u2019 sentences.", 
        "7": "So far, attempts were made at standard phrase-based SMT (PBSMT) models (Specia, 2010; S\u030ctajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016).", 
        "8": "Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, with reported good success (Glavas\u030c and S\u030ctajner, 2015; Paetzold and Specia, 2016).", 
        "9": "To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS.", 
        "10": "We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task.", 
        "11": "We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervised lexical simplification systems.", 
        "12": "2 Neural Text Simplification (NTS)  We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), hidden states of size 500 and 500 hidden units, and a 0.3 dropout probability (Srivastava et al., 2014).", 
        "13": "The vocabulary size is set to 50,000 and we train the model for 15 epochs with plain SGD optimizer, and after epoch 8 we halve the\n85\nlearning rate.", 
        "14": "At the end of each epoch we save the current state of the model and predict the perplexity values of the models on the development set.", 
        "15": "We employ early-stopping and select the model resulted from the epoch with the best perplexity to avoid over-fitting.", 
        "16": "The parameters are initialized over uniform distribution with support [-0.1, 0.1].", 
        "17": "Additionally, for the decoder we employ global attention in combination with input feeding as described by Luong et al.", 
        "18": "(2015).", 
        "19": "The architecture1 is depicted in Figure 1, with the input feeding approach represented only for the last hidden state of the decoder.", 
        "20": "For the attention layer, we compute a context vector ct by using the information provided from the hidden states of the source sentence and by computing a weighted average with the alignment weights at.", 
        "21": "The new hidden state is obtained using a concatenation of the previous hidden state and the context vector:\nh\u0303t = tanhW [ct;ht]\nThe global alignment weights at are being computed with a softmax function over the general scoring method for attention:\nat(s) = exphTt Wash\u0304s\u2211 s\u2032 exph T t Was\u2032 h\u0304s\u2032\nInput feeding is a process that sends the previous hidden state obtained using the alignment\n1The architecture configurations, data, and the pretrained models are released in https://github.com/ senisioi/NeuralTextSimplification\nmethod, to the input at the next step, presumably making the model keep track of anterior alignment decisions.", 
        "22": "Luong et al.", 
        "23": "(2015) showed this approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments.", 
        "24": "Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words and entities.", 
        "25": "Instead, we make use of alignment probabilities between the predictions and the original sentences to retrieve the original words.", 
        "26": "2.1 Word2vec Embeddings  Furthermore, we are interested to explore whether large scale pre-trained embeddings can improve text simplification models.", 
        "27": "Kauchak (2013) indicates that combining normal data with simplified data can increase the performance of ATS systems.", 
        "28": "Therefore, we construct a secondary model (NTSw2v) using a combination of pre-trained word2vec from Google News corpus (Mikolov et al., 2013a) of size 300 and locally trained embeddings of size 200.", 
        "29": "To ensure good representations of lowfrequency words, we use word2vec (R\u030cehu\u030ar\u030cek and Sojka, 2010; Mikolov et al., 2013b) to train skipgram with hierarchical softmax and we set a window of 10 words.", 
        "30": "Following Garten et al.", 
        "31": "(2015) who showed that simple concatenation can improve the word representations, we construct two different sets of embeddings for the encoder and for the decoder.", 
        "32": "The former are constructed using the word2vec trained on the original English texts combined with Google News and the later (decoder) embeddings are built from word2vec trained on the simplified version of the training data combined with Google News.", 
        "33": "To merge the local and global embeddings, we concatenate the representations for each word in the vocabulary, thus obtaining a new representation of size 500.", 
        "34": "If a word is missing in the global embeddings, we replace it with a sample from a Gaussian distribution with mean 0 and standard deviation of 0.9.", 
        "35": "The remaining parameters are left unchanged from the previous model description.", 
        "36": "2.2 Prediction Ranking  To ensure the best predictions and the best simplified sentences at each step, we use beam search to sample multiple outputs from the two systems\ndescribed previously (NTS and NTS-w2v).", 
        "37": "Beam search works by generating the first k hypotheses at each step ordered by the log-likelihood of the target sentence given the input sentence.", 
        "38": "By default, we use a beam size of 5 and take the first hypothesis, but we also observe that higher beam size and lower-ranked hypotheses can generate good simplification results.", 
        "39": "Therefore, we generate the first two candidate hypotheses for each beam size from 5 to 12.", 
        "40": "We then attempt to find the best beam size and hypothesis based on two metrics: the traditional MT-evaluation metric, BLEU (Papineni et al., 2002; Bird et al., 2009) with NIST smoothing (Bird et al., 2009), and SARI (Xu et al., 2016), a recent text-simplification metric.", 
        "41": "2.3 Dataset  To train our models, we use the publicly available dataset provided by Hwang et al.", 
        "42": "(2015) based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia (EW\u2013SEW).", 
        "43": "We discard the uncategorized matches, and use only good matches and partial matches which were above the 0.45 threshold (Hwang et al., 2015), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches).", 
        "44": "It is one of the largest freely available resources for text simplification, and unlike the previously used EW\u2013SEW corpus2 (Kauchak, 2013), which only contains full matches (167K pairs), the newer dataset also contains partial matches.", 
        "45": "Therefore, it is not only larger, but it also allows for learning sentence shortening (dropping irrelevant parts) transformations (see Table 3, Appendix A).", 
        "46": "We use the Stanford NER system (Finkel et al., 2005) to get an approximate number of locations, persons, organizations and miscellaneous entities\n2http://www.cs.pomona.edu/\u02dcdkauchak/ simplification/\nin the corpus.", 
        "47": "A brief analysis of the vocabulary is rendered in Table 1.", 
        "48": "The dataset we use contains an abundant amount of named entities and consequently a large amount of low frequency words, but the majority of entities are not part of the model\u2019s 50,000 words vocabulary due to their small frequency.", 
        "49": "These words are replaced with \u2019UNK\u2019 symbols during training.", 
        "50": "At prediction time, we replace the unknown words with the highest probability score from the attention layer.", 
        "51": "We believe it is important to ensure that the models learn good word representations, either during the model training or through word2vec, in order to accurately create alignments between source and target sentences.", 
        "52": "Given that in TS there is not only one best simplification, and that the quality of simplifications in Simple English Wikipedia has been disputed before (Amancio and Specia, 2014; Xu et al., 2015), for tuning and testing we use the dataset previously released by Xu et al.", 
        "53": "(2016), which contains 2000 sentences for tuning and 359 for testing, each with eight simplification variants obtained by eight Amazon Mechanical Turkers.3 The tune subset is also used as reference corpus in combination with BLEU and SARI to select the best beam size and hypothesis for prediction reranking.", 
        "54": "3 Evaluation  For the first 70 original sentences of the Xu et al.\u2019s (2016) test set4 we perform three types of human evaluation to assess the output of our best systems and three ATS systems of different architectures: (1) the PBSMT system with reranking of n-best outputs (Wubben et al., 2012), which represent the best PBSMT approach to ATS, trained and tuned over the same datasets as our systems; (2) the state-of-the-art SBMT system (Xu et al., 2016) with modified tuning function (using SARI) and using PPDB paraphrase database (Ganitkevitch et al., 2013);5 and (3) one of the state-of-theart unsupervised lexical simplification (LS) systems that leverages word-embeddings (Glavas\u030c and\n3None of the 359 test sentences was present in the datasets we used for training and tuning.", 
        "55": "4https://github.com/cocoxu/ simplification/\n5For the first two systems, we use publicly available output at: https://github.com/ cocoxu/simplification/tree/master/data/ systemoutputs\nS\u030ctajner, 2015).6\nWe evaluate the output of all systems using three types of human evaluation.", 
        "56": "Correctness and Number of Changes.", 
        "57": "First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g.", 
        "58": "\u201cbecome defunct\u201d \u2192 \u201cwas dissolved\u201d) as one change.", 
        "59": "Those changes that preserve the original meaning and grammaticality of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers) are marked as Correct.", 
        "60": "In the case of content reduction, we instructed the annotators to count the deletion of each array of consecutive words as one change and consider the meaning unchanged if the main information of the sentence was retained and unchanged.", 
        "61": "The sentences for which the two annotators did not agree were given to a third annotator to obtain the majority vote.", 
        "62": "Grammaticality and Meaning Preservation.", 
        "63": "Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each (whole) sentence with at least one change on a 1\u20135 Likert scale (1 \u2013 very bad; 5 \u2013 very good).", 
        "64": "The obtained inter-annotator agreement (quadratic Cohens kappa) was 0.78 for G and 0.63 for M.\nSimplicity of sentences.", 
        "65": "Third, the three nonnative fluent English speakers were shown original (reference) sentences and target (output) sentences, one pair at the time, and asked whether the target sentence is: +2 \u2013 much simpler; +1 \u2013 somewhat simpler; 0 \u2013 equally difficult; -1 \u2013 somewhat more difficult; -2 \u2013 much more difficult, than the reference sentence.", 
        "66": "The obtained inter-annotator agreement (quadratic Cohens kappa) was 0.66.", 
        "67": "While the correctness of changes takes into account the influence of each individual change on grammaticality, meaning and simplicity of a sentence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.", 
        "68": "4 Results and Discussion  The results of the human evaluation (Table 2) revealed that all NTS models achieve higher percentage of correct changes and more simplified output than any of the state-of-the-art ATS systems\n6For the LightLS system (Glavas\u030c and S\u030ctajner, 2015) we use the output of the original system provided by the authors.", 
        "69": "with different architectures (PBSMT-R, SBMT, and LightLS).", 
        "70": "We also notice that the best models according to BLEU are obtained with hypothesis 1 and the maximum beam size for both models, while the SARI re-ranker prefers hypothesis 2 and beam size 5 for the first NTS and the maximum beam size for the custom word embeddings model.", 
        "71": "The NTS with custom word2vec embeddings ranked with the text simplification specific metric (SARI) obtained the highest total number of changes among the neural systems, one of the highest percentage of correct changes, the second highest simplicity score, and solid grammaticality and meaning preservation scores.", 
        "72": "An example of the output of different systems is presented in Table 4 (Appendix A).", 
        "73": "The use of different metrics for ranking the NTS predictions optimizes the output towards different evaluation objectives: SARI leads to the highest number of total changes, BLEU to the highest percentage of correct changes, and the default beam scores to the best grammaticality (G) and meaning preservation (M).", 
        "74": "In addition, custom composed global and local word embeddings in combination with SARI metric improve the default translation system, given the joint scores for each evaluation criterion.", 
        "75": "Here is important to note that for ATS systems, the precision of the system (correctness of changes, grammaticality, meaning preservation, and simplicity of the output) is more important than the recall (the total number of changes made).", 
        "76": "The low recall would just leave the sentences similar to their originals thus not improving much the understanding or reading speed of the target users, or not improving much the NLP systems in which they are used as a pre-processing step.", 
        "77": "A low precision, on the other hand, would make texts even more difficult to read and understand, and would worsen the performances of the NLP systems in which ATS is used as a pre-processing step.", 
        "78": "5 Conclusions  We presented a first attempt at modelling sentence simplification with a neural sequence to sequence model.", 
        "79": "Our extensive human evaluation showed that our NTS systems, if the output is ranked with the right metric, can significantly7 outperform the best phrase-based and syntax-based MT approaches, and unsupervised lexical ATS approach,\n7Wilcoxon\u2019s signed rank test, p < 0.001.\nby grammaticality, meaning preservation and simplicity of the output sentences, the percentage of correct transformations, while at the same time achieving more than 1.5 changes per sentence, on average.", 
        "80": "Furthermore, we discovered that NTS systems are capable of correctly performing significant content reduction, thus being the only TS models proposed so far which can jointly perform lexical simplification and content reduction.", 
        "81": "Acknowledgments  This work has been partially supported by a grant of the Romanian National Authority for Scientific Research and Innovation, CNCS/CCCDI UEFISCDI, project number PN-III-P2-2.1-53BG/2016, within PNCDI III, and by the SFB 884 on the Political Economy of Reforms at the University of Mannheim (project C4), funded by the German Research Foundation (DFG).", 
        "82": "A Appendix - Data Sample and System Output"
    }, 
    "document_id": "P17-2014.pdf.json"
}
