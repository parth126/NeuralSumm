{
    "abstract_sentences": {
        "1": "We develop a language-independent, deep learning-based approach to the task of morphological disambiguation.", 
        "2": "Guided by the intuition that the correct analysis should be \u201cmost similar\u201d to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation.", 
        "3": "Our approach improves on the languagedependent state of the art for two agglutinative languages (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 666\u2013671 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2105  1 Introduction  Morphological disambiguation (MD) is a long standing problem in processing morphologically complex languages (MCL).", 
        "2": "POS tagging is a somewhat related problem, however in MD, in addition to POS tags, one typically has to predict lemmata (roots hereinafter) that surface forms stem from and morphemes1 they bear.", 
        "3": "For example, depending on the context, a Turkish word adam can be analyzed as: (i) a man \u2013 [adam]1+[Noun]2+[A3sg+Pnon+Nom]3 or (ii) my island \u2013 [ada]1+[Noun]2+[A3sg+P1sg +Nom]3 (Hakkani-Tu\u0308r et al., 2002).", 
        "4": "Thus, if one counts analyses as tags, MD can be cast as a tagging problem with an extremely large tagset.", 
        "5": "This fact discourages direct application of the state of the art approaches designed for small fixed tagsets.", 
        "6": "To develop a language independent dense representation of the analyses, we segment2 an analysis\n1We use the term morpheme for its universal recognition within the community.", 
        "7": "A more appropriate term might be grammeme, i.e.", 
        "8": "a value of grammatical category.", 
        "9": "2Such a segmentation is denoted by the squared brackets numbered in the respective order (cf.", 
        "10": "Turkish example).", 
        "11": "into (i) the root, (ii) its POS and (iii) the morpheme chain (MC).", 
        "12": "We then proceed to jointly learn the embeddigns for the root and the POS segments and to combine them and the MC segment representation into a single dense representation.", 
        "13": "MC segments are represented as binary vectors that, for a given analysis, encode presence or absence of each morpheme found in the train set.", 
        "14": "This ensures language independence and contrasts previous work (at least on Turkish and Kazakh), where only certain morphemes are chosen as features depending on their position (Assylbekov et al., 2016; Hakkani-Tu\u0308r et al., 2002) or presence (Makhambetov et al., 2015) in an analysis, or the authors\u2019 intuition (Yildiz et al., 2016; Tolegen et al., 2016; Sak et al., 2007).", 
        "15": "Apart from the sparseness of analyses distribution MCL notoriously raise free word order and long dependency issues.", 
        "16": "Thus, decoding analysis sequences using only the leftmost context may not be enough.", 
        "17": "To address this we leverage the rightmost context as well.", 
        "18": "We model the left- and rightmost surface context in two ways: using (i) BiLSTM (Greff et al., 2015) with a character-based sub-layer (Ling et al., 2015) and (ii) with a feed forward network on word embeddings.", 
        "19": "We then entertain the idea that given a word with multiple analyses and its surface context, the correct analysis might be \u201ccloser\u201d to the context.", 
        "20": "Following our intuition, we have tried computing the distance between the analysis and the context representations, and a simple dot product (as in unnormalized cosine similarity) has yielded the best performance.", 
        "21": "We evaluate our approach on Turkish and Kazakh data sets, using several baselines (including the state of the art methods for both languages) and a variety of settings and metrics.", 
        "22": "In terms of general accuracy our approach has achieved a nearly 1% improvement over the state of the art for Turkish and a marginal improvement for Kazakh.", 
        "23": "666\nOur contribution amounts to the following: (i) a general MD framework for MCL that can be analyzed in <root, POS, MC> triplets; (ii) improvement on language-dependent state of the art for Turkish and Kazakh.", 
        "24": "2 Models  In this section we describe our approach to encoding morphological analyses and the context into the embeddings and combining them to perform morphological disambiguation.", 
        "25": "2.1 Morphological Representation  We treat a morphological analysis as a combination of three main constituents: the root, its POS and the morpheme chain.", 
        "26": "These constituents are represented as dr, dp, and dm-dimensional vectors respectively.", 
        "27": "The former two vectors correspond to dense word embeddings (Collobert et al., 2011), and the latter is a binary vector which encodes the presence of a certain morpheme in the chain.", 
        "28": "The size of the binary vector, dm, is equal to the size of the morpheme dictionary obtained from the data.", 
        "29": "Given a sentence and the j-th surface word form withN analyses, we represent the k-th analysis as:\nAkj = tanh(W rrk + Wppk + Wmmk) (1)\nwhere Aj \u2208 Rdh\u00d7|N |, dh is the dimension of each analysis embedding, rk \u2208 Rdr\u00d71, pk \u2208 Rdp\u00d71,mk \u2208 {0, 1}dp\u00d71 are constituent vectors of the k-th analysis, and Wr \u2208 Rdh\u00d7dr , Wp \u2208 Rdh\u00d7dp ,Wm \u2208 Rdh\u00d7dm are the model parameters.", 
        "30": "The bias term was left out for clarity.", 
        "31": "This representation is shown on Figure 1 (bottom).", 
        "32": "2.2 Recurrent Neural Disambiguation  The model architecture is shown on Figure 1.", 
        "33": "It consists of two main blocks that learn the surface context (top) and the morphological analyses representations (bottom).", 
        "34": "When it comes to modeling context via word embeddings for morphologically complex languages, it is impractical to actually store vectors for all words, since majority of words in such languages has a large number of surface realizations.", 
        "35": "Our solution to this problem is to construct a surface word representation from characters that not only reduces data sparseness, but also help in dealing with the out-of-vocabulary (OOV) words (Ling et al., 2015).", 
        "36": "We represent each character of each word as a vector xi \u2208 Rdc and the\nentire embedding matrix as Ec \u2208 Rdc\u00d7|C|, where C is the character vocabulary extracted from the training set including alphanumeric characters and other possible symbols.", 
        "37": "Given an input surface word wi with its character embeddings x1, ..., xn, the hidden state ht at the time step t can be computed via the following Vanilla LSTM calculations:\nit = \u03c3(Wixt + Uiht\u22121 + bi) (2)\nft = \u03c3(Wfxt + Ufht\u22121 + bf ) (3) ot = \u03c3(Woxt + Uoht\u22121 + bo) (4) zt = tanh(Wzxt + Uzht\u22121 + bz) (5) ct = ft ct\u22121 + it zt (6) ht = ot tanh(ct) (7)\nwhere \u03c3(\u00b7) and tanh(\u00b7) are the non-linear functions.", 
        "38": "it, ft, ot are referred to three gates: input, forget, output that control the information flow of inputs.", 
        "39": "Parameters of the LSTM are W \u2217, U\u2217, b\u2217, where \u2217 can be any of {i, f, o, g}.", 
        "40": "The peephole connections were left out for clarity.", 
        "41": "We use both forward and backward LSTM to learn word representations obtained by concatenation of the last states in both direction hf and hb, e.g.", 
        "42": "hw = [ hf , hb ] .", 
        "43": "Character-based word embeddings obtained in this manner do not yet con-\ntain the context information on a sentence level.", 
        "44": "Thus, we adopt another LSTM to learn contextsensitive information for each word in both directions.", 
        "45": "We denote the concatenation of the embeddings learned from the forward and backward LSTM states as hc(j) \u2208 R2hs\u00d71 (where hs is the output size for the j-th word), and represent surface context as:\nSj = tanh(Dchc(j) + bj) (8)\nwhere Sj \u2208 Rdh\u00d71 is a hidden layer output, hc(j) \u2208 R2hs\u00d71is a context vector of the j-th word, and bj \u2208 Rdh\u00d71 is a bias term.", 
        "46": "For the final prediction, we score each analysis by computing the inner product between its representation and the context\u2019s representations:\nP kj = Sj \u00b7 Akj (9)\nwhere Sj and Akj are computed as equations (8) and (1) respectively.", 
        "47": "We normalize the obtained scores using softmax and choose the analysis with the maximum score as the correct one.", 
        "48": "In what follows we refer to this model as BiLSTM.", 
        "49": "Finally, in a separate setting, in addition to the surface context in the hidden layer we also incorporate the immediate (left and right) morphological context in the form of the average of the analyses representations:\nS\u2217j = tanh(Dchc(j) + DaLj + bj) (10)\nwhere Lj \u2208 R2dh\u00d71 is concatenation of averaged representations of the leftmost and rightmost analyses, and Dc \u2208 Rdh\u00d72hs and Da \u2208 Rdh\u00d72dh are the model parameters.", 
        "50": "This advanced variation is referred to as BiLSTM*.", 
        "51": "2.3 Alternative Context Representation  We also experiment with an alternative context model that uses a feed-forward NN architecture (Collobert et al., 2011; Zheng et al., 2013).", 
        "52": "In this model word embeddings of fixed window size are fed to the hidden layer, and the output represents the context.", 
        "53": "The remaining parts of the architecture stay the same: we use the same morphological representation and choose the correct analysis exactly as we did for BiLSTM model.", 
        "54": "As in the case with BiLSTM, we leverage morphological context, by performing a Viterbi decoding conditioned on the leftmost analysis.", 
        "55": "We refer to this\nmodel as DNN (Deep NN), an advanced variation of which uses the averaged rightmost morphological context as well, and is referred to as DNN*.", 
        "56": "2.4 Training  In all models, the top layer of the networks has a softmax that computes the normalized scores over morphological candidates given the input word.", 
        "57": "The networks are trained to minimize the cross entropy of the predicted and true morphological analyses.", 
        "58": "Back-propagation is employed to compute the gradient of the corresponding object function with respect to the model parameters.", 
        "59": "3 Experiments and Evaluation    3.1 Data Sets  We conduct our experiments on Kazakh (Assylbekov et al., 2016) and Turkish (Yuret and Tu\u0308re, 2006) data sets3.", 
        "60": "Table 1 shows the corpora statistics.", 
        "61": "Kazakh data set is almost 50 times smaller than that of Turkish, with four times the OOV rate and almost twice as many analyses per word on average.", 
        "62": "Given such a drastic difference in the resources it would be interesting to see how our models perform on otherwise similar languages (both Turkic).", 
        "63": "Lastly, while the corpora provide train and test splits, there are no tuning sets, so we withdraw small portions from the training sets for tuning hyper-parameters4.", 
        "64": "3.2 Baselines  We compare our models to three other approaches.", 
        "65": "For Kazakh we use an HMM based tagger and its version extended with the rule-based constraint grammar (Assylbekov et al., 2016), which is considered the state of the art for the language.", 
        "66": "We\n3For Turkish, we used a test set that was manually reannotated by Yildiz et al.", 
        "67": "(2016).", 
        "68": "4The following hyper-parameters are used in all the experiments: character embedding size dc = 35, character and context LSTM states are 50, root and POS embedding sizes are all set to 50, hidden layer size dh = 200, learning rate is set to 0.01.", 
        "69": "The window size of DNN is set to 5.", 
        "70": "For regularization we use dropout (Srivastava et al., 2014) with probability 0.5 on the hidden layers.", 
        "71": "We further constrain the norm of gradient to be below 2 by using gradient clipping.", 
        "72": "refer to these baselines as HMM and HMMCG.", 
        "73": "Another baseline is a voted perceptron (Collins, 2002) based tagger.", 
        "74": "We use our implementation of this baseline for Kazakh and the model developed by Sak et al.", 
        "75": "(2007) for Turkish.", 
        "76": "Lastly, we use a neural network model proposed by Yildiz et al.", 
        "77": "(2016), which is considered state of the art for Turkish.", 
        "78": "For this baseline too we use our own implementation (for both languages) and refer to it as MANN5.", 
        "79": "3.3 Experimental Setup  As described in the previous section, each of our models has two settings: the one that does not incorporate surrounding morphological context and the one that does (the starred one).", 
        "80": "In addition to that we use pre-trained embeddings, by training word2vec (Mikolov et al., 2013) skip-gram model on Wikipedia texts.", 
        "81": "This setting is denoted by a double dagger (\u2021).", 
        "82": "We perform a single run evaluation in terms of token- and sentence- based accuracy.", 
        "83": "We consider four types of tokens: (i) all tokens; (ii) ambiguous tokens (the ones with at least two analyses); (iii) OOV tokens; (iv) ambiguous OOV tokens.", 
        "84": "Thus, we use a total of five metrics.", 
        "85": "In terms of strictness we deem correct only the predictions that match the golden truth completely, i.e.", 
        "86": "in root, POS and MC (up to a single morpheme tag).", 
        "87": "5Note that all of the baselines are language dependent to a certain degree, with MANN being the least dependent and HMMCG the most.", 
        "88": "The latter baseline employs handengineered constraint grammar rules to perform initial disambiguation, followed by application of the HMM tagger, which cherry-picks the most informative grammatical features.", 
        "89": "3.4 Results and Discussion  The results are given in Table 2.", 
        "90": "Unless stated otherwise we refer to the general (all tokens) accuracy when comparing model performances.", 
        "91": "For Kazakh, DNN conditioned on the leftmost analysis yields 86.33% accuracy.", 
        "92": "DNN* that in addition uses the rightmost analysis embeddings, improves almost 1% over that result (87.25%).", 
        "93": "On the other hand BiLSTM, whose context representation uses surface forms only, performs even better (87.49%).", 
        "94": "When this model incorporates immediate morphological context, it (BiLSTM*) performs at 90.92% and beats the HMMCG baseline.", 
        "95": "However, the latter being a very strong language dependent baseline still outperforms our model in ambiguous OOV and sentence accuracy.", 
        "96": "When we evaluate our model under equal conditions (BiLSTM*\u2021+CG) it beats HMMCG on all of the metrics.", 
        "97": "We separate this comparison from the rest because of a language-dependent set up.", 
        "98": "In contrast, for Turkish DNN models outperform BiLSTM on seen tokens and yield an almost equal 92.2% accuracy regardless of using the rightmost morphological context.", 
        "99": "This performance is also higher than that of all baselines, including the state of the art MANN.", 
        "100": "However BiLSTM* is still better than DNN* in OOV token accuracy, both overall and ambiguous.", 
        "101": "As it can be seen, pre-training boosts the performance of DNN* and BiLSTM* across all metrics.", 
        "102": "For Kazakh pre-training results in .14% improvement in general token accuracy for BiLSTM*, which amounts to .67% improvement over the state of the art.", 
        "103": "For Turkish this results in an\nalmost 1% net improvement in overall token accuracy over MANN, the state of the art6.", 
        "104": "A cross-linguistic comparison reveals that although Kazakh data set is much smaller than that of Turkish and has more analyses per word on average and higher OOV rate, on certain metrics the models perform on par or even better for Kazakh7.", 
        "105": "To investigate this further we have made data sets comparable in size by randomly choosing 20.6K+ and 3.4K from Turkish training and test sets.", 
        "106": "On this data BiLSTM*\u2021 yields 91.18, 82.0% general and ambiguous token accuracy and respective scores for OOV are 87.0, 74.6%.", 
        "107": "This result follows the pattern, where for Turkish only the general accuracy is higher than that of Kazakh.", 
        "108": "It turns out that Turkish data contains many unambiguous tokens: 49% and 48% for full and small data sets (train + test average), against 36% for Kazakh.", 
        "109": "This suggests that the higher general accuracy on Turkish data can be explained by the higher rate of the unambiguous tokens.", 
        "110": "Also Turkish has a more complex derivational morphology, which \u201clengthens\u201d the analyses, e.g.", 
        "111": "an average number of morphemes per analysis is higher for Turkish (5.25) than for Kazakh (4.6).", 
        "112": "This adds sparseness to the morpheme chains and certainly further complicates disambiguation, especially in an OOV scenario.", 
        "113": "We also observe that BiLSTM*\u2021 works best on all metrics for Kazakh, but for Turkish it beats DNN*\u2021 only on the OOV part.", 
        "114": "Due to BiLSTM*\u2021 being computationally prohibitive we ran it with significantly less number of epochs than DNN, and it also being a character-based model, we speculate that it was able to learn character aware context embeddings hence better at OOV.", 
        "115": "4 Related Work  A morphology-aware NN (MANN) for MD was proposed by Yildiz et al.", 
        "116": "(2016), and has been reported to achieve ambiguous token accuracies of 84.12, 88.35 and 93.78% for Turkish, Finish and Hungarian respectively.", 
        "117": "This approach differs from ours in a number of ways.", 
        "118": "(i) Our\n6For the un-pretrained model original work reports 84.12% accuracy on ambiguous tokens (Yildiz et al., 2016), which is lower than 84.14% that un-pretrained DNN achieves on this metric.", 
        "119": "7 For instance, BiLSTM*\u2021 applied to Kazakh performs better than any other model for Turkish in terms of sentence, ambiguous and OOV token accuracy.", 
        "120": "Moreover all of the models (including the baselines) perform better on Kazakh in terms of ambiguous OOV accuracy.", 
        "121": "analysis representation treats morpheme tags in a language-independent manner considering every tag found in the training set, whereas in MANN certain tags are chosen with a specific language in mind.", 
        "122": "(ii) MANN is a feed-forward NN that, unlike our approach, does not account for the surface context.", 
        "123": "(iii) As we understood, at the decoding step MANN makes use of the golden truth, whereas our models have no need for that.", 
        "124": "Although several statistical models have been proposed for Kazakh MD, such as HMM- (Makazhanov et al., 2014; Makhambetov et al., 2015; Assylbekov et al., 2016), voted perceptron- (Tolegen et al., 2016) and transformation-based (Kessikbayeva and Cicekli, 2016) taggers, to our knowledge ours is the first deep learning-based approach to the problem that is also purely language independent.", 
        "125": "It is becoming increasingly popular to use richer architectures to learn better embeddings from characters/words (Yessenbayev and Makazhanov, 2016; Ling et al., 2015; Wieting et al., 2016).", 
        "126": "Ling et al.", 
        "127": "(2015) used a BiLSTM to learn word vectors, showing strong performance on language modeling and POS tagging.", 
        "128": "Melamud et al.", 
        "129": "(2016) proposed context2vec, a BiLSTM based model to learn context embedding of target words and achieved state-of-the-art results on sentence completion and word sense disambiguation.", 
        "130": "5 Conclusion  We have proposed a general MD framework for MCL that can be analyzed in <root, POS, MC> triplets.", 
        "131": "We have showed that the surface context can be useful to MD, especially if combined with morphological context.", 
        "132": "Our next step would be to assess our claims on a larger number of typologically distant languages.", 
        "133": "Acknowledgments  This work has been conducted under the targeted program O.0743 (0115PK02473) of the Committee of Science of the Ministry of Education and Science of the Republic of Kazakhstan, and the research grant 129-2017/022-2017 of the Nazarbayev University.", 
        "134": "The authors would like to thank Xiaoqing Zheng for tremendously helpful discussions, as well as Eray Yildiz and Zhenisbek Assylbekov for the data sets used in this study and prompt replies to all questions regarding those."
    }, 
    "document_id": "P17-2105.pdf.json"
}
