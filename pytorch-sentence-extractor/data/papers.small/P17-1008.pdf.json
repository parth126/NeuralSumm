{
    "abstract_sentences": {
        "1": "Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth.", 
        "2": "Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation.", 
        "3": "We address these gaps by critically surveying the state of the art in the field."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 77\u201389 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1008\nSemantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth.", 
        "2": "Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation.", 
        "3": "We address these gaps by critically surveying the state of the art in the field.", 
        "4": "1 Introduction  Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way.", 
        "5": "There has recently been an influx of proposals for semantic representations and corpora, e.g.", 
        "6": "GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016).", 
        "7": "Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones.", 
        "8": "An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects.", 
        "9": "In this paper we begin to chart the various proposals for semantic schemes according to the content they support.", 
        "10": "As not many semantic queries on texts can at present be answered with near human-like reliability without using manual symbolic annotation, we will mostly focus on schemes\nthat represent semantic distinctions explicitly.1\nWe begin by discussing the goals of SRT in Section 2.", 
        "11": "Section 3 surveys major represented meaning components, including predicate-argument relations, discourse relations and logical structure.", 
        "12": "Section 4 details the various concrete proposals for SRT schemes and annotated resources, while Sections 5 and 6 discuss criteria for their evaluation and their relation to syntax, respectively.", 
        "13": "We find that despite the major differences in terms of formalism and interface with syntax, in terms of their content there is a great deal of convergence of SRT schemes.", 
        "14": "Principal differences between schemes are mostly related to their ability to abstract away from formal and syntactic variation, namely to assign similar structures to different constructions that have a similar meaning, and to assign different structures to constructions that have different meanings, despite their surface similarity.", 
        "15": "Other important differences are in the level of training they require from their annotators (e.g., expert annotators vs. crowd-sourcing) and in their cross-linguistic generality.", 
        "16": "We discuss the complementary strengths of different schemes, and suggest paths for future integration.", 
        "17": "2 Defining Semantic Representation  The term semantics is used differently in different contexts.", 
        "18": "For the purposes of this paper we define a semantic representation as one that reflects the meaning of the text as it is understood by a language speaker.", 
        "19": "A semantic representation should thus be paired with a method for extracting information from it that can be directly evaluated by humans.", 
        "20": "The extraction process should be reliable and computationally efficient.", 
        "21": "1Note that even a string representation of text can be regarded as semantic given a reliable enough parser.", 
        "22": "77\nWe stipulate that a fundamental component of the content conveyed by SRTs is argument structure \u2013 who did what to whom, where, when and why, i.e., events, their participants and the relations between them.", 
        "23": "Indeed, the fundamental status of argument structure has been recognized by essentially all approaches to semantics both in theoretical linguistics (Levin and Hovav, 2005) and in NLP, through approaches such as Semantic Role Labeling (SRL; Gildea and Jurafsky, 2002), formal semantic analysis (e.g., Bos, 2008), and Abstract Meaning Representation (AMR; Banarescu et al., 2013).", 
        "24": "Many other useful meaning components have been proposed, and are discussed at a greater depth in Section 3.", 
        "25": "Another approach to defining an SRT is through external (extra-textual) criteria or applications.", 
        "26": "For instance, a semantic representation can be defined to support inference, as in textual entailment (Dagan et al., 2006) or natural logic (Angeli and Manning, 2014).", 
        "27": "Other examples include defining a semantic representation in terms of supporting knowledge base querying (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), or defining semantics through a different modality, for instance interpreting text in terms of images that correspond to it (Kiros et al., 2014), or in terms of embodied motor and perceptual schemas (Feldman et al., 2010).", 
        "28": "A different approach to SRT is taken by Vector Space Models (VSM), which eschew the use of symbolic structures, instead modeling all linguistic elements as vectors, from the level of words to phrases and sentences.", 
        "29": "Proponents of this approach generally invoke neural network methods, obtaining impressive results on a variety of tasks including lexical tasks such as cross-linguistic word similarity (Ammar et al., 2016), machine translation (Bahdanau et al., 2015), and dependency parsing (Andor et al., 2016).", 
        "30": "VSMs are also attractive in being flexible enough to model non-local and gradient phenomena (e.g., Socher et al., 2013).", 
        "31": "However, more research is needed to clarify the scope of semantic phenomena that such models are able to reliably capture.", 
        "32": "We therefore only lightly touch on VSMs in this survey.", 
        "33": "Finally, a major consideration in semantic analysis, and one of its great potential advantages, is its cross-linguistic universality.", 
        "34": "While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as-\nsumed to be much closer in terms of their semantic content (Bar-Hillel, 1960; Fodor, 1975).", 
        "35": "See Section 5 for further discussion.", 
        "36": "A terminological note: within formal linguistics, semantics is often the study of the relation between symbols (e.g., words, syntactic constructions) and what they signify.", 
        "37": "In this sense, semantics is the study of the aspects of meaning that are overtly expressed by the lexicon and grammar of a language, and is thus tightly associated with a theory of the syntax-semantics interface.", 
        "38": "We note that this definition of semantics is somewhat different from the one intended here, which defines semantic schemes as theories of meaning.", 
        "39": "3 Semantic Content  We turn to discussing the main content types encoded by semantic representation schemes.", 
        "40": "Due to space limitations, we focus only on text semantics, which studies the meaning relationships between lexical items, rather than the meaning of the lexical items themselves.2 We also defer discussion of more targeted semantic distinctions, such as sentiment, to future work.", 
        "41": "We will use the following as a running example:\n(1) Although Ann was leaving, she gave the present to John.", 
        "42": "Events.", 
        "43": "Events (sometimes called frames, propositions or scenes) are the basic building blocks of argument structure representations.", 
        "44": "An event includes a predicate (main relation, frame-evoking element), which is the main determinant of what the event is about.", 
        "45": "It also includes arguments (participants, core elements) and secondary relations (modifiers, non-core elements).", 
        "46": "Example 1 is usually viewed as having two events, evoked by \u201cleaving\u201d and \u201cgave\u201d.", 
        "47": "Schemes commonly provide an ontology or a lexicon of event types (also a predicate lexicon), which categorizes semantically similar events evoked by different lexical items.", 
        "48": "For instance, FrameNet defines frames as schematized story fragments evoked by a set of conceptually similar predicates.", 
        "49": "In (1), the frames evoked by \u201cleaving\u201d and \u201cgave\u201d are DEPARTING and GIVING, but DEPARTING may also be evoked by \u201cdepart\u201d and \u201cexit\u201d, and GIVING by \u201cdonate\u201d and \u201cgift\u201d.", 
        "50": "2 We use the term \u201cText Semantics\u201d, rather than the commonly used \u201cSentence Semantics\u201d to include inter-sentence semantic relations as well.", 
        "51": "The events discussed here should not be confused with events as defined in Information Extraction and related tasks such as event coreference (Humphreys et al., 1997), which correspond more closely to the everyday notion of an event, such as a political or financial event, and generally consist of multiple events in the sense discussed here.", 
        "52": "The representation of such events is recently receiving considerable interest within NLP, e.g.", 
        "53": "the Richer Event Descriptions framework (RED; Ikuta et al., 2014).", 
        "54": "Predicates and Arguments.", 
        "55": "While predicateargument relations are universally recognized as fundamental to semantic representation, the interpretation of the terms varies across schemes.", 
        "56": "Most SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered.", 
        "57": "For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives.", 
        "58": "FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as \u201cpresident\u201d.", 
        "59": "Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015).", 
        "60": "Core and Non-core Arguments.", 
        "61": "Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003).", 
        "62": "While it is possible to define the distinction distributionally as one between obligatory and optional arguments, here we focus on the semantic dimension, which distinguishes arguments whose meaning is predicate-specific and are necessary components of the described event (core), and those which are predicate-general (non-core).", 
        "63": "For example, FrameNet defines core arguments as conceptually necessary components of a frame, that make the frame unique and different from other frames, and peripheral arguments as those that introduce additional, independent or distinct relations from that of the frame such as time, place, manner, means and degree (Ruppenhofer et al., 2016, pp.", 
        "64": "23-24).", 
        "65": "Semantic Roles.", 
        "66": "Semantic roles are categories of arguments.", 
        "67": "Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that\nevoke the same frame type, such as \u201cleave\u201d and \u201cdepart\u201d), and PropBank (where roles are verbspecific).", 
        "68": "PropBank\u2019s role sets were extended by subsequent projects such as AMR.", 
        "69": "Another prominent semantic role inventory is VerbNet (Kipper et al., 2008) and subsequent projects (Bonial et al., 2011; Schneider et al., 2015), which define a closed set of abstract semantic roles (such as AGENT, PATIENT and INSTRUMENT) that apply to all predicate arguments.", 
        "70": "Co-reference and Anaphora.", 
        "71": "Co-reference allows to abstract away from the different ways to refer to the same entity, and is commonly included in semantic resources.", 
        "72": "Coreference interacts with argument structure annotation, as in its absence each argument is arbitrarily linked to one of its textual instances.", 
        "73": "Most SRL schemes would mark \u201cAnn\u201d in (1) as an argument of \u201cleaving\u201d and \u201cshe\u201d as an argument of \u201cgave\u201d, although on semantic grounds \u201cAnn\u201d is an argument of both.", 
        "74": "Some SRTs distinguish between the cases of argument sharing which is encoded by the syntax and is thus explicit (e.g., in \u201cJohn went home and took a shower\u201d, \u201cJohn\u201d is both an argument of \u201cwent home\u201d and of \u201ctook a shower\u201d), and cases where the sharing of arguments is inferred (as in (1)).", 
        "75": "This distinction may be important for text understanding, as the inferred cases tend to be more ambiguous (\u201cshe\u201d in (1) might not refer to \u201cAnn\u201d).", 
        "76": "Other schemes, such as AMR, eschew this distinction and use the same terms to represent all cases of coreference.", 
        "77": "Temporal Relations.", 
        "78": "Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time.", 
        "79": "Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013).", 
        "80": "A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010).", 
        "81": "For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order.", 
        "82": "Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications,\nincluding planning and entailment.", 
        "83": "See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types.", 
        "84": "Mostafazadeh et al.", 
        "85": "(2016) integrated causal and TimeML-style temporal relations into a unified representation.", 
        "86": "The internal temporal structure of events has been less frequently tackled.", 
        "87": "Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., \u201cclimbing a mountain\u201d), or its culmination (\u201creaching its top\u201d).", 
        "88": "Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010).", 
        "89": "Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP.", 
        "90": "Spatial Relations.", 
        "91": "The representation of spatial relations is pivotal in cognitive theories of meaning (e.g., Langacker, 2008), and in application domains such as geographical information systems or robotic navigation.", 
        "92": "Important tasks in this field include Spatial Role Labeling (Kordjamshidi et al., 2012) and the more recent SpaceEval (Pustejovsky et al., 2015).", 
        "93": "The tasks include the identification and classification of spatial elements and relations, such as places, paths, directions and motions, and their relative configuration.", 
        "94": "Discourse Relations encompass any semantic relation between events or larger semantic units.", 
        "95": "For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type CONCESSION, evoked by \u201calthough\u201d.", 
        "96": "Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012).", 
        "97": "The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between them into a hierarchical, closed category set, including high-level relation types like TEMPORAL, COMPARISON and CONTINGENCY and finer-grained ones such as JUSTIFICATION and EXCEPTION.", 
        "98": "Another commonly used resource is the RST Discourse Tree-\nbank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT\u2019s, which focuses on local discourse structure.", 
        "99": "Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow structures of discourse units categorized either according to their topic or according to their function within the text.", 
        "100": "An example is the segmentation of scientific papers into functional segments and their labeling with categories such as BACKGROUND and DISCUSSION (Liakata et al., 2010).", 
        "101": "See (Webber et al., 2011) for a survey of discourse structure in NLP.", 
        "102": "Discourse relations beyond the scope of a single sentence are often represented by specialized semantic resources and not by general ones, despite the absence of a clear boundary line between them.", 
        "103": "This, however, is beginning to change with some schemes, e.g., GMB and UCCA, already supporting cross-sentence semantic relations.3\nLogical Structure.", 
        "104": "Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well.", 
        "105": "Common representations are often based on variants of predicate calculus, and are useful for applications that require mapping text into an external, often executable, formal language, such as a querying language (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or robot instructions (Artzi and Zettlemoyer, 2013).", 
        "106": "Logical structures are also useful for recognizing entailment relations between sentences, as some entailments can be computed from the text\u2019s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013).", 
        "107": "Inference and Entailment.", 
        "108": "A primary motivation for many semantic schemes is their ability to support inference and entailment.", 
        "109": "Indeed, means for predicting logical entailment are built into many forms of semantic representations.", 
        "110": "A different approach was taken in the tasks of Recognizing Textual Entailment (Dagan et al., 2013), and Natural Logic (van Eijck, 2005), which considers an inference valid if a reasonable annotator would find the hypothesis likely to hold given\n3AMR will also support discourse structure in its future versions (N. Schneider; personal communication).", 
        "111": "the premise, even if it cannot be deduced from it.", 
        "112": "See (Manning, 2006) for a discussion of this point.", 
        "113": "Such inference relations are usually not included in semantic treebanks, but annotated in specialized resources (e.g., Dagan et al., 2006; Bowman et al., 2015).", 
        "114": "4 Semantic Schemes and Resources  This section briefly surveys the different schemes and resources for SRT.", 
        "115": "We focus on design principles rather than specific features, as the latter are likely to change as the schemes undergo continuous development.", 
        "116": "In general, schemes discussed in Section 3 are not repeated here.", 
        "117": "Semantic Role Labeling.", 
        "118": "SRL schemes diverge in their event types, the type of predicates they cover, their granularity, their cross-linguistic applicability, their organizing principles and their relation with syntax.", 
        "119": "Most SRL schemes define their annotation relative to some syntactic structure, such as parse trees of the PTB in the case of PropBank, or specialized syntactic categories defined for SRL purposes in the case of FrameNet.", 
        "120": "Other than PropBank, FrameNet and VerbNet discussed above, other notable resources include Semlink (Loper et al., 2007) that links corresponding entries in different resources such as PropBank, FrameNet, VerbNet and WordNet, and the Preposition Supersenses project (Schneider et al., 2015), which focuses on roles evoked by prepositions.", 
        "121": "See (Palmer et al., 2010, 2013) for a review of SRL schemes and resources.", 
        "122": "SRL schemes are often termed \u201cshallow semantic analysis\u201d due to their focus on argument structure, leaving out other relations such as discourse events, or how predicates and arguments are internally structured.", 
        "123": "AMR.", 
        "124": "AMR covers predicate-argument relations, including semantic roles (adapted from PropBank) that apply to a wide variety of predicates (including verbal, nominal and adjectival predicates), modifiers, co-reference, named entities and some time expressions.", 
        "125": "AMR does not currently support relations above the sentence level, and is admittedly Englishcentric, which results in an occasional conflation of semantic phenomena that happen to be similarly realized in English, into a single semantic category.", 
        "126": "AMR thus faces difficulties when assessing the invariance of its structures across translations (Xue et al., 2014).", 
        "127": "As an example,\nconsider the sentences \u201cI happened to meet Jack in the office\u201d, and \u201cI asked to meet Jack in the office\u201d.", 
        "128": "While the two have similar syntactic forms, the first describes a single \u201cmeeting\u201d event, where \u201chappened\u201d is a modifier, while the second describes two distinct events: asking and meeting.", 
        "129": "AMR annotates both in similar terms, which may be suitable for English, where aspectual relations are predominantly expressed as subordinating verbs (e.g., \u201cbegin\u201d, \u201cwant\u201d), and are syntactically similar to primary verbs that take an infinitival complement (such as \u201cask to meet\u201d or \u201clearn to swim\u201d).", 
        "130": "However, this approach is less suitable cross-linguistically.", 
        "131": "For instance, when translating the sentences to German, the divergence between the semantics of the two sentences is clear: in the first \u201chappened\u201d is translated to an adverb: \u201cIch habe Jack im Bu\u0308ro zufa\u0308llig getroffen\u201d (lit.", 
        "132": "\u201cI have Jack in-the office by-chance met\u201d), and in the second \u201casked\u201d is translated to a verb: \u201cIch habe gebeten, Jack im Bu\u0308ro zu treffen\u201d (lit.", 
        "133": "\u201cI have asked, Jack in-the office to meet\u201d).", 
        "134": "UCCA.", 
        "135": "UCCA (Universal Conceptual Cognitive Annotation) (Abend and Rappoport, 2013a,b) is a cross-linguistically applicable scheme for semantic annotation, building on typological theory, primarily on Basic Linguistic Theory (Dixon, 2010).", 
        "136": "UCCA\u2019s foundational layer of categories focuses on argument structures of various types and relations between them.", 
        "137": "In its current state, UCCA is considerably more coarse-grained than the above mentioned schemes (e.g., it does not include semantic role information).", 
        "138": "However, its distinctions tend to generalize well across languages (Sulem et al., 2015).", 
        "139": "For example, unlike AMR, it distinguishes between primary and aspectual verbs, so cases such as \u201chappened to meet\u201d are annotated similarly to cases such as \u201cmet by chance\u201d, and differently from \u201casked to meet\u201d.", 
        "140": "Another design principle UCCA evokes is support for annotation by non-experts.", 
        "141": "To do so the scheme reformulates some of the harder distinctions into more intuitive ones.", 
        "142": "For instance, the core/non-core distinction is replaced in UCCA with the distinction between pure relations (Adverbials) and those evoking an object (Participants), which has been found easier for annotators to apply.", 
        "143": "UDS.", 
        "144": "Universal Decompositional Semantics (White et al., 2016) is a multi-layered scheme, which currently includes semantic role anno-\ntation, word senses and aspectual classes (e.g., realis/irrealis).", 
        "145": "UDS emphasizes accessible distinctions, which can be collected through crowd-sourcing.", 
        "146": "However, the skeletal structure of UDS representations is derived from syntactic dependencies, and only includes verbal argument structures that can be so extracted.", 
        "147": "Notably, many of the distinctions in UDS are defined using feature bundles, rather than mutually exclusive categories.", 
        "148": "For instance, a semantic role may be represented as having the features +VOLITION and +AWARENESS, rather than as having the category AGENT.", 
        "149": "The Prague Dependency Treebank (PDT) Tectogrammatical Layer (PDT-TL) (Sgall, 1992; Bo\u0308hmova\u0301 et al., 2003) covers a rich variety of functional and semantic distinctions, such as argument structure (including semantic roles), tense, ellipsis, topic/focus, co-reference, word sense disambiguation and local discourse information.", 
        "150": "The PDT-TL results from an abstraction over PDT\u2019s syntactic layers, and its close relation with syntax is apparent.", 
        "151": "For instance, the PDT-TL encodes the distinction between a governing clause and a dependent clause, which is primarily syntactic in nature, so in the clauses \u201cJohn came just as we were leaving\u201d and \u201cWe were leaving just as John came\u201d the governing and dependent clause are swapped, despite their semantic similarity.", 
        "152": "CCG-based Schemes.", 
        "153": "CCG (Steedman, 2000) is a lexicalized grammar (i.e., nearly all semantic content is encoded in the lexicon), which defines a theory of how lexical information is composed to form the meaning of phrases and sentences (see Section 6.2), and has proven effective in a variety of semantic tasks (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Artzi and Zettlemoyer, 2013, inter alia).", 
        "154": "Several projects have constructed logical representations by associating CCG with semantic forms (by assigning logical forms to the leaves).", 
        "155": "For example, Boxer (Bos, 2008) and GMB, which builds on Boxer, use Discourse Representation Structures (Kamp and Reyle, 1993), while Lewis and Steedman (2013) used Davidsonian-style \u03bb-expressions, accompanied by lexical categorization of the predicates.", 
        "156": "These schemes encode events with their argument structures, and include an elaborate logical structure, as well as lexical and discourse information.", 
        "157": "HPSG-based Schemes.", 
        "158": "Related to CCG-based schemes are SRTs based on Head-driven Phrase\nStructure Grammar (HPSG; Pollard and Sag, 1994), where syntactic and semantic features are represented as feature bundles, which are iteratively composed through unification rules to form composite units.", 
        "159": "HPSG-based SRT schemes commonly use the Minimal Recursion Semantics (Copestake et al., 2005) formalism.", 
        "160": "Annotated corpora and manually crafted grammars exist for multiple languages (Flickinger, 2002; Oepen et al., 2004; Bender and Flickinger, 2005, inter alia), and generally focus on argument structural and logical semantic phenomena.", 
        "161": "The Broad-coverage Semantic Dependency Parsing shared task and corpora (Oepen et al., 2014, 2015) include corpora annotated with the PDT-TL, and dependencies extracted from the HPSG grammars Enju (Miyao, 2006) and the LinGO English Reference Grammar (ERG; Flickinger, 2002).", 
        "162": "Like the PDT-TL, projects based on CCG, HPSG, and other expressive grammars such as LTAG (Joshi and Vijay-Shanker, 1999) and LFG (Kaplan and Bresnan, 1982) (e.g., GlueTag (Frank and van Genabith, 2001)), yield semantic representations that are coupled with syntactic ones.", 
        "163": "While this approach provides powerful tools for inference, type checking, and mapping into external formal languages, it also often results in difficulties in abstracting away from some syntactic details.", 
        "164": "For instance, the dependencies derived from ERG in the SDP corpus use the same label for different senses of the English possessive construction, regardless of whether they correspond to ownership (e.g., \u201cJohn\u2019s dog\u201d) or to a different meaning, such as marking an argument of a nominal predicate (e.g., \u201cJohn\u2019s kick\u201d).", 
        "165": "See Section 6.", 
        "166": "OntoNotes is a useful resource with multiple inter-linked layers of annotation, borrowed from different schemes.", 
        "167": "The layers include syntactic, SRL, co-reference and word sense disambiguation content.", 
        "168": "Some properties of the predicate, such as which nouns are eventive, are encoded as well.", 
        "169": "To summarize, while SRT schemes differ in the types of content they support, schemes evolve to continuously add new content types, making these differences less consequential.", 
        "170": "The fundamental difference between the schemes is the extent that they abstract away from syntax.", 
        "171": "For instance, AMR and UCCA abstract away from syntax as part of their design, while in most other schemes syntax and semantics are more tightly coupled.", 
        "172": "Schemes also differ in other aspects discussed in Sections 5 and 6.", 
        "173": "5 Evaluation  Human evaluation is the ultimate criterion for validating an SRT scheme given our definition of semantics as meaning as it is understood by a language speaker.", 
        "174": "Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT.", 
        "175": "Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation.", 
        "176": "Hartshorne et al.", 
        "177": "(2013) and Reisinger et al.", 
        "178": "(2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets.", 
        "179": "Another evaluation approach is task-based evaluation.", 
        "180": "Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural.", 
        "181": "For instance, a major motivation for AMR is its applicability to machine translation, making MT a natural (albeit hitherto unexplored) testbed for AMR evaluation.", 
        "182": "Another example is using question answering to evaluate semantic parsing into knowledge-base queries.", 
        "183": "Another common criterion for evaluating a semantic scheme is invariance, where semantic analysis should be similar across paraphrases or translation pairs (Xue et al., 2014; Sulem et al., 2015).", 
        "184": "For instance, most SRL schemes abstract away from the syntactic divergence between the sentences (1) \u201cHe gave a present to John\u201d and (2) \u201cIt was John who was given a present\u201d (although a complete analysis would reflect the difference of focus between them).", 
        "185": "Importantly, these evaluation criteria also apply in cases where the representation is automatically induced, rather than manually defined.", 
        "186": "For instance, vector space representations are generally evaluated either through task-based evaluation, or in terms of semantic features computed from them, whose validity is established by human annotators (e.g., Agirre et al., 2013, 2014).", 
        "187": "Finally, where semantic schemes are induced through manual annotation (and not through au-\ntomated procedures), a common criterion for determining whether the guidelines are sufficiently clear, and whether the categories are well-defined is to measure agreement between annotators, by assigning them the same texts and measuring the similarity of the resulting structures.", 
        "188": "Measures include the SMATCH measure for AMR (Cai and Knight, 2013), and the PARSEVAL F-score (Black et al., 1991) adapted for DAGs for UCCA.", 
        "189": "SRT schemes diverge in the background and training they require from their annotators.", 
        "190": "Some schemes require extensive training (e.g., AMR), while others can be (at least partially) collected by crowdsourcing (e.g., UDS).", 
        "191": "Other examples include FrameNet, which requires expert annotators for creating new frames, but employs less trained in-house annotators for applying existing frames to texts; QASRL, which employs non-expert annotators remotely; and UCCA, which uses inhouse non-experts, demonstrating no advantage to expert over non-expert annotators after an initial training period.", 
        "192": "Another approach is taken by GMB, which uses online collaboration where expert collaborators participate in manually correcting automatically created representations.", 
        "193": "They further employ gamification strategies for collecting some aspects of the annotation.", 
        "194": "Universality.", 
        "195": "One of the great promises of semantic analysis (over more surface forms of analysis) is its cross-linguistic potential.", 
        "196": "However, while the theoretical and applicative importance of universality in semantics has long been recognized (Goddard, 2011), the nature of universal semantics remains unknown.", 
        "197": "Recently, projects such as BabelNet (Ehrmann et al., 2014), UBY (Gurevych et al., 2012) and Open Multilingual Wordnet4, constructed huge multi-lingual semantic nets, by linking resources such as Wikipedia and WordNet and processing them using modern NLP.", 
        "198": "However, such projects currently focus on lexical semantic and encyclopedic information rather than on text semantics.", 
        "199": "Symbolic SRT schemes such as SRL schemes and AMR have also been studied for their crosslinguistic applicability (Pado\u0301 and Lapata, 2009; Sun et al., 2010; Xue et al., 2014), indicating partial portability across languages.", 
        "200": "Translated versions of PropBank and FrameNet have been constructed for multiple languages (e.g., Akbik et al., 2016; Hartmann and Gurevych, 2013).", 
        "201": "How-\n4http://compling.hss.ntu.edu.sg/omw/\never, as both PropBank and FrameNet are lexicalized schemes, and as lexicons diverge wildly across languages, these schemes require considerable adaptation when ported across languages (Kozhevnikov and Titov, 2013).", 
        "202": "Ongoing research tackles the generalization of VerbNet\u2019s unlexicalized roles to a universally applicable set (e.g., Schneider et al., 2015).", 
        "203": "Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the LinGO Grammar Matrix (Bender and Flickinger, 2005), both of which draw on typological theory.", 
        "204": "Vector space models, which embed words and sentences in a vector space, have also been applied to induce a shared cross-linguistic space (Klementiev et al., 2012; Rajendran et al., 2015; Wu et al., 2016).", 
        "205": "However, further evaluation is required in order to determine what aspects of meaning these representations reflect reliably.", 
        "206": "6 Syntax and Semantics    6.1 Syntactic and Semantic Generalization  Syntactic distinctions are generally guided by a combination of semantic and distributional considerations, where emphasis varies across schemes.", 
        "207": "Consider phrase-based syntactic structures, common examples of which, such as the Penn Treebank for English (Marcus et al., 1993) and the Penn Chinese Treebank (Xue et al., 2005), are adaptations of X-bar theory.", 
        "208": "Constituents are commonly defined in terms of distributional criteria, such as whether they can serve as conjuncts, be passivized, elided or fronted (Carnie, 2002, pp.", 
        "209": "50-53).", 
        "210": "Moreover, phrase categories are defined according to the POS category of their headword, such as Noun Phrase, Verb Phrase or Preposition Phrase, which are also at least partly distributional, motivated by their similar morphological and syntactic distribution.", 
        "211": "In contrast, SRT schemes tend to abstract away from these realizational differences and directly reflect the argument structure of the sentence using the same set of categories, irrespective of the POS of the predicate, or the case marking of its arguments.", 
        "212": "Distributional considerations are also apparent with functional syntactic schemes (the most commonly used form of which in NLP are lexicalist dependency structures), albeit to a lesser extent.", 
        "213": "A prominent example is Universal Dependencies (UD; Nivre et al., 2016), which aims at produc-\ning a cross-linguistically consistent dependencybased annotation, and whose categories are motivated by a combination of distributional and semantic considerations.", 
        "214": "For example, UD would distinguish between the dependency type between \u201cJohn\u201d and \u201cbrother\u201d in \u201cJohn, my brother, arrived\u201d and \u201cJohn, who is my brother, arrived\u201d, despite their similar semantics.", 
        "215": "This is due to the former invoking an apposition, and the latter a relative clause, which are different in their distribution.", 
        "216": "As an example of the different categorization employed by UD and by purely semantic schemes such as AMR and UCCA consider (1) \u201cfounding of the school\u201d, (2) \u201cpresident of the United States\u201d and (3) \u201cUnited States president\u201d.", 
        "217": "UD is faithful to the syntactic structure and represents (1) and (2) similarly, while assigning a different structure to (3).", 
        "218": "In contrast, AMR and UCCA perform a semantic generalization and represents examples (2) and (3) similarly and differently from (1).", 
        "219": "6.2 The Syntax-Semantics Interface  A common assumption on the interface between syntax and semantics is that semantics of phrases and sentences is compositional \u2013 it is determined recursively by the meaning of its immediate constituents and their syntactic relationships, which are generally assumed to form a closed set (Montague, 1970, and much subsequent work).", 
        "220": "Thus, the interpretation of a sentence can be computed bottom-up, by establishing the meaning of individual words, and recursively composing them, to obtain the full sentential semantics.", 
        "221": "The order and type of these compositions are determined by the syntactic structure.", 
        "222": "Compositionality is employed by linguistically expressive grammars, such as those based on CCG and HPSG, and has proven to be a powerful method for various applications.", 
        "223": "See (Bender et al., 2015) for a recent discussion of the advantages of compositional SRTs.", 
        "224": "Nevertheless, a compositional account meets difficulties when faced with multi-word expressions and in accounting for cases like \u201che sneezed the napkin off the table\u201d, where it is difficult to determine whether \u201csneezed\u201d or \u201coff\u201d account for the constructional meaning.", 
        "225": "Construction Grammar (Fillmore et al., 1988; Goldberg, 1995) answers these issues by using an open set of construction-specific compositional operators, and supporting lexical en-\ntries of varying lengths.", 
        "226": "Several ongoing projects address the implementation of the principles of Construction Grammar into explicit grammars, including Sign-based Construction Grammar (Fillmore et al., 2012), Embodied Construction Grammar (Feldman et al., 2010) and Fluid Construction Grammar (Steels and de Beules, 2006).", 
        "227": "The achievements of machine learning methods in many areas, and optimism as to its prospects, have enabled the approaches to semantics discussed in this paper.", 
        "228": "Machine learning allows to define semantic structures on purely semantic grounds and to let algorithms identify how these distinctions are mapped to surface/distributional forms.", 
        "229": "Some of the schemes discussed in this paper take this approach in its pure form (e.g., AMR and UCCA).", 
        "230": "7 Conclusion  Semantic representation in NLP is undergoing rapid changes.", 
        "231": "Traditional semantic work has either used shallow methods that focus on specific semantic phenomena, or adopted formal semantic theories which are coupled with a syntactic scheme through a theory of the syntax-semantics interface.", 
        "232": "Recent years have seen increasing interest in an alternative approach that defines semantic structures independently from any syntactic or distributional criteria, much due to the availability of semantic treebanks that implement this approach.", 
        "233": "Semantic schemes diverge in whether they are anchored in the words and phrases of the text (e.g., all types of semantic dependencies and UCCA) or not (e.g., AMR and logic-based representations).", 
        "234": "We do not view this as a major difference, because most unanchored representations (including AMR) retain their close affinity with the words of the sentence, possibly because of the absence of a workable scheme for lexical decomposition, while dependency structures can be converted into logic-based representations (Reddy et al., 2016).", 
        "235": "In practice, anchoring facilitates parsing, while unanchored representations are more flexible to use where words and semantic components are not in a one-to-one correspondence.", 
        "236": "Our survey concludes that the main distinguishing factors between schemes are their relation to syntax, their degree of universality, and the expertise and training they require from annotators, an important factor in addressing the annotation bottleneck.", 
        "237": "We hope this survey of the state of the art in semantic representation will promote discus-\nsion, expose more researchers to the most pressing questions in semantic representation, and lead to the wide adoption of the best components from each scheme.", 
        "238": "Acknowledgements.", 
        "239": "We thank Nathan Schneider for his helpful comments.", 
        "240": "The work was support by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
    }, 
    "document_id": "P17-1008.pdf.json"
}
