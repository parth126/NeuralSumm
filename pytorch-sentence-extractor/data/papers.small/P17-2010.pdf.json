{
    "abstract_sentences": {
        "1": "Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation.", 
        "2": "We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment.", 
        "3": "Compound splitting has great potential for this novel task that is both transparent and well-defined.", 
        "4": "Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by taskinternal mechanisms in statistical machine translation.", 
        "5": "We show significant improvements using different compound splitting methods on a German textual entailment dataset."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 58\u201363 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2010  1 Introduction  Closed compounding, i.e., the formation of a oneword unit composing several lexemes, is a common linguistic phenomenon in several languages such as German, Dutch, Greek, and Finnish.", 
        "2": "The goal of compound splitting is to obtain the constituents of a compound to increase its semantic transparency.", 
        "3": "For example, for the German compound Apfelsaft \u2018apple1 juice2\u2019 the desired output of a compound splitter is Apfel1 Saft2.", 
        "4": "Intrinsic evaluation of compound splitting measures the correctness of the determined split point (Riedl and Biemann, 2016) and the resulting lemmas by means of precision, recall, F1-score and accuracy (e.g., Koehn and Knight (2003)).", 
        "5": "In extrinsic evaluation setups, compound splitting is applied to the input data of an external natural language processing (NLP) task that benefits from split compounds.", 
        "6": "As closed compounding introduces semantic opaqueness and vastly increases the vocabulary size of a language, many NLP tasks\nbenefit from compound splitters.", 
        "7": "Still, previous work that evaluates compound splitting with extrinsic evaluation methods mostly focuses on statistical machine translation (SMT) (e.g., Nie\u00dfen and Ney (2000), Koehn and Knight (2003)).", 
        "8": "Some other external tasks such as information retrieval (Kraaij and Pohlmann, 1998) or speech recognition (Larson et al., 2000) have been shown to benefit from prior compound splitting, yet these works have not compared the extrinsic performance of different compound splitting methods.", 
        "9": "Interestingly, the performance found in intrinsic evaluations does not automatically propagate to performance in downstream evaluations as shown in (Fritzinger and Fraser, 2010) for SMT, where oversplit compounds are simply learned as phrases (Dyer, 2009; Weller et al., 2014).", 
        "10": "Oversplitting is an example of a feature that might not be measured in intrinsic evaluations, because some available gold standards contain positive examples only (Ziering and van der Plas, 2016).", 
        "11": "It is highly relevant to increase the number of extrinsic tasks for the evaluation of compound splitting to be able to evaluate features that intrinsic evaluations and known extrinsic evaluations ignore.", 
        "12": "In this paper we investigate the suitability of Recognizing Textual Entailment (RTE) for the task of compound splitting, inspired by the fact that previous work in RTE underlined the potential benefits of compound splitting for this task (Zeller, 2016).", 
        "13": "Textual Entailment (TE) is a directional relationship between an entailing text fragment T and an entailed hypothesis, H, saying that the meaning of T entails (or implies) the meaning of H. This relation holds if \u2018typically, a human, reading T, would infer that H is most likely true\u2019 (Dagan et al., 2006).", 
        "14": "The following is an example of an entailing T-H pair:\nT: Yoko Ono unveiled a bronze statue of her late husband, John Lennon.", 
        "15": "H: Yoko Ono is John Lennon\u2019s widow.", 
        "16": "58\nWe opted for exploring the use of RTE as an extrinsic evaluation for compound splitting for three main reasons: first, in contrast to SMT systems, most RTE systems are less complex.", 
        "17": "In fact, we deliberately chose an RTE system that reaches good performance with a method that is transparent, i.e., a method that allows for exploring the effect of compounding.1 It is not our goal to reach state-of-the-art performance for the RTE task.", 
        "18": "We aim to find a suitable alternative extrinsic evaluation for compound splitting.", 
        "19": "Second, human agreement on the binary RTE decisions is very high, e.g., on the dataset used in our experiments, an average agreement rate of 87.8% with a \u03ba level of 0.75 was reported (Giampiccolo et al., 2007).", 
        "20": "Third, the potential benefits for RTE are large.", 
        "21": "According to Zeller (2016, p. 182) the number of T/H pairs in their phenomenon-specific RTE dataset would rise by about 16 percentage points by compound splitting.", 
        "22": "In the dataset we use in our experiments, about three-quarters of the T-H pairs contain at least one closed compound.", 
        "23": "2 Relevance of Compound Splitting for RTE  The approach to RTE taken in this paper follows the Lexical Overlap Hypothesis (LOH), which states that the higher the number of lexical matches between T and H, the more likely the T-H pair is entailing rather than non-entailing (Zeller, 2016).", 
        "24": "In other words, H is more likely to be entailed by T if most of its lexical content also occurs in T. While this hypothesis is a simplification of the TE problem, it has been shown to perform reasonably well for some datasets (Noh et al., 2015).", 
        "25": "We argue that the brittleness of the chosen LOH-based RTE system may actually be a strength in terms of evaluation, since it will penalize oversplitting more severely than, e.g., an RNNbased RTE system or a phrase-based MT method that can recover from systematic oversplitting by chunking the splits.", 
        "26": "Under the LOH, the problem caused by the opacity of closed compounds becomes evident.", 
        "27": "As shown in the example below, missing information on the constituents of closed compounds hinders the matching of words from T in H1.", 
        "28": "Conversely, compound splitting also helps to detect\n1We did not opt for neural RTE systems (Bowman et al., 2015), albeit state-of-the-art, in this first study because of the opacity of the models and the inclusion of phrase-level information, which will make interpretation of the effect harder.", 
        "29": "non-entailing T-H pairs.", 
        "30": "By compound splitting, we increase the number of uncovered tokens in H2, which makes a non-entailment decision more likely2.", 
        "31": "T: Kinder lieben Fruchtsa\u0308fte1 aus A\u0308pfeln2 \u2018Children love fruit juices1 made of apples2\u2019 H1: Peters Sohn liebt Apfel3saft4 \u2018Peter\u2019s son loves apple3 juice4\u2019 H2: Peters Sohn liebt Apfel5kuchen6stu\u0308cke7 \u2018Peter\u2019s son loves pieces7 of apple5 pie6\u2019  3 Materials and Methods  In this section we explain the splitters and the RTE framework used in our experiments.", 
        "32": "3.1 Inspected Compound Splitters  Our proposed extrinsic evaluation approach for compound splitting is language-independent as we do not use any language-specific parameters.", 
        "33": "However, in the present work we test it on the most prominent closed-compounding language, German (Ziering and van der Plas, 2014).", 
        "34": "We inspect the impact of three different types of automatic compound splitting3 methods that follow a generate-and-rank principle, where the candidate splits are ranked according to the geometric mean of the constituents\u2019 frequencies in a given training corpus (Koehn and Knight, 2003).", 
        "35": "FF2010 The compound splitter by Fritzinger and Fraser (2010) relies on the output of the German morphological analyzer SMOR (Schmid et al., 2004) to generate several plausible compound splits (e.g., due to word sense ambiguity).", 
        "36": "WH2012 As an alternative method, we use the statistical approach presented in Weller and Heid (2012) for German compound splitting.", 
        "37": "Instead of using the knowledge-rich SMOR, it includes an extensive list of hand-crafted transformation rules that allows to map constituents to corpus lemmas (e.g., by truncating linking morphemes) to generate all possible splits with up to four constituents per compound.", 
        "38": "Moreover, misleading lemmas are removed from the training corpus using hand-crafted filters.", 
        "39": "2Note that we need to apply lemmatization prior to determining the lexical matches between T and H.\n3The compound splitters are designed to split compounds with any content word as head, i.e., noun compounds (Hunde|hu\u0308tte \u2018doghouse\u2019), verb compounds (eis|laufen \u2018to ice-skate\u2019) and adjective compounds (hunde|mu\u0308de \u2018dogtired\u2019) and disregard constructions with a functional modifier (as in the particle verb auf|stehen \u2018to stand up\u2019).", 
        "40": "ZvdP2016 Finally, the method using least language-specific knowledge was proposed by Ziering and van der Plas (2016).", 
        "41": "Instead of using a morphological analyzer or manually compiling a hand-crafted list of rules, they recursively generate all possible binary splits by learning constituent transformations from regular inflection derived from a monolingual lemmatized corpus, e.g., the s-suffix in the case of a genitive marker is often used as linking morpheme.", 
        "42": "The recursion stops if a non-splitting (atomic) analysis is ranked highest.", 
        "43": "Additionally, to provide an upper bound, we manually split development and test data.", 
        "44": "3.2 RTE Framework  We conduct our RTE experiments using the opensource Excitement Open Platform (EOP) (Pado\u0301 et al., 2015; Magnini et al., 2014), which provides comprehensive implementations of algorithms and lexical resources for textual inference.", 
        "45": "We use the alignment-based algorithm P1EDA (Noh et al., 2015) in all our experiments as it has been shown to be simple and transparent while yielding relatively good results.", 
        "46": "P1EDA is based on the LOH for RTE explained in Section 2.", 
        "47": "The algorithm works in three steps: First, it extracts all possible alignments between sequences of identical lemmas in T and H. Then, it extracts various features4 from the alignments.", 
        "48": "Finally, these features are given as input to a multinomial logistic regression classifier which is trained on annotated data.", 
        "49": "For the sake of simplicity, for now we only use one basic aligner which aligns (sequences of) words in T and H that consist of identical lemmas.", 
        "50": "We will investigate the impact of prior compound splitting given additional lexical resources (such as a derivational morphology lex-\n4We use a similar feature set as Noh et al.", 
        "51": "(2015), namely the ratio of aligned vs. unaligned words in H with respect to all words, content words, and named entities.", 
        "52": "icon (Zeller et al., 2013)) in future work.", 
        "53": "We use TreeTagger (Schmid, 1995) as integrated in EOP to provide tokenization, lemmatization and Partof-Speech tagging as linguistic preprocessing.", 
        "54": "We train and evaluate all models on the German translation of the RTE-3 dataset (Dagan et al., 2006; Magnini et al., 2014).", 
        "55": "The training and test dataset contain 800 T-H pairs each.", 
        "56": "In both sets, entailing and non-entailing T-H pairs are equally distributed (chance baseline of 50% accuracy).", 
        "57": "We apply a compound splitter on the RTE training and test dataset before we input the data to the EOP pipeline.", 
        "58": "We replace all compounds by their constituents, separated by white-space.", 
        "59": "Thus, they are subsequently treated as individual words by EOP and the lexical aligner can benefit from the increased transparency of the compounds.", 
        "60": "4 Results and Discussion  Table 1 shows accuracy, precision, recall and F1score for the entailment and non-entailment class on the RTE-3 dataset.", 
        "61": "As reflected in the results, reducing the opacity of compounds via the application of a compound splitter improves the subsequent RTE performance.", 
        "62": "This holds for all compound splitters that we used in our experiments.", 
        "63": "It is also noticeable that the different compound splitters yield different results in the downstream task, with FF2010 being the most beneficial and significantly5 outperforming the initial RTE setup without prior compound splitting (INIT) by up to four percentage points in accuracy and F1-score.", 
        "64": "As expected, manual splitting performs best overall.", 
        "65": "The performance difference with FF2010 is however not statistically significant.", 
        "66": "This is not surprising because FF2010 reaches an accuracy of around 90% in intrinsic evaluations (Ziering and van der Plas, 2016) and the small underperfor-\n5McNemar test (McNemar, 1947), p < 0.05\nmance is leveled out by the small size of the test set.", 
        "67": "Moreover, manual inspections revealed that FF2010 has a higher recall than manual splitting in the non-entailment class due to its undersplitting which results in less lexical overlap between T and H, pointing to the non-entailment class.", 
        "68": "When we compare these results from the extrinsic evaluation with intrinsic evaluation results (in terms of splitting accuracy) reported in Ziering and van der Plas (2016), we see the same performance ordering with respect to the three compound splitters, while the current extrinsic evaluation on RTE differentiates between the best system (FF2010) and the two others in that only the former reached statistically significant improvements over the INIT baseline.", 
        "69": "To analyze the possible causes of difference in performance between the systems and to see the benefits of using RTE for compound splitting evaluation we performed a manual error analysis.", 
        "70": "First, we examined all entailment classifications that were correct using FF2010 and incorrect when using the INIT baseline.", 
        "71": "Using FF2010, the classifier was able to correctly classify an additional 36 entailing and 25 non-entailing T-H pairs.", 
        "72": "As expected, most of the hypotheses in these pairs contained correctly split compounds where the RTE system could benefit from the increased transparency.", 
        "73": "Conversely, we also examined the 28 T-H pairs that the classifier missed to identify as entailing while they were correct in INIT.", 
        "74": "Most of the examples were cases in which there was almost no lexical overlap between T and H even with compound splitting.", 
        "75": "Furthermore, we compared the correct entailment classifications of FF2010 with the other two splitters.", 
        "76": "For ZvdP2016, most errors can be attributed to oversplitting.", 
        "77": "Precisely, 25 out of its 37 (67.5%) misclassifications compared to FF2010 can be attributed to this problem.", 
        "78": "For example, ZvdP2016 oversplit the name Landowska into Line Dow Ska6 that appeared in both T and H in an non-entailing pair, which artificially increased the coverage ratio of words in H and therefore pointed to the incorrect entailment classification.", 
        "79": "For WH2012, oversplitting is also a major contributor of RTE errors, however it appeares not as predominant as for ZvdP2016.", 
        "80": "10 out of its 29 (34.5%) misclassifications compared to FF2010\n6Misleading knowledge about verbal inflection automatically derived from a lemmatized corpus is responsible for the oversplitting by ZvdP2016.", 
        "81": "can be attributed to oversplitting, while 4 (13.8%) missclassifications are due to undersplitting.", 
        "82": "For example, in an entailing T-H pair WH2012 correctly split Amazonas-Regenwald \u2018Amazon rainforest\u2019 in H into Amazonas Regen Wald, however it oversplit Amazonas in T into Amazon As \u2018Amazon ace\u2019 and thus, Amazonas in H remained unaligned.", 
        "83": "To the contrary, FF2010 did not split Amazonas in T, which lead to a higher token coverage ratio in H. Again, in the H Die EU senkt die Fangquoten \u2018The EU lowers the fishing quota\u2019 of another entailing T-H pair, WH2010 correctly split Fang1quoten2 \u2018fishing quota\u2019 in H into fangen1 Quote2 but failed to split EU-Quote in T, failing to cover both EU and Quote in H.\nOur closer inspections also showed that compound splitting does not always suffice to reveal a lexical match between T and H as shown in the following example:\nT: Ben fa\u0308hrt1 einen Mercedes2 \u2018Ben drives1 a Mercedes2\u2019\nH: Ben ist Auto3fahrer4 \u2018Ben is a car3 driver4\u2019 Given a correct splitting of Autofahrer to Auto Fahrer, a derivational morphology resource (Zeller et al., 2013) would be required to discover the relationship between fahren and Fahrer and a synonym database to find that Mercedes is a hyponym of Auto.", 
        "84": "This does not weaken the claim that RTE is useful for evaluating compound splitters.", 
        "85": "It just shows that deeper, semantic compound analysis could improve RTE further.", 
        "86": "Besides, the error analysis shed some light on the treatment of compound heads and modifiers.", 
        "87": "It seems advisable to weight the compound head and modifiers differently when computing the ratio of aligned tokens in H. As illustrated by the following example, coverage of the head should be more important for the entailment decision than of the modifiers.", 
        "88": "Given a correct split of Kinder1buch2 into Kind1 Buch2, H1 and H2 have the same token coverage ratio while only H1 is entailed by T.\nT: Yuki kauft ein Kinder1buch2 \u2018Yuki buys a children\u2019s1 book2\u2019\nH1: Yuki kauft ein Buch \u2018Yuki buys a book\u2019 H2: Yuki ist ein Kind \u2018Yuki is a child\u2019\nIt should be noted that the transparency gain using compound splitting is limited to closed compounds that are compositional with respect to at least one constituent.", 
        "89": "Splitting compounds in\nH that are fairly non-compositional with respect to all constituents (e.g., Maulwurf \u2018mole\u2019 (lit.", 
        "90": "\u2018mouth throw\u2019)) is counterproductive.", 
        "91": "However, since most compounds (in particular ad-hoc productions) are compositional, this is only a side issue.", 
        "92": "In fact, we did not observe any cases of noncompositional compounds in the course of our error analysis.", 
        "93": "In summary, compound splitting is a complex task that comprises many subtasks.", 
        "94": "The multiple evaluation methods available, both intrinsic and extrinsic, vary in their suitability to evaluate them.", 
        "95": "One of these subtasks concerns the ability of compound splitters to determine whether to split or not, which is an integral part of compound analysis.", 
        "96": "While aspects such as oversplitting were not consistently evaluated in previous intrinsic evaluations, or compensated for by task-internal mechanisms in SMT, RTE proved more strict in this respect.", 
        "97": "Moreover, the transparency of the models made it possible to better estimate the impact of splitting.", 
        "98": "Despite the small size of the dataset, we were able to show significant differences, partly due to the clear definition of this binary classification task.", 
        "99": "On a side note, to the best of our knowledge, the result we obtained using the FF2010 compound splitter is the best result on the German RTE-3 dataset that has been reported using EOP.", 
        "100": "Notably, we obtain an accuracy which is almost three percentage points higher than the results of Noh et al.", 
        "101": "(2015), although they include further (language-specific) linguistic knowledge.", 
        "102": "5 Conclusion  Inspired by the potential benefits of compound splitting from the RTE literature and supported by the transparency of the models and the clear definition of this binary classification task, we set out to explore whether RTE is a suitable method to extrinsically evaluate the performance of compound splitting.", 
        "103": "We compared several compound splitters on a German textual entailment dataset and found that compound splitting is helpful for RTE across the board.", 
        "104": "More importantly, we found that certain aspects of compound splitters, neglected in previous evaluations, such as oversplitting, had a large impact on this task and nicely differentiated the systems tested.", 
        "105": "We conclude that RTE represents a suitable alternative to SMT for the extrinsic evaluation of compound splitters.", 
        "106": "In future work, we would like to investigate the interaction between additional lexical resources (such as GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) or DErivBase (Zeller et al., 2013)) and compound splitting, and the impact on the RTE performance.", 
        "107": "Acknowledgments  We thank Sebastian Pado\u0301 and Britta Zeller for their inspiring ideas about using compound splitting for improving RTE, and for their support in using the RTE framework.", 
        "108": "We are also grateful to the anonymous reviewers for their helpful feedback.", 
        "109": "This research was funded by the German Research Foundation (Collaborative Research Centre 732, Project D11)."
    }, 
    "document_id": "P17-2010.pdf.json"
}
