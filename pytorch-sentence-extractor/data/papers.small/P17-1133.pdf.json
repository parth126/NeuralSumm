{
    "abstract_sentences": {
        "1": "What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares?", 
        "2": "We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically.", 
        "3": "In particular, what kinds of information can be leveraged to uncover the potential prerequisite relation between knowledge concepts.", 
        "4": "We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts.", 
        "5": "Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0% by F1-score) comparing with existing methods."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1447\u20131456 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1133  1 Introduction  Mastery learning was first formally proposed by Benjamin Bloom in 1968 (Bloom, 1981), suggesting that students must achieve a level of mastery (e.g., 90% on a knowledge test) in prerequisite knowledge before moving forward to learn subsequent knowledge concepts.", 
        "2": "From then on, prerequisite relations between knowledge concepts become a cornerstone for designing curriculum in schools and universities.", 
        "3": "Prerequisite relations essentially can be considered as the dependency among knowledge concepts.", 
        "4": "It is crucial for people to learn, organize, apply, and generate knowledge (Laurence and Margolis, 1999).", 
        "5": "Figure 1 shows a real example from Coursera.", 
        "6": "The student wants to learn \u201cConditional Random Field\u201d (in video18 of CS229).", 
        "7": "The prerequisite knowledge might be \u201cHidden Markov Model\u201d (in video25 of\nCS224), whose prerequisite knowledge is \u201cMaximum Likelihood\u201d (in video12 of Math112).", 
        "8": "Organizing the knowledge structure with prerequisite relations in education improves tasks such as curriculum planning (Yang et al., 2015), automatic reading list generation (Jardine, 2014), and improving education quality (Rouly et al., 2015).", 
        "9": "For example, as shown in Figure 1, with explicit prerequisite relations among concepts (in red), a coherent and reasonable learning sequence can be recommended to the student (in blue).", 
        "10": "Before, the prerequisite relationships were provided by teachers or teaching assistants (Novak, 1990); however in the era of MOOCs, it is becoming infeasible as the teachers would find that they are facing with hundreds of thousands of students with various background.", 
        "11": "Meanwhile, the rapid growth of Massive Open Online Courses has offered thousands of courses, and students are free to choose any course from the thousands of candidates.", 
        "12": "Therefore, there is a clear need for methods to automatically dig out the prerequisite relationships among knowledge concepts from the large course space, so that the students from different background can easily explore the knowledge space and better design their personalized learning schedule.", 
        "13": "There are a few efforts aiming to automatically detect prerequisite relations for knowledge base.", 
        "14": "For example, Talukdar and Cohen (2012) proposed a method for inferring prerequisite relationships between entities in Wikipedia and Liang et al.", 
        "15": "(2015) presented a more general approach\n1447\nto predict prerequisite relationships.", 
        "16": "A few other works intend to extract prerequisite relationships from textbooks (Yosef et al., 2011; Wang et al., 2016).", 
        "17": "However, it is far from sufficient to directly apply these methods to the MOOC environments due to the following reasons.", 
        "18": "First, the focus of most previous attempts has been on prerequisite inference of Wikipedia concepts (either Wikipedia articles or Wikipedia concepts in textbooks).", 
        "19": "Many course concepts are not included in Wikipedia (Schweitzer, 2008; Okoli et al., 2014).", 
        "20": "We can leverage Wikipedia, in particular the existing entity relationships in Wikipedia, but cannot only rely on Wikipedia for detecting prerequisite relations in MOOCs.", 
        "21": "Second, with the thousands of courses from different universities and also very different disciplinaries, the MOOC scenario is much more complicated \u2014 there are not only inter-course concept relationships, but also intracourse and even intra-disciplinary relationships.", 
        "22": "Moreover, user interactions with the MOOC system might be also helpful to identify the prerequisite relations.", 
        "23": "How to fully leverage the different information to obtain a better performance for inferring prerequisite relations in MOOCs is a challenging issue.", 
        "24": "In this paper, we attempt to figure out what kinds of information in MOOCs can be used to uncover the prerequisite relations among concepts.", 
        "25": "Specifically, we consider it from three aspects, including course concept semantics, course video context and course structure.", 
        "26": "First, semantic relatedness plays an important role in prerequisite relations between concepts.", 
        "27": "If two concepts have very different semantic meanings (e.g., \u201cmatrix\u201d and \u201canthropology\u201d), it is unlikely that they have prerequisite relations.", 
        "28": "However, statistical features in MOOCs do not provide sufficient information for capturing the concept semantics because of the short length of course videos in MOOCs, we propose an embedding-based method to incorporate external knowledge from Wikipedia to learn semantic representations of concepts in MOOCs.", 
        "29": "Based on it, we propose one semantic feature to calculate the semantic relatedness between concepts.", 
        "30": "Second, motivated by the reference distance (RefD) (Liang et al., 2015), we propose three new contextual features, i.e., Video Reference Distance, Sentence Reference Distance and Wikipedia Reference Distance, to infer prerequisite relations in MOOCs based on context information from different aspects, which\nare more general and informative than RefD and overcome its sparsity problem.", 
        "31": "Third, we examine different distributional patterns for concepts in MOOCs, including appearing position, distributional asymmetry, video coverage and survival time.", 
        "32": "We further propose three structural features to utilize these patterns to help prerequisite inference in MOOCs.", 
        "33": "To evaluate the proposed method, we construct three datasets, each of which consists of multiple real courses in a specific domain from Coursera 1, the largest MOOC platform in the world.", 
        "34": "We also compare our method with the representative works of prerequisite learning and make a deep analysis of the feature contribution proposed in the paper.", 
        "35": "The experimental results show that our method achieves the state-of-the-art results in the prerequisite relation discovery in MOOCs.", 
        "36": "In summary, our contributions include: a) the first attempt, to the best of our knowledge, to detect prerequisite relations among concepts in MOOCs; b) proposal of a set of novel features that utilize contextual, structural and semantic information in MOOCs to identify prerequisite relations; c) design of three useful datasets based on real courses of Coursera to evaluate our method.", 
        "37": "2 Problem Formulation  In this section, we first give some necessary definitions and then formulate the problem of prerequisite relation learning in MOOCs.", 
        "38": "A MOOC corpus is composed by n courses in the same subject area, denoted as D = {C1, \u00b7 \u00b7 \u00b7 , Ci, \u00b7 \u00b7 \u00b7 , Cn}, where Ci is one course.", 
        "39": "Each course C can be further represented as a video sequence C = (V1, \u00b7 \u00b7 \u00b7 ,Vi, \u00b7 \u00b7 \u00b7 ,V|C|), where Vi denotes the i-th teaching video of course C. Finally, we view each video V as a document of its video texts (video subtitles or speech script), i.e., V = (s1 \u00b7 \u00b7 \u00b7 si \u00b7 \u00b7 \u00b7 s|V|), where si is the i-th sentence of the video texts.", 
        "40": "Course concepts are subjects taught in the course, i.e., the concepts not only mentioned but also discussed and taught in the course.", 
        "41": "Let us denote the course concept set of D as K = K1 \u222a \u00b7 \u00b7 \u00b7 \u222aKn, where Ki is the set of course concepts in Ci.", 
        "42": "Prerequisite relation learning in MOOCs is formally defined as follows.", 
        "43": "Given a MOOC corpus D and its corresponding course concepts\n1https://www.coursera.org/\nK, the objective is to learn a function P : K2 \u2192 {0, 1} that maps a concept pair \u3008a, b\u3009, where a, b \u2208 K, to a binary class that predicts whether a is a prerequisite concept of b.", 
        "44": "In order to learn this mapping, we need to answer two crucial questions.", 
        "45": "How could we represent a course concept?", 
        "46": "What information regarding a concept pair is helpful to capture their prerequisite relation?", 
        "47": "We first propose an embedding-based method to learn appropriate semantic representations for each course concept in K. Based on the learned representations, we propose 7 novel features to capture whether a concept pair has prerequisite relation.", 
        "48": "These features utilize different aspects of information and can be classified into 1 semantic feature, 3 contextual features and 3 structural features.", 
        "49": "In the following section, we first describe the semantic representations in detail, and then formally introduce our proposed features.", 
        "50": "3 Method    3.1 Concept Representation & Semantic Relatedness  We first learn appropriate representations for course concepts.", 
        "51": "Given the course concepts K as input, we utilize a Wikipedia corpus to learn semantic representations for concepts in K. A Wikipedia corpusW is a set of Wikipedia articles and can be represented as a sequence of words W = \u3008w1 \u00b7 \u00b7 \u00b7wi \u00b7 \u00b7 \u00b7wm\u3009, where wi denotes a word and m is the length of the word sequence.", 
        "52": "Our method consists of two steps: (1) entity annotation, and (2) representation learning.", 
        "53": "Entity Annotation.", 
        "54": "We first automatically annotate the entities in W to obtain an entity set E and an entity-annotated Wikipedia corpus W \u2032 = \u3008x1 \u00b7 \u00b7 \u00b7xi \u00b7 \u00b7 \u00b7xm\u2032\u3009, where xi corresponds to a word w \u2208 W or an entity e \u2208 E .", 
        "55": "Note that m\u2032 < m because multiple adjacent words could be labeled as one entity.", 
        "56": "Many entity linking tools are available for entity annotation, e.g.", 
        "57": "TAGME (Ferragina and Scaiella, 2010), AIDA (Yosef et al., 2011) and TremenRank (Cao et al., 2015).", 
        "58": "However, the rich hyperlinks created by Wiki editors provide a more natural way.", 
        "59": "In our experiments, we simply use the hyperlinks in Wikipedia articles as annotated entities.", 
        "60": "Representation Learning.", 
        "61": "We then learn word embeddings (Mikolov et al., 2013b,a) on W \u2032 to obtain low-dimensional, real-valued vector repre-\nsentations for entities in E and words in W .", 
        "62": "Let us denote ve and vw as the vector of e \u2208 E and w \u2208 W , respectively.", 
        "63": "For a course concept a \u2208 K, suppose a is a N -gram term \u3008g1 \u00b7 \u00b7 \u00b7 gN \u3009 and g1, \u00b7 \u00b7 \u00b7 , gN \u2208 W , we obtain its semantic representations va as follows.", 
        "64": "va = { ve, if a \u2261 e and e \u2208 E vg1 + \u00b7 \u00b7 \u00b7+ vgN , otherwise\n(1)\nIt means that if a is a Wikipedia entity, we can directly obtain its semantic representations; otherwise, we obtain its vector via the vector addition of its individual word vectors.", 
        "65": "In this way, a has no corresponding vector only if any of its constituent word is absence in the whole Wikipedia corpus.", 
        "66": "This case is unusual because a large online encyclopedia corpus can easily cover almost all individual words of the vocabulary.", 
        "67": "Our experimental results verify that over 98% of the course concepts have vector representations.", 
        "68": "Feature 1: Semantic Relatedness For a given concept pair \u3008a, b\u3009, the semantic relatedness between a and b, denoted as \u03c9(a, b), is our first feature (the only semantic feature).", 
        "69": "With learned semantic representations, semantic relatedness of two concepts can be easily reflected by their distance in the vector space.", 
        "70": "We define \u03c9(a, b) \u2208 [0, 1] as the normalized cosine distance between va and vb, as follows.", 
        "71": "\u03c9(a, b) = 1\n2 (1 + va \u00b7 vb \u2016va\u2016 \u00b7 \u2016vb\u2016 ) (2)  3.2 Contextual Features  Context information in course videos provides important clues to infer prerequisite relations.", 
        "72": "In videos where concept A is taught, if the teacher also mentions concept B for a lot but not vice versa, then B is more likely to be a prerequisite of A than A of B.", 
        "73": "For example, \u201cgradient descent\u201d is a prerequisite concept of \u201cback propagation\u201d.", 
        "74": "In teaching videos of \u201cback propagation\u201d, the concept \u201cgradient descent\u201d is frequently mentioned when illustrating the optimization detail of back propagation.", 
        "75": "On the contrary, however, \u201cback propagation\u201d is unlikely to be mentioned when teaching \u201cgradient descent\u201d.", 
        "76": "A similar observation also exists in Wikipedia, based on which Liang et al.", 
        "77": "(2015) proposed an indicator, namely reference distance (RefD), to infer prerequisite relations among Wikipedia articles.", 
        "78": "However, RefD is computed based on the link structure of Wikipedia, thus is only feasible for Wikipedia\nconcepts and is not applicable in plain text.", 
        "79": "We overcome the above shortcomings of RefD to propose three novel features, which utilize different aspects of context information\u2014course videos, video sentences and Wikipedia articles\u2014to infer prerequisite relations in MOOCs.", 
        "80": "Feature 2: Video Reference Distance Given a concept pair \u3008a, b\u3009 where a, b \u2208 K, we propose the video reference weight (V rw) to quantify how b is referred by videos of a, defined as follows.", 
        "81": "V rw (a, b) =\n\u2211 C\u2208D \u2211 V\u2208C\nf (a,V) \u00b7 r (V, b) \u2211 C\u2208D \u2211 V\u2208C f (a,V) (3)\nwhere f (a,V) indicates the term frequency of concept a in video V , which reflects how important is concept a to this video.", 
        "82": "r (V, b) \u2208 {0, 1} denotes whether concept b appears in video V .", 
        "83": "Intuitively, if b appears in more important videos of a, V rw (a, b) tends to be larger, and the range of V rw (a, b) is between 0 and 1.", 
        "84": "Then, the video reference distance (V rd) is defined as the difference of V rw between two concepts, as follows.", 
        "85": "V rd (a, b) = V rw (b, a)\u2212 V rw (a, b) (4)\nIn practice, this feature may be too sparse if the MOOC corpus is small.", 
        "86": "For an arbitrary concept pair, they may have no co-occurrence in all course videos.", 
        "87": "We expend the video reference distance to a more general version by considering the semantic relatedness among concepts.", 
        "88": "Besides the conditions in which A refers to B, we also consider the cases in which A-related concepts refer to B.", 
        "89": "We first define the generalized video reference weight (GV rw) as follows.", 
        "90": "GV rw (a, b) = \u2211M i=1 V rw (ai, b) \u00b7 \u03c9 (ai, b)\u2211M\ni=1 \u03c9 (ai, b) (5)\nwhere a1, \u00b7 \u00b7 \u00b7 , aM \u2208 K are the top-M most similar concepts of a, measured by the semantic relatedness function \u03c9(\u00b7, \u00b7) in feature 1.", 
        "91": "GV rw is the weighted average of V rw (ai, b), indicating how b is referred by a-related concepts in their corresponding videos.", 
        "92": "Note that a1 = a, thus GV rw (a, b) \u2261 V rw (a, b) when M = 1.", 
        "93": "Similarly, we define the generalized video reference distance (GV rd) as follows.", 
        "94": "GV rd (a, b) = GV rw (b, a)\u2212GV rw (a, b) (6)\nIntuitively, if most of b-related concepts refer to a but not vice versa, then a is likely to be a prerequisite of b.", 
        "95": "For example, it is plausible\nfor the related concepts of \u201cgradient descent\u201d, e.g., \u201csteepest descent\u201d and \u201cNewton\u2019s method\u201d, to mention \u201cmatrix\u201d but clearly not vice versa.", 
        "96": "Feature 3: Sentence Reference Distance Sentence reference distance is similar to feature 2, but stands on the sentence level.", 
        "97": "Following the same design pattern of feature 2, we define the sentence reference weight (Srw) and sentence reference distance (Srd) as follows.", 
        "98": "Srw (a, b) =\n\u2211 C\u2208D \u2211 V\u2208C \u2211 s\u2208V\nr(s, a) \u00b7 r(s, b) \u2211 C\u2208D \u2211 V\u2208C \u2211 s\u2208V r(s, a) (7)\nSrd (a, b) = Srw (b, a)\u2212 Srw (a, b) (8)\nwhere r (s, a) \u2208 {0, 1} is an indicator of whether concept a appears in sentence s. Srw(a, b) calculates the ratio of B appearing in the sentences of a.", 
        "99": "We also define generalized sentence reference weight (GSrw) and generalized sentence reference distance (GSrd) as follows.", 
        "100": "GSrw (a, b) = \u2211M i=1 Srw (ai, b) \u00b7 \u03c9 (ai, b)\u2211M\ni=1 \u03c9 (ai, b) (9)\nGSrd (a, b) = GSrw (b, a)\u2212GSrw (a, b) (10)\nFeature 4: Wikipedia Reference Distance Contextual information of Wikipedia is also useful for detecting prerequisite relations.", 
        "101": "As mention before, RefD is not general enough to be applied in our settings, because it is limited to Wikipedia concepts.", 
        "102": "Therefore, we improve this indicator to a more general one, which is also suitable for non-wiki concepts.", 
        "103": "Specifically, for a concept a \u2208 K, let us denote the top-M most related wiki entities of a as Ra = \u3008e1, \u00b7 \u00b7 \u00b7 , eM \u3009, where e1, \u00b7 \u00b7 \u00b7 , eM \u2208 E .", 
        "104": "Because concepts in K and entities in E are jointly embedded in the same vector space in Section 3.1, we can easily obtain Ra with the semantic relatedness metric \u03c9(\u00b7, \u00b7) in Feature 1.", 
        "105": "We then define the wikipedia reference weight (Wrw) as follows.", 
        "106": "Wrw (a, b) =\n\u2211 e\u2208Ra Erw (e, b) \u00b7 \u03c9 (e, a) \u2211\ne\u2208Ra \u03c9 (e, a)\n(11)\nwhere Erw(e, a) is a binary indicator, in which Erw(e, a) = 1 if the Wikipedia article of e refers to any entity in Ra, and Erw(e, a) = 0 otherwise.", 
        "107": "Wrw (a, b) measures how frequently that arelated wiki entities refer to b-related wiki entities.", 
        "108": "Finally, wikipedia reference distance (Wrd) is\ndefined as the difference of Wrw between a and b, i.e., Wrd (a, b) =Wrw (b, a)\u2212Wrw (a, b).", 
        "109": "3.3 Structural Features  Since course concepts are usually introduced based on their learning dependencies, the structure of MOOC courses also significantly contribute to prerequisite relation inference in MOOCs.", 
        "110": "However, structure-based features for prerequisite detection have not been well-studied in previous works.", 
        "111": "In this section, we investigate different structural information, including appearing positions of concepts, learning dependencies of videos and complexity levels of concepts, to propose three novel features to infer prerequisite relations in MOOCs.", 
        "112": "Before introducing these features, let us define two useful notations as follows.", 
        "113": "C(a) are the courses in which a is a course concept, i.e., C(a) = {Ci|Ci \u2208 D, a \u2208 Ki}.", 
        "114": "I(C, a) are the video indexes that contain concept a in course C. For example, if a appears in the first and the 4-th video of C, then I(C, a) = {1, 4}.", 
        "115": "Feature 5: Average Position Distance In a course, for a specific concept, its prerequisite concepts tend to be introduced before this concept and its subsequent concepts tend to be introduced after this concept.", 
        "116": "Based on this observation, for a concept pair \u3008a, b\u3009, we calculate the distance of the average appearing position of a and b as one feature, namely average position distance (Apd).", 
        "117": "If C(a) \u2229 C(b) 6= \u2205, Apd (a, b) is formally defined as follows.", 
        "118": "Apd (a, b) =\n\u2211 C\u2208C(a)\u2229C(b)\n\u2223\u2223\u2223 \u2211\ni\u2208I(C,a) i |I(C,a)| \u2212 \u2211 j\u2208I(C,b) j |I(C,b)| \u2223\u2223\u2223\n|C(a) \u2229 C(b)| (12)\nIf C(a) \u2229 C(b) = \u2205, we set Apd (a, b) = 0.", 
        "119": "Feature 6: Distributional Asymmetry Distance We also use the learning dependency of course videos to help infer learning dependency of course concepts.", 
        "120": "Based on our observation, the chance that a prerequisite concept is frequently mentioned in its subsequent videos is larger than that a subsequent concept is talked about in its prerequisite videos.", 
        "121": "Specifically, if video Va is a precursor video of Vb, and a is a prerequisite concept of b, then it is likely that f(b,Va) < f(a,Vb), where f(a,V) denotes the term frequency of a in video V .", 
        "122": "We thus define another feature, namely distributional asymmetry distance (Dad), to calculate the extent that a given concept pair satisfies this\ndistributional asymmetry pattern.", 
        "123": "Formally, in course C, for a given concept pair \u3008a, b\u3009, we first define S(C) = {(i, j)|i \u2208 I(C, a), j \u2208 I(C, b), i < j}, i.e., all possible video pairs of \u3008a, b\u3009 that have sequential relation.", 
        "124": "Then, the distributional asymmetry distance of \u3008a, b\u3009 is formally defined as follows.", 
        "125": "Dad (a, b) =\n\u2211 C\u2208C(a)\u2229C(b)\n\u2211 (i,j)\u2208S(C) f(a,VCi )\u2212f(b,VCj )\n|S(C)|\n|C(a) \u2229 C(b)| (13)\nwhere VCi denotes the i-th video of course C. If C(a) \u2229 C(b) = \u2205, we set Dad (a, b) = 0.", 
        "126": "Feature 7: Complexity Level Distance Two related concepts with prerequisite relationship tend to have a difference in their complexity level, meaning that one concept is basic while another one is advanced.", 
        "127": "For example, \u201cdata set\u201d and \u201ctraining set\u201d have learning dependencies and the latter concept is more advanced than the former one.", 
        "128": "However, \u201ctest set\u201d and \u201ctraining set\u201d have no such relation when their complexity levels are similar.", 
        "129": "Complexity level of a course concept is implicit in its distribution in courses.", 
        "130": "Specifically, we observe that, for a concept in MOOCs, if it covers more videos in a course or it survives longer time in a course, then it is more likely to be a basic concept rather than an advanced one.", 
        "131": "We then formally define the average video coverage (avc) and the average survival time (ast) of a concept a as follows.", 
        "132": "avc (a) = 1 |C(a)| \u2211\nC\u2208C(a)\n|I(C, a)| |C| (14)\nast (a) = 1 |C(a)| \u2211\nC\u2208C(a)\nmax(I(C, a))\u2212min(I(C, a)) + 1 |C|\n(15)\nwheremax/min(I(C, a)) obtains the video index where a appears the last/first time in course C. Based on the above equations, we define the complexity level distance (Cld) between concept a and b as follows.", 
        "133": "Cld (a, b) = avc (a) \u00b7 ast (a)\u2212 avc (b) \u00b7 ast (b) (16)  4 Experiments    4.1 Data Sets  In order to validate the efficiency of our features, we conducted experiments on three MOOC corpus with different domains: \u201cMachine Learning\u201d (ML), \u201cData Structure and Algorithms\u201d (DSA), and \u201cCalculus\u201d (CAL).", 
        "134": "To the best of our knowledge, there is no public data set for mining\nprerequisite relations in MOOCs.", 
        "135": "We created the experimental data sets through a three-stage process.", 
        "136": "First, for each chosen domain, we select its relevant courses from Coursera, one of the leading MOOC platforms, and download all course materials using coursera-dl 2, a widely-used tool for automatically downloading Coursera.org videos.", 
        "137": "For example, for ML, we select 5 related courses 3 from 5 different universities and obtain a total of 548 course videos.", 
        "138": "Then, we manually label course concepts for each course: (1) Extract candidate concepts from documents of video subtitles following the method of Parameswaran et al.", 
        "139": "(2010).", 
        "140": "(2) Label the candidates as \u201ccourse concept\u201d or \u201cnot course concept\u201d and obtain a set of course concepts for this course.", 
        "141": "Finally, we manually annotate the prerequisite relations among the labeled course concepts.", 
        "142": "If the number of course concepts is n, the number of all possible pairs to be checked could reach n \u00d7 (n \u2212 1)/2, which requires arduous human labeling work.", 
        "143": "Therefore, for each dataset, we randomly select 25 percent of all possible pairs for evaluation.", 
        "144": "For each course concept pair \u3008a, b\u3009, three human annotators majoring in the corresponding domain were asked to label them as \u201ca is b\u2019s prerequisite\u201d, \u201cb is a\u2019s prerequisite\u201d or \u201cno prerequisite relationship\u201d using their own knowledge background and additional textbook resources.", 
        "145": "We take a majority vote of the annotators to create final labels and access the interannotator agreement using the average of pairwise \u03ba statistics (Landis and Koch, 1981) between all pairs of the three annotators.", 
        "146": "The statistics of the three datasets are listed in Table 1, where #courses and #videos are the total number of courses and videos in each dataset and #concepts is the number of labeled course concepts.", 
        "147": "The #pairs denotes the number of labeled concept pairs for evaluation, in which \u2018+\u2019\n2https://github.com/coursera-dl/coursera-dl 3These courses are: \u201cMachine Learning (Stanford)\u201d, \u201cMachine Learning (Washington)\u201d, \u201cPractical Machine Learning (JHU)\u201d, \u201cMachine Learning With Big Data (UCSD)\u201d and \u201cNeural Networks for Machine Learning (UofT)\u201d\ndenotes the number of positive instances, i.e.", 
        "148": "pairs who have prerequisite relations, and \u2018\u2212\u2019 denotes the number of negative instances.", 
        "149": "4.2 Evaluation Results  For each dataset, we apply 5-fold cross validation to evaluate the performance of the proposed method, i.e., testing our method on one fold while training the classifier using the other 4 folds.", 
        "150": "Usually, there are much fewer positive instances than negative instances, so we balance the training set by oversampling the positive instances (Yosef et al., 2011; Talukdar and Cohen, 2012).", 
        "151": "In our experiments, we employ 4 different binary classifiers, including Na\u0131\u0308veBayes (NB), Logistic Regression (LR), SVM with linear kernel (SVM) and Random Forest (RF).", 
        "152": "We use precision (P ), recall (R), and F1-score (F1) to evaluate the prerequisite classification results.", 
        "153": "The experimental results are presented in Table 2.", 
        "154": "Contextual features are shaped by the parameter M , i.e., the number of related concepts being considered.", 
        "155": "In our experiments, we tried different settings of M and report the results when M=1 and M=10 in Table 2.", 
        "156": "As for the semantic representation, we use the latest publicly available Wikipedia dump 4 and apply the skip-gram model (Mikolov et al., 2013b) to train word embeddings using the Python library gensim 5 with default parameters.", 
        "157": "As shown in Table 2, the evaluation results varies by different classifiers.", 
        "158": "It turns out that Na\u0131\u0308veBayes performs the worst.", 
        "159": "This seems to be caused by the fact that the independence assumption is not satisfied for our features; for\n4https://dumps.wikimedia.org/enwiki/20170120/ 5http://radimrehurek.com/gensim/\nexample, Feature 2 and Feature 3 both utilize the local context information, only with different granularity, thus are quite co-related.", 
        "160": "Random Forest beats others, with best F1 across all three datasets.", 
        "161": "Its average F1 outperforms SVM, NB and LR by 7.0%, 11.1% and 8.3%, respectively (M=10).", 
        "162": "The reason is as follows.", 
        "163": "Instead of a simple descriptive feature, each of our proposed feature determines whether a concept pair has prerequisite relation from a specific aspect; its function is similar to an independent weak classifier.", 
        "164": "Therefore, rather than using a linear combination of features for classification (e.g., SVM and LR), a boosting model (e.g., Random Forest) is more suitable for this task.", 
        "165": "The performance is slightly better when M=10 for all classifiers, with +0.20% for SVM, +0.53% for NB, +0.73% for LR and +3.63% for RF, with respect to the average F1.", 
        "166": "The results verify the effectiveness of considering related concepts in contextual features.", 
        "167": "We use RF and set M=10 in the following experiments.", 
        "168": "4.3 Comparison with Baselines  We further compare our approach with three representative methods for prerequisite inference.", 
        "169": "4.3.1 Baseline Approaches  Hyponym Pattern Method (HPM).", 
        "170": "Prerequisite relationships often exists between hyponymhypernym concept pairs (e.g., \u201cMachine Learning\u201d and \u201cSupervised Learning\u201d).", 
        "171": "As a baseline, we adopt the 10 lexico-syntactic patterns used by Wang et al.", 
        "172": "(2016) to extract hyponym relationships between concepts.", 
        "173": "If a concept pair matches at least one of these patterns in the MOOC corpus, we judge them to have prerequisite relations.", 
        "174": "Reference Distance (RD) We also employ the RefD proposed by Liang et al.", 
        "175": "(2015) as one of our baselines.", 
        "176": "However, this method is only appliable to Wikipedia concepts.", 
        "177": "To make it comparable with our method, for each of our datasets, we construct a subset of it by picking out the concept pairs \u3008a, b\u3009 in which a and b are both Wikipedia concepts.", 
        "178": "For example, we find 49% of course concepts in ML have their corresponding Wikipedia articles and 28% percent of concept pairs in ML meet the above condition.", 
        "179": "We use the new datasets constructed from ML, DSA and CAL, namely W-ML, W-DSA, and W-CAL, to compare our method with RefD.", 
        "180": "Supervised Relationship Identification (SRI) Wang et al.", 
        "181": "(2016) has employed several fea-\ntures to infer prerequisite relations of Wikipedia concepts in textbooks, including 3 Textbook features and 6 Wikipedia features.", 
        "182": "Based on these features, they performed a binary classification using SVM to identify prerequisite relationships and has achieved state-of-the-art results.", 
        "183": "Because the Wikipedia features can only be applied to Wikipedia concepts, in order to make a comparison, we create two versions of their method: (1) T-SRI: only textbook features are used to train the classifier and (2) F-SRI: the original version, all features are used.", 
        "184": "We compare the performance of our method with T-SRI on ML, DSA and CAL datasets; we also compare our method with F-SRI on W-ML, W-DSA and W-CAL datasets.", 
        "185": "4.3.2 Performance Comparison  In Table 3 we summarize the comparing results of different methods across different datasets (\u201cMOOC\u201d refers to our method).", 
        "186": "We find that our method outperforms baseline methods across all six datasets 6.", 
        "187": "For example, the F1 of our method on ML outperforms T-SRI and HPM by 10.5% and 43.6%, respectively.", 
        "188": "Specifically, we have the following observations.", 
        "189": "First, HPM achieves relatively high precision but low recall.", 
        "190": "This is because when A \u201cis a\u201d B, a prerequisite relation often exists from B to A, but clearly not vise versa.", 
        "191": "Second, T-SRI has certain effectiveness for learning prerequisite relations, with F1 ranging from 62.1 to 65.2%.", 
        "192": "However, T-SRI only considers relatively simple features, such as the sequential and co-occurrence among concepts.", 
        "193": "With more\n6The improvements are all statistically significant tested with bootstrap re-sampling with 95% confidence.", 
        "194": "comprehensive feature engineering, the F1 of our method significantly outperforms T-SRI (+10.5% on ML, +9.1% on DSA and +7.1% on CAL).", 
        "195": "Third, incorporating Wikipedia-based features (FSRI) achieves certain promotion in performance (+0.93% comparing with T-SRI in average F1).", 
        "196": "4.4 Feature Contribution Analysis  In order to get an insight into the importance of each feature in our method, we perform a contribution analysis with different features.", 
        "197": "Here, we run our approach 10 times on the ML dataset.", 
        "198": "In each of the first 7 times, one feature is removed; in each of the rest 3 times, one group of features are removed, e.g., removing contextual features means removing Gvrd, Gsrd and Wrd at the same time.", 
        "199": "We record the decrease of F1-score for each setting.", 
        "200": "Table 4 lists the evaluation results after ignoring different features.", 
        "201": "According to the decrement of F1-scores, we find that all the proposed features are useful in predicting prerequisite relations.", 
        "202": "Especially, we observe that Cld (Feature 7), decreasing our best F1score by 7.4%, plays the most important role.", 
        "203": "This suggests that most concepts do exist difference in complexity level.", 
        "204": "For two concepts, the difference of their coverage and survival times in courses are important for prerequisite relation detection.", 
        "205": "On the contrary, with 1.9% decrease, Sr (Feature 1) is relatively less important.", 
        "206": "We may easily find two concepts which have related semantic meanings (e.g., \u201ctest set\u201d and \u201ctraining set\u201d) but have no prerequisite relationship.", 
        "207": "However, semantic relatedness is critical for the contextual features because it overcomes the problem of the sparsity of context in calculation.", 
        "208": "We experience a decrease of 5.4% when we further do not consider related concepts in contextual features, i.e., set M=1.", 
        "209": "As for the feature group contribution, we observe that Structural Features, with a decrease of 9.2%, has a greater impact than the other two groups.", 
        "210": "This is as expected because it includes Cld.", 
        "211": "Among the three structural features, Apd makes relatively less contribution.", 
        "212": "The reason is that sometimes the professor may frequently mention a prerequisite concept after introducing a subsequent concept orally, for helping students better understand the concept.", 
        "213": "5 Related Works  To the best of our knowledge, there has been no previous work on mining prerequisite relations\namong concepts in MOOCs.", 
        "214": "Some researchers have been engaged in detecting other type of prerequisite relations.", 
        "215": "For example, Yang et al.", 
        "216": "(2015) proposed to induce prerequisite relations among courses to support curriculum planning.", 
        "217": "Liu et al.", 
        "218": "(2011) studied learning-dependency between knowledge units, a special text fragment containing concepts, using a classification-based method.", 
        "219": "In the area of education, researchers have tried to find general prerequisite structures from students\u2019 test performance (Vuong et al., 2011; Scheines et al., 2014; Huang et al., 2015).", 
        "220": "Different from them, we focus on more finegrained prerequisite relations, i.e., the prerequisite relations among course concepts.", 
        "221": "Among the few related works of mining prerequisite relations among concepts, Liang et al.", 
        "222": "(2015) and Talukdar and Cohen (Talukdar and Cohen, 2012) studied prerequisite relationships between Wikipedia articles.", 
        "223": "They assumed that hyperlinks between Wikipedia pages indicate a prerequisite relationship and design several useful features.", 
        "224": "Based on these Wikipedia features plus some textbook features, Wang et al.", 
        "225": "(Wang et al., 2016) proposed a method to construct a concept map from textbooks, which jointly learns the key concepts and their prerequisite relations.", 
        "226": "However, the investigation of only Wikipedia concepts is also the bottleneck of their studies.", 
        "227": "In our work, we propose more general features to infer prerequisite relations among concepts, regardless of whether the concept is in Wikipedia or not.", 
        "228": "Liang et al.", 
        "229": "(2017) propose an optimization based framework to discover concept prerequisite relations from course dependencies.", 
        "230": "Gordon et al.", 
        "231": "(2016) utilize cross-entropy to learn concept dependencies in scientific corpus.", 
        "232": "Besides local statistical information, our method also utilize external knowledge to enrich concept semantics, which is more informativeness.", 
        "233": "Our work is also related to the study of automatic relation extraction.", 
        "234": "Different research lines have been proposed around this topic, including hypernym-hyponym relation extraction (Ritter et al., 2009; Wei et al., 2012), entity relation extraction (Zhou et al., 2006; Fan et al., 2014; Lin et al., 2015) and open relation extraction (Fader et al., 2011).", 
        "235": "However, previous works mainly focus on factual relations, the extraction of cognitive relations (e.g.", 
        "236": "prerequisite relations) has not been well studied yet.", 
        "237": "6 Conclusions and Future Work  We conducted a new investigation on automatically inferring prerequisite relations among concepts in MOOCs.", 
        "238": "We precisely define the problem and propose several useful features from different aspects, i.e., contextual, structural and semantic features.", 
        "239": "Moreover, we apply an embeddingbased method that jointly learns the semantic representations of Wikipedia concepts and MOOC concepts to help implement the features.", 
        "240": "Experimental results on online courses with different domains validate the effectiveness of the proposed method.", 
        "241": "Promising future directions would be to investigate how to utilize user interaction in MOOCs for better prerequisite learning, as well as how deep learning models can be used to automatically learn useful features to help infer prerequisite relations.", 
        "242": "Acknowledgments  This work is supported by 973 Program (No.", 
        "243": "2014CB340504), NSFC Key Program (No.", 
        "244": "61533018), Fund of Online Education Research Center, Ministry of Education (No.", 
        "245": "2016ZD102), Key Technologies Research and Development Program of China (No.", 
        "246": "2014BAK04B03) and NSFC-NRF (No.", 
        "247": "61661146007)."
    }, 
    "document_id": "P17-1133.pdf.json"
}
