{
    "abstract_sentences": {
        "1": "Automatically estimating a user\u2019s socioeconomic profile from their language use in social media can significantly help social science research and various downstream applications ranging from business to politics.", 
        "2": "The current paper presents the first study where user cognitive structure is used to build a predictive model of income.", 
        "3": "In particular, we first develop a classifier using a weakly supervised learning framework to automatically time-tag tweets as past, present, or future.", 
        "4": "We quantify a user\u2019s overall temporal orientation based on their distribution of tweets, and use it to build a predictive model of income.", 
        "5": "Our analysis uncovers a correlation between future temporal orientation and income.", 
        "6": "Finally, we measure the predictive power of future temporal orientation on income by performing regression."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 659\u2013665 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2104  1 Introduction  User-generated content in social media such as Twitter has enabled the study of author profiling on an unprecedented scale.", 
        "2": "Author profiling in social media aims at inferring various attributes of the user from the text that they have written.", 
        "3": "Most of the prior studies in this field have focused on age, gender prediction (Marquardt et al., 2014; Sap et al., 2014), psychological well-being (Dodds et al., 2011; Choudhury et al., 2013), and a host of other behavioural, psychological and medical phenomena (Kosinski et al., 2013).", 
        "4": "However, there has been a lack of work looking at the socioeconomic characteristics of Twitter users.", 
        "5": "In this paper, we focus on automatic estimation of Twitter users\u2019 income from their Twitter language.", 
        "6": "An income predictor of social media users can be useful for both social science research and a range of\ndownstream applications in banking, marketing, and politics.", 
        "7": "Previous social science studies on income demonstrate that income of people is correlated with various factors such as demographic feature (the congressional district in which the respondent lived), educational categories, sex, age, age squared, gender, race categories, marital status categories, and height (Kahneman and Deaton, 2010).", 
        "8": "Other studies reveal that psychological traits related to extroversion (e.g.", 
        "9": "larger social networks) and conscientiousness (e.g.", 
        "10": "orderliness) have a positive correlation with income, while neurotic traits (e.g.", 
        "11": "anger, anxiety) are anticorrelated (Roberts et al., 2007).", 
        "12": "Human temporal orientation refers to individual differences in the relative emphasis one places on the past, present, or future (Zimbardo and Boyd, 2015).", 
        "13": "Past studies have established consistent links between temporal orientation and most of the above-mentioned income predictor factors such as age, sex, gender, education, and psychological traits (Webley and Nyhus, 2006; Adams and Nettle, 2009; Schwartz et al., 2013; Zimbardo and Boyd, 2015).", 
        "14": "Accordingly, this begs the question as to whether there is any link between an individual\u2019s temporal orientation and their income level.", 
        "15": "Traditionally, temporal orientation has been assessed by self-report questionnaires.", 
        "16": "In this paper, we assess temporal orientation based on language use in Twitter.", 
        "17": "Our method uses a tweet-level classifier of past, present, and future, grouped over users to create user-level assessments.", 
        "18": "Our learning framework uses convolutional neural networks (CNNs) (Goodfellow et al., 2016) to infer tweet vector representations, and considers them as the feature to develop a classification model that can automatically detect the time orientation (oriented towards past, present, and future) of tweets.", 
        "19": "The framework leverages weak supervision signals provided by a list of manu-\n659\nally selected eighty (80) high-precision seed terms (and automatically extracted similar terms) representing past, present, and future to train the CNN.", 
        "20": "For example, tweets exclusively containing past (resp.", 
        "21": "present and future) seed terms were marked with weak labels past (resp.", 
        "22": "present and future).", 
        "23": "We used the tweet-level temporal classifier to automatically classify a large dataset consisting of \u224810 million tweets from 5,191 users mapped to their income, using fine-grained user occupation as a proxy.", 
        "24": "Finally, we tested whether individual differences in past, present, and future orientation are related to income.", 
        "25": "In particular, we frame the income prediction task as regression using linear as well as non-linear learning algorithms where temporal orientation served as predictive features.", 
        "26": "To the best of our knowledge, this represents the first work to study a temporal orientation-based income prediction using Twitter language.", 
        "27": "In summary the proposed approach is different from the previous works (Schwartz et al., 2015; Preot\u0327iuc-Pietro et al., 2015; Park et al., 2017) in several ways.", 
        "28": "Unlike Schwartz et al.", 
        "29": "(2015), we used a weakly supervised approach.", 
        "30": "The generation of training data is semi-automatic in our case.", 
        "31": "Rather than manually identifying features, tweet vectors are fed to a CNN classifier.", 
        "32": "Furthermore while Schwartz et al.", 
        "33": "(2015) studied temporal orientation of facebook data in order to predict different human correlates like conscientiousness, age, and gender, our current work focuses on predicting the income of a user using temporal orientation of their tweets.", 
        "34": "In Preot\u0327iuc-Pietro et al.", 
        "35": "(2015), the authors predict user income based on different demographic and psychological features of users.", 
        "36": "However, the process of extracting these features is computationally complex.", 
        "37": "The current study is therefore, the first of its kind to explore the use of temporal orientation of user-tweets to predict income.", 
        "38": "2 Related Work  Existing message-/sentence-level temporal classification methods generally fall into two categories: (1) rule-based methods, and (2) supervised machine-learning methods.", 
        "39": "Rule-based methods mainly rely on manually designed classification rules for each temporal class (Nie et al., 2015).", 
        "40": "Despite their effectiveness, this kind of method requires substantial efforts in rule design.", 
        "41": "Most research on machine learning-based sentence tem-\nporal classification has revolved around feature engineering for better classification performance.", 
        "42": "Different kinds of features have been explored such as bag-of-words, time expressions, part-ofspeech tags, and temporal class-specific lexicons (Schwartz et al., 2015).", 
        "43": "Temporal class specific lexicon creation and feature engineering also cost a lot of human efforts.", 
        "44": "In addition, creation of a large-scale training data set for supervised machine-learning approaches is also very laborious.", 
        "45": "3 Methodology  In this section, we describe our proposed methodology to identify the underlying temporal orientation of tweets and a set of contrastive systems that we used as baselines for comparative study.", 
        "46": "3.1 Tweet Temporal Orientation Classifier  The task can be defined as given a tweet t and its posting date d, predict its temporal class c \u2208 { past, present, or future} with reference to its issuing date.", 
        "47": "Proposed Architecture: The proposed framework has two main steps as: (i) training the model parameters, and (ii) using the model to tag unseen tweets.", 
        "48": "During training, we use the weakly labeled tweets to learn the parameters of the CNN and temporal orientation classifier.", 
        "49": "For classification, a linear Support Vector Machine (lSVM)1 is used.", 
        "50": "In particular, we trained three binary classifiers (one per class)2 using one-vs.-rest, and label a tweet with the class that assigned the highest score.", 
        "51": "In the second step, we pass tweets through these two optimized components to detect their temporal orientation.", 
        "52": "1Trained using the Weka implementation of LIBSVM with linear kernels (polynomial kernels yielded worse performance).", 
        "53": "2Multi-class classification yielded worse performance.", 
        "54": "The choice of CNN for feature extraction is motivated by:\n\u2022 CNNs have been successfully used as feature extractors in various computer vision tasks and achieved better results compared to handcrafted features.", 
        "55": "Research has shown that CNN feature maps can be used with SVM to yield classification results that outperform the original CNN (Athiwaratkun et al., 2015)\n\u2022 Superior accuracies have also been achieved by following a similar line of research in the context of NLP tasks (Kim, 2014; Poria et al., 2015).", 
        "56": "Convolutional Neural Networks (CNNs): The task is challenging as tweets are short and noisy.", 
        "57": "Moreover, English, like many languages, uses a wide variety of ways to refer to the past, present, and future.", 
        "58": "Unlike previous approaches which mainly rely on hand-crafted rules and feature engineering, we automatically extract features for tweets to build our tweet-level Temporal Orientation Classifier.", 
        "59": "In particular, we use CNNs to automatically extract tweet vectors as the features for classification.", 
        "60": "Recently, CNNs have been shown to be useful in many natural language processing and information retrieval tasks by effectively modeling natural language semantics (Collobert et al., 2011).", 
        "61": "For our experiments, we trained a simple CNN with one convolution layer followed by one max pooling layer (Collobert et al., 2011; Kim, 2014).", 
        "62": "In the CNN model, we use 3 filters with window sizes of 5, 6 and 7 with 100 feature maps each.", 
        "63": "These window sizes will capture 5-gram, 6-gram, and 7-gram information in the tweets.", 
        "64": "We employ dropout for regularization with a dropout rate of 0.5 which is a reasonable default.", 
        "65": "We also use rectified linear units and mini-batches with size of 50.", 
        "66": "The parameters of the CNN were fixed based on the performance of 3-fold crossvalidation.", 
        "67": "The tweet representations are trained on top of pre-trained word vectors which are updated during CNN training.", 
        "68": "We use the publicly available word2vec3 vectors that are trained on Google News corpus as well our own Word2vec vectors4 trained during the labeled-data creation phase.", 
        "69": "During the training phase, the parameters\n3https://code.google.com/p/word2vec/ 4trained using gensim library available at\nhttps://radimrehurek.com/gensim/intro.html\nof the CNN model are learned by passing multiple filters over word vectors and then applying the max-over-time pooling operation to generate features which are used in a fully connected softmax layer.", 
        "70": "Finally, we use the cross-entropy loss function for learning the parameters of the model.", 
        "71": "Similar to Kim (2014), we use dropout (Hinton et al., 2012) to regularize the change of parameters by randomly setting some weights to zero that prevents overfitting.", 
        "72": "3.2 Income Predictor Model  Similar to Preot\u0327iuc-Pietro et al.", 
        "73": "(2015), we formulate the income prediction task as regression using user-level temporal orientation as features.", 
        "74": "First, the tweet temporal orientation classifier is used to label whether a tweet focuses on past, present, or future.", 
        "75": "Afterwards, at user-level, we produce three categories of temporal orientation (three separate variables summing to one), defined simply as the proportion of a user\u2019s total tweets (tweets(user)all) classified in the given temporal category (c \u2208 { past, present, or future}), as in (1):\norientationc(user) = |tweetsc(user)| |tweetsall(user)| (1)\nWe use linear and non-linear methods.", 
        "76": "The linear method is logistic regression (LR) (Freedman, 2009) with Elastic Net regularisation.", 
        "77": "In order to capture the non-linear relationship between a user\u2019s temporal orientation and their income, we use Gaussian Processes (GP) (Rasmussen and Nickisch, 2010) for regression.", 
        "78": "Given that our dataset is very large and the number of features is high, for GP inference we use the fully independent training conditional approximation (Snelson and Ghahramani, 2005) with 500 random inducing points.", 
        "79": "4 Data Sets    4.1 Training Data  Tweets are collected using the Twitter streaming API.5 We downloaded English tweets during the period 01.01.2015\u201331.01.2015, which generated about 40 million tweets.", 
        "80": "After collecting the tweets, we filter past-, present-, and futureoriented tweets using a manually selected high precision list of 50 seed terms.", 
        "81": "These are terms\n5https://dev.twitter.com/streaming/ overview.", 
        "82": "that capture temporal dimensions of tweets with very few false positives, though the recall of these terms is low.", 
        "83": "In order to increase the recall, and to capture new terms that are good paradigms of past, present, and future, we expand our initial seed terms using a query expansion technique.", 
        "84": "We employ a continuous distributed vector representation of words using the continuous Skip-gram model (also known as Word2Vec) proposed by Mikolov et al.", 
        "85": "(2013).", 
        "86": "The model is trained on the whole collection of 40 million tweets with dimension and window size set to 300 and 7, respectively.", 
        "87": "Given the vector representations for the terms, we calculate the similarity scores between pairs of terms in our vocabulary using cosine similarity.", 
        "88": "The top 10 similar terms for each seed term are selected for the expansion of the initial seed list.", 
        "89": "We again filter the whole collection of tweets using the newly added seed terms.", 
        "90": "We finally select 120,000 tweets equally distributed in past (=40,000 tweets), present (=40,000), and future (=40,000) temporal categories.6 Examples of filtered tweets are as follows:\n\u2022 Thank you so much for coming in for our show yesterday.", 
        "91": "(seed=yesterday)\n\u2022 @**** is currently out of the office working his other job.", 
        "92": "(seed=currently)\n\u2022 I promise you don\u2019t have to be afraid.", 
        "93": "(seed=promise)\nTable 1 shows some examples of expanded terms for some of the initial seed terms.", 
        "94": "There are some unrelated keywords in the expanded seed list due to the automatic process of keyword selection.", 
        "95": "4.2 Test Set  In order to evaluate the tweet temporal orientation classification model, 2035 tweets were manually annotated by three human annotators in four different categories: past, present, future and doubtful.", 
        "96": "Majority voting is applied to assign the final output class to a given tweet.", 
        "97": "Tweets whose temporal orientation was not resolved by majority voting were deleted from the test set.7 The final distribution of annotated tweets was: past=423, present=1252, future=325, doubtful=35.", 
        "98": "6Similar to Schwartz et al.", 
        "99": "(2015), we only considered past, present and future categories.", 
        "100": "7Note that we approached the authors of Schwartz et al.", 
        "101": "(2015) to obtain their dataset but they did not share the data because of copyright issues.", 
        "102": "This is the reason for generating our own gold-standard test set.", 
        "103": "4.3 Income data of Users  We used a dataset developed by Preot\u0327iuc-Pietro et al.", 
        "104": "(2015), which contains 5,191 Twitter users along with their platform statistics and \u224810 million historical tweets.", 
        "105": "The dataset is based on mapping a Twitter user to a job title and using this as a proxy for the mean income for that specific occupation.", 
        "106": "5 Experimental Results  Temporal Orientation Classification Results: The performance of our tweet temporal orientation classifier is evaluated using the manually annotated test set.", 
        "107": "We compare our approach with two baselines that are the most relevant for our research: (i) Baseline1: a rule-based method (Nie et al., 2015) and (ii) Baseline2: a supervised learning strategy with bag-of-words, time expressions, part-of-speech tags, and temporal class-specific lexicon features (Schwartz et al., 2015).", 
        "108": "Comparative evaluation results are presented in Table 2.", 
        "109": "The results show that our weakly supervised framework outperforms rule-based and supervised learning technique in terms of accuracy.", 
        "110": "We examine the impact of the size of labeled training data on each method\u2019s performance.", 
        "111": "Baseline1 (rule-based approach) is not involved since this does not depend on labeled training data.", 
        "112": "We randomly select d% of the training data to train the classifiers and test them on the test set, with d ranging from 10 to 90.", 
        "113": "For each d, we generate the training set 20 times and the averaged performance is recorded.", 
        "114": "Accuracies of both approaches over the test data are presented in Table 3.", 
        "115": "Results show that our proposed framework performs consistently better than its counterpart.", 
        "116": "In particular, results show that with 30K training examples, better results can be obtained by our approach than relying on 120K training items for the state-of-theart supervised machine learning approach (Baseline2).", 
        "117": "Income Prediction Results: Similar to Preot\u0327iucPietro et al.", 
        "118": "(2015), we measure the predictive power of temporal orientation by performing regression on the user income.", 
        "119": "Performance is measured using 10-fold cross-validation: in each round, 80% of the data is used to train the model, 10% is used to tune model parameters using grid search and a different 10% is held out for testing.", 
        "120": "The final results are computed over the aggregate set of results of all 10 folds.", 
        "121": "Results us-\ning linear and non-linear regression methods and past, present, future temporal orientation features are presented in Table4.", 
        "122": "Performance is measured using two standard metrics: Pearson\u2019s correlation coefficient r and Mean Absolute Error (MAE) between inferred and target values.", 
        "123": "Results show that\ncorrelation between a user\u2019s future temporal orientation and their income is the highest, i.e.", 
        "124": "people with higher future temporal orientation tend to have higher income levels.", 
        "125": "Results also demonstrate that predictive models with future temporal orientation as a feature can predict income with high accuracy compared to past and present temporal orientation.", 
        "126": "Our findings are consistent\nwith previous research that suggests that futureoriented thinking is linked to academic achievement, increased social involvement, lower distress, extroversion, and conscientiousness.", 
        "127": "These factors are also positively correlated with income (Kahana et al., 2005; Roberts et al., 2007).", 
        "128": "Note also that, the non-linear methods outperform the linear methods by a wide margin, showing the importance of modeling non-linear relationships in our data.", 
        "129": "6 Conclusions  We presented the first large-scale study aiming to predict the income of Twitter users from their temporal orientation.", 
        "130": "Temporal orientation of users is assessed from their tweets.", 
        "131": "Our weakly supervised learning framework automatically time-tags tweets according to its underlying temporal orientation: past, present, or future.", 
        "132": "The associations we found between user-level future temporal orientation and income are novel in the context of well-established temporal orientation correlates.", 
        "133": "As future work, we are in the process of improving the temporal orientation classification accuracy by incorporating linguistic and sentiment related features into the deep learning phase.", 
        "134": "Acknowledgments  The ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund."
    }, 
    "document_id": "P17-2104.pdf.json"
}
