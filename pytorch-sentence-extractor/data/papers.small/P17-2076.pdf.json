{
    "abstract_sentences": {
        "1": "Twitter should be an ideal place to get a fresh read on how different issues are playing with the public, one that\u2019s potentially more reflective of democracy in this new media age than traditional polls.", 
        "2": "Pollsters typically ask people a fixed set of questions, while in social media people use their own voices to speak about whatever is on their minds.", 
        "3": "However, the demographic distribution of users on Twitter is not representative of the general population.", 
        "4": "In this paper, we present a demographic classifier for gender, age, political orientation and location on Twitter.", 
        "5": "We collected and curated a robust Twitter demographic dataset for this task.", 
        "6": "Our classifier uses a deep multi-modal multitask learning architecture to reach a stateof-the-art performance, achieving an F1score of 0.89, 0.82, 0.86, and 0.68 for gender, age, political orientation, and location respectively."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 478\u2013483 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2076  1 Introduction  While the most ambitious polls are based on standardized interviews with a few thousand people, millions are tweeting freely and publicly in their own voices about issues they care about.", 
        "2": "This data offers a vibrant 24/7 snapshot of people\u2019s response to various events and topics.", 
        "3": "However, the people using Twitter are not representative of the general US population (Greenwood et al., 2016).", 
        "4": "Therefore, if one is to use Twitter to understand the public\u2019s views on various, it is essential to understand the demographic of the users on Twitter.", 
        "5": "A robust demographic classification algorithm can also be utilized for detection of \u2217The first two authors contributed equally to this work.", 
        "6": "non-human account, especially in the context of bots involved in the spread of rumors and false information on Twitter (Vosoughi, 2015).", 
        "7": "In this paper, we present a state-of-the-art demographic classifier for Twitter.", 
        "8": "We focus on four different demographic categories: (a) Gender, (b) Age, (c) Political Orientation and (d) Location.", 
        "9": "We implement different variants of the deep multimodal multi-task learning architecture to infer these demographic categories.", 
        "10": "2 Features  Our deep multi-modal multi-task learning models (DMT-Demographic Models) use features extracted from the users\u2019 Twitter profile (such as name, profile picture, and description), the users following network and the users historical tweets (what they have said in the past).", 
        "11": "Below, we explain how these features are extracted and used.", 
        "12": "2.1 Name  The name specified by users in their profile is mainly used for gender prediction.", 
        "13": "We used three datasets for gender associations of common names:\n\u2022 We used the data from US Census Bureau data which contains male and female1 first names along with frequency of names for the sample male and female population with respect to 1990 census.", 
        "14": "\u2022 We obtained yearly data for the 100 most popular female and male names between 1960 and 2010 and calculate the overall frequency of a name being used in each list.", 
        "15": "\u2022 We also have a list of European names and popular first from other countries associated\n1https://www2.census.gov/topics/genealogy/1990surnames/\n478\nwith gender information2.", 
        "16": "Using the above datasets, we associate each name with a vector of size 2 representing the probability that the name occurs in male list and female list based on frequency available in the datasets.", 
        "17": "2.2 Following Network  Network Features can be a signal in prediction of some of the demographic parameters.", 
        "18": "But it is difficult to utilize the complete list of followers and following information of each and every user due to curse of dimensionality.", 
        "19": "Therefore, we build an binary vector of size Ndim for each user with each index of the vector representing a popular Twitter profiles associated with age, political orientation or location and the value (1, 0) represents if the user is following the profile or not.", 
        "20": "These profiles are short-listed based on the following techniques:\n\u2022 We search for user accounts on Twitter for task specific keywords like teenager, 80s, 90s for Age prediction; Democrat, Republican for political orientation and state names for location prediction.", 
        "21": "\u2022 We take advantage of the data collected from our earlier work (Vijayaraghavan et al., 2016) in processing news stories, classifying named entities into various categories and mapping them to Twitter handles.", 
        "22": "We use the political personalities mapped onto Twitter to the list of twitter profiles that can potentially act as a signal for our prediction tasks.", 
        "23": "Sample Twitter handles associated with each of the tasks are given in Table 1.", 
        "24": "For gender, the handles were too generic, so we expect that there are inherent latent features that can contribute towards gender prediction based on the shortlisted Twitter handles.", 
        "25": "We experiment with one or two fullyconnected layers and compress the information to a Nemb-sized vector.", 
        "26": "2.3 Profile Description  The profile description can be really useful to predict all the demographic parameters.", 
        "27": "Since, GRU is computationally less expensive than LSTM and performs better than standard RNN, we use a gated recurrent network (GRU) (Cho et al., 2014; Chung et al., 2014).", 
        "28": "At each time step t, GRU unit takes a 2https://hackage.haskell.org/package/gender0.1.1.0/src/data/nam dict.txt.UTF8\nword embedding xt and a hidden state ht as input.", 
        "29": "The internal transition operations of the GRU are defined as follows:\nzt = \u03c3(W (z)xt + U (z)ht\u22121 + b(z)) (1)\nrt = \u03c3(W (r)xt + U (r)ht\u22121 + b(r)) (2)\nh\u0303t = tanh(Wxt + rt \u00b7 Uht\u22121 + b(h) (3)\nht = zt \u00b7 ht\u22121 + (1\u2212 zt)h\u0303t (4)\nwhere W (z),W (r),W \u2208 IRdh\u00d7di , U (z), U (r), U \u2208 IRdh\u00d7dh and \u00b7 is an element-wise product.", 
        "30": "The dimensions dh and di are hyperparameters representing the hidden state size and input embedding size respectively.", 
        "31": "In our experiments, we represent the description as a (a) vector using GRU\u2019s final hidden state i.e.", 
        "32": "the hidden state representation (referred as DF \u2208 IRdh) at the last time step (b) matrix using all the time steps of hidden state, represented as DM \u2208 IRL\u00d7dh , where L is the sequence length of the user description.", 
        "33": "2.4 Profile Picture  Age and gender prediction can exploit the features extracted from profile picture.", 
        "34": "We extract dense feature representation from the image using the Inception architecture (Szegedy et al., 2015).", 
        "35": "Since we deal with multiple tasks, we experiment with two different layers (pool3 and mixed10) representations from the Inception architecture.", 
        "36": "The output vector sizes are IV = 2048 and IM = 64\u00d7 2048 respectively.", 
        "37": "2.5 Tweets  Finally, we also use tweets for our multitask learning problem.", 
        "38": "In our experiments, we restrict it to user\u2019s recent K tweets.", 
        "39": "The list of tweets, each of which is word sequence (Si = [wi1, w i 2, ..., w i N ]), are encoded using a positional encoding scheme as used in (Sukhbaatar et al., 2015).", 
        "40": "(For a more sophisticated encoding of the tweets, one can use the Tweet2Vec by Vosoughi et al.", 
        "41": "(Vosoughi et al., 2016), however the algorithm requires a massive training dataset, which might not be available to everyone) For the positional encoding scheme, the sentence representation is computed by\nPi =\nN\u2211\nj=1\nlj \u00b7 wij (5)\nN is the maximum number of words in a sentence and lj is a column vector with structure\nlpj = (1\u2212 j/N)\u2212 (p/q)(1\u2212 2j/N) (6)\nwhere p is the embedding index and q is the dimension of the embedding.", 
        "42": "The tweet representation obtained from the positional encoding summarizes the word tokens in the sentence.", 
        "43": "We explore tweet features as (1) a vector by summing up all the K-tweet embeddings Tq \u2208 IRq, (2) a matrix by concatenating all the K-tweet embeddings TKq \u2208 IRK\u00d7q  3 DMT-Demographic Models  Some of the latent information from one task can be useful to predict another task.", 
        "44": "Therefore, we propose three variants of deep multi-modal multitask learning demographic models to leverage the multi-modal nature of data.", 
        "45": "Figure 1 gives an illustration of our proposed models.", 
        "46": "In this section, we explain the single task output layer followed by the various models.", 
        "47": "3.1 Vanilla DMT-Demographic Model  This model takes vector features extracted from various user details (explained in section 2) represented byDF , Tq, Nemb, IV for description, tweet, network and image features respectively.", 
        "48": "The feature vectors are concatenated and passed through a fully-connected layer.", 
        "49": "The output of the fullyconnected layer is a compressed latent feature vector of size h. This shared latent vector is given to a task-specific output layer explained in Section 4.", 
        "50": "For gender prediction task, name features are concatenated with latent vector before feeding it to the output layer.", 
        "51": "3.2 Attention-based DMT-Demographic Model  All the modalities do not contribute equally to each of our tasks.", 
        "52": "Hence, for each task, we concatenate the weighted modal feature representations obtained through attention mechanism and then pass it through a fully-connected layer to get\na latent feature vector.", 
        "53": "Formally, the extracted features vectors represented by DF , Tq, Nemb, IV are concatenated to get a matrix M \u2208 IRd\u00d74 where d is the dimension of each feature.", 
        "54": "If the extracted features are not of the same dimension d, then we introduce a fully-connected layer and transform it to a d-sized vector.", 
        "55": "The attention over different modal features are computed as follows.", 
        "56": "\u03b1 = softmax(W (2)tanh(W (1)M + b(1)) + b(2)) (7) where \u03b1 \u2208 IR1\u00d7d.", 
        "57": "We multiply each of the feature vectors by their corresponding \u03b1 value to get a weighted feature representation.", 
        "58": "These weighted representation are concatenated before passing it through a fully-connected layer.", 
        "59": "The latent vector obtained from the fully connected layer is now task-specific and not shared between tasks.", 
        "60": "The latent vector is given to a task-specific output layer.", 
        "61": "3.3 Hierarchical Attention-based DMT - Demographic Model  This model is a slight variant of the previous model.", 
        "62": "In this model, we introduce another level of attention mechanism over the extracted features.", 
        "63": "The main intuition behind this approach is to have more attention on individual features based on their importance for a task.", 
        "64": "For example, certain words like \u2019male\u2019,\u2019husband\u2019 in user\u2019s description might be more suitable for gender prediction than any other task.", 
        "65": "So we weigh such words higher than the other words in the description during gender prediction task.", 
        "66": "However, these weights might not be applicable for a location prediction task.", 
        "67": "Hence, we implement a hierarchical attention mechanism that has task-specific weighted feature extraction followed by task-specific attention over the modalities.", 
        "68": "The rest of the architecture is similar to the attention-based model.", 
        "69": "This model uses the matrix representation associated with each of the features like description (DM ), tweets (TKq) and profile picture (IM ).", 
        "70": "However, the network features (Nemb) remain unchanged.", 
        "71": "The attention applied over the extracted features is similar to Equation 7 where the dimen-\nsions of weight parameters are feature-specific.", 
        "72": "For the sake of convenience, let \u03b2(F ) be the weights similar to \u03b1 associated with a feature F .", 
        "73": "For each feature F, we perform a weighted sum over the extracted representation matrix to obtain a vector representation.", 
        "74": "Let M (F ) denote the matrix representation of an extracted feature F, then the vector representation V (F ) of the feature F can be computed as follows.", 
        "75": "V (F ) =\nrF\u2211\nr=1\n\u03b2(F )r M (F ) r (8)\nwhere rF is the maximum number of rows in the representation matrix M (F ) associated with feature F. These vector representations of all the features are fed to layers similar to attention-based DMT model.", 
        "76": "It is important to note that all the models incorporate name features with the final latent vector representation for gender prediction task.", 
        "77": "4 Output Layer  Given a specific task A, we feed the latent feature vector h(A), obtained after applying any of the explained models, to a softmax layer depending on the classification task.", 
        "78": "So the task-specific representations are fed to task-specific output layers.", 
        "79": "y\u0303(A) = softmax(W (A)h(A) + b(A)), (9)\nwhere y\u0303(A) is a distribution over various categories associated with task A.", 
        "80": "For each task A, we minimize the cross-entropy of the predicted and true distributions.", 
        "81": "L(A)(y\u0303(A), y(A)) =\nN\u2211\ni=1\nC(A)\u2211\nj=1\ny j(A) i log(y\u0303 j(A) i )\n(10) where yj(A)i and y\u0303i\nj(A) are the true label and prediction probabilities for task A, N denotes the total number of training samples and C(A) is the total number of classes associated with the task A.", 
        "82": "Thus, the parameters of our network are optimized for global objective function given by:\n\u03b7 = \u2211\nA\u2208X L(A)(y\u0303(A), y(A)) (11)\nwhere X={Age, Location, Political Orientation, Gender}  5 Data Collection & Evaluation  We agglomerated data based on user tweets and their profile description.", 
        "83": "With access to Twitter API, we were able to get the timeline and profile information of a subset of users.", 
        "84": "We perform simple analysis of tweets and user description and those that contain phrases like \u201dI\u2019m a girl / woman / guy / man / husband / wife / mother/ father\u201d, \u201dI am a democrat / republican / liberal / conservative\u201d or \u201dI support hillary / trump\u201d, \u201dHappy 30th birthday to me\u201d,\u201dI\u2019m 30 years old\u201d, \u201dBorn in 1980\u201d etc.", 
        "85": "and their variants are shortlisted.", 
        "86": "These phrases act as indicators of gender, political orientation and age.", 
        "87": "For location prediction task, we used a combination of two different Twitter fields to collect\ndata: (a) latitude, longitude from geo-tagged user tweets, (b) Location field in user profile information.", 
        "88": "The various categories associated with each of the tasks are: (a) Gender: M,F (b) Age: < 30, 30\u2212 60, > 60 (c) Political Orientation: Democrat, Republican (d) Location: All states in USA.", 
        "89": "In order to avoid selection bias in the dataset collected, we introduce some noise in the training set by randomly removing the terms (from tweet or description) used for shortlisting the user profile.", 
        "90": "The total size of the training set is 50,859.", 
        "91": "We evaluate our models on task-specific annotated (mechanical turk) data or data collected based on different phrase indicators from user\u2019s tweet or description that was not a part of training set.", 
        "92": "The details of the test set are given in Table 2.", 
        "93": "The macro F1-score of different DMT-Demographic models (plus two baseline non-neural network based models) on the test data can be seen in Table 3.", 
        "94": "Hierarchical-Attention model performs well ahead of the other two models for almost all the tasks.", 
        "95": "However, the performance of all the models fall flat for location prediction task.", 
        "96": "Location-specific feature augmentation can be explored to improve its performance further.", 
        "97": "6 Related Work  The main distinctions of several of these models with DMT-Demographic models are that (a) most previous literature use only tweet content analysis to predict demographic information (Nguyen et al., 2013) while our model leverages different modals of user information including profile picture, (b) though some of the works use interesting network information they do not leverage other user details as potential signals (Colleoni et al., 2014; Culotta et al., 2015), (c) many of the models involve a lot of feature engineering like extracting location indicative words for geolocation prediction, etc.", 
        "98": "(Han et al., 2014; Sloan et al., 2015), (d) our model learns shared and task-specific layer parameters as we handle the demographic prediction\nproblem as a multi-task learning problem using different modalities like image (profile picture), text (tweets and user description) and network features (following).", 
        "99": "7 Conclusion  In this paper, we presented a state-of-the-art demographic classifier for identifying the gender, age, political orientation and the location of users on Twitter.", 
        "100": "We also collected and curated a novel Twitter demographic dataset and explored different variants of deep multi-modal multi-task learning architectures, settling on the HierarchicalAttention DMT as the top performing model, achieving an F1-score of 0.89, 0.82, 0.86, and 0.68 for gender, age, political orientation, and location respectively.", 
        "101": "In the future, we intend to use the demographic classifier presented in this paper to study the demographic biases present on Twitter."
    }, 
    "document_id": "P17-2076.pdf.json"
}
