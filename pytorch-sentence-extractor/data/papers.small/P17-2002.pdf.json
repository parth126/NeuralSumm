{
    "abstract_sentences": {
        "1": "This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar.", 
        "2": "During training, graph-to-string rules are learned using a heuristic extraction algorithm.", 
        "3": "At test time, a graph transducer is applied to collapse input AMRs and generate output sentences.", 
        "4": "Evaluated on a standard benchmark, our method gives the state-of-the-art result."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7\u201313 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2002\nAMR-to-text Generation with Synchronous Node Replacement Grammar\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea Department of Computer Science, University of Rochester, Rochester, NY 14627\nIBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Singapore University of Technology and Design\nAbstract\nThis paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar.", 
        "2": "During training, graph-to-string rules are learned using a heuristic extraction algorithm.", 
        "3": "At test time, a graph transducer is applied to collapse input AMRs and generate output sentences.", 
        "4": "Evaluated on a standard benchmark, our method gives the state-of-the-art result.", 
        "5": "1 Introduction  Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.", 
        "6": "AMR uses a graph to represent meaning, where nodes (such as \u201cboy\u201d, \u201cwant-01\u201d) represent concepts, and edges (such as \u201cARG0\u201d, \u201cARG1\u201d) represent relations between concepts.", 
        "7": "Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).", 
        "8": "AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations.", 
        "9": "Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).", 
        "10": "Flanigan et al.", 
        "11": "(2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer.", 
        "12": "Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string.", 
        "13": "However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them.", 
        "14": "Information loss in the graph-to-tree transformation step cannot be recovered.", 
        "15": "Song et al.", 
        "16": "(2016) directly generate sentences using graphfragment-to-string rules.", 
        "17": "They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences.", 
        "18": "However, their method does not learn hierarchical structural correspondences between AMR graphs and strings.", 
        "19": "We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules.", 
        "20": "As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs.", 
        "21": "At test time, we apply a graph transducer to collapse input\n7\nAMR graphs and generate output strings according to the learned grammar.", 
        "22": "Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding.", 
        "23": "It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset.", 
        "24": "2 Synchronous Node Replacement Grammar    2.1 Grammar Definition  A synchronous node replacement grammar (NRG) is a rewriting formalism: G = \u3008N,\u03a3,\u2206, P, S\u3009, where N is a finite set of nonterminals, \u03a3 and \u2206 are finite sets of terminal symbols for the source and target sides, respectively.", 
        "25": "S \u2208 N is the start symbol, and P is a finite set of productions.", 
        "26": "Each instance of P takes the form Xi \u2192 (\u3008F,E\u3009,\u223c), where Xi \u2208 N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over \u03a3 and node labels over N \u222a \u03a3, E is a corresponding target string over N \u222a\u2206 and \u223c denotes the alignment of nonterminal symbols between F and E. A classic NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism defining how F is connected to the rest of the graph when replacing Xi with F on the graph.", 
        "27": "Here we omit defining C and allow arbitrary connections.1 Following Chiang\n1This may over generate, but does not affect our case, as in our bottom-up decoding procedure (section 3) when F is replaced with Xi, nodes previously connected to F are reconnected to Xi\nData: training corpus C Result: rule instances R\n1 R\u2190 []; 2 for (Sent,AMR,\u223c) in C do 3 Rcur \u2190 FRAGMENTEXTRACT(Sent,AMR,\u223c); 4 for ri in Rcur do 5 R.APPEND(ri) ; 6 for rj in Rcur/{ri} do 7 if ri.CONTAINS(rj) then 8 rij \u2190 ri.COLLAPSE(rj); 9 R.APPEND(rij) ;\n10 end 11 end 12 end 13 end\nAlgorithm 1: Rule extraction\n(2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances.", 
        "28": "Figure 2 shows an example derivation process for the sentence \u201cthe boy wants to go\u201d given the rule set in Table 1.", 
        "29": "Given the start symbol S, which is first replaced with X1, rule (c) is applied to generate \u201cX2 to go\u201d and its AMR counterpart.", 
        "30": "Then rule (b) is used to generate \u201cX3 wants\u201d and its AMR counterpart from X2.", 
        "31": "Finally, rule (a) is used to generate \u201cthe boy\u201d and its AMR counterpart from X3.", 
        "32": "Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013).", 
        "33": "2.2 Induced Rules  There are three types of rules in our system, namely induced rules, concept rules and graph glue rules.", 
        "34": "Here we first introduce induced rules, which are obtained by a two-step procedure on a training corpus.", 
        "35": "Shown in Algorithm 1, the first step is to extract a set of initial rules from training \u3008sentence, AMR, \u223c\u30092 pairs (Line 2) using the phrase-to-graph-fragment extraction algorithm of Peng et al.", 
        "36": "(2015) (Line 3).", 
        "37": "Here an initial rule\n2\u223c denotes alignment between words and AMR labels.", 
        "38": "contains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj , and generate rij by collapsing ri with rj , if ri contains rj (Line 6-8).", 
        "39": "Here ri contains rj , if rj .F is a subgraph of ri.F and rj .E is a sub-phrase of ri.E.", 
        "40": "When collapsing ri with rj , we replace the corresponding subgraph in ri.F with a new non-terminal node, and the sub-phrase in ri.E with the same non-terminal.", 
        "41": "For example, we obtain rule (b) by collapsing (d) with (a) in Table 1.", 
        "42": "All initial and generated rules are stored in a rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set.", 
        "43": "2.3 Concept Rules and Glue Rules  In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations.", 
        "44": "For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of the node concept.", 
        "45": "A concept rule is used in case no induced rule can cover the node.", 
        "46": "We refer to the verbalization list3 and AMR guidelines4 for creating more complex concept rules.", 
        "47": "For example, one concept rule created from the verbalization list is \u201c(k / keep-01 :ARG1 (p / peace)) ||| peacekeeping\u201d.", 
        "48": "Inspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied.", 
        "49": "Three glue rules are defined for each type of edge label.", 
        "50": "Taking the edge label \u201cARG0\u201d as an example, we create the following glue rules:\nID.", 
        "51": "F E r1 (X1 / #X1# :ARG0 (X2 / #X2#)) #X1# #X2# r2 (X1 / #X1# :ARG0 (X2 / #X2#)) #X2# #X1# r3 (X1 / #X1# :ARG0 X1) #X1#\nwhere for both r1 and r2, F contains two nonterminal nodes with a directed edge connecting them, and E is the concatenation the two nonterminals in either the monotonic or the inverse order.", 
        "52": "For r3, F contains one non-terminal node with a self-pointing edge, and E is the nonterminal.", 
        "53": "With concept rules and glue rules in our final rule set, it is easily guaranteed that there are legal derivations for any input AMR graph.", 
        "54": "3 Model  We adopt a log-linear model for scoring search hypotheses.", 
        "55": "Given an input AMR graph, we find\n3http://amr.isi.edu/download/lists/verbalization-listv1.06.txt\n4https://github.com/amrisi/amr-guidelines\nthe highest scored derivation t\u2217 from all possible derivations t:\nt\u2217 = argmax t\nexp \u2211\ni\nwifi(g, t), (1)\nwhere g denotes the input AMR, fi(\u00b7, \u00b7) and wi represent a feature and the corresponding weight, respectively.", 
        "56": "The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3).", 
        "57": "The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).", 
        "58": "We perform bottom-up search to transduce input AMRs to surface strings.", 
        "59": "Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score.", 
        "60": "Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam.", 
        "61": "3.1 Translation Probabilities  Production rules serve as a basis for scoring hypotheses.", 
        "62": "We associate each synchronous NRG rule n \u2192 (\u3008F,E\u3009,\u223c) with a set of probabilities.", 
        "63": "First, phrase-to-fragment translation probabilities are defined based on maximum likelihood estimation (MLE), as shown in Equation 2, where c\u3008F,E\u3009 is the fractional count of \u3008F,E\u3009.", 
        "64": "p(F |E) = c\u3008F,E\u3009\u2211 F \u2032 c\u3008F \u2032,E\u3009\n(2)\nIn addition, lexicalized translation probabilities are defined as:\npw(F |E) = \u220f\nl\u2208F\n\u2211 w\u2208E p(l|w) (3)\nHere l is a label (including both edge labels such as \u201cARG0\u201d and concept labels such as \u201cwant-01\u201d) in the AMR fragment F , and w is a word in the phrase E. Equation 3 can be regarded as a \u201csoft\u201d version of the lexicalized translation probabilities adopted by SMT, which picks the alignment yielding the maximum lexicalized probability for each translation rule.", 
        "65": "In addition to p(F |E) and pw(F |E), we use features in the reverse direction, namely p(E|F ) and pw(E|F ), the definitions of which are omitted as they are consistent with\nEquations 2 and 3, respectively.", 
        "66": "The probabilities associated with concept rules and glue rules are manually set to 0.0001.", 
        "67": "3.2 Reordering Model  Although the word order is defined for induced rules, it is not the case for glue rules.", 
        "68": "We learn a reordering model that helps to decide whether the translations of the nodes should be monotonic or inverse given the directed connecting edge label.", 
        "69": "The probabilistic model using smoothed counts is defined as:\np(M |h, l, t) = 1.0 + \u2211 h \u2211 t c(h, l, t,M)\n2.0 + \u2211 o\u2208{M,I} \u2211 h \u2211 t c(h, l, t, o) (4)\nc(h, l, t,M) is the count of monotonic translations of head h and tail t, connected by edge l.  3.3 Moving Distance  The moving distance feature captures the distances between the subgraph roots of two consecutive rule matches in the decoding process, which controls a bias towards collapsing nearby subgraphs consecutively.", 
        "70": "4 Experiments    4.1 Setup  We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances.", 
        "71": "Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner.", 
        "72": "Rules are extracted from the training data, and model parameters are tuned on the dev set.", 
        "73": "For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances.", 
        "74": "We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.", 
        "75": "MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50.", 
        "76": "We investigate the effectiveness of rules and features by ablation tests: \u201cNoInducedRule\u201d does not adopt induced rules, \u201cNoConceptRule\u201d does not adopt concept rules, \u201cNoMovingDistance\u201d does not adopt the moving distance feature, and \u201cNoReorderModel\u201d disables the reordering model.", 
        "77": "Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate\nexisting translation fragments into a final translation, and if a subgraph can not be translated, the empty string is used as the output.", 
        "78": "We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al., 2016), on the same dataset.", 
        "79": "4.2 Main results  The results are shown in Table 2.", 
        "80": "First, All outperforms all baselines.", 
        "81": "NoInducedRule leads to the greatest performance drop compared with All, demonstrating that induced rules play a very important role in our system.", 
        "82": "On the other hand, NoConceptRule does not lead to much performance drop.", 
        "83": "This observation is consistent with the observation of Song et al.", 
        "84": "(2016) for their TSP-based system.", 
        "85": "NoMovingDistance leads to a significant performance drop, empirically verifying the fact that the translations of nearby subgraphs are also close.", 
        "86": "Finally, NoReorderingModel does not affect the performance significantly, which can be because the most important reordering patterns are already covered by the hierarchical induced rules.", 
        "87": "Compared with TSP-gen and JAMR-gen, our final model All improves the BLEU from 22.44 and 23.00 to 25.62, showing the advantage of our model.", 
        "88": "To our knowledge, this is the best result reported so far on the task.", 
        "89": "4.3 Grammar analysis  We have shown the effectiveness of our synchronous node replacement grammar (SNRG) on the AMR-to-text generation task.", 
        "90": "Here we further analyze our grammar as it is relatively less studied than the hyperedge replacement grammar (HRG) (Drewes et al., 1997).", 
        "91": "Statistics on the whole rule set We first categorize our rule set by the number of terminals and nonterminals in the AMR fragment F , and show the percentages of each type in Figure 3.", 
        "92": "Each rule contains at most 1 nonterminal, as we collapse each initial rule only once.", 
        "93": "First\nof all, the percentage of rules containing nonterminals are much more than those without nonterminals, as we collapse each pair of initial rules (in Algorithm 1) and the results can be quadratic the number of initial rules.", 
        "94": "In addition, most rules are small containing 1 to 3 terminals, meaning that they represent small pieces of meaning and are easier to matched on a new AMR graph.", 
        "95": "Finally, there are a few large rules, which represent complex meaning.", 
        "96": "Statistics on the rules used for decoding In addition, we collect the rules that our well-tuned system used for generating the 1-best output on the testset, and categorize them into 3 types: (1) glue rules, (2) nonterminal rules, which are not glue rules but contain nonterminals on the righthand side and (3) terminal rules, whose right-hand side only contain terminals.", 
        "97": "Over the rules used on the 1-best result, more than 30% are non-terminal rules, showing that the induced rules play an important role.", 
        "98": "On the other hand, 30% are glue rules.", 
        "99": "The reason is that the data sparsity for graph grammars is more severe than string-based grammars (such as CFG), as the graph structures are more complex than strings.", 
        "100": "Finally, terminal rules take the largest percentage, while most are induced rules, but not concept rules.", 
        "101": "Rule examples Finally, we show some rules in Table 4, where F and E are the right-hand-side AMR fragment and phrase, respectively.", 
        "102": "For the first rule, the root of F is a verb (\u201cgive-01\u201d) whose subject is a nonterminal and object is a AMR fragment \u201c(p / person :ARG0-of (u / use-01))\u201d, which means \u201cuser\u201d.", 
        "103": "So it is easy to see that the corresponding phrase E conveys the same meaning.", 
        "104": "For the second rule, \u201c(s3 / stay-01 :accompanier (i / i))\u201d means \u201cstay\nwith me\u201d, which is also covered by its phrase.", 
        "105": "4.4 Generation example  Finally, we show an example in Table 5, where the top is the input AMR graph, and the bottom is the generation result.", 
        "106": "Generally, most of the meaning of the input AMR are correctly translated, such as \u201c:example\u201d, which means \u201csuch as\u201d, and \u201cthing\u201d, which is an abstract concept and should not be translated, while there are a few errors, such as \u201cthat\u201d in the result should be \u201cwhat\u201d, and there should be an \u201cin\u201d between \u201ctmt\u201d and \u201cfairfax\u201d.", 
        "107": "5 Conclusion  We showed that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG in the training time, and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time.", 
        "108": "Our method performs better than the previous systems, empirically proving the advantages of our graph-to-string rules.", 
        "109": "Acknowledgement  This work was funded by a Google Faculty Research Award.", 
        "110": "Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education."
    }, 
    "document_id": "P17-2002.pdf.json"
}
