{
    "abstract_sentences": {
        "1": "We speed up Neural Machine Translation (NMT) decoding by shrinking run-time target vocabulary.", 
        "2": "We experiment with two shrinking approaches: Locality Sensitive Hashing (LSH) and word alignments.", 
        "3": "Using the latter method, we get a 2x overall speed-up over a highly-optimized GPU implementation, without hurting BLEU.", 
        "4": "On certain low-resource language pairs, the same methods improve BLEU by 0.5 points.", 
        "5": "We also report a negative result for LSH on GPUs, due to relatively large overhead, though it was successful on CPUs.", 
        "6": "Compared with Locality Sensitive Hashing (LSH), decoding with word alignments is GPU-friendly, orthogonal to existing speedup methods and more robust across language pairs."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 574\u2013579 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2091  1 Introduction  Neural Machine Translation (NMT) has been demonstrated as an effective model and been put into large-scale production (Wu et al., 2016; He, 2015).", 
        "2": "For online translation services, decoding speed is a crucial factor to achieve a better user experience.", 
        "3": "Several recently proposed training methods (Shen et al., 2015; Wiseman and Rush, 2016) aim to solve the exposure bias problem, but require decoding the whole training set multiple times, which is extremely time-consuming for millions of sentences.", 
        "4": "Slow decoding speed is partly due to the large target vocabulary size V, which is usually in the tens of thousands.", 
        "5": "The first two columns of Table 1 show the breakdown of the runtimes required by sub-modules to decode 1812 Japanese sentences to English using a sequence-to-sequence model with local attention (Luong et al., 2015).", 
        "6": "Softmax is the most computationally intensive part, where each hidden vector ht \u2208 Rd needs to dot-product with V target embeddings ei \u2208 Rd.", 
        "7": "It occupies 40% of the total decoding time.", 
        "8": "Another sub-module whose computation time is proportional to V is Beam Expansion, where we need to find the top B words among all V vocabulary according to their probability.", 
        "9": "It takes around 17% of the decoding time.", 
        "10": "Several approaches have proposed to improve decoding speed:\n1.", 
        "11": "Using special hardware, such as GPU and Tensor Processing Unit (TPU), and lowprecision calculation (Wu et al., 2016).", 
        "12": "2.", 
        "13": "Compressing deep neural models through knowledge distillation and weight pruning (See et al., 2016; Kim and Rush, 2016).", 
        "14": "574\n3.", 
        "15": "Several variants of Softmax have been proposed to solve its poor scaling properties on large vocabularies.", 
        "16": "Morin and Bengio (2005) propose hierarchical softmax, where at each step log2 V binary classifications are performed instead of a single classification on a large number of classes.", 
        "17": "Gutmann and Hyva\u0308rinen (2010) propose noise-contrastive estimation which discriminate between positive labels and k (k << V ) negative labels sampled from a distribution, and is applied successfully on natural language processing tasks (Mnih and Teh, 2012; Vaswani et al., 2013; Williams et al., 2015; Zoph et al., 2016).", 
        "18": "Although these two approaches provide good speedups for training, they still suffer at test time.", 
        "19": "Chen et al.", 
        "20": "(2016) introduces differentiated softmax, where frequent words have more parameters in the embedding and rare words have less, offering speedups on both training and testing.", 
        "21": "In this work, we aim to speed up decoding by shrinking the run-time target vocabulary size, and this approach is orthogonal to the methods above.", 
        "22": "It is important to note that approaches 1 and 2 will maintain or even increase the ratio of target word embedding parameters to the total parameters, thus the Beam Expansion and Softmax will occupy the same or greater portion of the decoding time.", 
        "23": "A small run-time vocabulary will dramatically reduce the time spent on these two portions and gain a further speedup even after applying other speedup methods.", 
        "24": "To shrink the run-time target vocabulary, our first method uses Locality Sensitive Hashing.", 
        "25": "Vijayanarasimhan et al.", 
        "26": "(2015) successfully applies it on CPUs and gains speedup on single step prediction tasks such as image classification and video identification.", 
        "27": "Our second method is to use word alignments to select a very small number of candidate target words given the source sentence.", 
        "28": "Recent works (Jean et al., 2015; Mi et al., 2016; L\u2019Hostis et al., 2016) apply a similar strategy and report speedups for decoding on CPUs on richsource language pairs.", 
        "29": "Our major contributions are:\n1.", 
        "30": "To our best of our knowledge, this work is the first attempt to apply LSH technique on sequence generation tasks on GPU other than single-step classification on CPU.", 
        "31": "We\nfind current LSH algorithms have a poor performance/speed trade-off on GPU, due to the large overhead introduced by many hash table lookups and list-merging involved in LSH.", 
        "32": "2.", 
        "33": "For our word alignment method, we find that only the candidate list derived from lexical translation table of IBM model 4 is adequate to achieve good BLEU/speedup trade-off for decoding on GPU.", 
        "34": "There is no need to combine the top frequent words or words from phrase table, as proposed in Mi et al.", 
        "35": "(2016).", 
        "36": "3.", 
        "37": "We conduct our experiments on GPU and provide a detailed analysis of BLEU/speedup trade-off on both resource-rich/poor language pairs and both attention/non-attention NMT models.", 
        "38": "We achieve more than 2x speedup on 4 language pairs with only a tiny BLEU drop, demonstrating the robustness and efficiency of our methods.", 
        "39": "2 Methods  At each step during decoding, the softmax function is calculated as:\nP (y = j|hi) = eh\nT i wj+bj\n\u2211V k=1 e hTi wk+bk (1)\nwhere P (y = j|hi) is the probability of word j = 1...V given the hidden vector hi \u2208 Rd, i = 1...B.", 
        "40": "B represents the beam size.", 
        "41": "wj \u2208 Rd is output word embedding and bj \u2208 R is the corresponding bias.", 
        "42": "The complexity is O(dBV ).", 
        "43": "To speed up softmax, we use word frequency, locality sensitive hashing, and word alignments respectively to select C (C << V ) potential words and evaluate their probability only, reducing the complexity to O(dBC + overhead).", 
        "44": "2.1 Word Frequency  A simple baseline to reduce target vocabulary is to select the top C words based on their frequency in the training corpus.", 
        "45": "There is no run-time overhead and the overall complexity is O(dBC).", 
        "46": "2.2 Locality Sensitive Hashing  The word j = argmaxk P (y = k|hi) will have the largest value of hTi wj + bj .", 
        "47": "Thus the argmax problem can be converted to finding the nearest neighbor of vector [hi; 1] among the vectors [wj ; bj ] under the distance measure of dot-product.", 
        "48": "Locality Sensitive Hashing (LSH) is a powerful technique for the nearest neighbor problem.", 
        "49": "We employ the winner-take-all (WTA) hashing (Yagnik et al., 2011) defined as:\nWTA(x \u2208 Rd) = [I1; ...; Ip; ...; IP ] (2) Ip = argmax K k=1 Permutep(x)[k] (3) WTAband(x) = [B1; ...;Bw; ...;BW ] (4)\nBw = [I(w\u22121)\u2217u+1; ...; I(w\u22121)\u2217u+i; ...; Iw\u2217u] (5)\nu = P/W (6)\nwhere P distinct permutations are applied and the index of the maximum value of the first K elements of each permutations is recorded.", 
        "50": "To perform approximate nearest neighbor searching, we follow the scheme used in (Dean et al., 2013; Vijayanarasimhan et al., 2015):\n1.", 
        "51": "Split the hash code WTA(x) into W bands (as shown in equation 4), with each band P W log2(K) bits long.", 
        "52": "2.", 
        "53": "Create W hash tables [T1, ..., Tw, ..., TW ], and hash every word index j into every table Tw using WTAband(wj)[w] as the key.", 
        "54": "3.", 
        "55": "Given the hidden vector hi, extract a list of word indexes from each table Tw using the key WTAband(hi)[w].", 
        "56": "Then we merge the W lists and count the number of the occurrences of each word index.", 
        "57": "Select the top C word indexes with the largest counts, and calculate their probability using equation 1.", 
        "58": "The 4 hyper-parameters that define a WTA-LSH are {K,P,W,C}.", 
        "59": "The run-time overhead comprises hashing the hidden vector, W times hash table lookups and W lists merging.", 
        "60": "The overall complexity isO(B(dC+K\u2217P+W+W\u2217Navg))), where Navg is the average number of the word indexes stored in a hash bin of Tw.", 
        "61": "Although the complexity is much smaller than O(dBV ), the runtime in practice is not guaranteed to be shorter, especially on GPUs, as hash table lookups introduce too many small kernel launches and list merging is hard to parallelize.", 
        "62": "2.3 Word Alignment  Intuitively, LSH shrinks the search space utilizing the spatial relationship between the query vector and database vectors in high dimension space.", 
        "63": "It\nis a task-independent technique.", 
        "64": "However, when focusing on our specific task (MT), we can employ translation-related heuristics to prune the run-time vocabulary precisely and efficiently.", 
        "65": "One simple heuristic relies on the fact that each source word can only be translated to a small set of target words.", 
        "66": "The word alignment model, a foundation of phrase-base machine translation, also follows the same spirit in its generative story: each source word is translated to zero, one, or more target words and then reordered to form target sentences.", 
        "67": "Thus, we apply the following algorithm to reduce the run-time vocabulary size:\n1.", 
        "68": "Apply IBM Model 4 and the grow-diag-final heuristic on the training data to get word alignments.", 
        "69": "Calculate the lexical translation table P(e|f) based on word alignments.", 
        "70": "2.", 
        "71": "For each word f in the source vocabulary of the neural machine translation model, store the top M target words according to P(e|f) in a hash table Tf2e = {f : [e1, ...eM ]}\n3.", 
        "72": "Given a source sentence s = [f1, ..., fN ], extract the candidate target word list from Tf2e for each source word fi.", 
        "73": "Merge the N lists to form the reduced target vocabulary Vnew;\n4.", 
        "74": "Construct the new embedding matrix and bias vector according to Vnew, then perform the normal beam search on target side.", 
        "75": "The only hyper-parameter is {M}, the number of candidate target words for each source word.", 
        "76": "Given a source sentence of length Ls, the run-time overhead includes Ls times hash table lookups and Ls lists merging.", 
        "77": "The complexity for each decoding step isO(dB|Vnew|+(Ls+LsM)/Lt), where Lt is the maximum number of decoding steps.", 
        "78": "Unlike LSH, these table lookups and list mergings are performed once per sentence, and do not depend on the any hidden vectors.", 
        "79": "Thus, we can overlap the computation with source side forward propagation.", 
        "80": "3 Experiments  To examine the robustness of these decoding methods, we vary experiment settings in different ways: 1) We train both attention (Luong et al., 2015) and non-attention (Sutskever et al., 2014) models; 2) We train models on\nboth resource-rich language pairs, French to English (F2E) and Japanese to English (J2E), and a resource-poor language pair, Uzbek to English (U2E); 3) We translate both to English (F2E, J2E, and U2E) and from English (E2J).", 
        "81": "We use 2- layer LSTM seq2seq models with different attention settings, hidden dimension sizes, dropout rates, and initial learning rates, as shown in Table 3.", 
        "82": "We use the ASPEC Japanese-English Corpus (Nakazawa et al., 2016), French-English Corpus from WMT2014 (Bojar et al., 2014), and Uzbek-English Corpus (Linguistic Data Consortium, 2016).", 
        "83": "Table 2 shows the decoding results of the three methods.", 
        "84": "Decoding with word alignments achieves the best performance/speedup trade-off across all four translation directions.", 
        "85": "It can halve the overall decoding time with less than 0.17 BLEU drop.", 
        "86": "Table 1 compares the detailed time breakdown of full-vocabulary decoding and WA50 decoding.", 
        "87": "WA50 can gain a speedup of 19.48x and 2.28x on softmax and beam expansion respectively, leading to an overall 2.08x speedup with only 0.03 BLEU drop.", 
        "88": "In contrast, decoding with top frequent words will hurt the BLEU rapidly as the speedup goes higher.", 
        "89": "We calculate the word type coverage (TC) for the test reference data as\nfollows:\nTC = |{run-time vocab} \u2229 {word types in test}| |{word types in test}| The top 1000 words only cover 14% word types of J2E test data, whereas WA10 covers 75%, whose run-time vocabulary is no more than 200 for a 20 words source sentence.", 
        "90": "The speedup of English-to-Uzbek translation is relatively low (around 1.7x).", 
        "91": "This is because the original full vocabulary size is small (25k), leaving less room for shrinkage.", 
        "92": "LSH achieves better BLEU than decoding with top frequent words of the same run-time vocabulary size C on attention models.", 
        "93": "However, it in-\ntroduces too large an overhead (50 times slower), especially when softmax is highly optimized on GPU.", 
        "94": "When doing sequential beam search, search error accumulates rapidly.", 
        "95": "To reach reasonable performance, we have to apply an adequately large number of permutations (P = 5000).", 
        "96": "We also find that decoding with word alignments can even improve BLEU on resource-poor languages (12.17 vs. 11.67).", 
        "97": "Our conjecture is that rare words are not trained enough, so neural models confuse them, and word alignments can provide a hard constraint to rule out the unreasonable word choices.", 
        "98": "4 Conclusion  We apply word alignments to shrink run-time vocabulary to speed up neural machine translation decoding on GPUs, and achieve more than 2x speedup on 4 translation directions without hurting BLEU.", 
        "99": "We also compare with two other speedup methods: decoding with top frequent words and decoding with LSH.", 
        "100": "Experiments and analyses demonstrate that word alignments provides accurate candidate target words and introduces only a tiny overhead over a highlyoptimized GPU implementation."
    }, 
    "document_id": "P17-2091.pdf.json"
}
