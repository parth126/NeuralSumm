{
    "abstract_sentences": {
        "1": "Word embeddings are well known to capture linguistic regularities of the language on which they are trained.", 
        "2": "Researchers also observe that these regularities can transfer across languages.", 
        "3": "However, previous endeavors to connect separate monolingual word embeddings typically require cross-lingual signals as supervision, either in the form of parallel corpus or seed lexicon.", 
        "4": "In this work, we show that such cross-lingual connection can actually be established without any form of supervision.", 
        "5": "We achieve this end by formulating the problem as a natural adversarial game, and investigating techniques that are crucial to successful training.", 
        "6": "We carry out evaluation on the unsupervised bilingual lexicon induction task.", 
        "7": "Even though this task appears intrinsically cross-lingual, we are able to demonstrate encouraging performance without any cross-lingual clues."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1959\u20131970 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1179  1 Introduction  As word is the basic unit of a language, the betterment of its representation has significant impact on various natural language processing tasks.", 
        "2": "Continuous word representations, commonly known as word embeddings, have formed the basis for numerous neural network models since their advent.", 
        "3": "Their popularity results from the performance boost they bring, which should in turn be attributed to the linguistic regularities they capture (Mikolov et al., 2013b).", 
        "4": "Soon following the success on monolingual tasks, the potential of word embeddings for crosslingual natural language processing has attracted much attention.", 
        "5": "In their pioneering work, Mikolov\n\u2217Corresponding author.", 
        "6": "Spanish English  et al.", 
        "7": "(2013a) observe that word embeddings trained separately on monolingual corpora exhibit isomorphic structure across languages, as illustrated in Figure 1.", 
        "8": "This interesting finding is in line with research on human cognition (Youn et al., 2016).", 
        "9": "It also means a linear transformation may be established to connect word embedding spaces, allowing word feature transfer.", 
        "10": "This has far-reaching implication on low-resource scenarios (Daume\u0301 III and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013), because word embeddings only require plain text to train, which is the most abundant form of linguistic resource.", 
        "11": "However, connecting separate word embedding spaces typically requires supervision from crosslingual signals.", 
        "12": "For example, Mikolov et al.", 
        "13": "(2013a) use five thousand seed word translation pairs to train the linear transformation.", 
        "14": "In a recent study, Vulic\u0301 and Korhonen (2016) show that at least hundreds of seed word translation pairs are needed for the model to generalize.", 
        "15": "This is unfortunate for low-resource languages and domains,\n1959\nbecause data encoding cross-lingual equivalence is often expensive to obtain.", 
        "16": "In this work, we aim to entirely eliminate the need for cross-lingual supervision.", 
        "17": "Our approach draws inspiration from recent advances in generative adversarial networks (Goodfellow et al., 2014).", 
        "18": "We first formulate our task in a fashion that naturally admits an adversarial game.", 
        "19": "Then we propose three models that implement the game, and explore techniques to ensure the success of training.", 
        "20": "Finally, our evaluation on the bilingual lexicon induction task reveals encouraging performance, even though this task appears formidable without any cross-lingual supervision.", 
        "21": "2 Models  In order to induce a bilingual lexicon, we start from two sets of monolingual word embeddings with dimensionality d. They are trained separately on two languages.", 
        "22": "Our goal is to learn a mapping function f : Rd \u2192 Rd so that for a source word embedding x, f (x) lies close to the embedding of its target language translation y.", 
        "23": "The learned mapping function can then be used to translate each\nsource word x by finding the nearest target embedding to f (x).", 
        "24": "We consider x to be drawn from a distribution px, and similarly y \u223c py.", 
        "25": "The key intuition here is to find the mapping function to make f (x) seem to follow the distribution py, for all x \u223c px.", 
        "26": "From this point of view, we design an adversarial game as illustrated in Figure 2(a): The generator G implements the mapping function f , trying to make f (x) passable as target word embeddings, while the discriminator D is a binary classifier striving to distinguish between fake target word embeddings f (x) \u223c pf(x) and real ones y \u223c py.", 
        "27": "This intuition can be formalized as the minimax game minGmaxD V (D,G) with value function\nV (D,G)\n=Ey\u223cpy [logD (y)] + Ex\u223cpx [log (1\u2212D (G (x)))] .", 
        "28": "(1)\nTheoretical analysis reveals that adversarial training tries to minimize the Jensen-Shannon divergence JSD ( py||pf(x) ) (Goodfellow et al., 2014).", 
        "29": "Importantly, the minimization happens at the distribution level, without requiring word\ntranslation pairs to supervise training.", 
        "30": "2.1 Model 1: Unidirectional Transformation  The first model directly implements the adversarial game, as shown in Figure 2(a).", 
        "31": "As hinted by the isomorphism shown in Figure 1, previous works typically choose the mapping function f to be a linear map (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015).", 
        "32": "We therefore parametrize the generator as a transformation matrix G \u2208 Rd\u00d7d.", 
        "33": "We also tried non-linear maps parametrized by neural networks, without success.", 
        "34": "In fact, if the generator is given sufficient capacity, it can in principle learn a constant mapping function to a target word embedding, which makes the discriminator impossible to distinguish, much like the \u201cmode collapse\u201d problem widely observed in the image domain (Radford et al., 2015; Salimans et al., 2016).", 
        "35": "We therefore believe it is crucial to grant the generator with suitable capacity.", 
        "36": "As a generic binary classifier, a standard feedforward neural network with one hidden layer is used to parametrize the discriminator D, and its loss function is the usual cross-entropy loss, as in the value function (1):\nLD = \u2212 logD (y)\u2212 log (1\u2212D (Gx)) .", 
        "37": "(2)\nFor simplicity, here we write the loss with a minibatch size of 1; in our experiments we use 128.", 
        "38": "The generator loss is given by\nLG = \u2212 logD (Gx) .", 
        "39": "(3)\nIn line with previous work (Goodfellow et al., 2014), we find this loss easier to minimize than the original form log (1\u2212D (Gx)).", 
        "40": "Orthogonal Constraint  The above model is very difficult to train.", 
        "41": "One possible reason is that the parameter search space Rd\u00d7d for the generator may still be too large.", 
        "42": "Previous works have attempted to constrain the transformation matrix to be orthogonal (Xing et al., 2015; Zhang et al., 2016b; Artetxe et al., 2016).", 
        "43": "An orthogonal transformation is also theoretically appealing for its self-consistency (Smith et al., 2017) and numerical stability.", 
        "44": "However, using constrained optimization for our purpose is cumbersome, so we opt for an orthogonal parametrization (Mhammedi et al., 2016) of the generator instead.", 
        "45": "2.2 Model 2: Bidirectional Transformation  The orthogonal parametrization is still quite slow.", 
        "46": "We can relax the orthogonal constraint and only require the transformation to be self-consistent (Smith et al., 2017): If G transforms the source word embedding space into the target language space, its transpose G> should transform the target language space back to the source.", 
        "47": "This can be implemented by two unidirectional models with a tied generator, as illustrated in Figure 2(b).", 
        "48": "Two separate discriminators are used, with the same cross-entropy loss as Equation (2) used by Model 1.", 
        "49": "The generator loss is given by\nLG = \u2212 logD1 (Gx)\u2212 logD2 ( G>x ) .", 
        "50": "(4)  2.3 Model 3: Adversarial Autoencoder  As another way to relax the orthogonal constraint, we introduce the adversarial autoencoder (Makhzani et al., 2015), depicted in Figure 2(c).", 
        "51": "After the generator G transforms a source word embedding x into a target language representation Gx, we should be able to reconstruct the source word embedding x by mapping back withG>.", 
        "52": "We therefore introduce the reconstruction loss measured by cosine similarity:\nLR = \u2212 cos ( x,G>Gx ) .", 
        "53": "(5)\nNote that this loss will be minimized if G is orthogonal.", 
        "54": "With this term included, the loss function for the generator becomes\nLG = \u2212 logD (Gx)\u2212 \u03bb cos ( x,G>Gx ) , (6)\nwhere \u03bb is a hyperparameter that balances the two terms.", 
        "55": "\u03bb = 0 recovers the unidirectional transformation model, while larger \u03bb should enforce a stricter orthogonal constraint.", 
        "56": "3 Training Techniques  Generative adversarial networks are notoriously difficult to train, and investigation into stabler training remains a research frontier (Radford et al., 2015; Salimans et al., 2016; Arjovsky and Bottou, 2017).", 
        "57": "We contribute in this aspect by reporting techniques that are crucial to successful training for our task.", 
        "58": "3.1 Regularizing the Discriminator  Recently, it has been suggested to inject noise into the input to the discriminator (S\u00f8nderby et al.,\n2016; Arjovsky and Bottou, 2017).", 
        "59": "The noise is typically additive Gaussian.", 
        "60": "Here we explore more possibilities, with the following types of noise, injected into the input and hidden layer:\n\u2022 Multiplicative Bernoulli noise (dropout) (Srivastava et al., 2014): \u223c Bernoulli (p).", 
        "61": "\u2022 Additive Gaussian noise: \u223c N ( 0, \u03c32 ) .", 
        "62": "\u2022 Multiplicative Gaussian noise: \u223c N ( 1, \u03c32 ) .", 
        "63": "As noise injection is a form of regularization (Bishop, 1995; Van der Maaten et al., 2013; Wager et al., 2013), we also try l2 regularization, and directly restricting the hidden layer size to combat overfitting.", 
        "64": "Our findings include:\n\u2022 Without regularization, it is not impossible for the optimizer to find a satisfactory parameter configuration, but the hidden layer size has to be tuned carefully.", 
        "65": "This indicates that a balance of capacity between the generator and discriminator is needed.", 
        "66": "\u2022 All forms of regularization help training by allowing us to liberally set the hidden layer size to a relatively large value.", 
        "67": "\u2022 Among the types of regularization, multiplicative Gaussian injected into the input is the most effective, and additive Gaussian is similar.", 
        "68": "On top of input noise, hidden layer noise helps slightly.", 
        "69": "In the following experiments, we inject multiplicative Gaussian into the input and hidden layer of the discriminator with \u03c3 = 0.5.", 
        "70": "3.2 Model Selection  From a typical training trajectory shown in Figure 3, we observe that training is not convergent.", 
        "71": "In fact, simply using the model saved at the end of training gives poor performance.", 
        "72": "Therefore we need a mechanism to select a good model.", 
        "73": "We observe there are sharp drops of the generator loss LG, and find they correspond to good models, as the discriminator gets confused at these points with its classification accuracy (D accuracy) dropping simultaneously.", 
        "74": "Interestingly, the reconstruction loss LR and the value of \u2225\u2225G>G\u2212 I \u2225\u2225 F\nexhibit synchronous drops, even if we use the unidirectional transformation model (\u03bb = 0).", 
        "75": "This means a good transformation matrix is indeed\nnearly orthogonal, and justifies our encouragement of G towards orthogonality.", 
        "76": "With this finding, we can train for sufficient steps and save the model with the lowest generator loss.", 
        "77": "As we aim to find the cross-lingual transformation without supervision, it would be ideal to determine hyperparameters without a validation set.", 
        "78": "The sharp drops can also be indicative in this case.", 
        "79": "If a hyperparameter configuration is poor, those values will oscillate without a clear drop.", 
        "80": "Although this criterion is somewhat subjective, we find it to be quite feasible in practice.", 
        "81": "3.3 Other Training Details  Our approach takes monolingual word embeddings as input.", 
        "82": "We train the CBOW model (Mikolov et al., 2013b) with default hyperparameters in word2vec.1 The embedding dimension d is 50 unless stated otherwise.", 
        "83": "Before feeding them into our system, we normalize the word embeddings to unit length.", 
        "84": "When sampling words for adversarial training, we penalize frequent words in a way similar to (Mikolov et al., 2013b).", 
        "85": "G is\n1https://code.google.com/archive/p/word2vec\ninitialized with a random orthogonal matrix.", 
        "86": "The hidden layer size ofD is 500.", 
        "87": "Adversarial training involves alternate gradient update of the generator and discriminator, which we implement with a simpler variant algorithm described in (Nowozin et al., 2016).", 
        "88": "Adam (Kingma and Ba, 2014) is used as the optimizer, with default hyperparameters.", 
        "89": "For the adversarial autoencoder model, \u03bb = 1 generally works well, but \u03bb = 10 appears stabler for the low-resource Turkish-English setting.", 
        "90": "4 Experiments  We evaluate the quality of the cross-lingual embedding transformation on the bilingual lexicon induction task.", 
        "91": "After a source word embedding is transformed into the target space, its M nearest target embeddings (in terms of cosine similarity) are retrieved, and compared against the entry in a ground truth bilingual lexicon.", 
        "92": "Performance is measured by top-M accuracy (Vulic\u0301 and Moens, 2013): If any of the M translations is found in the ground truth bilingual lexicon, the source word is considered to be handled correctly, and the accuracy is calculated as the percentage of correctly translated source words.", 
        "93": "We generally report the harshest top-1 accuracy, unless when comparing with published figures in Section 4.4.", 
        "94": "Baselines  Almost all approaches to bilingual lexicon induction from non-parallel data depend on seed lexica.", 
        "95": "An exception is decipherment (Dou and Knight, 2012; Dou et al., 2015), and we use it as our baseline.", 
        "96": "The decipherment approach is not based on distributional semantics, but rather views the source language as a cipher for the target language, and attempts to learn a statistical model to decipher the source language.", 
        "97": "We run the MonoGiza system as recommended by the toolkit.2 It can also utilize monolingual embeddings (Dou et al., 2015); in this case, we use the same embeddings as the input to our approach.", 
        "98": "Sharing the underlying spirit with our approach, related methods also build upon monolingual word embeddings and find transformation to link different languages.", 
        "99": "Although they need seed word translation pairs to train and thus not directly comparable, we report their performance with 50 and 100 seeds for reference.", 
        "100": "These methods are:\n2http://www.isi.edu/naturallanguage/software/monogiza release v1.0.tar.gz\n\u2022 Translation matrix (TM) (Mikolov et al., 2013a): the pioneer of this type of methods mentioned in the introduction, using linear transformation.", 
        "101": "We use a publicly available implementation.3\n\u2022 Isometric alignment (IA) (Zhang et al., 2016b): an extension of TM by augmenting its learning objective with the isometric (orthogonal) constraint.", 
        "102": "Although Zhang et al.", 
        "103": "(2016b) had subsequent steps for their POS tagging task, it could be used for bilingual lexicon induction as well.", 
        "104": "We ensure the same input embeddings for these methods and ours.", 
        "105": "The seed word translation pairs are obtained as follows.", 
        "106": "First, we ask Google Translate4 to translate the source language vocabulary.", 
        "107": "Then the target translations are queried again and translated back to the source language, and those that do not match the original source words are discarded.", 
        "108": "This helps to ensure the translation quality.", 
        "109": "Finally, the translations are discarded if they fall out of our target language vocabulary.", 
        "110": "3http://clic.cimec.unitn.it/\u02dcgeorgiana.dinu/down 4https://translate.google.com  4.1 Experiments on Chinese-English    Data  For this set of experiments, the data for training word embeddings comes from Wikipedia comparable corpora.5 Following (Vulic\u0301 and Moens, 2013), we retain only nouns with at least 1,000 occurrences.", 
        "111": "For the Chinese side, we first use OpenCC6 to normalize characters to be simplified, and then perform Chinese word segmentation and POS tagging with THULAC.7 The preprocessing of the English side involves tokenization, POS tagging, lemmatization, and lowercasing, which we carry out with the NLTK toolkit.8 The statistics of the final training data is given in Table 1, along with the other experimental settings.", 
        "112": "As the ground truth bilingual lexicon for evaluation, we use Chinese-English Translation Lexicon Version 3.0 (LDC2002L27).", 
        "113": "Overall Performance  Table 2 lists the performance of the MonoGiza baseline and our four variants of adversarial training.", 
        "114": "MonoGiza obtains low performance, likely due to the harsh evaluation protocol (cf.", 
        "115": "Section 4.4).", 
        "116": "Providing it with syntactic information can help (Dou and Knight, 2013), but in a lowresource scenario with zero cross-lingual information, parsers are likely to be inaccurate or even unavailable.", 
        "117": "5http://linguatools.org/tools/corpora/wikipediacomparable-corpora\n6https://github.com/BYVoid/OpenCC 7http://thulac.thunlp.org 8http://www.nltk.org\nThe unidirectional transformation model attains reasonable accuracy if trained successfully, but it is rather sensitive to hyperparameters and initialization.", 
        "118": "This training difficulty motivates our orthogonal constraint.", 
        "119": "But imposing a strict orthogonal constraint hurts performance.", 
        "120": "It is also about 20 times slower even though we utilize orthogonal parametrization instead of constrained optimization.", 
        "121": "The last two models represent different relaxations of the orthogonal constraint, and the adversarial autoencoder model achieves the best performance.", 
        "122": "We therefore use it in our following experiments.", 
        "123": "Table 3 lists some word translation examples given by the adversarial autoencoder model.", 
        "124": "Comparison With Seed-Based Methods  In this section, we investigate how many seeds TM and IA require to attain the performance level of our approach.", 
        "125": "There are a total of 1,280 seed translation pairs for Chinese-English, which are removed from the test set during the evaluation for this experiment.", 
        "126": "We use the most frequent S pairs for TM and IA.", 
        "127": "Figure 4 shows the accuracies with respect to\nS. When the seeds are few, the seed-based methods exhibit clear performance degradation.", 
        "128": "In this case, we also observe the importance of the orthogonal constraint from the superiority of IA to TM, which supports our introduction of this constraint as we attempt zero supervision.", 
        "129": "Finally, in line with the finding in (Vulic\u0301 and Korhonen, 2016), hundreds of seeds are needed for TM to generalize.", 
        "130": "Only then do seed-based methods catch up with our approach, and the performance difference is marginal even when more seeds are provided.", 
        "131": "Effect of Embedding Dimension  As our approach takes monolingual word embeddings as input, it is conceivable that their quality significantly affects how well the two spaces can be connected by a linear map.", 
        "132": "We look into this aspect by varying the embedding dimension d in Figure 5.", 
        "133": "As the dimension increases, the accuracy improves and gradually levels off.", 
        "134": "This indicates that too low a dimension hampers the encoding of linguistic information drawn from the corpus, and it is advisable to use a sufficiently large dimension.", 
        "135": "4.2 Experiments on Other Language Pairs    Data  We also induce bilingual lexica from Wikipedia comparable corpora for the following language pairs: Spanish-English, Italian-English, JapaneseChinese, and Turkish-English.", 
        "136": "For SpanishEnglish and Italian-English, we choose to use TreeTagger9 for preprocessing, as in (Vulic\u0301 and Moens, 2013).", 
        "137": "For the Japanese corpus, we use MeCab10 for word segmentation and POS tagging.", 
        "138": "For Turkish, we utilize the preprocessing tools (tokenization and POS tagging) provided in LORELEI Language Packs (Strassel and Tracey, 2016), and its English side is preprocessed by NLTK.", 
        "139": "Unlike the other language pairs, the frequency cutoff threshold for Turkish-English is 100, as the amount of data is relatively small.", 
        "140": "The ground truth bilingual lexica for SpanishEnglish and Italian-English are obtained from Open Multilingual WordNet11 through NLTK.", 
        "141": "For Japanese-Chinese, we use an in-house lexicon.", 
        "142": "For Turkish-English, we build a set of ground truth translation pairs in the same way as how we obtain seed word translation pairs from Google Translate, described above.", 
        "143": "Results  As shown in Table 4, the MonoGiza baseline still does not work well on these language pairs, while our approach achieves much better performance.", 
        "144": "The accuracies are particularly high for SpanishEnglish and Italian-English, likely because they are closely related languages, and their embedding spaces may exhibit stronger isomorphism.", 
        "145": "The\n9http://www.cis.uni-muenchen.de/\u02dcschmid/tools/ TreeTagger\n10http://taku910.github.io/mecab 11http://compling.hss.ntu.edu.sg/omw\nperformance on Japanese-Chinese is lower, on a comparable level with Chinese-English (cf.", 
        "146": "Table 2), and these languages are relatively distantly related.", 
        "147": "Turkish-English represents a low-resource scenario, and therefore the lexical semantic structure may be insufficiently captured by the embeddings.", 
        "148": "The agglutinative nature of Turkish can also add to the challenge.", 
        "149": "4.3 Large-Scale Settings  We experiment with large-scale Chinese-English data from two sources: the whole Wikipedia dump and Gigaword (LDC2011T13 and LDC2011T07).", 
        "150": "We also simplify preprocessing by removing the noun restriction and the lemmatization step (cf.", 
        "151": "preprocessing decisions for the above experiments).", 
        "152": "Although large-scale data may benefit the training of embeddings, it poses a greater challenge to bilingual lexicon induction.", 
        "153": "First, the degree of non-parallelism tends to increase.", 
        "154": "Second, with cruder preprocessing, the noise in the corpora may take its toll.", 
        "155": "Finally, but probably most importantly, the vocabularies expand dramatically compared to previous settings (see Table 1).", 
        "156": "This means a word translation has to be retrieved from a much larger pool of candidates.", 
        "157": "For these reasons, we consider the performance of our approach presented in Table 5 to be encouraging.", 
        "158": "The imbalanced sizes of the Chinese and English Wikipedia do not seem to cause a problem for the structural isomorphism needed by our method.", 
        "159": "MonoGiza does not scale to such large vocabularies, as it already takes days to train in our Italian-English setting.", 
        "160": "In contrast, our approach is immune from scalability issues by working with embeddings provided by word2vec, which is well known for its fast speed.", 
        "161": "With the network\nconfiguration used in our experiments, the adversarial autoencoder model takes about two hours to train for 500k minibatches on a single CPU.", 
        "162": "4.4 Comparison With (Cao et al., 2016)  In order to compare with the recent method by Cao et al.", 
        "163": "(2016), which also uses zero cross-lingual signal to connect monolingual embeddings, we replicate their French-English experiment to test our approach.12 This experimental setting has important differences from the above ones, mostly in the evaluation protocol.", 
        "164": "Apart from using top-5 accuracy as the evaluation metric, the ground truth bilingual lexicon is obtained by performing word alignment on a parallel corpus.", 
        "165": "We find this automatically constructed bilingual lexicon to be noisier than the ones we use for the other language pairs; it often lists tens of translations for a source word.", 
        "166": "This lenient evaluation protocol should explain MonoGiza\u2019s higher numbers in Table 6 than what we report in the other experiments.", 
        "167": "In this setting, our approach is able to considerably outperform both MonoGiza and the method by Cao et al.", 
        "168": "(2016).", 
        "169": "5 Related Work    5.1 Cross-Lingual Word Embeddings for Bilingual Lexicon Induction  Inducing bilingual lexica from non-parallel data is a long-standing cross-lingual task.", 
        "170": "Except for the decipherment approach, traditional statistical methods all require cross-lingual signals (Rapp, 1999; Koehn and Knight, 2002; Fung and Cheung, 2004; Gaussier et al., 2004; Haghighi et al., 2008; Vulic\u0301 et al., 2011; Vulic\u0301 and Moens, 2013).", 
        "171": "Recent advances in cross-lingual word embeddings (Vulic\u0301 and Korhonen, 2016; Upadhyay et al.,\n12As a confirmation, we ran MonoGiza in this setting and obtained comparable performance as reported.", 
        "172": "2016) have rekindled interest in bilingual lexicon induction.", 
        "173": "Like their traditional counterparts, these embedding-based methods require crosslingual signals encoded in parallel data, aligned at document level (Vulic\u0301 and Moens, 2015), sentence level (Zou et al., 2013; Chandar A P et al., 2014; Hermann and Blunsom, 2014; Koc\u030cisky\u0301 et al., 2014; Gouws et al., 2015; Luong et al., 2015; Coulmance et al., 2015; Oshikiri et al., 2016), or word level (i.e.", 
        "174": "seed lexicon) (Gouws and S\u00f8gaard, 2015; Wick et al., 2016; Duong et al., 2016; Shi et al., 2015; Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Faruqui and Dyer, 2014; Lu et al., 2015; Ammar et al., 2016; Zhang et al., 2016a, 2017; Smith et al., 2017).", 
        "175": "In contrast, our work completely removes the need for cross-lingual signals to connect monolingual word embeddings, trained on non-parallel text corpora.", 
        "176": "As one of our baselines, the method by Cao et al.", 
        "177": "(2016) also does not require cross-lingual signals to train bilingual word embeddings.", 
        "178": "It modifies the objective for training embeddings, whereas our approach uses monolingual embeddings trained beforehand and held fixed.", 
        "179": "More importantly, its learning mechanism is substantially different from ours.", 
        "180": "It encourages word embeddings from different languages to lie in the shared semantic space by matching the mean and variance of the hidden states, assumed to follow a Gaussian distribution, which is hard to justify.", 
        "181": "Our approach does not make any assumptions and directly matches the mapped source embedding distribution with the target distribution by adversarial training.", 
        "182": "A recent work also attempts adversarial training for cross-lingual embedding transformation (Barone, 2016).", 
        "183": "The model architectures are similar to ours, but the reported results are not positive.", 
        "184": "We tried the publicly available code on our data, but the results were not positive, either.", 
        "185": "Therefore, we attribute the outcome to the difference in the loss and training techniques, but not the model architectures or data.", 
        "186": "5.2 Adversarial Training  Generative adversarial networks are originally proposed for generating realistic images as an implicit generative model, but the adversarial training technique for matching distributions is generalizable to much more tasks, including natural\nlanguage processing.", 
        "187": "For example, Ganin et al.", 
        "188": "(2016) address domain adaptation by adversarially training features to be domain invariant, and test on sentiment classification.", 
        "189": "Chen et al.", 
        "190": "(2016) extend this idea to cross-lingual sentiment classification.", 
        "191": "Our research deals with unsupervised bilingual lexicon induction based on word embeddings, and therefore works with word embedding distributions, which are more interpretable than the neural feature space of classifiers in the above works.", 
        "192": "In the field of neural machine translation, a recent work (He et al., 2016) proposes dual learning, which also involves a two-agent game and therefore bears conceptual resemblance to the adversarial training idea.", 
        "193": "The framework is carried out with reinforcement learning, and thus differs greatly in implementation from adversarial training.", 
        "194": "6 Conclusion  In this work, we demonstrate the feasibility of connecting word embeddings of different languages without any cross-lingual signal.", 
        "195": "This is achieved by matching the distributions of the transformed source language embeddings and target ones via adversarial training.", 
        "196": "The success of our approach signifies the existence of universal lexical semantic structure across languages.", 
        "197": "Our work also opens up opportunities for the processing of extremely low-resource languages and domains that lack parallel data completely.", 
        "198": "Our work is likely to benefit from advances in techniques that further stabilize adversarial training.", 
        "199": "Future work also includes investigating other divergences that adversarial training can minimize (Nowozin et al., 2016), and broader mathematical tools that match distributions (Mohamed and Lakshminarayanan, 2016).", 
        "200": "Acknowledgments  We thank the anonymous reviewers for their helpful comments.", 
        "201": "This work is supported by the National Natural Science Foundation of China (No.", 
        "202": "61522204), the 973 Program (2014CB340501), and the National Natural Science Foundation of China (No.", 
        "203": "61331013).", 
        "204": "This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme."
    }, 
    "document_id": "P17-1179.pdf.json"
}
