{
    "abstract_sentences": {
        "1": "We propose novel radical features from automatic translation for event extraction.", 
        "2": "Event detection is a complex language processing task for which it is expensive to collect training data, making generalisation challenging.", 
        "3": "We derive meaningful subword features from automatic translations into target language.", 
        "4": "Results suggest this method is particularly useful when using languages with writing systems that facilitate easy decomposition into subword features, e.g., logograms and Cangjie.", 
        "5": "The best result combines logogram features from Chinese and Japanese with syllable features from Korean, providing an additional 3.0 points f-score when added to state-of-the-art generalisation features on the TAC KBP 2015 Event Nugget task."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 293\u2013298 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2046  1 Introduction  Event trigger detection is the task of identifying the mention that predicates the occurrence of an event and assigning it an event type (e.g., attack).", 
        "2": "Typical training data for event trigger detection includes fewer than 200 annotated documents (Ellis et al., 2015).", 
        "3": "Yet systems attempt to identify many event types (e.g., 38 for the data used here), making data sparsity a particular challenge (Ji, 2009; Zhu et al., 2014).", 
        "4": "Existing approaches use two main strategies for handling data sparsity.", 
        "5": "One strategy is to use lexical databases.", 
        "6": "Lexical databases have become a standard feature set for event detection.", 
        "7": "They make it easy to include synonyms and word-class information through hypernym relations.", 
        "8": "However, they require substantial human effort to build and can have low coverage.", 
        "9": "Another approach is to induce word-class information through cluster-\ning.", 
        "10": "Here cluster co-membership can be used to find synonyms and cluster identifiers provide abstracted word-class information.", 
        "11": "We propose novel semantic features for English event detection derived from automatic translations into thirteen languages.", 
        "12": "In particular, we explore the use of Cangjie1 radicals in Chinese and Japanese.", 
        "13": "Where characters represent concepts, they have often been composed of smaller pictographic units, called radicals.", 
        "14": "For example: \u660e(bright) is composed of two radicals\u65e5,\u6708(sun, moon) with corresponding Latin letter sequence \u201dAB\u201d.", 
        "15": "While this composition is often not productive, we hypothesise that the recurrence of some radicals among related concepts\u2019 logograms may be exploited to identify semantic affinity.", 
        "16": "Results suggest that (1) translated language features are especially useful if the target language has a writing system facilitating easy decomposition into useful subword features; (2) logograms (e.g., Chinese, Japanese), radicals (e.g., Chinese, Japanese) and syllables (e.g., Japanese, Korean) prove beneficial and complementary; and (3) Chinese characters are particularly useful, comparable to WordNet.", 
        "17": "Adding the best translated language features to the final system improves F1 by 3.0 points over a state-of-the-art feature set on the TAC KBP 2015 nugget type detection task.", 
        "18": "2 Background  Multilingual resources have been successfully applied to various NLP tasks such as named entity recognition (Klementiev and Roth, 2006), paraphrasing (Bannard and Callison-Burch, 2005), sentiment analysis (Wan, 2008), and word sense disambiguation (Lefever and Hoste, 2010).", 
        "19": "1https://en.wikipedia.org/wiki/ Cangjie_input_method\n293\nJi (2009) reports significantly improved event trigger extraction via cross-lingual clusters of English translations to Chinese trigger words over large corpora.", 
        "20": "At runtime, these are used to replace low-confidence event triggers with other high-confidence predicates from the same cluster.", 
        "21": "We describe an approach leveraging cross-lingual information not only from words, but also at the level of characters and radicals.", 
        "22": "Like Zhu et al.", 
        "23": "(2014), we use Google Translate and build bilingual feature vectors from the translations as well as original English sentences.", 
        "24": "While they address event trigger type classification only, we address both trigger detection and classification.", 
        "25": "We use new translated language features and evaluate with a range of languages.", 
        "26": "Li et al.", 
        "27": "(2012) show that monolingual Chinese event trigger extraction benefits from using compositional semantics inferred from Chinese characters.", 
        "28": "We use similar Chinese character information as features for English event trigger detection also using maximum entropy modelling.", 
        "29": "Furthermore, we introduce new radical features that take advantage of semantic compositionality of Chinese characters.", 
        "30": "2.1 Task  We address the event nugget detection task from the Text Analysis Conference Knowledge Base Population (TAC KBP) 2015 shared task (Mitamura and Hovy, 2015), which includes trigger detection and classification.", 
        "31": "An event trigger is the smallest extent of text (usually a word or short continuous phrase) that predicates the occurrence of an event (LDC, 2015).", 
        "32": "The task defines 9 event types and and 38 subtypes.", 
        "33": "Like most task participants, we formulate event trigger detection as a token-level classification task.", 
        "34": "We use a maximum entropy classifier here, with IOB encoding (Sang and Veenstra, 1999) to represent multi-word mentions.", 
        "35": "For comparison, we implement the baseline and lexical generalisation features from Hong et al.", 
        "36": "(2015).", 
        "37": "This was the best-performing system in the TAC 2015 nugget type detection task, with an F1 of 58.3.", 
        "38": "We do not replicate their semisupervised techniques here as we want to isolate the comparison of translated language features to other generalisation features.", 
        "39": "Since translated language features leverage off-the-shelf automatic translation, we believe the results here will gener-\nalise to semi-supervised learning as well.", 
        "40": "Baseline Features (BASE) Our baseline system uses standard surface features used for event extraction.", 
        "41": "Features of the current token include the full word token as it appears in the sentence, its lemma, its part of speech (POS), its entity type, and a feature that indicates whether the first character of the token is capitalised.", 
        "42": "Context features are computed for a window of one token on either side of the current token.", 
        "43": "They include lemma bigrams, POS bigrams and entity type bigrams.", 
        "44": "Finally, grammatical features are computed based on a dependency parse of the sentence.", 
        "45": "These include dependency relation types for the governor and any dependents, conjoined relation type and lemma, conjoined relation type and POS, and conjoined relation type and entity type.", 
        "46": "Lexical generalisation Features (LEX) We include three generalisation feature sets from the literature as a benchmark.", 
        "47": "The first lexical resource we use is Nomlex (Macleod et al., 1998) \u2013 a dictionary of nouns that are generated from another verb class, usually verbs.", 
        "48": "We also use Brown clusters trained on the Reuters corpus (Brown et al., 1992; Turian et al., 2010).", 
        "49": "Brown clusters group words into classes by performing a hierarchical clustering over distributional representations of the contexts in which they appear.", 
        "50": "Finally, we use WordNet (Miller, 1995) \u2013 a lexical database that includes synonym relations and semantic type-of/hypernym relationships.", 
        "51": "These relations have been used to extend feature sets beyond observed tokens which can help with identification of rare or unseen event triggers.", 
        "52": "3 Approach  We use machine translation (MT) service to obtain translated text.", 
        "53": "The translation is done at sentence level.", 
        "54": "We cache the translation results on files to ensure the experiments are repeatable.", 
        "55": "Below are example sentences translated from English into Chinese and Spanish.", 
        "56": "EN The attack by insurgents happened yesterday.", 
        "57": "ZH \u53db\u4e82\u5206\u5b50\u7684\u8972\u64ca\u767c\u751f\u5728\u6628\u5929\u3002 ES El ataque de los insurgentes paso\u0301 ayer.", 
        "58": "(1)  3.1 Translated Language Features (TRANS)  We generate three types of logogram features and use stem features for non-logogram languages.", 
        "59": "Word features (word) Different words in English can be translated into the same word in another language.", 
        "60": "For example there are 201 unique\nEnglish trigger words for attack events and only 160 unique words in their Chinese translations.", 
        "61": "Therefore if an English trigger word is not in the training data, the model might still recognise the trigger if it has seen the Chinese translation before.", 
        "62": "Logogram character features (char) Chinese and Japanese logograms are compositions of one or more characters defining their meanings.", 
        "63": "Therefore, different words representing the same event often contain similar characters.", 
        "64": "There are 195 unique Chinese characters for the attack event triggers in the corpus.", 
        "65": "The most frequently appearing characters are \u201c\u64ca\u201d (strike, attack), \u201c\u6230\u201d (war, fight), \u201c\u6bba\u201d (kill), \u201c\u722d\u201d (fight, dispute), and \u201c\u70b8\u201d (bomb, explode).", 
        "66": "Logogram Cangjie features (Cangjie) Chinese and Japanese characters can be further decomposed to smaller components called radicals.", 
        "67": "Certain radicals are more commonly found for a particular event type (Table 1).", 
        "68": "Cangjie is one of the methods to decompose Chinese characters.", 
        "69": "It was designed to use on computers with QWERTY keyboards so the radicals can be easily stored, indexed and searched by most computer systems.", 
        "70": "In addition to word and character features, we compute Cangjie features for logographic languages.", 
        "71": "Stem features (stem) For many languages character and radical features cannot be generated.", 
        "72": "We generate stem features in addition to the word features where available.", 
        "73": "We use the NLTK Snowball stemmer for German, Spanish, Finnish, Hungarian, Dutch and Russian; and the NLTK ISRI stemmer for Arabic.", 
        "74": "By including a range of languages, we hope to separate the effect of syllabic from semantic components of logograms.", 
        "75": "3.2 Translation Alignment  Translated language features require each English word to be aligned to one in the translated sentence.", 
        "76": "We use the translation service obtain all possible translations of a given English word, e.g.", 
        "77": ":\nEN attack\nZH \u9032\u653b,\u7830\u64ca,\u767c\u4f5c,\u653b\u64ca,\u653b\u6253,\u638a\u64ca,\u62a8\u64ca, ... ES acometida, ataque contra, agresio\u0301n, ...\n(2)\nIf one of these is in the translated sentence, then an alignment is made.", 
        "78": "If not, then we use the most likely word translation (underlined above).", 
        "79": "4 Experiments  We use the TAC KBP 2015 English event nugget data (Ellis et al., 2015) for the experiments.", 
        "80": "Development experiments use the training data (LDC2015E73) and the evaluation data (LDC2015R26) is held out for final results.", 
        "81": "The development corpus contains a total of 158 documents from two genres: 81 newswire documents and 77 discussion forum documents.", 
        "82": "We split this into 80% for training and 20% for development testing.", 
        "83": "We use Google Translate to obtain sentence and word translations into target languages and derive translated language features to help with the English task.", 
        "84": "Evaluation uses the official scorer from the shared task, where a trigger is counted as correct if both the trigger span and its event subtype are correctly identified.", 
        "85": "Comparing languages First, we explore how translated language features perform across the thirteen languages.", 
        "86": "Figure 1 shows how much each target language improves BASE on development data.", 
        "87": "We include all word, stem, character and Cangjie features as available for each language.", 
        "88": "Chinese, Japanese and Korean stand out, with improvements as high as 19.17 points f-score due mostly to large increases in recall.", 
        "89": "These results suggest that languages with writing systems that facilitate easy decomposition into meaningful subword features are particularly useful.", 
        "90": "Combining languages Next, we test whether system performance can be further improved using TRANS features from multiple languages.", 
        "91": "We add target languages one at a time in order of individual performance, and find that Traditional Chinese, Japanese and Korean to Simplified Chinese together improve F1 by 2.5 points.", 
        "92": "This combined feature set is used in the remaining analysis and experimental results.", 
        "93": "Error analysis We explore characteristic errors for BASE+LEX versus BASE+TRANS for the attack event on evaluation data.", 
        "94": "We randomly sample twenty instances where one is correct and the other is incorrect.", 
        "95": "Of six LEX FN errors, two are triggers not seen in the training data, e.g., \u2018wages\u2019 (Transfer-Money), and \u2018resignation\u2019 (End-Position).", 
        "96": "In other cases, there seem to be too few training instances, e.g., \u2018pardoning\u2019 (Pardon) only appears once in the training data.", 
        "97": "The TRANS FN error is due to a bad translation in which \u2018strike\u2019 (Attack) is a translated to the \u2018work stoppage\u2019 sense instead of the \u2018forceful hit\u2019 sense.", 
        "98": "For both systems, most FP errors correspond to cases with challenging ambiguity.", 
        "99": "For instance, both systems label \u2018appeal\u2019 as Justice.Appeal event in two sentences where the word \u2018appeal\u2019 means \u2018ask for aid\u2019, instead of \u2018taking a court case to a higher court\u2019.", 
        "100": "The translation was incorrect in this case.", 
        "101": "Similarly, \u2018report\u2019 appears six times in the training data as three different event types (Broadcast, Correspondence, Move-Person).", 
        "102": "Long-tail generalisation Table 2 shows typelevel results for BASE+LEX and BASE+TRANS compared to BASE alone.", 
        "103": "The generalisation feature sets outperform the baseline for all but three of the 38 event types.", 
        "104": "For Pardon, BASE obtains 97 F1 so there is little room for improvement.", 
        "105": "For Execute, LEX features have no effect while TRANS doubles BASE F1.", 
        "106": "Contact is the only type where generalisation features are harmful.", 
        "107": "Ignoring ties, BASE+TRANS performs best on more types (13) than BASE+LEX (11).", 
        "108": "TRANS appears to help more with long-tail entity types that have fewer training instances (e.g., Bankruptcy, Appeal, Born).", 
        "109": "Encouragingly, this\nanalysis also suggests that LEX and TRANS can be complementary, with LEX doing particularly well on some types (e.g., Trial-Hearing, Correspond) and TRANS doing particularly well on others (e.g., Transfer-Money, Release-Parole).", 
        "110": "5 Final Results and Discussion  Table 3 contains final results on the held-out evaluation data.", 
        "111": "The final translated language feature set (TRANS) comprises word, character and Cangjie features from Traditional Chinese, Simplified Chinese, Japanese and Korean.", 
        "112": "TRANS features provide a large F1 improvement of 17.4 over the baseline (BASE), similar to the benchmark lexical generalisation features (LEX).", 
        "113": "They differ in precision-recall tradeoff, with higher recall but lower precision from TRANS.", 
        "114": "LEX and TRANS are complementary, giving F1 of 55.0.", 
        "115": "This is 20.6 points higher than the baseline features alone, and improves both the precision of LEX and the recall of TRANS.", 
        "116": "The main appeal of the approach here is that translated character and radical features are easy to obtain using off-the-shelf tools.", 
        "117": "This provides a simple technique to capture semantic information and leverage the word sense disambiguation encoded in translation models trained over very large datasets.", 
        "118": "Given the positive results here, we plan to explore translation and alignment strategies to improve precision.", 
        "119": "We also plan to quantify the effect of different translation systems and system change over time.", 
        "120": "6 Conclusion  We described an event detection system leveraging features from off-the-shelf automatic translation to improve generalisation to new data.", 
        "121": "Chinese, Japanese and Korean prove especially useful as they provide natural decomposition into informative subword features, i.e., characters (Chinese and Japanese), radicals (Chinese and Japanese) and syllables (Korean).", 
        "122": "None of the nine other languages explored provide similar levels of natural decomposition and none provided additional benefit.", 
        "123": "The best system includes Chinese, Japanese and Korean character features.", 
        "124": "These translated language features improve f-score by 3 points on top of the English-only generalisation features from WordNet, Nomlex and Brown clusters.", 
        "125": "Acknowledgments  We wish to thank Will Radford and the anonymous reviewers for their helpful feedback.", 
        "126": "This research is funded by the Capital Markets Co-operative Research Centre.", 
        "127": "Ben Hachey is the recipient of an Australian Research Council Discovery Early Career Researcher Award (DE120102900)."
    }, 
    "document_id": "P17-2046.pdf.json"
}
