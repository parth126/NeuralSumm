{
    "abstract_sentences": {
        "1": "The paper presents a procedure of building an evaluation dataset1.", 
        "2": "for the validation of compositional distributional semantics models estimated for languages other than English.", 
        "3": "The procedure generally builds on steps designed to assemble the SICK corpus, which contains pairs of English sentences annotated for semantic relatedness and entailment, because we aim at building a comparable dataset.", 
        "4": "However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the need for language-specific transformation rules.", 
        "5": "The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset.", 
        "6": "The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment.", 
        "7": "The dataset may be used for the evaluation of compositional distributional semantics models of Polish."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 784\u2013792 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1073  1 Introduction and related work    1.1 Distributional semantics  The basic idea of distributional semantics, i.e.", 
        "2": "determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists \u2013 Harris (1954) and Firth (1957).", 
        "3": "John R. Firth drew attention to the contextdependent nature of meaning especially with his\n1The dataset is obtainable at: http://zil.ipipan.waw.pl/Scwad/CDSCorpus\nfamous maxim \u201cYou shall know a word by the company it keeps\u201d (Firth, 1957, p. 11).", 
        "4": "Nowadays, distributional semantics models are estimated with various methods, e.g.", 
        "5": "word embedding techniques (Bengio et al., 2003, 2006; Mikolov et al., 2013).", 
        "6": "To ascertain the purport of a word, e.g.", 
        "7": "bath, you can use the context of other words that surround it.", 
        "8": "If we assume that the meaning of this word expressed by its lexical context is associated with a distributional vector, the distance between distributional vectors of two semantically similar words, e.g bath and shower, should be smaller than between vectors representing semantically distinct words, e.g.", 
        "9": "bath and tree.", 
        "10": "1.2 Compositional distributional semantics  Based on empirical observations that distributional vectors encode certain aspects of word meaning, it is expected that similar aspects of the meaning of phrases and sentences can also be represented with vectors obtained via composition of distributional word vectors.", 
        "11": "The idea of semantic composition is not new.", 
        "12": "It is well known as the principle of compositionality:2 \u201cThe meaning of a compound expression is a function of the meaning of its parts and of the way they are syntactically combined.\u201d (Janssen, 2012, p. 19).", 
        "13": "Modelling the meaning of textual units larger than words using compositional and distributional information is the main subject of compositional distributional semantics (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, to name a few studies).", 
        "14": "The fundamental principles of compositional distributional semantics, henceforth referred to as CDS, are mainly propagated with papers written on the topic.", 
        "15": "Apart from the papers, it was the SemEval-2014 Shared Task 1\n2As the principle of compositionality is attributed to Gottlob Frege, it is often called Frege\u2019s principle.", 
        "16": "784\n(Marelli et al., 2014) that essentially contributed to the expansion of CDS and increased an interest in this domain.", 
        "17": "The goal of the task was to evaluate CDS models of English in terms of semantic relatedness and entailment on proper sentences from the SICK corpus.", 
        "18": "1.3 The SICK corpus  The SICK corpus (Bentivogli et al., 2014) consists of 10K pairs of English sentences containing multiple lexical, syntactic, and semantic phenomena.", 
        "19": "It builds on two external data sources \u2013 the 8K ImageFlickr dataset (Rashtchian et al., 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al., 2012).", 
        "20": "Each sentence pair is human-annotated for relatedness in meaning and entailment.", 
        "21": "The relatedness score corresponds to the degree of semantic relatedness between two sentences and is calculated as the average of ten human ratings collected for this sentence pair on the 5-point Likert scale.", 
        "22": "This score indicates the extent to which the meanings of two sentences are related.", 
        "23": "The entailment relation between two sentences, in turn, is labelled with entailment, contradiction, or neutral.", 
        "24": "According to the SICK guidelines, the label assigned by the majority of human annotators is selected as the valid entailment label.", 
        "25": "1.4 Motivation and organisation of the paper  Studying approaches to various natural language processing (henceforth NLP) problems, we have observed that the availability of language resources (e.g.", 
        "26": "training or testing data) stimulates the development of NLP tools and the estimation of NLP models.", 
        "27": "English is undoubtedly the most prominent in this regard and English resources are the most numerous.", 
        "28": "Therefore, NLP methods are mostly designed for English and tested on English data, even if there is no guarantee that they are universal.", 
        "29": "In order to verify whether an NLP algorithm is adequate, it is not enough to evaluate it solely for English.", 
        "30": "It is also valuable to have high-quality resources for languages typologically different to English.", 
        "31": "Hence, we aim at building datasets for the evaluation of CDS models in languages other than English, which are often underresourced.", 
        "32": "We strongly believe that the availability of test data will encourage development of CDS models in these languages and allow to better test the universality of CDS methods.", 
        "33": "We start with a high-quality dataset for Polish, which is a completely different language than English in at least two dimensions.", 
        "34": "First, it is a rather under-resourced language in contrast to the resource-rich English.", 
        "35": "Second, it is a fusional language with a relatively free word order in contrast to the isolated English with a relatively fixed word order.", 
        "36": "If some heuristics is tested on e.g.", 
        "37": "Polish, the evaluation results can be approximately generalised to other Slavic languages.", 
        "38": "We hope the Slavic NLP community will be interested in designing and evaluating methods of semantic modelling for Slavic languages.", 
        "39": "The procedure of building an evaluation dataset for validating compositional distributional semantics models of Polish generally builds on steps designed to assemble the SICK corpus (described in Section 1.3) because we aim at building an evaluation dataset which is comparable to the SICK corpus.", 
        "40": "However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for Polish (see Section 2.1) and the need for Polish-specific transformation rules (see Section 2.2).", 
        "41": "Furthermore, the rules of arranging sentences into pairs (see Section 2.3) are defined anew taking into account the characteristic of data and bidirectional entailment annotations, since an entailment relation between two sentences must not be symmetric.", 
        "42": "Even if our assumptions of annotating sentence pairs coincide with the SICK principles to a certain extent (see Section 3.1), the annotation process differs from the SICK procedure, in particular by introducing an element of human verification of correctness of automatically transformed sentences (see Section 3.2) and some additional post-corrections (see Section 3.3).", 
        "43": "Finally, a summary of the dataset is provided in Section 4.1 and the dataset evaluation is given in Section 4.2.", 
        "44": "2 Procedure of collecting data    2.1 Selection and description of images  The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012).", 
        "45": "Since we are not aware of accessibility of analogous resources for Polish, we have to select images first and then describe the selected images.", 
        "46": "Images are selected from the 8K ImageFlickr\ndataset (Rashtchian et al., 2010).", 
        "47": "At first we wanted to take only these images the descriptions of which were selected for the SICK corpus.", 
        "48": "However, a cursory check shows that these images are quite homogeneous, with a predominant number of dogs depictions.", 
        "49": "Therefore, we independently extract 1K images and split them into 46 thematic groups (e.g.", 
        "50": "children, musical instruments, motorbikes, football, dogs).", 
        "51": "The numbers of images within individual thematic groups vary from 6 images in the volleyball and telephoning groups to 94 images in the various people group.", 
        "52": "The second largest groups are children and dogs with 50 images each.", 
        "53": "The chosen images are given to two authors who independently of each other formulate their descriptions based on a short instruction.", 
        "54": "The authors are instructed to write one single sentence (with a sentence predicate) describing the action in a displayed image.", 
        "55": "They should not describe an imaginable context or an interpretation of what may lie behind the scene in the picture.", 
        "56": "If some details in the picture are not obvious, they should not be described either.", 
        "57": "Furthermore, the authors should avoid multiword expressions, such as idioms, metaphors, and named entities, because those are not compositional linguistic phenomena.", 
        "58": "Finally, descriptions should contain Polish diacritics and proper punctuation.", 
        "59": "2.2 Transformation of descriptions  The second step of building the SICK corpus consisted in pre-processing extracted sentences, i.e.", 
        "60": "normalisation and expansion (Bentivogli et al., 2014, p. 3\u20134).", 
        "61": "Since the authors of Polish descriptions are asked to follow the guidelines (presented in Section 2.1), the normalisation step is not essential for our data.", 
        "62": "The expansion step, in turn, is implemented and the sentences provided by the authors are lexically and syntactically transformed in order to obtain derivative sentences with similar, contrastive, or neutral meanings.", 
        "63": "The following transformations are implemented:\n1. dropping conjunction concerns sentences with coordinated predicates sharing a subject, e.g.", 
        "64": "Rowerzysta odpoczywa i obserwuje morze.", 
        "65": "(Eng.", 
        "66": "\u2018A cyclist is resting and watching the sea.\u2019).", 
        "67": "The finite form of one of the coordinated predicates is transformed into:\n\u2022 an active adjectival participle, e.g.", 
        "68": "Odpoczywaja\u0328cy rowerzysta obserwuje\nmorze.", 
        "69": "(Eng.", 
        "70": "\u2018A resting cyclist is watching the sea.\u2019) or Obserwuja\u0328cy morze rowerzysta odpoczywa.", 
        "71": "(Eng.", 
        "72": "\u2018A cyclist, who is watching the sea, is resting.\u2019), \u2022 a contemporary adverbial participle,\ne.g.", 
        "73": "Rowerzysta, odpoczywaja\u0328c, obserwuje morze.", 
        "74": "(Eng.", 
        "75": "\u2018A cyclist is watching the sea, while resting.\u2019) or Rowerzysta odpoczywa, obserwuja\u0328c morze.", 
        "76": "(Eng.", 
        "77": "\u2018A cyclist is resting, while watching the sea.\u2019).", 
        "78": "2. removing conjunct in adjuncts, i.e.", 
        "79": "the deletion of one of coordinated elements of an adjunct, e.g.", 
        "80": "Ma\u0142y, ale zwinny kot miauczy.", 
        "81": "(Eng.", 
        "82": "\u2018A small but agile cat miaows.\u2019) can be changed into either Ma\u0142y kot miauczy.", 
        "83": "(Eng.", 
        "84": "\u2018A small cat miaows.\u2019) or Zwinny kot miauczy.", 
        "85": "(Eng.", 
        "86": "\u2018An agile cat miaows.\u2019).", 
        "87": "3. passivisation, e.g.", 
        "88": "Cz\u0142owiek ujez\u0307dz\u0307a byka.", 
        "89": "(Eng.", 
        "90": "\u2018A man is breaking a bull in.\u2019) can be transformed into Byk jest ujez\u0307dz\u0307any przez cz\u0142owieka.", 
        "91": "(Eng.", 
        "92": "\u2018A bull is being broken in by a man.\u2019).", 
        "93": "4. removing adjuncts, e.g.", 
        "94": "Dwa bia\u0142e kr\u00f3liki siedza\u0328 na trawie.", 
        "95": "(Eng.", 
        "96": "\u2018Two small rabbits are sitting on the grass.\u2019) can be changed into Kr\u00f3liki siedza\u0328.", 
        "97": "(Eng.", 
        "98": "\u2018The rabbits are sitting.\u2019).", 
        "99": "5. swapping relative clause for participles, i.e.", 
        "100": "a relative clause swaps with a participle (and vice versa), e.g.", 
        "101": "Kobieta przytula psa, kt\u00f3rego trzyma na smyczy.", 
        "102": "(Eng.", 
        "103": "\u2018A woman hugs a dog which she keeps on a leash.\u2019).", 
        "104": "The relative clause is interchanged for a participle construction, e.g.", 
        "105": "Kobieta przytula trzymanego na smyczy psa.", 
        "106": "(Eng.", 
        "107": "\u2018A woman hugs a dog kept on a leash.\u2019).", 
        "108": "6. negation, e.g.", 
        "109": "Me\u0328z\u0307czyz\u0301ni w turbanach na g\u0142owach siedza\u0328 na s\u0142oniach.", 
        "110": "(Eng.", 
        "111": "\u2018Men in turbans on their heads are sitting on elephants.\u2019) can be transformed into Nikt nie siedzi na s\u0142oniach.", 
        "112": "(Eng.", 
        "113": "\u2018Nobody is sitting on elephants.\u2019), Z\u0307adni me\u0328z\u0307czyz\u0301ni w turbanach na g\u0142owach nie siedza\u0328 na s\u0142oniach.", 
        "114": "(Eng.", 
        "115": "\u2018No men in turbans on their heads are sitting on elephants.\u2019), and Me\u0328z\u0307czyz\u0301ni w turbanach na g\u0142owach nie siedza\u0328 na s\u0142oniach.", 
        "116": "(Eng.", 
        "117": "\u2018Men in turbans on their heads are not sitting on elephants.\u2019).", 
        "118": "7. constrained mixing of dependents from various sentences, e.g.", 
        "119": "Dwoje dzieci siedzi na wielb\u0142a\u0328dach w pobliz\u0307u wysokich g\u00f3r.", 
        "120": "(Eng.", 
        "121": "\u2018Two children are sitting on camels near high mountains.\u2019) can be changed into Dwoje dzieci siedzi przy zastawionym stole w pobliz\u0307u wysokich g\u00f3r.", 
        "122": "(Eng.", 
        "123": "\u2018Two children are sitting at the table laid with food near high mountains.\u2019).", 
        "124": "The first five transformations are designed to produce sentences with a similar meaning, the sixth transformation outputs sentences with a contradictory meaning, and the seventh transformation should generate sentences with a neutral (or unrelated) meaning.", 
        "125": "All transformations are performed on the dependency structures of input sentences (Wr\u00f3blewska, 2014).", 
        "126": "Some of the transformations are very productive (e.g.", 
        "127": "mixing dependents).", 
        "128": "Other, in turn, are sparsely represented in the output (e.g.", 
        "129": "dropping conjunction).", 
        "130": "The number of transformed sentences randomly selected to build the dataset is in the second column of Table 1.", 
        "131": "2.3 Data ensemble  The final step of building the SICK corpus consisted in arranging normalised and expanded sentences into pairs.", 
        "132": "Since our data diverges from SICK data, the process of arranging Polish sentences into pairs also differs from pairing in the SICK corpus.", 
        "133": "The general idea behind the pair-ensembling procedure was to introduce sentence pairs with different levels of relatedness into the dataset.", 
        "134": "Apart from pairs connecting two sentences originally written by humans (as described in Section 2.1), there are also pairs in which an original sentence is connected with\na transformed sentence.", 
        "135": "For each of the 1K images, the following 10 pairs are constructed (for A being the set of all sentences originally written by the first author, B being the set of all sentences originally written by the second author, a \u2208 A and b \u2208 B being the original descriptions of the picture):\n1.", 
        "136": "(a,b)\n2.", 
        "137": "(a,a1), where a1 \u2208 t(a), and t(a) is the set of all transformations of the sentence a\n3.", 
        "138": "(b,b1), where b1 \u2208 t(b)\n4.", 
        "139": "(a,b2), where b2 \u2208 t(b)\n5.", 
        "140": "(b,a2), where a2 \u2208 t(a)\n6.", 
        "141": "(a,a3), where a3 \u2208 t(a\u2032),a\u2032 \u2208 A, T (a\u2032) = T (a),a\u2032 6= a, for T (a) being the thematic group3 of a\n7.", 
        "142": "(b,b3), where b3 \u2208 t(b\u2032),b\u2032 \u2208 B, T (b\u2032) = T (b),b\u2032 6= b\n8.", 
        "143": "(a,a4), where a4 \u2208 A, T (a4) 6= T (a)4\n9.", 
        "144": "(b,b4), where b4 \u2208 B, T (b4) 6= T (b)\n10.", 
        "145": "(a,a5), where a5 \u2208 t(a),a5 6= a1 for 50% images, (b,b5) (analogously) for other 50%.5\nFor each sentence pair (a,b) created according to this procedure, its reverse (b,a) is also included in our corpus.", 
        "146": "As a result, the working set consists of 20K sentence pairs.", 
        "147": "3 Corpus annotation    3.1 Annotation assumptions  The degree of semantic relatedness between two sentences is calculated as the average of all human ratings on the Likert scale with the range from 0 to 5.", 
        "148": "Since we do not want to excessively influence\n3The thematic group of a sentence a corresponds to the thematic group of an image being the source of a (as described in Section 2.1).", 
        "149": "4The pairs (a,a4) of the same authors\u2019 descriptions of two images from different thematic groups are expected to be unrelated.", 
        "150": "The same applies to (b,b4).", 
        "151": "5A repetition of point 2 with a restriction that a different pair is created (pairs of very related sentences are expected).", 
        "152": "We alternate between authors A and B to obtain equal author proportions in the final ensemble of pairs.", 
        "153": "the annotations, the guidelines given to annotators are mainly example-based:6\n\u2022 5 (very related): Kot siedzi na p\u0142ocie.", 
        "154": "(Eng.", 
        "155": "\u2018A cat is sitting on the fence.\u2019) vs. Na p\u0142ocie jest duz\u0307y kot.", 
        "156": "(Eng.", 
        "157": "\u2018There is a large cat on the fence.\u2019),\n\u2022 1\u20134 (more or less related): Kot siedzi na p\u0142ocie.", 
        "158": "(Eng.", 
        "159": "\u2018A cat is sitting on the fence.\u2019) vs. Kot nie siedzi na p\u0142ocie.", 
        "160": "(Eng.", 
        "161": "\u2018A cat is not sitting on the fence.\u2019); Kot siedzi na p\u0142ocie.", 
        "162": "(Eng.", 
        "163": "\u2018A cat is sitting on the fence.\u2019) vs. W\u0142as\u0301ciciel da\u0142 kotu chrupki.", 
        "164": "(Eng.", 
        "165": "\u2018The owner gave kibble to his cat.\u2019); Kot siedzi na p\u0142ocie.", 
        "166": "(Eng.", 
        "167": "\u2018A cat is sitting on the fence.\u2019) vs. Kot miauczy pod p\u0142otem.", 
        "168": "(Eng.", 
        "169": "\u2018A cat miaows by the fence.\u2019).", 
        "170": "\u2022 0 (unrelated): Kot siedzi na p\u0142ocie.", 
        "171": "(Eng.", 
        "172": "\u2018A cat is sitting on the fence.\u2019) vs. Zacza\u0328\u0142 padac\u0301 deszcz.", 
        "173": "(Eng.", 
        "174": "\u2018It started to rain.\u2019).", 
        "175": "Apart from these examples, there is a note in the annotation guidelines indicating that the degree of semantic relatedness is not equivalent to the degree of semantic similarity.", 
        "176": "Semantic similarity is only a special case of semantic relatedness, semantic relatedness is thus a more general term than the other one.", 
        "177": "Polish entailment labels correspond directly to the SICK labels (i.e.", 
        "178": "entailment, contradiction, neutral).", 
        "179": "The entailment label assigned by the majority of human judges is selected as the gold label.", 
        "180": "The entailment labels are defined as follows:\n\u2022 a wynika z b (b entails a) \u2013 if a situation or an event described by sentence b occurs, it is recognised that a situation or an event described by a occurs as well, i.e.", 
        "181": "a and b refer to the same event or the same situation,\n\u2022 a jest zaprzeczeniem b (a is the negation of b) \u2013 if a situation or an event described by b occurs, it is recognised that a situation or an event described by a may not occur at the same time,\n6We realise that the boundary between semantic perception of a sentence by various speakers is fuzzy (it depends on speakers\u2019 education, origin, age, etc.).", 
        "182": "It was thus our wellthought-out decision to draw only general annotation frames and to enable annotators to rely on their feel for language.", 
        "183": "\u2022 a jest neutralne wobec b (a is neutral to b) \u2013 the truth of a situation described by a cannot be determined on the basis of b.", 
        "184": "3.2 Annotation procedure  Similar to the SICK corpus, each Polish sentence pair is human-annotated for semantic relatedness and entailment by 3 human judges experienced in Polish linguistics.7 Since for each annotated pair (a,b), its reverse (b,a) is also subject to annotation, the entailment relation is in practice determined \u2018in both directions\u2019 for 10K sentence pairs.", 
        "185": "For the task of relatedness annotation, the order of sentences within pairs seems to be irrelevant, we can thus assume to obtain 6 relatedness scores for 10K unique pairs.", 
        "186": "Since the transformation process is fully automatic and to a certain extent based on imperfect dependency parsing, we cannot ignore errors in the transformed sentences.", 
        "187": "In order to avoid annotating erroneous sentences, the annotation process is divided into two stages:\n1. a sentence pair is sent to a judge with the leader role, who is expected to edit and to correct the transformed sentence from this pair before annotation, if necessary,\n2. the verified and possibly enhanced sentence pair is sent to the other two judges, who can only annotate it.", 
        "188": "The leader judges should correct incomprehensible and ungrammatical sentences with a minimal number of necessary changes.", 
        "189": "Unusual sentences which could be accepted by Polish speakers should not be modified.", 
        "190": "Moreover, the modified sentence may not be identical with the other sentence in the pair.", 
        "191": "The classification and statistics of distinct corrections made by the leader judges are provided in Table 2.", 
        "192": "A strict classification of error types is quite hard to provide because some sentences contain more than one error.", 
        "193": "We thus order the error types from the most serious errors (i.e.", 
        "194": "\u2018sense\u2019 errors) to the redundant corrections (i.e.", 
        "195": "\u2018other\u2019 type).", 
        "196": "If a sentence contains several errors, it is qualified for the higher order error type.", 
        "197": "In the case of sentences with \u2018sense\u2019 errors, the need for correction is uncontroversial and\n7Our annotators have relatively strong linguistic background.", 
        "198": "Five of them have PhD in linguistics, five are PhD students, one is a graduate, and one is an undergraduate.", 
        "199": "arises from an internal logical contradiction.8 The sentences with \u2018semantic\u2019 changes are syntactically correct, but deemed unacceptable by the leader annotators from the semantic or pragmatic point of view.9 The \u2018grammatical\u2019 errors mostly concern missing agreement.10 The majority of \u2018word order\u2019 corrections are unnecessary, but we found some examples which can be classified as actual word or phrase order errors.11 The correction of punctuation consists in adding or deleting a comma.12 The sentences in the \u2018other\u2019 group, in turn, could as well have been left unchanged because they are proper Polish sentences, but were apparently considered odd by the leader annotators.", 
        "200": "8An example of \u2018sense\u2019 error: the sentence Ch\u0142opak w zielonej bluzie i czapce zjez\u0307dz\u0307a na rolkach na lez\u0307a\u0328co.", 
        "201": "(Eng.", 
        "202": "\u2018A boy in a green sweatshirt and a cap roller-skates downhill in a lying position.\u2019) is corrected into Ch\u0142opak w zielonej bluzie i czapce zjez\u0307dz\u0307a na rolkach.", 
        "203": "(Eng.", 
        "204": "\u2018A boy in a green sweatshirt and a cap roller-skates downhill.\u2019).", 
        "205": "9An example of \u2018semantic\u2019 correction: the sentence Dziewczyna trzyma w pysku patyk.", 
        "206": "(Eng.", 
        "207": "\u2018A girl holds a stick in her muzzle.\u2019) is corrected into Dziewczyna trzyma w ustach patyk.", 
        "208": "(Eng.", 
        "209": "\u2018A girl holds a stick in her mouth.\u2019).", 
        "210": "10An example of \u2018grammatical\u2019 error: the sentence Grupasg.nom us\u0301miechaja\u0328cych sie\u0328 ludzi tan\u0301cza\u0328pl.", 
        "211": "(Eng.", 
        "212": "*\u2018A group of smiling people are dancing.\u2019) is corrected into Grupasg.nom us\u0301miechaja\u0328cych sie\u0328 ludzi tan\u0301czysg .", 
        "213": "(Eng.", 
        "214": "\u2018A group of smiling people is dancing.\u2019).", 
        "215": "11An example of word order error: the sentence Samoch\u00f3d, kt\u00f3ry jest uszkodzony, koloru bia\u0142ego stoi na lawecie duz\u0307ego auta.", 
        "216": "(lit.", 
        "217": "\u2018A car that is damaged, of the white color stands on the trailer of a large car.\u2019, Eng.", 
        "218": "\u2018A white car that is damaged is standing on the trailer of a large car.\u2019) is corrected into Samoch\u00f3d koloru bia\u0142ego, kt\u00f3ry jest uszkodzony, stoi na lawecie duz\u0307ego auta.", 
        "219": "12An example of punctuation correction: the wrong comma in the sentence Nad brzegiem wody, stoja\u0328 dwaj me\u0328z\u0307czyz\u0301ni z we\u0328dkami.", 
        "220": "(lit.", 
        "221": "\u2018On the water\u2019s edge, two men are standing with rods.\u2019; Eng.", 
        "222": "\u2018Two men with rods are standing on the water\u2019s edge.\u2019) should be deleted, i.e.", 
        "223": "Nad brzegiem wody stoja\u0328 dwaj me\u0328z\u0307czyz\u0301ni z we\u0328dkami.", 
        "224": "3.3 Impromptu post-corrections  During the annotation process it came out that sentences accepted by some human annotators are unacceptable for other annotators.", 
        "225": "We thus decided to garner annotators\u2019 comments and suggestions for improving sentences.", 
        "226": "After validation of these suggestions by an experienced linguist, it turns out that most of these proposals concern punctuation errors (e.g.", 
        "227": "missing comma) and typos in 312 distinct sentences.", 
        "228": "These errors are fixed directly in the corpus because they should not impact the annotations of sentence pairs.", 
        "229": "The other suggestions concern more significant changes in 29 distinct sentences (mostly minor grammatical or semantic problems overlooked by the leader annotators).", 
        "230": "The annotations of pairs with modified sentences are resent to the annotators so that they can verify and update them.", 
        "231": "4 Corpus summary and evaluation    4.1 Corpus statistics  Tables 3 and 4 summarise the annotations of the resulting 10K sentence pairs corpus.", 
        "232": "Table 3 aggregates the occurrences of 6 possible relatedness scores, calculated as the mean of all 6 individual annotations, rounded to an integer.", 
        "233": "Table 4 shows the number of the particular entailment labels in the corpus.", 
        "234": "Since each sentence pair is annotated for entailment in both directions, the final entailment label is actually a pair of two labels:\n\u2022 entailment+neutral points to \u2018one-way\u2019 entailment,\n\u2022 contradiction+neutral points to \u2018one-way\u2019 contradiction,\n\u2022 entailment+entailment, contradiction+contradiction, and neutral+neutral point to equivalence.", 
        "235": "While the actual corpus labels are ordered in the sense that there is a difference between e.g.", 
        "236": "entailment+neutral and neutral+entailment (the entailment occurs in different directions), we treat all labels as unordered for the purpose of this summary (e.g.", 
        "237": "entailment+neutral covers neutral+entailment as well, representing the same type of relation between two sentences).", 
        "238": "4.2 Inter-annotator agreement  The standard measure of inter-annotator agreement in various natural language labelling tasks is Cohen\u2019s kappa (Cohen, 1960).", 
        "239": "However, this coefficient is designed to measure agreement between two annotators only.", 
        "240": "Since there are three annotators of each pair of ordered sentences, we decided to apply Fleiss\u2019 kappa13 (Fleiss, 1971) designed for measuring agreement between multiple raters who give categorical ratings to a fixed number of items.", 
        "241": "An additional advantage of this measure is that different items can be rated by different human judges, which doesn\u2019t impact measurement.", 
        "242": "The normalised Fleiss\u2019 measure of inter-annotator agreement is:\n\u03ba = P\u0304 \u2212 P\u0304e 1\u2212 P\u0304e\nwhere the quantity P\u0304 \u2212 P\u0304e measures the degree of agreement actually attained in excess of chance, while \u201c[t]he quantity 1 \u2212 P\u0304e measures the degree of agreement attainable over and above what would be predicted by chance\u201d (Fleiss, 1971, p. 379).", 
        "243": "We recognise Fleiss\u2019 kappa as particularly useful for measuring inter-annotator agreement with respect to entailment labelling in our evaluation dataset.", 
        "244": "First, there are more than two raters.", 
        "245": "Second, entailment labels are categorically.", 
        "246": "Measured\n13As Fleiss\u2019 kappa is actually the generalisation of Scott\u2019s \u03c0 (Scott, 1955), it is sometimes referred to as Fleiss\u2019 multi-\u03c0, cf.", 
        "247": "Artstein and Poesio (2008).", 
        "248": "with Fleiss\u2019 kappa, there is an inter-annotator agreement of \u03ba = 0.734 for entailment labels in Polish evaluation dataset, which is quite satisfactory as for a semantic labelling task.", 
        "249": "Relative to semantic relatedness, the distinction in meaning of two sentences made by human judges is often very subtle.", 
        "250": "This is also reflected in the inter-annotator agreement scores measured with Fleiss\u2019 kappa.", 
        "251": "Inter-annotator agreement measured for six semantic relatedness groups corresponding to points on the Likert scale is quite low: \u03ba = 0.337.", 
        "252": "If we measure interannotator agreement for three classes corresponding to the three relatedness groups from the annotation guidelines (see Section 3.1), i.e.", 
        "253": "<0>, <1, 2, 3, 4>, and <5>, the Fleiss\u2019 score is significantly higher: \u03ba = 0.543.", 
        "254": "Hence, we conclude that Fleiss\u2019 kappa is not a reliable measure of inter-annotator agreement in relation to relatedness scores.", 
        "255": "Therefore, we decided to use Krippendorff\u2019s \u03b1 instead.", 
        "256": "Krippendorff\u2019s \u03b1 (Krippendorff, 1980, 2013) is a coefficient appropriate for measuring the interannotator agreement of a dataset which is annotated with multiple judges and characterised by different magnitudes of disagreement and missing values.", 
        "257": "Krippendorff proposes distance metrics suitable for various scales: binary, nominal, interval, ordinal, and ratio.", 
        "258": "In ordinal measurement14 the attributes can be rank-ordered, but distances between them do not have any meaning.", 
        "259": "Measured with Krippendorff\u2019s ordinal \u03b1, there is an inter-annotator agreement of \u03b1 = 0.780 for relatedness scores in the Polish evaluation dataset, which is quite satisfactory as well.", 
        "260": "Hence, we conclude that our dataset is a reliable resource for the purpose of evaluating compositional distributional semantics model of Polish.", 
        "261": "5 Conclusions  The goal of this paper is to present the procedure of building a Polish evaluation dataset for the validation of compositional distributional semantics models.", 
        "262": "As we aim at building an evalua-\n14Nominal measurement is useless for measuring agreement between relatedness scores (\u03b1 = 0.340 is the identical value as Fleiss\u2019 kappa, since all disagreements are considered equal).", 
        "263": "We also test interval measurement, in which the distance between the attributes does have meaning and an average of an interval variable is computed.", 
        "264": "The interval score measured for relatedness annotations is quite high \u03b1 = 0.785, but we doubt whether the distance between relatedness scores is meaningful in this case.", 
        "265": "tion dataset which is comparable to the SICK corpus, the general assumptions of our procedure correspond to the design principles of the SICK corpus.", 
        "266": "However, the procedure of building the SICK corpus cannot be adapted without modifications.", 
        "267": "First, the Polish seed-sentences have to be written based on the images which are selected from 8K ImageFlickr dataset and split into thematic groups, since usable datasets are not publicly available.", 
        "268": "Second, since the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for Polish have to be defined from scratch.", 
        "269": "Third, the process of arranging Polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations.", 
        "270": "The discrepancies relative to the SICK procedure also concern the annotation process itself.", 
        "271": "Since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions.", 
        "272": "Furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections.", 
        "273": "The presented procedure of building a dataset was tested on Polish.", 
        "274": "However, it is very likely that the annotation framework will work for other Slavic languages (e.g.", 
        "275": "Czech with an excellent dependency parser).", 
        "276": "The presented procedure results in building the Polish test corpus of relatively high quality, confirmed by the inter-annotator agreement coefficients of \u03ba = 0.734 (measured with Fleiss\u2019 kappa) for entailment labels and of \u03b1 = 0.780 (measured with Krippendorff\u2019s ordinal alpha) for relatedness scores.", 
        "277": "Acknowledgments  We would like to thank the reliable and tenacious annotators of our dataset: Alicja DziedzicRawska, Boz\u0307ena Itoya, Magdalena Kr\u00f3l, Anna Latusek, Justyna Ma\u0142ek, Ma\u0142gorzata Michalik, Agnieszka Norwa, Ma\u0142gorzata Szajbel-Keck, Alicja Walichnowska, Konrad Zielin\u0301ski, and some other.", 
        "278": "The research presented in this paper was supported by SONATA 8 grant no 2014/15/D/HS2/03486 from the National Science Centre Poland."
    }, 
    "document_id": "P17-1073.pdf.json"
}
