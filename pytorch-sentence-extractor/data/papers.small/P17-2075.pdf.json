{
    "abstract_sentences": {
        "1": "In social media, demographic inference is a critical task in order to gain a better understanding of a cohort and to facilitate interacting with one\u2019s audience.", 
        "2": "Most previous work has made independence assumptions over topological, textual and label information on social networks.", 
        "3": "In this work, we employ recursive neural networks to break down these independence assumptions to obtain inference about demographic characteristics on Twitter.", 
        "4": "We show that our model performs better than existing models including the state-of-theart."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 471\u2013477 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2075  1 Introduction  Social media is a popular public platform for communicating, sharing information and expressing opinions.", 
        "2": "Millions of users discuss a variety of topics such as politics or sports.", 
        "3": "Valuable insights can be obtained by analysing social media content (e.g., mining consumer preferences), and, consequently, social media data is now a valuable resource.", 
        "4": "Accordingly, social media analytics have received much attention among researchers and companies (Wan and Paris, 2014; Valdes et al., 2015; Zubiaga et al., 2016).", 
        "5": "Inferring demographic characteristics from social media is a useful mechanism to gain a better understanding of a cohort and one\u2019s audience, and to facilitate interacting with that audience.", 
        "6": "Many researchers have studied ways to infer demographic attributes of Twitter users, such as age (Mislove et al., 2011; Mohammady Ardehaly and Culotta, 2015), gender (Filippova, 2012; Taniguchi et al., 2015), occupation (Preot\u0327iucPietro et al., 2015; Kim et al., 2016), location (Jurgens et al., 2015; Jayasinghe et al., 2016) or politi-\ncal preferences (Volkova et al., 2014; McCormick et al., 2015).", 
        "7": "A common approach to infer demographic characteristics is the use of supervised classifiers trained on textual features.", 
        "8": "The main limitation of this approach is that it makes little use of the network topology.", 
        "9": "Several network embedding methods have been proposed to learn distributed dense representations for vertices in graphs: DeepWalk (Perozzi et al., 2014) or LINE (Tang et al., 2015).", 
        "10": "While these two models can capture the topological structure of social networks, their performances are still limited, as the text features associated with vertices are not considered.", 
        "11": "For instance, text messages that Twitter users post, tweets, can offer great potential to enhance the vertex embeddings.", 
        "12": "Yang et al.", 
        "13": "(2015) proposed the Text-Associated DeepWalk (TADW) to enhance the discriminative power of the vertex embeddings by incorporating the text information into the embedding generation process.", 
        "14": "Although this matrix factorisation framework is effective on the vertex classification task, it can produce suboptimal embeddings.", 
        "15": "This is because the label information is not exploited in the unsupervised framework.", 
        "16": "More recently, Pan et al.", 
        "17": "(2016) proposed the TriParty Deep Network Representation (TriDNR), a method that incorporates the label information in addition to the text and topological information.", 
        "18": "However, in TriDNR, the text in a vertex is assumed to be independent of neighbour vertices.", 
        "19": "Furthermore, the two optimisation problems (learning vertex embeddings and training discriminative classifiers) are tackled separately.", 
        "20": "This is also true of TADW.", 
        "21": "In this paper, we tackle the problem of inferring demographic characteristics from social networks as a vertex classification task on graphs.", 
        "22": "We employ recursive neural networks (RNNs) to infer three demographic attributes of Twitter users\n471\n(age, gender and user type) based on network topology, text content and label information.", 
        "23": "Our model breaks down the independence assumption by leveraging RNNs on paths in graphs.", 
        "24": "We show that our model achieves better performance compared to existing models including the state-ofthe-art.", 
        "25": "While high performance is achieved using solely neural network models, more importantly, we find that different demographic inference tasks benefit to varying the topological size of RNNs.", 
        "26": "To our knowledge, there has been little previous work applying neural network based methods to the problem of inferring social media demographics.", 
        "27": "2 Graph Recursive Neural Networks  RNNs are deep learning models that recursively compose the vector of a parent unit from those of child units over a given structure in topological order (Pollack, 1990).", 
        "28": "They have shown to be very effective for various natural language processing (NLP) tasks, capturing syntactic and semantic composition (Socher et al., 2011; Qian et al., 2015).", 
        "29": "In this section, we describe the framework of Graph RNNs (GRNNs) (Xu et al., 2017) to classify the vertices of a graph.", 
        "30": "This framework allows us to infer the demographic characteristics of social media users.", 
        "31": "We formally define the problem of Twitter vertex classification as follows: A Twitter social network is defined as G = (V,E), where V is the set of vertices (users) and E is the set of edges (relationships) between the vertices.", 
        "32": "Each edge e \u2208 E is an ordered pair e = (vi, vj), where vi, vj \u2208 V , that is unweighted1 (wij = 0) but di-\n1While the edges in the Twitter social network can be weighted (e.g., based on the importance of relationship), we only take into account an unweighted social network in this study.", 
        "33": "rected ((i, j) 6\u2261 (j, i)).", 
        "34": "Each vi is associated with a pair of (xi, yi), where xi \u2208 X is a feature vector and yi \u2208 Y is a particular label that depends on xi.", 
        "35": "X and Y thus denote a set of feature vectors and a set of possible predicted labels in G, respectively.", 
        "36": "Our goal is then to predict the most likely label y\u0302t \u2208 Y for vt \u2208 V , which is the target vertex to be classified: y\u0302t = argmaxyt\u2208Y P\u03b8(yt|vt, G,X ) using a RNN with parameters \u03b8.\nGRNNs contain five components that will be presented in this section: 1) Graph-to-Tree Conversion, 2) Word Embedding layer, 3) Recursive Neural Unit layer and 4) Softmax Output layer, as illustrated in Figure 1.", 
        "37": "2.1 Graph-to-Tree Conversion  A graph is converted to tree structures before constructing a RNN for each tree.", 
        "38": "Specifically, we construct a tree T = (VT , ET ) of depth d rooted at vt using a breadth-first search algorithm from G. VT and ET are the sets of vertices and edges in the tree.", 
        "39": "(v, w) \u2208 ET denotes an edge from a parent vertex v to a child vertex w.  2.2 Word Embeddings  Let Si = {w1, w2, ..., wR} be texts (e.g., tweets) consisting of R words, which are associated with a vertex vi.", 
        "40": "Every word wr is converted into a real-valued vector er by looking up the embedding matrix E \u2208 Rdw|V |, where dw is the size of word embedding and |V | is a vocabulary size.", 
        "41": "The matrix E is initialised using the Skip-gram model (Mikolov et al., 2013).", 
        "42": "Si is then fed into the next layer as a real-valued feature vector xi = {e1, e2, ..., eR}.", 
        "43": "2.3 Recursive Neural Units  Once we construct a tree from a graph, we build a RNN using one of two types of recursive neural units (RNUs) for each vertex vk \u2208 T : Naive\nRecursive Neural Unit (NRNU) and Long ShortTerm Memory Unit (LSTMU).", 
        "44": "2.3.1 Graph Naive Recursive Neural Net  Each NRNU for a vertex vk take its feature vector xk and a hidden state h\u0303k as input.", 
        "45": "Max pooling produces h\u0303k from all hidden state vectors hr of the child vertices vr of vk.2 A hidden state vector hk of vk is obtained using weight matrices, followed by a non-linear function tanh:\nh\u0303k = max vr\u2208C(vk)\n{hr}\nhk = tanh(W (h)xk + U (h)h\u0303k + b (h))\nwhere C(vk) is the set of child vertices of vk (vr \u2208 C(vk)) and hr is a hidden state of a child vertex vr.", 
        "46": "W (h) and U (h) are weight matrices, and b(h) is a bias vector for model parameters.", 
        "47": "In this paper, we refer to Graph Naive Recursive Neural Network as GNRNN incorporating NRNU as a RNU.", 
        "48": "2.3.2 Graph Long Short-Term Memory Net  LSTMU (Hochreiter and Schmidhuber, 1997) was originally proposed to tackle a sequential labelling problem and it is able to model long-range dependencies by incorporating gated memory cells.", 
        "49": "At each time step, LSTMU takes the sequential input vector and the previous hidden state vector to produce the next hidden state.", 
        "50": "In this study, LSTMU is employed as a RNU to represent a vertex in a tree, and it naturally captures the relationships between vertices.", 
        "51": "For a vertex vk, LSTMU takes xk and h\u0303k as input, and generates the input, forget and output gate signals, denoted as ik, fk and ok respectively.", 
        "52": "It produces a memory cell state ck and hidden state hk with respect to a vertex vk:\nh\u0303k = max vr\u2208C(vk)\n{hr}\nc\u0303k = tanh(W (c)xk + U (c)h\u0303k + b (c)) ik = \u03c3(W (i)xk + U (i)h\u0303k + b (i)) fkr = \u03c3(W (f)xk + U (f)hr + b (f)) ok = \u03c3(W (o)xk + U (o)h\u0303k + b (o)) ck = ik c\u0303k + \u2211\nvr\u2208C(vk) fkr cr\nhk = ok tanh(ck)\nwhere refers to element-wise product and \u03c3 indicates the sigmoid function.", 
        "53": "W (\u2217), U (\u2217) and b(\u2217)\n2Our preliminary results demonstrated that the max pooling strategy achieved better performance than sum and average poolings.", 
        "54": "are LSTM parameters.", 
        "55": "We call a tree-structured network topology consisting of LSTMUs as Graph Long Short-Term Memory Net (GLSTMN).", 
        "56": "2.4 Softmax Output  At the end, the hidden state ht is fed into a softmax classifier to predict the label yt of the target vertex vt after calculating the hidden states of all vertices in T : P\u03b8(yt|vt, G,X ) = softmax(W (s)ht+b(s)) y\u0302t = argmaxyt\u2208Y P\u03b8(yt|vt, G,X ).", 
        "57": "3 Experimental Setup  In this section, we provide an overview of datasets and the models that are evaluated in the experiments.", 
        "58": "3.1 Datasets  We evaluate the effectiveness of our model on three types of social networks: gender, age and user type classification.", 
        "59": "Twitter users follow others or are followed, and two types of relationships are used to build the social networks: friend and follower.", 
        "60": "A user is associated with others via the following relationship, the user\u2019s friend in Twitter\u2019s terminology.", 
        "61": "Follower relationships indicate that a user receives all the tweets from those the user follows.", 
        "62": "\u2022 Gender (Volkova, 2014) is a Twitter social network encoding friend relationships between users.", 
        "63": "The labels of this network are Male and Female.", 
        "64": "\u2022 Age (Volkova, 2014) is a Twitter social network encoding friend relationships between users.", 
        "65": "The labels of this network are Young (18-23 years old) and Old (25-30 years old).", 
        "66": "\u2022 UserType (Kim et al., 2017) is a Twitter social network encoding follower relationships between users.", 
        "67": "The labels represent the types of Twitter users: Individual, Organisation and other.", 
        "68": "To generate text features of vertices, up to 1K tweets per user are used in Gender and Age, whereas Twitter user profile descriptions are used in UserType.", 
        "69": "All words are stemmed, and then stop words and words with document frequency less than 10 are removed.", 
        "70": "The statistics of the datasets are summarised in Table 1.", 
        "71": "3.2 Evaluated Models  We compare the GRNN model with several existing models to assess vertex classification performance.", 
        "72": "\u2022 Lexica (LX) (Sap et al., 2014): a lexiconbased method produced from Twitter to calculate the scores of gender and age.", 
        "73": "These scores are used to predict their labels for users.", 
        "74": "\u2022 Logistic Regression (LR) (Hosmer Jr et al., 2013): a commonly used linear model in the NLP community, only using textual contents in vertices.", 
        "75": "Bag-of-words feature vectors are generated without incorporating any topological information of a network.", 
        "76": "\u2022 Label Propagation (LP) (Wang and Zhang, 2006): a graph-based semi-supervised learning model, where label probabilities are propagated to all unlabelled neighbours.", 
        "77": "The probability derivation steps are terminated for the remaining vertices when all label probabilities converge.", 
        "78": "\u2022 Text-Associated DeepWalk (TADW) (Yang et al., 2015): an unsupervised vertex embedding learning method.", 
        "79": "Low-dimensional representations of vertices are induced both from their texts and graph relationships based on inductive matrix factorisation.", 
        "80": "\u2022 Tri-Party Deep Network Representation (TriDNR) (Pan et al., 2016): two neural networks incorporating the texts, relationships and labels of vertices in graphs.", 
        "81": "As in TADW, unlabelled vertices are classified using Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) trained on learned vertex embeddings.", 
        "82": "3.3 Experiment Settings  In our experiments, we follow the standard experimental protocol for vertex classification task.", 
        "83": "More specifically, we evaluate classification ac-\ncuracy3 with different training ratios, increasing from 20% to 80%.", 
        "84": "For each training ratio, we randomly generate 5 different training datasets.", 
        "85": "For each training dataset, we run 10 trials and record the highest accuracy on each testing dataset.", 
        "86": "We then report the average accuracy for the same ratio of training datasets.", 
        "87": "We test six different model architectures using NRNU and LSTMU with three different tree depths (d = 0 , d = 1 , d = 2 ).", 
        "88": "The tree depth corresponds to the number of hops between users in a social network.", 
        "89": "For all the GRNN models, the size of the hidden units is set to 200.", 
        "90": "We use Adagrad (Duchi et al., 2011) with a batch size of 20 as the optimisation method that automatically adapts the learning rate in training.", 
        "91": "The initial learning rate is set to 0.1 for LR and LP, and 0.01 for all GRNNs.", 
        "92": "4 Results and Analysis  In this section, we present the experimental results and analysis on vertex classification for the three\n3We report accuracy following previous work for Age and Gender datasets, consisting of balanced label distributions.", 
        "93": "Although UserType is imbalanced, accuracy is reported in this paper for simplicity and consistency.", 
        "94": "Previous work also reports classification accuracy for this task.", 
        "95": "networks.", 
        "96": "Numbers in bold represent the highest performance in each column in all tables.", 
        "97": "As shown in Table 2, GNRNN and GLSTMN perform better than the evaluated models for the task of gender prediction, with no performance gain when the depth of the tree is increased.", 
        "98": "Namely, GLSTMN d0 is the best performing model for this task.", 
        "99": "Table 3 shows that GLSTMN also achieves the best performance for age prediction.", 
        "100": "For this task, however, performance increases with the depth of the tree.", 
        "101": "For the task of user type classification, Table 4 shows that we attain the best performance with tree depth d = 1 for GLSTMN for the training ratio of 80%.", 
        "102": "Note that we could not train the GRNN models with tree depth 2 (GNRNN d2 and GLSTMN d2) on the UserType dataset on our Linux server with 64G memory due to the lack of memory.", 
        "103": "As shown in Table 1, the average degree of UserType is roughly 10 times larger than that of Gender or Age.", 
        "104": "The degree of a vertex in a graph indicates the number of edges connected to adjacent vertices.", 
        "105": "It means that the GRNN models could have approximately 625 (= 25 \u00d7 25) vertices in average for the tree depth of 2.", 
        "106": "Our experiments show that a dense graph consisting of high degree vertices is intractable under the GRNN model.", 
        "107": "Interestingly, LR and LP are effective methods for Gender and Age compared to TADW and TriDNR.", 
        "108": "In particular, TADW performs poorly on Gender, and TriDNR marginally outperforms GLSTMN on the dense social network, UserType.", 
        "109": "To summarise, we observed four findings:\n1.", 
        "110": "The GRNN models (GNRNN and GLSTMN) overall outperform the existing models by a noticeable margin in most cases (except for UserType), showing the benefit of RNNs for social network inferences on vertices.", 
        "111": "2.", 
        "112": "The LSTM unit in GLSTMN gives superior\nperformance over the NRN unit in GNRNN regardless of datasets.", 
        "113": "Our results are in line with other findings, showing that the LSTM works consistently better than the standard recurrent neural network.", 
        "114": "3.", 
        "115": "GRNNs have different optimal tree depths for each demographic inference task.", 
        "116": "Tree depth does not improve inference performance for Gender, indicating that only text information is sufficient without incorporating network information.", 
        "117": "For age, tree depth allows GRNNs to be more effective although its increase does not consistently lead to better performance for GLSTMN.", 
        "118": "Similarly, tree depth increases the performance of GRNNs for UserType.", 
        "119": "We hypothesise this may be related to the nature of social interactions (e.g., Twitter users in the similar age group are more likely to interact).", 
        "120": "4.", 
        "121": "The matrix factorisation-based methods (TADW and TriDNR) relatively work well on datasets having high degree vertices, whereas the GRNN models achieve relatively good performance on graphs containing low degree vertices.", 
        "122": "5 Conclusions and Future Work  In this paper we tackled the demographic inference problem on Twitter as vertex classification on a graph using GRNNs, demonstrating their effectiveness against strong models for selected datasets.", 
        "123": "The RNN framework provides an effective way to incorporate network, text and label information for Twitter demographic inference.", 
        "124": "However, different demographic inference tasks benefit to varying the tree depth of GRNN models.", 
        "125": "As our future work, we plan to employ other state-of-the-art deep learning models (Yang et al., 2016; Kipf and Welling, 2017) that vary in the nature of the dependency between network, text and label information for demographic inference to confirm the effectiveness of our proposals.", 
        "126": "Acknowledgments  The authors are grateful to three anonymous ACL reviewers.", 
        "127": "We would also like to thank Brian Jin, who ran the data collection for this study."
    }, 
    "document_id": "P17-2075.pdf.json"
}
