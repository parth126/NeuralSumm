{
    "abstract_sentences": {
        "1": "The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself.", 
        "2": "Stancetaking underlies a wide range of interactional phenomena, relating to formality, politeness, affect, and subjectivity.", 
        "3": "We present a computational approach to stancetaking, in which we build a theoretically-motivated lexicon of stance markers, and then use multidimensional analysis to identify a set of underlying stance dimensions.", 
        "4": "We validate these dimensions intrinsically and extrinsically, showing that they are internally coherent, match pre-registered hypotheses, and correlate with social phenomena."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 884\u2013895 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1082  1 Introduction  What does it mean to be welcoming or standoffish, light-hearted or cynical?", 
        "2": "Such interactional styles are performed primarily with language, yet little is known about how linguistic resources are arrayed to create these social impressions.", 
        "3": "The sociolinguistic concept of interpersonal stancetaking attempts to answer this question, by providing a conceptual framework that accounts for a range of interpersonal phenomena, subsuming formality, politeness, and subjectivity (Du Bois, 2007).1 This\n1Stancetaking is distinct from the notion of stance which corresponds to a position in a debate (Walker et al., 2012).", 
        "4": "Similarly, Freeman et al.", 
        "5": "(2014) correlate phonetic features with the strength of such argumentative stances.", 
        "6": "framework has been applied almost exclusively through qualitative methods, using close readings of individual texts or dialogs to uncover how language is used to position individuals with respect to their interlocutors and readers.", 
        "7": "We attempt the first large-scale operationalization of stancetaking through computational methods.", 
        "8": "Du Bois (2007) formalizes stancetaking as a multi-dimensional construct, reflecting the relationship of discourse participants to (a) the audience or interlocutor; (b) the topic of discourse; (c) the talk or text itself.", 
        "9": "However, the multidimensional nature of stancetaking poses problems for traditional computational approaches, in which labeled data is obtained by relying on annotator intuitions about scalar concepts such politeness (Danescu-Niculescu-Mizil et al., 2013) and formality (Pavlick and Tetreault, 2016).", 
        "10": "Instead, our approach is based on a theoretically-guided application of unsupervised learning, in the form of factor analysis, applied to lexical features.", 
        "11": "Stancetaking is characterized in large part by an array of linguistic features ranging from discourse markers such as actually to backchannels such as yep (Kiesling, 2009).", 
        "12": "We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and Finegan (1989) and the Switchboard Dialogue Act Corpus (Jurafsky et al., 1998).", 
        "13": "We then extend this lexicon to the social media domain using word embeddings.", 
        "14": "Finally, we apply multi-dimensional analysis of co-occurrence patterns to identify a small set of stance dimensions.", 
        "15": "To measure the internal coherence (construct validity) of the stance dimensions, we use a word\n884\nintrusion task (Chang et al., 2009) and a set of preregistered hypotheses.", 
        "16": "To measure the utility of the stance dimensions, we perform a series of extrinsic evaluations.", 
        "17": "A predictive evaluation shows that the membership of online communities is determined in part by the interactional stances that predominate in those communities.", 
        "18": "Furthermore, the induced stance dimensions are shown to align with annotations of politeness and formality.", 
        "19": "Contributions We operationalize the sociolinguistic concept of stancetaking as a multidimensional framework, making it possible to measure at scale.", 
        "20": "Specifically,\n\u2022 we contribute a lexicon of stance markers based on prior work and adapted to the genre of online interpersonal discourse;\n\u2022 we group stance markers into latent dimensions; \u2022 we show that these stance dimensions are inter-\nnally coherent;\n\u2022 we demonstrate that the stance dimensions predict and correlate with social phenomena.2  2 Related Work  From a theoretical perspective, we build on prior work on interactional meaning in language.", 
        "21": "Methodologically, our paper relates to prior work on lexicon-based analysis and contrastive studies of social media communities.", 
        "22": "2.1 Linguistic Variation and Social Meaning  In computational sociolinguistics (Nguyen et al., 2016), language variation has been studied primarily in connection with macro-scale social variables, such as age (Argamon et al., 2007; Nguyen et al., 2013), gender (Burger et al., 2011; Bamman et al., 2014), race (Eisenstein et al., 2011; Blodgett et al., 2016), and geography (Eisenstein et al., 2010).", 
        "23": "This parallels what Eckert (2012) has called the \u201cfirst wave\u201d of language variation studies in sociolinguistics, which also focused on macro-scale variables.", 
        "24": "More recently, sociolinguists have dedicated increased attention to situational and stylistic variation, and the interactional meaning that such variation can convey (Eckert and Rickford, 2001).", 
        "25": "This linguistic research can be aligned with computational efforts to quantify phenomena such\n2Lexicons and stance dimensions are available at https://github.com/umashanthi-research/ multidimensional-stance-lexicon\nas subjectivity (Riloff and Wiebe, 2003), sentiment (Wiebe et al., 2005), politeness (DanescuNiculescu-Mizil et al., 2013), formality (Pavlick and Tetreault, 2016), and power dynamics (Prabhakaran et al., 2012).", 
        "26": "While linguistic research on interactional meaning has focused largely on qualitative methodologies such as discourse analysis (e.g., Bucholtz and Hall, 2005), these computational efforts have made use of crowdsourced annotations to build large datasets of, for example, polite and impolite text.", 
        "27": "These annotation efforts draw on the annotators\u2019 intuitions about the meaning of these sociolinguistic constructs.", 
        "28": "Interpersonal stancetaking represents an attempt to unify concepts such as sentiment, politeness, formality, and subjectivity under a single theoretical framework (Jaffe, 2009; Kiesling, 2009).", 
        "29": "The key idea, as articulated by Du Bois (2007), is that stancetaking captures the speaker\u2019s relationship to (a) the topic of discussion, (b) the interlocutor or audience, and (c) the talk (or writing) itself.", 
        "30": "Various configurations of these three legs of the \u201cstance triangle\u201d can account for a range of phenomena.", 
        "31": "For example, epistemic stance relates to the speaker\u2019s certainty about what is being expressed, while affective stance indicates the speaker\u2019s emotional position with respect to the content (Ochs, 1993).", 
        "32": "The framework of stancetaking has been widely adopted in linguistics, particularly in the discourse analytic tradition, which involves close reading of individual texts or conversations (Ka\u0308rkka\u0308inen, 2006; Keisanen, 2007; Precht, 2003; White, 2003).", 
        "33": "But despite its strong theoretical foundation, we are aware of no prior efforts to operationalize stancetaking at scale.", 
        "34": "Since annotators may not have strong intuitions about stance \u2014 in the way that they do about formality and politeness \u2014 we cannot rely on the annotation methodologies employed in prior work.", 
        "35": "We take a different approach, performing a multidimensional analysis of the distribution of likely stance markers.", 
        "36": "2.2 Lexicon-based Analysis  Our operationalization of stancetaking is based on the induction of lexicons of stance markers.", 
        "37": "The lexicon-based methodology is related to earlier work from social psychology, such as the General Inquirer (Stone, 1966) and LIWC (Tausczik and Pennebaker, 2010).", 
        "38": "In LIWC, the basic categories were identified first, based on psychological\nconstructs (e.g., positive emotion, cognitive processes, drive to power) and syntactic groupings of words and phrases (e.g., pronouns, prepositions, quantifiers).", 
        "39": "The lexicon designers then manually contructed lexicons for each category, augmenting their intuitions by using distributional statistics to suggest words that may have been missed (Pennebaker et al., 2015).", 
        "40": "In contrast, we follow the approach of Biber (1991), using multidimensional analysis to identify latent groupings of markers based on co-occurrence statistics.", 
        "41": "We then use crowdsourcing and extrinsic comparisons to validate the coherence of these dimensions.", 
        "42": "2.3 Multicommunity Studies  Social media platforms such as Reddit, Stack Exchange, and Wikia can be considered multicommunity environments, in that they host multiple subcommunities with distinct social and linguistic properties.", 
        "43": "Such subcommunities can be contrasted in terms of topics (Adamic et al., 2008; Hessel et al., 2014) and social networks (Backstrom et al., 2006).", 
        "44": "Our work focuses on Reddit, emphasizing community-wide differences in norms for interpersonal interaction.", 
        "45": "In the same vein, Tan and Lee (2015) attempt to characterize stylistic differences across subreddits by focusing on very common words and parts-of-speech; Tran and Ostendorf (2016) use language models and topic models to measure similarity across threads within a subreddit.", 
        "46": "One distinction of our approach is that the use of multidimensional analysis gives us interpretable dimensions of variation.", 
        "47": "This makes it possible to identify the specific interpersonal features that vary across communities.", 
        "48": "3 Data  Reddit, one of the internet\u2019s largest social media platforms, is a collection of subreddits organized around various topics of interest.", 
        "49": "As of January 2017, there were more than one million subreddits and nearly 250 million users, discussing topics ranging from politics (r/politics) to horror stories (r/nosleep).3 Although Reddit was originally designed for sharing hyperlinks, it also provides the ability to post original textual content, submit comments, and vote on content quality (Gilbert, 2013).", 
        "50": "Reddit\u2019s conversation-like threads are therefore well suited for the study of interpersonal social and linguistic phenomena.", 
        "51": "3http://redditmetrics.com/\nFor example, the following are two comments from the subreddit r/malefashionadvice, posted in response to a picture posted by a user asking for fashion advise.", 
        "52": "U1: \u201cI think the beard looks pretty good.", 
        "53": "Definitely not the goatee.", 
        "54": "Clean shaven is always the safe option.\u201d\nU2: \u201cDefinitely the beard.", 
        "55": "But keep it trimmed.\u201d\nThe phrases in bold face are markers of stance, indicating a evaluative stance.", 
        "56": "The following example is a part of a thread in the subreddit r/photoshopbattles where users discuss an edited image posted by the original poster OP.", 
        "57": "The phrases in bold face are markers of stance, indicating an involved and interactional stance.", 
        "58": "U3: \u201cHa ha awesome!\u201d U4: \u2018\u2018are those..... furries?\u201d\nOP: \u201cyes, sir.", 
        "59": "They are!\u201d U4: \u201cOh cool.", 
        "60": "That makes sense!\u201d\nWe used an archive of 530 million comments posted on Reddit in 2014, retrieved from the public archive of Reddit comments.4 This dataset consists of each post\u2019s textual content, along with metadata that identifies the subreddit, thread, author, and post creation time.", 
        "61": "More statistics about the full dataset are shown in Table 1.", 
        "62": "4 Stance Lexicon  Interpersonal stancetaking can be characterized in part by an array of linguistic features such as hedges (e.g., might, kind of ), discourse markers (e.g., actually, I mean), and backchannels (e.g., yep, um).", 
        "63": "Our analysis focuses on these markers, which we collect into a lexicon.", 
        "64": "4.1 Seed lexicon  We began with a seed lexicon of stance markers from Biber and Finegan (1989), who compiled an\n4https://archive.org/details/2015_ reddit_comments_corpus\nextensive list by surveying dictionaries, previous studies on stance, and texts in several genres of English.", 
        "65": "This list includes certainty adverbs (e.g., actually, of course, in fact), affect markers (e.g., amazing, thankful, sadly), and hedges (e.g., kind of, maybe, something like) among other adverbial, adjectival, verbal, and modal markers of stance.", 
        "66": "In total, this list consists of 448 stance markers.", 
        "67": "The Biber and Finegan (1989) lexicon is primarily based on written genres from the pre-social media era.", 
        "68": "Our dataset \u2014 like much of the recent work in this domain \u2014 consists of online discussions, which differ significantly from printed texts (Eisenstein, 2013).", 
        "69": "One difference is that online discussions contain a number of dialog act markers that are characteristic of spoken language, such as oh yeah, nah, wow.", 
        "70": "We accounted for this by adding 74 dialog act markers from the Switchboard Dialog Act Corpus (Jurafsky et al., 1998).", 
        "71": "The final seed lexicon consists of 517 unique markers, from these two sources.", 
        "72": "Note that the seed lexicon also includes markers that contain multiple tokens (e.g.", 
        "73": "kind of, I know).", 
        "74": "4.2 Lexicon expansion  Online discussions differ not only from written texts, but also from spoken discussions, due to their use of non-standard vocabulary and spellings.", 
        "75": "To measure stance accurately, these genre differences must be accounted for.", 
        "76": "We therefore expanded the seed lexicon using automated techniques based on distributional statistics.", 
        "77": "This is similar to prior work on the expansion of sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Hamilton et al., 2016).", 
        "78": "Our lexicon expansion approach used word embeddings to find words that are distributionally similar to those in the seed set.", 
        "79": "We trained word embeddings on a corpus of 25 million Reddit comments and a vocabulary of 100K most frequent words on Reddit using the structured skip-gram models of both WORD2VEC (Mikolov et al., 2013) and WANG2VEC (Ling et al., 2015) with default parameters.", 
        "80": "The WANG2VEC method augments WORD2VEC by accounting for word order information.", 
        "81": "We found the similarity judgments obtained from WANG2VEC to be qualitatively more meaningful, so we used these embeddings to construct the expanded lexicon.5\n5We used the following default parameters: 100 dimensions, a window size of five, a negative sampling size of ten, five-epoch iterations, and a sub-sampling rate of 10\u22124.", 
        "82": "To perform lexicon expansion, we constructed a dictionary of candidate terms, consisting of all unigrams that occur with a frequency rate of at least 10\u22127 in the Reddit comment corpus.", 
        "83": "Then, for each single-token marker in the seed lexicon, we identified all terms from the candidate set whose embedding has cosine similarity of at least 0.75 with respect to the seed marker.6 Table 2 shows examples of seed markers and related terms we extracted from word embeddings.", 
        "84": "Through this procedure, we identified 228 additional markers based on similarity to items in the seed list from Biber and Finegan (1989), and 112 additional markers based on the seed list of dialog acts.", 
        "85": "In total, our stance lexicon contains 812 unique markers.", 
        "86": "5 Linguistic Dimensions of Stancetaking  To summarize the main axes of variation across the lexicon of stance markers, we apply a multidimensional analysis (Biber, 1992) to the distributional statistics of stance markers across subreddit communities.", 
        "87": "Each dimension of variation can then be viewed as a spectrum, characterized by the stance markers and subreddits that are associated with the positive and negative extremes.", 
        "88": "Multidimensional analysis is based on singular value decomposition, which has been applied successfully to a wide range of problems in natural language processing and information retrieval (e.g., Landauer et al., 1998).", 
        "89": "While Bayesian topic models are an appealing alternative, singular value decomposition is fast and deterministic, with a minimal number of tuning parameters.", 
        "90": "6We tried different thresholds on the similarity value and the corpus frequency, and the reported values were chosen based on the quality of the resulting related terms.", 
        "91": "This was done prior to any of the validations or extrinsic analyses described later in the paper.", 
        "92": "5.1 Extracting Stance Dimensions  Our analysis is based on the co-occurrence of stance markers and subreddits.", 
        "93": "This is motivated by our interest in comparisons of the interactional styles of online communities within Reddit, and by the premise that these distributional differences reflect socially meaningful communicative norms.", 
        "94": "A pilot study applied the same technique to the cooccurrence of stance markers and individual authors, and the resulting dimensions appeared to be less stylistically coherent.", 
        "95": "Singular value decomposition is often used in combination with a transformation of the cooccurrence counts by pointwise mutual information (Bullinaria and Levy, 2007).", 
        "96": "This transformation ensures that each cell in the matrix indicates how much more likely a stance marker is to cooccur with a given subreddit than would happen by chance under an independence assumption.", 
        "97": "Because negative PMI values tend to be unreliable, we use positive PMI (PPMI), which involves replacing all negative PMI values with zeros (Niwa and Nitta, 1994).", 
        "98": "Therefore, we obtain stance dimensions by applying singular value decomposition to the matrix constructed as follows:\nXm,s =\n( log Pr(marker = m, subreddit = s)\nPr(marker = m) Pr(subreddit = s)\n)\n+\n.", 
        "99": "Truncated singular value decomposition performs the approximate factorization X \u2248 U\u03a3V >, where each row of the matrix U is a k-dimensional description of each stance marker, and each row of V is a k-dimensional description of each subreddit.", 
        "100": "We included the 7,589 subreddits that received at least 1,000 comments in 2014.", 
        "101": "5.2 Results: Stance Dimensions  From the SVD analysis, we extracted the six principal latent dimensions that explain the most variation in our dataset.7 The decision to include only the first six dimensions was based on the strength of the singular values corresponding to the dimensions.", 
        "102": "Table 3 shows the top five stance markers for each extreme of the six dimensions.", 
        "103": "The stance dimensions convey a range of concepts, such as involved versus informational language, narrative\n7Similar to factor analysis, the top few dimensions of SVD explain the most variation, and tend to be most interpretable.", 
        "104": "A scree plot (Cattell, 1966) showed that the amount of variation explained dropped after the top six dimensions, and qualitative interpretation showed that the remaining dimension were less interpretable.", 
        "105": "versus dialogue-oriented writing, standard versus non-standard variation, and positive versus negative affect.", 
        "106": "Figure 1 shows the distribution of subreddits along two of these dimensions.", 
        "107": "6 Construct Validity  Evaluating model output against gold-standard annotations is appropriate when there is some notion of a correct answer.", 
        "108": "As stancetaking is a multidimensional concept, we have taken an unsupervised approach.", 
        "109": "Therefore, we use evaluation techniques based on the notion of validity, which is the extent to which the operationalization of a construct truly captures the intended quantity or concept.", 
        "110": "Validation techniques for unsupervised content analysis are widely found in the social science literature (Weber, 1990; Quinn et al., 2010) and have also been recently used in the NLP and machine learning communities (e.g., Chang et al., 2009; Murphy et al., 2012; Sim et al., 2013).", 
        "111": "We used several methods to validate the stance dimensions extracted from the corpus of Reddit comments.", 
        "112": "This section describes intrinsic evaluations, which test whether the extracted stance dimensions are linguistically coherent and mean-\ningful, thereby testing the construct or content validity of the proposed stance dimensions (Quinn et al., 2010).", 
        "113": "Extrinsic evaluations are presented in section 7.", 
        "114": "6.1 Word Intrusion Task  A word intrusion task is used to measure the coherence and interpretability of a group of words.", 
        "115": "Human raters are presented with a list of terms, all but one of which are selected from a target concept; their task is to identify the intruder.", 
        "116": "If the target concept is internally coherent, human raters should be able to perform this task accurately; if not, their selections should be random.", 
        "117": "Word intrusion tasks have previously been used to validate the interpretability of topic models (Chang et al., 2009) and vector space models (Murphy et al., 2012).", 
        "118": "We deployed a word intrusion task on Amazon Mechanical Turk (AMT), in which we presented the top four stance markers from one end of a dimension, along with an intruder marker selected from the top four markers of the opposite end of that dimension.", 
        "119": "In this way, we created four word intrusion tasks for each end of each dimension.", 
        "120": "The main reason for including only the top four words in each dimension is the expense of conducting crowd-sourced evaluations.", 
        "121": "In the most relevant prior work, Chang et al.", 
        "122": "(2009) used the top five words from each topic in their evaluation of topic models.", 
        "123": "Worker selection We required that the AMT workers (\u201cturkers\u201d) have completed a minimum of 1,000 HITs and have at least 95% approval rate\nFurthermore, because our task is based on analysis of English language texts, we required the turkers to be native speakers of English living in one of the majority English speaking countries.", 
        "124": "As a further requirement, we required the turkers to obtain a qualification which involves an English comprehension test similar to the questions in standardized English language tests.", 
        "125": "These requirements are based on best practices identified by CallisonBurch and Dredze (2010).", 
        "126": "Task specification Each AMT human intelligence task (HIT) consists of twelve word intrusion tasks, one for each end of the six dimensions.", 
        "127": "We provided minimal instructions regarding the task, and did not provide any examples, to avoid introducing bias.8 As a further quality control, each HIT included three questions which ask the turkers to pick the best synonym for a given word from a list of five answers, where one answer was clearly correct; Turkers who gave incorrect answers were to be excluded, but this situation did not arise in practice.", 
        "128": "Altogether each HIT consists of 15 questions, and was paid US$1.50.", 
        "129": "Five different turkers performed each HIT.", 
        "130": "Results We measured the interrater reliability using Krippendorf\u2019s \u03b1 (Krippendorff, 2007) and the model precision metric of Chang et al.", 
        "131": "(2009).", 
        "132": "Results on both metrics were encouraging.", 
        "133": "We obtained a value of \u03b1 = 0.73, on a scale where\n8The prompt for the word intrusions task was: \u201cSelect the intruder word/phrase: you will be given a list of five English words/phrases and asked to pick the word/phrase that is least similar to the other four words/phrases when used in online discussion forums.\u201d\n\u03b1 = 0 indicates chance agreement and \u03b1 = 1 indicates perfect agreement.", 
        "134": "The model precision was 0.82; chance precision is 0.20.", 
        "135": "To offer a sense of typical values for this metric, Chang et al.", 
        "136": "(2009) report model precisions in the range 0.7\u20130.83 in their analysis of topic models.", 
        "137": "Overall, these results indicate that the multi-dimensional analysis has succeeded at identifying dimensions that reflect natural groupings of stance markers.", 
        "138": "6.2 Pre-registered Hypotheses  Content validity was also assessed using a set of pre-registered hypotheses.", 
        "139": "The practice of preregistering hypotheses before an analysis and testing the correctness is widely used in the social sciences; it was adopted by Sim et al.", 
        "140": "(2013) to evaluate the induction of political ideological models from text.", 
        "141": "Before performing the mutidimensional analysis, we identified two groups of hypotheses that are expected to hold with respect to the latent stancetaking dimensions using our prior linguistic knowledge:\n\u2022 Hypothesis I: Stance markers that are synonyms should not appear on the opposite ends of a stance dimension.", 
        "142": "\u2022 Hypothesis II: If at least one stance marker\nfrom a predefined stance feature group (defined below) appears on one end of a stance dimension, then other markers from the same feature group will tend not to appear at the opposite end of the same dimension.", 
        "143": "6.2.1 Synonym Pairs  For each marker in our stance lexicon, we extracted synonyms from Wordnet, focusing on markers that appear in only one Wordnet synset, and not including pairs in which one term was an inflection of the other.9 Our final list contains 73 synonym pairs (e.g., eventually/finally, grateful/thankful, yea/yeah).", 
        "144": "Of these pairs, there were 59 cases in which both terms appeared in either the top or bottom 200 positions of a stance dimension.", 
        "145": "In 51 of these cases (86%), the two terms appeared on the same side of the dimension.", 
        "146": "The chance rate would be 50%, so this supports Hypothesis I and\n9It is possible that inflections are semantically similar, because by definition they are changes in the form of a word to mark distinctions such as tense, person, or number.", 
        "147": "However, different inflections of a single word form might be used to mark different stances (e.g., some stances might be associated with the past while others might be associated with the present or future).", 
        "148": "further validates the stance dimensions.", 
        "149": "More details of the results are shown in Table 4.", 
        "150": "Note that synonym pairs may differ in aspects such as formality (e.g., said/informed, want/desire), which is one of the main dimensions of stancetaking.", 
        "151": "Therefore, perfect support for Hypothesis I is not expected.", 
        "152": "6.2.2 Stance Feature Groups  Biber and Finegan (1989) group stance markers into twelve \u201cfeature groups\u201d, such as certainty adverbs, doubt adverbs, affect expressions, and hedges.", 
        "153": "Ideally, the stance dimensions should preserve these groupings.", 
        "154": "To test this, for each of the seven feature groups with at least ten stance markers in the lexicon, we counted the number of terms appearing among the top 200 positions in both ends (high/low) of each dimension.", 
        "155": "Under the null hypothesis, the stance dimensions are random with respect to the feature groups, so we would expect roughly an equal number of markers on both ends.", 
        "156": "As shown in Table 5, for five of the seven feature groups, it is possible to reject the null hypothesis at p < .007, which is the significance threshold at \u03b1 = 0.05, after correcting for multiple comparisons using the Bonferroni correction.", 
        "157": "This indicates that the stance dimensions are aligned with predefined stance feature groups.", 
        "158": "7 Extrinsic Evaluations  The evaluations in the previous section test internal validity; we now describe evaluations testing whether the stance dimensions are relevant to external social and interactional phenomena.", 
        "159": "7.1 Predicting Cross-posting  Online communities can be considered as communities of practice (Eckert and McConnell-Ginet, 1992), where members come together to engage in shared linguistic practices.", 
        "160": "These practices\nevolve simultaneously with membership, coalescing into shared norms.", 
        "161": "The memberships of multiple subreddits on the same topic (e.g., r/science and r/askscience) often do not overlap considerably.", 
        "162": "Therefore we hypothesize that users of Reddit have preferred interactional styles, and that participation in subreddit communities is governed not only by topic interest, but also by these interactional preferences.", 
        "163": "The proposed stancetaking dimensions provide a simple measure of interactional style, allowing us to test whether it is predictive of community membership decisions.", 
        "164": "Classification task We design a classification task, in which the goal is to determine whether a pair of subreddits is high-crossover or lowcrossover.", 
        "165": "In high-crossover subreddit pairs, individuals are especially likely to participate in both.", 
        "166": "For the purpose of this evaluation, individuals are considered to participate in a subreddit if they contribute posts or comments.", 
        "167": "We compute the pointwise mutual information (PMI) with respect to cross-participation among the 100 most popular subreddits.", 
        "168": "For each subreddit s, we identify the five highest and lowest PMI pairs \u3008s, t\u3009, and add these to the high-crossover and low-crossover sets, respectively.", 
        "169": "Example pairs are shown in Table 6.", 
        "170": "After eliminating redundant pairs, we identify 437 unique high-crossover pairs, and 465 unique lowcrossover pairs.", 
        "171": "All evaluations are based on multiple random training/test splits over this dataset.", 
        "172": "Classification approaches A simple classification approach is to predict that subreddits with similar text will have high crossover.", 
        "173": "We measure similarity using TF-IDF weighted cosine similarity, using two possible lexicons: the 8,000 most frequent words on reddit (BOW), and the stance lexicon (STANCE MARKERS).", 
        "174": "The similarity threshold between high-crossover and low-\ncrossover pairs was estimated on the training data.", 
        "175": "We also tested the relevance of multi-dimensional analysis, by applying SVD to both lexicons.", 
        "176": "For each pair of subreddits, we computed a feature set of the absolute difference across the top six latent dimensions, and applied a logistic regression classifier.", 
        "177": "Regularization was tuned by internal crossvalidation.", 
        "178": "Results Table 7 shows average accuracies for these models.", 
        "179": "The stance-based SVD features are considerably more accurate than the BOWbased SVD features, indicating that interactional style does indeed predict cross-posting behavior.10 Both are considerably more accurate than the bagof-words models based on cosine similarity.", 
        "180": "7.2 Politeness and Formality  The utility of the induced stance dimensions depends on their correlation with social phenomena of interest.", 
        "181": "Prior work has used crowdsourcing to annotate texts for politeness and formality.", 
        "182": "We now evaluate the stancetaking properties of these annotated texts.", 
        "183": "Data We used the politeness corpus of Wikipedia edit requests from Danescu-NiculescuMizil et al.", 
        "184": "(2013), which includes the textual content of the edit requests, along with scalar annotations of politeness.", 
        "185": "Following the original\n10We use BOW+SVD as the most comparable contentbased alternative to our stancetaking dimensions.", 
        "186": "While there may be more accurate discriminative approaches, our goal is a direct comparison of stance and content-based features, not an exhaustive comparison of classification approaches.", 
        "187": "authors, we compare the text for the messages ranked in the first and fourth quartiles of politeness scores.", 
        "188": "For formality, we used the corpus from Pavlick and Tetreault (2016), focusing on the blogs domain, which is most similar to our domain of Reddit.", 
        "189": "Each sentence in this corpus was annotated for formality levels from \u22123 to +3.", 
        "190": "We considered only the sentences with mean formality score greater than +1 (more formal) and less than \u22121 (less formal).", 
        "191": "Stance dimensions For each document in the above datasets, we compute the stance properties, as follows: for each dimension, we compute the total frequency of the hundred most positive terms and the hundred most negative terms, and then take the difference.", 
        "192": "Instances containing no terms from either list are excluded.", 
        "193": "We focus on stance dimensions two and five (summarized in Table 3), because they appeared to be most relevant to politeness and formality.", 
        "194": "Dimension two contrasts informational and argumentative language against emotional and non-standard language.", 
        "195": "Dimension five contrasts positive and formal language against non-standard and somewhat negative language.", 
        "196": "Results A kernel density plot of the resulting differences is shown in Figure 2.", 
        "197": "The effect sizes of the resulting differences are quantified using Cohen\u2019s d statistic (Cohen, 1988).", 
        "198": "Effect sizes for all differences are between 0.3 and 0.4, indicating small-to-medium effects \u2014 except for the evaluation of formality on dimension five, where the effect size is close to zero.", 
        "199": "The relatively modest effect sizes are unsurprising, given the short length of the texts.", 
        "200": "However, these differences lend insight to the relationship between formality and politeness, which may seem to be closely related concepts.", 
        "201": "On dimension two, it is possible to be polite while using non-standard language such as hehe and awww, so long as the sentiment expressed is positive; however, these markers are not consistent with formality.", 
        "202": "On dimension five, we see that positive sentiment terms such as lovely and stunning are consistent with politeness, but not with formality.", 
        "203": "Indeed, the distribution of dimension five indicates that both ends of dimension five are consistent only with informal texts.", 
        "204": "Overall, these results indicate that interactional phenomena such as politeness and formality are reflected in our stance dimensions, which are induced in an unsupervised manner.", 
        "205": "Future work\nmay consider the utility of these stance dimensions to predict these social phenomena, particularly in cross-domain settings where lexical classifiers may overfit.", 
        "206": "8 Conclusion  Stancetaking provides a general perspective on the various linguistic phenomena that structure social interactions.", 
        "207": "We have identified a set of several hundred stance markers, building on previouslyidentified lexicons by using word embeddings to perform lexicon expansion.", 
        "208": "We then used multidimensional analysis to group these markers into stance dimensions, which we show to be internally coherent and extrinsically useful.", 
        "209": "Our hope is that these stance dimensions will be valuable as a convenient building block for future research on interactional meaning.", 
        "210": "Acknowledgments Thanks to the anonymous reviewers for their useful and constructive feedback on our submission.", 
        "211": "This research was supported by Air Force Office of Scientific Research award FA9550-14-1-0379, by National Institutes of Health award R01-GM112697, and by the National Science Foundation awards 1452443 and 1111142.", 
        "212": "We thank Tyler Schnoebelen for helpful discussions; C.J.", 
        "213": "Hutto, Tanushree Mitra, and Sandeep Soni for assistance with Mechanical Turk experiments; and Ian Stewart for assistance with creating word embeddings.", 
        "214": "We also thank the Mechanical Turk workers for performing the word intrusion task, and for feedback on a pilot task."
    }, 
    "document_id": "P17-1082.pdf.json"
}
