{
    "abstract_sentences": {
        "1": "Parallel corpora are widely used in a variety of Natural Language Processing tasks, from Machine Translation to cross-lingual Word Sense Disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale.", 
        "2": "In this paper we present EUROSENSE, a multilingual sense-annotated resource based on the joint disambiguation of the Europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a languageindependent unified sense inventory.", 
        "3": "We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for Word Sense Disambiguation."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 594\u2013600 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2094  1 Introduction  One of the long-standing challenges in Natural Language Processing (NLP) lies in automatically identifying the meaning of words in context.", 
        "2": "Various lines of research have been geared towards achieving this goal, most notably Word Sense Disambiguation (Navigli, 2009, WSD) and Entity Linking (Rao et al., 2013, EL).", 
        "3": "In both tasks, supervised approaches (Zhong and Ng, 2010; Melamud et al., 2016; Iacobacci et al., 2016; Ka\u030ageba\u0308ck and Salomonsson, 2016) tend to obtain the best performances over standard benchmarks but, from a practical standpoint, they lose ground to knowledge-based approaches (Agirre et al., 2014; Moro et al., 2014b; Weissenborn et al., 2015), which scale better in terms of scope and number of languages.", 
        "4": "In fact, the development of supervised disambiguation systems depends crucially on the availability of re-\nliable sense-annotated corpora, which are indispensable in order to provide solid training and testing grounds (Pilehvar and Navigli, 2014).", 
        "5": "However, hand-labeled sense annotations are notoriously difficult to obtain on a large scale, and manually curated corpora (Miller et al., 1993; Passonneau et al., 2012) have a limited size.", 
        "6": "Given that scaling the manual annotation process becomes practically unfeasible when both lexicographic and encyclopedic knowledge is addressed (Schubert, 2006), recent years have witnessed efforts to produce larger sense-annotated corpora automatically (Moro et al., 2014a; Taghipour and Ng, 2015a; Scozzafava et al., 2015; Raganato et al., 2016).", 
        "7": "Even though these automatic approaches produce noisier corpora, it has been shown that training on them leads to better supervised and semi-supervised models (Taghipour and Ng, 2015b; Raganato et al., 2016; Yuan et al., 2016; Raganato et al., 2017), as well as to effective embedded representations for senses (Iacobacci et al., 2015; Flekova and Gurevych, 2016).", 
        "8": "A convenient way of generating sense annotations is to exploit parallel corpora and word alignments (Taghipour and Ng, 2015a): indeed, parallel corpora exist in many flavours (Tiedemann, 2012) and are widely used across the NLP community for a variety of different tasks.", 
        "9": "In this paper we focus on Europarl (Koehn, 2005)1, one of the most popular multilingual corpora, originally designed to provide aligned parallel text for Machine Translation (MT) systems.", 
        "10": "Extracted from the proceedings of the European Parliament, the latest release of the Europarl corpus comprises parallel text for 21 European languages, with more than 743 million tokens overall.", 
        "11": "Apart from its prominent role in MT as a training set, the Europarl corpus has been used\n1http://opus.lingfil.uu.se/Europarl.", 
        "12": "php\n594\nfor cross-lingual WSD (Lefever and Hoste, 2010, 2013), including, more recently, preposition sense disambiguation (Gonen and Goldberg, 2016), and widely exploited to develop cross-lingual word embeddings (Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Vyas and Carpuat, 2016; Vulic\u0301 and Korhonen, 2016; Artetxe et al., 2016) as well as multi-sense embeddings (Ettinger et al., 2016; S\u030custer et al., 2016).", 
        "13": "In this paper, our aim is to augment Europarl with sense-level information for multiple languages, thereby constructing a large-scale senseannotated multilingual corpus which has the potential to boost both WSD and MT research.", 
        "14": "We follow an approach that has already proved effective in a definitional setting (CamachoCollados et al., 2016a): unlike previous crosslingual approaches, we do not rely on word alignments against a pivot language, but instead leverage all languages at the same time in a joint disambiguation procedure that is subsequently refined using distributional similarity.", 
        "15": "We draw on the wide-coverage multilingual encyclopedic dictionary of BabelNet (Navigli and Ponzetto, 2012)2, which enables us to seamlessly cover lexicographic and encyclopedic knowledge in multiple languages within a unified sense inventory.", 
        "16": "As a result of our disambiguation pipeline we obtain and make available to the community EUROSENSE, a multilingual sense-annotated corpus with almost 123 million sense annotations of more than 155 thousand distinct concepts and named entities drawn from the multilingual sense inventory of BabelNet, and covering all the 21 languages of the Europarl corpus.", 
        "17": "As such EUROSENSE constitutes, to our knowledge, the largest corpus of its kind.", 
        "18": "2 Related Work  Extending sense annotations to multiple languages is a demanding endeavor, especially when manual intervention is required.", 
        "19": "Despite the fact that sense-annotated corpora for a number of languages have been around for more than a decade (Petrolito and Bond, 2014), they either include few samples per word sense, or only cover a restricted set of ambiguous words (Passonneau et al., 2012); as a result, multilingual WSD was until recently almost exclusively tackled using knowledge-based approaches (Agirre et al., 2014;\n2http://babelnet.org\nMoro et al., 2014b).", 
        "20": "Nowadays, however, the rapid development of NLP pipelines for languages other than English has been opening up the possibilities for the automatic generation of multilingual sense-annotated data.", 
        "21": "Nevertheless, the few approaches that have been proposed so far are either focused on treating each individual language in isolation (Otegi et al., 2016), or limited to short and concise definitional text (Camacho-Collados et al., 2016a).", 
        "22": "On the other hand, the use of parallel text to perform WSD (Ng et al., 2003; Lefever et al., 2011; Yao et al., 2012; Bonansinga and Bond, 2016) or even Word Sense Induction (Apidianaki, 2013) has been widely explored in the literature, and has demonstrated its effectiveness in producing high-quality sense-annotated data (Chan and Ng, 2005).", 
        "23": "This strategy, however, requires word alignments for each language pair to be taken into account, with alignment errors that might propagate and hamper subsequent stages unless human supervision is employed to correct erroneous annotations (Taghipour and Ng, 2015a).", 
        "24": "Moreover, cross-language disambiguation using parallel text requires a language-independent annotation framework that goes beyond monolingual WordNet-like sense inventories (Lefever et al., 2011) in order for the annotations obtained to be used effectively within end-to-end applications.", 
        "25": "With EUROSENSE, instead, the key idea is to exploit at best parallel sentences to provide enriched context for a joint multilingual disambiguation.", 
        "26": "Using BabelNet, a unified multilingual sense inventory, we obtain language-independent sense annotations for a wide variety of concepts and named entities, which can be seamlessly mapped to individual semantic resources (e.g WordNet, Wikipedia, DBpedia) via BabelNet\u2019s inter-resource mappings.", 
        "27": "3 Building EUROSENSE  Following Camacho-Collados et al.", 
        "28": "(2016a), our fully automatic disambiguation pipeline for constructing EUROSENSE couples a graph-based multilingual joint WSD/EL system, Babelfy (Moro et al., 2014b)3, and a language-independent vector representation of concepts and entities, NASARI (Camacho-Collados et al., 2016b).4 It comprises two stages: multilingual disambigua-\n3http://babelfy.org 4http://lcl.uniroma1.it/nasari\ntion (Section 3.1) and refinement based on distributional similarity (Section 3.2).", 
        "29": "3.1 Stage 1: Multilingual Disambiguation  As a preprocessing step, we part-of-speech tag and lemmatize the whole corpus using TreeTagger (Schmid, 1995)5.", 
        "30": "We perform disambiguation at the sentence level.", 
        "31": "However, instead of disambiguating each sentence in isolation, language by language, we first identify all available translations of a given sentence and then gather these together into a single multilingual text.", 
        "32": "Then, we disambiguate this multilingual text using Babelfy.", 
        "33": "Given that Babelfy is capable of handling text with multiple languages at the same time, this multilingual extension effectively increases the amount of context for each sentence, and directly helps in dealing with highly ambiguous words in any particular language (as the translations of these words may be less ambiguous in some different language).", 
        "34": "Moreover, given the multilingual nature of our sense inventory, Babelfy\u2019s high-coherence approach favors naturally sense assignments that are consistent across languages at the sentence level (i.e.", 
        "35": "those having fewer distinct senses shared by more translations of the same sentence).", 
        "36": "As a result, we obtain a full, high-coverage version of EUROSENSE where each disambiguated word or multi-word expression (disambiguated instance) is associated with a coherence score.6  3.2 Stage 2: Similarity-based Refinement  In this stage we aim at improving the sense annotations obtained in the previous step (Section 3.1), with a procedure specifically targeted at correcting and extending these sense annotations.", 
        "37": "In general, graph-based WSD systems, such as Babelfy, have been shown to be heavily biased towards the Most Common Sense (MCS) (Calvo and Gelbukh, 2015).", 
        "38": "In order to get a handle on this bias and improve our pipeline\u2019s disambiguation accuracy we adopt a refinement based on distributional similarity, which is not affected by the MCS.", 
        "39": "To this end, we exploit the 300-dimensional embedded representations of concepts and entities of NASARI to discard or refine disam-\n5We rely on the internal preprocessing pipeline of Babelfy for those languages not supported by TreeTagger.", 
        "40": "6As in Camacho-Collados et al.", 
        "41": "(2016a), coherence score is given by the normalized number of connections of a given concept within the sentence.", 
        "42": "biguated instances that are less semantically coherent.", 
        "43": "These NASARI vector representations were constructed by combining structural and distributional knowledge from Wikipedia and WordNet with Word2Vec word embeddings (Mikolov et al., 2013) trained on textual corpora.", 
        "44": "For each sentence, we first identify a subset D of high-confidence disambiguations7 from among those given by Babelfy in the previous step.", 
        "45": "Then, we calculate the centroid of all the NASARI vectors corresponding to the elements of D, and we re-disambiguate the mentions associated with the remaining low-confidence disambiguated instances (i.e.", 
        "46": "those not in D), by picking, for each mention w, the concept or entity s\u0302 whose NASARI vector8 is closest to the centroid of the sentence:\ns\u0302 = argmax s\u2208Sw cos (\u2211 d\u2208D ~d |D| , ~s ) (1)\nwhere Sw is the set of all candidate senses for mention w according to BabelNet.", 
        "47": "Cosine similarity (cos) is used as similarity measure.", 
        "48": "Finally, in order to discard less confident annotations, we consider the cosine value associated with each refined disambiguation as confidence score, and use it to compare each disambiguated instance against an empirically validated threshold of 0.75.", 
        "49": "As a result, we obtain the refined high-precision version of EUROSENSE, where each disambiguated instance is associated with both a coherence score and a distributional similarity score.", 
        "50": "4 Corpus and Statistics  Table 1 reports general statistics on EUROSENSE regarding both its high-coverage (cf.", 
        "51": "Section 3.1) and high-precision (cf.", 
        "52": "Section 3.2) versions.", 
        "53": "Joint multilingual disambiguation with Babelfy generated more than 215M sense annotations of 247k distinct concepts and entities, while similarity-based refinement retained almost 123M high-confidence instances (56.96% of the total), covering almost 156k distinct concepts and entities.", 
        "54": "42.40% of these retained annotations were corrected or validated using distributional similarity.", 
        "55": "As expected, the distribution over parts of speech is skewed towards nominal senses (64.79% before refinement and 81.79% after refinement)\n7We follow Camacho-Collados et al.", 
        "56": "(2016a) and consider disambiguated instances with a coherence score above 0.125.", 
        "57": "8Given a concept or entity s we indicate with ~s its corresponding NASARI vector.", 
        "58": "followed by verbs (19.26% and 12.22%), adjectives (11.46% and 5.24%) and adverbs (4.48% and 0.73%).", 
        "59": "We note that the average coherence score increases from 0.19 to 0.29 after refinement, suggesting that distributional similarity tends to favor sense annotations that are also consistent across different languages.", 
        "60": "Table 1 also includes language-specific statistics on the 4 languages of the intrinsic evaluation, where the average lexical ambiguity ranges from 1.12 senses per lemma (German) to 2.26 (English) and, as expected, decreases consistently after refinement.", 
        "61": "Interestingly enough, if we consider all the 21 languages, the total number of distinct lemmas covered is more than twice the total number of distinct senses: this is a direct consequence of having a unified, language-independent sense inventory (BabelNet), a feature that sets EUROSENSE apart from previous multilingual sense-annotated corpora (Otegi et al., 2016).", 
        "62": "Finally we note from the global figures on the number of covered senses that 109 591 senses (44.2% of the total) are not covered by the English sense annotations: this suggests that EUROSENSE relies heavily on multilinguality in integrating concepts or named entities that are tied to specific social or cultural aspects of a given language (and hence would be underrepresented in an English-specific sense inventory).", 
        "63": "5 Experimental Evaluation  We assessed the quality of EUROSENSE\u2019s sense annotations both intrinsically, by means of a manual evaluation on four samples of randomly extracted sentences in different languages (Section 5.1), as well as extrinsically, by augmenting the training set of a state-of-the-art supervised WSD system (Zhong and Ng, 2010) and showing that\nit leads to consistent performance improvements over two standard WSD benchmarks (Section 5.2).", 
        "64": "5.1 Intrinsic Evaluation: Annotation Quality  In order to assess annotation quality directly, we carried out a manual evaluation on 4 different languages (English, French, German and Spanish) with 2 human judges per language.", 
        "65": "We sampled 50 random sentences across the subset of sentences in EUROSENSE featuring a translation in all 4 languages, totaling 200 sentences overall.", 
        "66": "For each sentence, we evaluated all sense annotations both before and after the refinement stage, along with the sense annotations obtained by a baseline that disambiguates each sentence in isolation with Babelfy.", 
        "67": "Overall, we manually verified a total of 5818 sense annotations across the three configurations (1518 in English, 1564 in French, 1093 in German and 1643 in Spanish).", 
        "68": "In every language the two judges agreed in more than 85% of the cases, with an inter-annotator agreement in terms of Cohen\u2019s kappa (Cohen, 1960) above 60% in all evaluations (67.7% on average).", 
        "69": "Results, reported in Table 2, show that joint multilingual disambiguation improves consistently over the baseline.", 
        "70": "The similarity-based refinement boosts precision even further, at the expense of a reduced coverage (whereas both Babelfy and the baseline attempt an answer for every disambiguation target).", 
        "71": "Over the 4 languages, sense annotations appear to be most reliable for German, which is consistent with its lower lexical ambiguity on the corpus (cf.", 
        "72": "Section 4).", 
        "73": "5.2 Extrinsic Evaluation: Word Sense Disambiguation  We additionally carried out an extrinsic evaluation of EUROSENSE by using its refined sense an-\nnotations for English as a training set for a supervised all-words WSD system, It Makes Sense (Zhong and Ng, 2010, IMS).", 
        "74": "Following Taghipour and Ng (2015a), we started with SemCor (Miller et al., 1993) as initial training dataset, and then performed a subsampling of EUROSENSE up to 500 additional training examples per word sense.", 
        "75": "We then trained IMS on this augmented training set and tested on the two most recent standard benchmarks for all-words WSD: the SemEval2013 task 12 (Navigli et al., 2013) and the SemEval-2015 task 13 (Moro and Navigli, 2015) test sets.", 
        "76": "As baselines we considered IMS trained on SemCor only and OMSTI, the sense-annotated dataset constructed by Taghipour and Ng (2015a) which also includes SemCor.", 
        "77": "Finally, we report the results of UKB, a knowledge-based system (Agirre et al., 2014).9 As shown in Table 3, IMS trained on our augmented training set consistently outperforms all baseline models, showing the reliability of EUROSENSE as training corpus, even against sense annotations obtained semiautomatically (Taghipour and Ng, 2015a).", 
        "78": "6 Release  EUROSENSE is available at http://lcl.", 
        "79": "uniroma1.it/eurosense.", 
        "80": "We release two different versions of the corpus:\n\u2022 A high-coverage version, obtained after the first stage of the pipeline, i.e.", 
        "81": "multilingual joint disambiguation with Babelfy.", 
        "82": "Here, each sense annotation is associated with a coherence score (cf.", 
        "83": "Section 3.1);\n\u2022 A high-precision version, obtained after the similarity-based refinement with NASARI.", 
        "84": "In this version, sense annotations are associated\n9We include its two implementations using the full WordNet graph and the disambiguated glosses of WordNet as connections: default and word by word (w2w).", 
        "85": "with both a coherence score and a distributional similarity score (cf.", 
        "86": "Section 3.2).", 
        "87": "7 Conclusion  In this paper we presented EUROSENSE, a large multilingual sense-annotated corpus based on Europarl, and constructed automatically via a disambiguation pipeline that exploits the interplay between a joint multilingual disambiguation algorithm and a language-independent vector-based representation of concepts and entities.", 
        "88": "Crucially, EUROSENSE relies on the wide-coverage unified sense inventory of BabelNet, which enabled the disambiguation process to exploit at best parallel text and enforces cross-language coherence among sense annotations.", 
        "89": "We evaluated EUROSENSE both intrinsically and extrinsically, showing that it provides reliable sense annotations that improve supervised models for WSD.", 
        "90": "Acknowledgments  The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE Contract No.", 
        "91": "726487.", 
        "92": "Claudio Delli Bovi is supported by a Sapienza Research Grant \u2018Avvio alla Ricerca 2016\u2019.", 
        "93": "Jose Camacho-Collados is supported by a Google PhD Fellowship in Natural Language Processing."
    }, 
    "document_id": "P17-2094.pdf.json"
}
