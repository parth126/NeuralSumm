{
    "abstract_sentences": {
        "1": "Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment.", 
        "2": "In other words, \u201dSarcasm is the giant chasm between what I say, and the person who doesn\u2019t get it.\u201d.", 
        "3": "In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one.", 
        "4": "We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges.", 
        "5": "Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures.", 
        "6": "We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm.", 
        "7": "We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN\u2019s interpretations are scored higher by humans for adequacy and sentiment polarity.", 
        "8": "We conclude with a discussion on future research directions for our new task.1"
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1690\u20131700 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1155  1 Introduction  Sarcasm is a sophisticated form of communication in which speakers convey their message in an indirect way.", 
        "2": "It is defined in the MerriamWebster dictionary (Merriam-Webster, 1983) as the use of words that mean the opposite of what\n1Our dataset, consisting of 3000 sarcastic tweets each augmented with five interpretations, is available in the project page: https://github.com/Lotemp/ SarcasmSIGN.", 
        "3": "The page also contains the sarcasm interpretation guidelines, the code of the SIGN algorithms and other materials related to this project.", 
        "4": "one would really want to say in order to insult someone, to show irritation, or to be funny.", 
        "5": "Considering this definition, it is not surprising to find frequent use of sarcastic language in opinionated user generated content, in environments such as Twitter, Facebook, Reddit and many more.", 
        "6": "In textual communication, knowledge about the speaker\u2019s intent is necessary in order to fully understand and interpret sarcasm.", 
        "7": "Consider, for example, the sentence \u201dwhat a wonderful day\u201d.", 
        "8": "A literal analysis of this sentence demonstrates a positive experience, due to the use of the word wonderful.", 
        "9": "However, if we knew that the sentence was meant sarcastically, wonderful would turn into a word of a strong negative sentiment.", 
        "10": "In spoken language, sarcastic utterances are often accompanied by a certain tone of voice which points out the intent of the speaker, whereas in textual communication, sarcasm is inherently ambiguous, and its identification and interpretation may be challenging even for humans.", 
        "11": "In this paper we present the novel task of interpretation of sarcastic utterances.", 
        "12": "We define the purpose of the interpretation task as the capability to generate a non-sarcastic utterance that captures the meaning behind the original sarcastic text.", 
        "13": "Our work currently targets the Twitter domain since it is a medium in which sarcasm is prevalent, and it allows us to focus on the interpretation of tweets marked with the content tag #sarcasm.", 
        "14": "And so, for example, given the tweet \u201dhow I love Mondays.", 
        "15": "#sarcasm\u201d we would like our system to generate interpretations such as \u201dhow I hate Mondays\u201d or \u201dI really hate Mondays\u201d.", 
        "16": "In order to learn such interpretations, we constructed a parallel corpus of 3000 sarcastic tweets, each of which has five non-sarcastic interpretations (Section 3).", 
        "17": "Our task is complex since sarcasm can be expressed in many forms, it is ambiguous in nature and its understanding may require world knowl-\n1690\nedge.", 
        "18": "Following are several examples taken from our corpus:\n1. loving life so much right now.", 
        "19": "#sarcasm 2.", 
        "20": "Way to go California!", 
        "21": "#sarcasm 3.", 
        "22": "Great, a choice between two excellent can-\ndidates, Donald Trump or Hillary Clinton.", 
        "23": "#sarcasm\nIn example (1) it is quite straightforward to see the exaggerated positive sentiment used in order to convey strong negative feelings.", 
        "24": "Examples (2) and (3), however, do not contain any excessive sentiment.", 
        "25": "Instead, previous knowledge is required if one wishes to fully understand and interpret what went wrong with California, or who Hillary Clinton and Donald Trump are.", 
        "26": "Since sarcasm is a refined and indirect form of speech, its interpretation may be challenging for certain populations.", 
        "27": "For example, studies show that children with deafness, autism or Asperger\u2019s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014).", 
        "28": "Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004).", 
        "29": "Extracting the honest meaning behind the sarcasm may alleviate such issues.", 
        "30": "In order to design an automatic sarcasm interpretation system, we first rely on previous work in established similar tasks (section 2), particularly machine translation (MT), borrowing algorithms as well as evaluation measures.", 
        "31": "In section 4 we discuss the automatic evaluation measures we apply in our work and present human based measures for: (a) the fluency of a generated nonsarcastic utterance, (b) its adequacy as interpretation of the original sarcastic tweet\u2019s meaning, and (c) whether or not it captures the sentiment of the original tweet.", 
        "32": "Then, in section 5, we explore the performance of prominent phrase-based and neural MT systems on our task in development data experiments.", 
        "33": "We next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator, section 6), our novel MT based algorithm which puts a special emphasis on sentiment words.", 
        "34": "Lastly, in Section 7 we assess the performance of the various algorithms and show that while they perform similarly in terms of automatic MT evaluation, SIGN is superior according\nto the human measures.", 
        "35": "We conclude with a discussion on future research directions for our task, regarding both algorithms and evaluation.", 
        "36": "2 Related Work  The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature.", 
        "37": "In computational work, the interest in sarcasm has dramatically increased over the past few years.", 
        "38": "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).", 
        "39": "Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; Gonza\u0301lez-Iba\u0301nez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.", 
        "40": "Therefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization.", 
        "41": "Sarcasm Detection Recent computational work on sarcasm revolves mainly around detection.", 
        "42": "Due to the large volume of detection work, we survey only several representative examples.", 
        "43": "Tsur et al.", 
        "44": "(2010) and Davidov et al.", 
        "45": "(2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset.", 
        "46": "Gonza\u0301lez-Iba\u0301nez et al.", 
        "47": "(2011) used lexical and pragmatic features, e.g.", 
        "48": "emojis and whether the utterance is a comment to another person, in order to train a classifier that distinguishes sarcastic utterances from tweets of positive and negative sentiment.", 
        "49": "Riloff et al.", 
        "50": "(2013) observed that a certain type of sarcasm is characterized by a contrast between a positive sentiment and a negative situation.", 
        "51": "Consequently, they described a bootstrapping algorithm that learns distinctive phrases connected to negative situations along with a positive sentiment and used these phrases to train their classifier.", 
        "52": "Barbieri et al.", 
        "53": "(2014) avoided using word patterns and\ninstead employed features such as the length and sentiment of the tweet, and the use of rare words.", 
        "54": "Despite the differences between detection and interpretation, this line of work is highly relevant to ours in terms of feature design.", 
        "55": "Moreover, it presents fundamental notions, such as the sentiment polarity of the sarcastic utterance and of its interpretation, that we adopt.", 
        "56": "Finally, when utterances are not marked for sarcasm as in the Twitter domain, or when these labels are not reliable, detection is a necessary step before interpretation.", 
        "57": "Machine Translation We approach our task as one of monolingual MT, where we translate sarcastic English into non-sarcastic English.", 
        "58": "Therefore, our starting point is the application of MT techniques and evaluation measures.", 
        "59": "The three major approaches to MT are phrase based (Koehn et al., 2007), syntax based (Koehn et al., 2003) and the recent neural approach.", 
        "60": "For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations.", 
        "61": "Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference.", 
        "62": "Here we employ the phrase based Moses system (Koehn et al., 2007) and an RNN-encoder-decoder architecture, based on Cho et al.", 
        "63": "(2014).", 
        "64": "Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task.", 
        "65": "Paraphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task.", 
        "66": "Quirk et al.", 
        "67": "(2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005).", 
        "68": "Xu et al.", 
        "69": "(2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT.", 
        "70": "Work on paraphrasing and summarization is often evaluated using MT evaluation measures such as BLEU.", 
        "71": "As BLEU is precision-oriented,\ncomplementary recall-oriented measures are often used as well.", 
        "72": "A prominent example is ROUGE (Lin, 2004), a family of measures used mostly for evaluation in automatic summarization: candidate summaries are scored according to the fraction of n-grams from the human references they contain.", 
        "73": "We also utilize PINC (Chen and Dolan, 2011), a measure which rewards paraphrases for being different from their source, by introducing new n-grams.", 
        "74": "PINC is often combined with BLEU due to their complementary nature: while PINC rewards n-gram novelty, BLEU rewards similarity to the reference.", 
        "75": "The highest correlation with human judgments is achieved by the product of PINC with a sigmoid function of BLEU (Chen and Dolan, 2011).", 
        "76": "3 A Parallel Sarcastic Tweets Corpus  To properly investigate our task, we collected a dataset, first of its kind, of sarcastic tweets and their non-sarcastic (honest) interpretations.", 
        "77": "This data, as well as the instructions provided for our human judges, will be made publicly available and will hopefully provide a basis for future work regarding sarcasm on Twitter.", 
        "78": "Despite the focus of the current work on the Twitter domain, we consider our task as a more general one, and hope that our discussion, observations and algorithms will be beneficial for other domains as well.", 
        "79": "Using the Twitter API2, we collected tweets marked with the content tag #sarcasm, posted between Januray and June of 2016.", 
        "80": "Following Tsur et al.", 
        "81": "(2010), Gonza\u0301lez-Iba\u0301nez et al.", 
        "82": "(2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link.", 
        "83": "This results in 3000 sarcastic tweets containing text only, where the average sarcastic tweet length is 13.87 utterances, the average interpretation length is 12.10 words and the vocabulary size is 8788 unique words.", 
        "84": "In order to obtain honest interpretations for our sarcastic tweets, we used Fiverr3 \u2013 a platform for selling and purchasing services from independent suppliers (also referred to as workers).", 
        "85": "We em-\n2http://apiwiki.twitter.com 3https://www.fiverr.com\nployed ten Fiverr workers, half of them from the field of comedy writing, and half from the field of literature paraphrasing.", 
        "86": "The chosen workers were made sure to have an active Twitter account, in order to ensure their acquaintance with social networks and with Twitter\u2019s colorful language (hashtags, common acronyms such as LOL, etc.).", 
        "87": "We then randomly divided our tweet corpus to two batches of size 1500 each, and randomly assigned five workers to each batch.", 
        "88": "We instructed the workers to translate each sarcastic tweet into a non sarcastic utterance, while maintaining the original meaning.", 
        "89": "We encouraged the workers to use external knowledge sources (such as Google) if they came across a subject they were not familiar with, or if the sarcasm was unclear to them.", 
        "90": "Although our dataset consists only of tweets that were marked with the hashtag #sarcasm, some of these tweets were not identified as sarcastic by all or some of our Fiverr workers.", 
        "91": "In such cases the workers were instructed to keep the original tweet unchanged (i.e, uninterpreted).", 
        "92": "We keep such tweets in our dataset since we expect a sarcasm interpretation system to be able to recognize non-sarcastic utterances in its input, and to leave them in their original form.", 
        "93": "Table 1 presents two examples from our corpus.", 
        "94": "The table demonstrates the tendency of the workers to generally agree on the core meaning of the sarcastic tweets.", 
        "95": "Yet, since sarcasm is inherently vague, it is not surprising that the interpretations differ from one worker to another.", 
        "96": "For example, some workers change only one or two words from the original sarcastic tweet, while others rephrase the entire utterance.", 
        "97": "We regard this as beneficial, since it brings a natural, human variance into the task.", 
        "98": "This variance makes the evaluation of automatic sarcasm interpretation algorithms challenging, as we further discuss in the next section.", 
        "99": "4 Evaluation Measures  As mentioned above, in certain cases world knowledge is mandatory in order to correctly evaluate sarcasm interpretations.", 
        "100": "For example, in the case of the second sarcastic tweet in table 1, we need to know that 2:30 is considered a late hour so that staying up till 2:30 and staying up late would be considered equivalent despite the lexical difference.", 
        "101": "Furthermore, we notice that transforming a sarcastic utterance into a non sarcastic one often requires to change a small number of words.", 
        "102": "For example, a single word change in the sarcastic tweet \u201dHow I love Mondays.", 
        "103": "#sarcasm\u201d leads to the non-sarcastic utterance How I hate Mondays.", 
        "104": "This is not typical for MT, where usually the entire source sentence is translated to a new sentence in the target language and we would expect lexical similarity between the machine generated translation and the human reference it is compared to.", 
        "105": "This raises a doubt as to whether n-gram based MT evaluation measures such as the aforementioned are suitable for our task.", 
        "106": "We hence asses the quality of an interpretation using automatic evaluation measures from the tasks of MT, paraphrasing, and summarization (Section 2), and compare these measures to human-based measures.", 
        "107": "Automatic Measures We use BLEU and ROUGE as measures of n-gram precision and recall, respectively.", 
        "108": "We report scores of ROUGE-1, ROUGE-2 and ROUGE-L (recall based on unigrams, bigrams and longest common subsequence between candidate and reference, respectively).", 
        "109": "In order to asses the n-gram novelty of interpretations (i.e, difference from the source), we report PINC and PINC\u2217sigmoid(BLEU) (see Section 2).", 
        "110": "Human judgments We employed an additional group of five Fiverr workers and asked them to score each generated interpretations with two scores on a 1-7 scale, 7 being the best.", 
        "111": "The scores\nare: adequacy: the degree to which the interpretation captures the meaning of the original tweet; and fluency: how readable the interpretation is.", 
        "112": "In addition, reasoning that a high quality interpretation is one that captures the true intent of the sarcastic utterance by using words suitable to its sentiment, we ask the workers to assign the interpretation with a binary score indicating whether the sentiment presented in the interpretation agrees with the sentiment of the original sarcastic tweet.4\nThe human measures enjoy high agreement levels between the human judges.", 
        "113": "The averaged root mean squared error calculated on the test set across all pairs of judges and across the various algorithms we experiment with are: 1.44 for fluency and 1.15 for adequacy.", 
        "114": "For sentiment scores the averaged agreement at the same setup is 93.2%.", 
        "115": "5 Sarcasm Interpretations as MT  As our task is about the generation of one English sentence given another, a natural starting point is treating it as monolingual MT.", 
        "116": "We hence begin with utilizing two widely used MT systems, representing two different approaches: Phrase Based MT vs. Neural MT.", 
        "117": "We then analyze the performance of these two systems, and based on our conclusions we design our SIGN model.", 
        "118": "4For example, we consider \u201dBest day ever #sarcasm\u201d and its interpretation \u201dWorst day ever\u201d to agree on the sentiment, despite the use of opposite sentiment words.", 
        "119": "Phrase Based MT We employ Moses5, using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy.", 
        "120": "We use phrases of up to 8 words to build our phrase table, and do not filter sentences according to length since tweets contain at most 140 characters.", 
        "121": "We employ the KenLM algorithm (Heafield, 2011) for language modeling, and train it on the non-sarcastic tweet interpretations (the target side of the parallel corpus).", 
        "122": "Neural Machine Translation We use GroundHog, a publicly available implementation of an RNN encoder-decoder, with LSTM hidden states.6 Our encoder and decoder contain 250 hidden units each.", 
        "123": "We use the minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model, where each SGD update is computed using a minibatch of 16 utterances.", 
        "124": "Following Sutskever et al.", 
        "125": "(2014), we use beam search for test time decoding.", 
        "126": "Henceforth we refer to this system as RNN.", 
        "127": "Performance Analysis We divide our corpus into training, development and test sets of sizes 2400, 300 and 300 respectively.", 
        "128": "We train Moses and the RNN on the training set and tune their parameters on the development set.", 
        "129": "Table 3 presents development data results, as these are preliminary experiments that aim to asses the compatibility of MT algorithms to our task.", 
        "130": "Moses scores much higher in terms of BLEU and ROUGE, meaning that compared to the RNN its interpretations capture more n-grams appearing in the human references while maintaining high precision.", 
        "131": "The RNN outscores Moses in terms of PINC and PINC\u2217sigmoid(BLEU), meaning that its interpretations are more novel, in terms of ngrams.", 
        "132": "This alone might not be a negative trait; However, according to human judgments Moses performs better in terms of fluency, adequacy and sentiment, and so the novelty of the RNN\u2019s interpretations does not necessarily contribute to their\n5http://www.statmt.org/moses 6https://github.com/lisa-groundhog/\nGroundHog\nquality, and even possibly reduces it.", 
        "133": "Table 2 illustrates several examples of the interpretations generated by both Moses and the RNN.", 
        "134": "While the interpretations generated by the RNN are readable, they generally do not maintain the meaning of the original tweet.", 
        "135": "We believe that this is the result of the neural network overfitting the training set, despite regularization and dropout layers, probably due to the relatively small training set size.", 
        "136": "In light of these results when we experiment with the SIGN algorithm (Section 7), we employ Moses as its MT component.", 
        "137": "The final example of Table 2 is representative of cases where both Moses and the RNN fail to capture the sarcastic sense of the tweet, incorrectly interpreting it or leaving it unchanged.", 
        "138": "In order to deal with such cases, we wish to utilize a property typical of sarcastic language.", 
        "139": "Sarcasm is mostly used to convey a certain emotion by using strong sentiment words that express the exact opposite of their literal meaning.", 
        "140": "Hence, many sarcastic utterances can be correctly interpreted by keeping most of their words, replacing only sentiment words with expressions of the opposite sentiment.", 
        "141": "For example, the sarcasm in the utterance \u201dYou\u2019re the best.", 
        "142": "#sarcasm\u201d is hidden in best, a word of a strong positive sentiment.", 
        "143": "If we transform this word into a word of the opposite sentiment, such as worst, then we get a non-sarcastic utterance with the correct sentiment.", 
        "144": "We next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator), an algorithm which capitalizes on sentiment words in order to produce accurate interpretations.", 
        "145": "6 The Sarcasm SIGN Algorithm  SIGN (Figure 1) targets sentiment words in sarcastic utterances.", 
        "146": "First, it clusters sentiment words according to semantic relatedness.", 
        "147": "Then, each sen-\ntiment word is replaced with its cluster 7 and the transformed data is fed into an MT system (Moses in this work), at both its training and test phases.", 
        "148": "Consequently, at test time the MT system outputs non-sarcastic utterances with clusters replacing sentiment words.", 
        "149": "Finally, SIGN performs a declustering process on these MT outputs, replacing sentiment clusters with suitable words.", 
        "150": "In order to detect the sentiment of words, we turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990).", 
        "151": "Using SentiWordNet\u2019s positivity and negativity scores, we collect from our training data a set of distinctly positive words (\u223c 70) and a set of distinctly negative words (\u223c 160).8 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)9 and cluster each set using the k-means algorithm with L2 distance.", 
        "152": "We aim to have ten words on average in each cluster, and so the positive set is clustered into 7 clusters, and the negative set into 16 clusters.", 
        "153": "Table 4 presents examples from our clusters.", 
        "154": "Upon receiving a sarcastic tweet, at both training and test, SIGN searches it for sentiment words according to the positive and negative sets.", 
        "155": "If such\n7This means that we replace a word with cluster-j where j is the number of the cluster to which the word belongs.", 
        "156": "8The scores are in the [0,1] range.", 
        "157": "We set the threshold of 0.6 for both distinctly positive and distinctly negative words.", 
        "158": "9https://levyomer.wordpress.com/2014/ 04/25/dependency-based-word-embeddings/.", 
        "159": "We choose these embeddings since they are believed to better capture the relations between a word and its context, having been trained on dependency-parsed sentences.", 
        "160": "a word is found, it is replaced with its cluster.", 
        "161": "For example, given the sentence \u201dHow I love Mondays.", 
        "162": "#sarcasm\u201d, love will be recognized as a positive sentiment word, and the sarcastic tweet will become: \u201dHow I cluster-i Mondays.", 
        "163": "#sarcasm\u201d where i is the cluster number of the word love.", 
        "164": "During training, this process is also applied to the non-sarcastic references.", 
        "165": "And so, if one such reference is \u201dI dislike Mondays.\u201d, then dislike will be identified and the reference will become \u201dI cluster-j Mondays.\u201d, where j is the cluster number of the word dislike.", 
        "166": "Moses is then trained on these new representations of the corpus, using the exact same setup as before.", 
        "167": "This training process produces a mapping between positive and negative clusters, and outputs sarcastic interpretations with clustered sentiment words (e.g, \u201dI cluster-j Mondays.\u201d).", 
        "168": "At test time, after Moses generates an utterance containing clusters, a de-clustering process takes place: the clusters are replaced with the appropriate sentiment words.", 
        "169": "We experiment with several de-clustering approaches: (1) SIGN-centroid: the chosen sentiment word will be the one closest to the centroid of cluster j.", 
        "170": "For example in the tweet \u201dI cluster-j Mondays.\u201d, the sentiment word closest to the centroid of cluster j will be chosen; (2) SIGNcontext: the cluster is replaced with its word that has the highest average Pointwise Mutual Information (PMI) with the words in a symmetric context window of size 3 around the cluster\u2019s location in the output.", 
        "171": "For example, for \u201dI cluster-j Mondays.\u201d, the sentiment word from cluster j which has the highest average PMI with the words in {\u2019I\u2019,\u2019Mondays\u2019} will be chosen.", 
        "172": "The PMI values are computed on the training data; and (3) SIGNOracle: an upper bound where a person manually chooses the most suitable word from the cluster.", 
        "173": "We expect this process to improve the quality of sarcasm interpretations in two aspects.", 
        "174": "First, as mentioned earlier, sarcastic tweets often differ from their non sarcastic interpretations in a small\nnumber of sentiment words (sometimes even in a single word).", 
        "175": "SIGN should help highlight the sentiment words most in need of interpretation.", 
        "176": "Second, under the pre-processing SIGN performs to the input examples of Moses, the latter is inclined to learn a mapping from positive to negative clusters, and vice versa.", 
        "177": "This is likely to encourage the Moses output to generate outputs of the same sentiment as the original sarcastic tweet, but with honest sentiment words.", 
        "178": "For example, if the sarcastic tweet expresses a negative sentiment with strong positive words, the non-sarcastic interpretation will express this negative sentiment with negative words, thus stripping away the sarcasm.", 
        "179": "7 Experiments and Results  We experiment with SIGN and the Moses and RNN baselines at the same setup of section 5.", 
        "180": "We report test set results for automatic and human measures, in Tables 5 and 6 respectively.", 
        "181": "As in the development data experiments (Table 3), the RNN presents critically low adequacy scores of 2.11 across the entire test set and of 1.89 in cases where the interpretation and the tweet differ.", 
        "182": "This, along with its low fluency scores (5.74 and 5.43\nrespectively) and its very low BLEU and ROUGE scores make us deem this model immature for our task and dataset, hence we exclude it from this section\u2019s tables and do not discuss it further.", 
        "183": "In terms of automatic evaluation (Table 5), SIGN and Moses do not perform significantly different.", 
        "184": "When it comes to human evaluation (Table 6) however, SIGN-context presents substantial gains.", 
        "185": "While for fluency Moses and SIGN-context perform similarly, SIGN-context performs much better in terms of adequacy and the percentage of tweets with the correct sentiment.", 
        "186": "The differences are substantial as well as statistically significant: adequacy of 3.61 for SIGN-context compared to 2.55 of Moses, and correct sentiment for 46.2% of the SIGN interpretations, compared to only 25.7% of the Moses interpretations.", 
        "187": "Table 6 further provides an initial explanation to the improvement of SIGN over Moses: Moses tends to keep interpretations identical to the original sarcastic tweet, altering them in only 42.3% of the cases, 10 while SIGN-context\u2019s interpretations differ from the original sarcastic tweet in 68.5% of the cases, which comes closer to the 73.8% in the gold standard human interpretations.", 
        "188": "If for each of the algorithms we only regard to interpretations that differ from the original sarcastic tweet, the differences between the models are less substantial.", 
        "189": "Nonetheless, SIGN-context still presents improvement by correctly changing sentiment in 67.5% of the cases compared to 60.8% for Moses.", 
        "190": "Both tables consistently show that the contextbased selection strategy of SIGN outperforms the centroid alternative.", 
        "191": "This makes sense as, being context-ignorant, SIGN-centroid might produce non-fluent or inadequate interpretations for a given context.", 
        "192": "For example, the tweet \u201dAlso gotta move a piano as well.", 
        "193": "joy #sarcasm\u201d is changed to \u201dAlso gotta move a piano as well.", 
        "194": "bummer\u201d by SIGN-context, while SIGN-centroid changes it to the less appropriate \u201dAlso gotta move a piano as well.", 
        "195": "boring\u201d.", 
        "196": "Nonetheless, even this naive de-clustering approach substantially improves adequacy and sentiment accuracy over Moses.", 
        "197": "Finally, comparison to SIGN-oracle reveals that the context selection strategy is not far from human performance with respect to both automatic and human evaluation measures.", 
        "198": "Still, some gain can be achieved, especially for the human measures on tweets that were changed at interpreta-\n10We elaborate on this in section 8.\ntion.", 
        "199": "This indicates that SIGN can improve mostly through a better clustering of sentiment words, rather than through a better selection strategy.", 
        "200": "8 Discussion and Future Work  Automatic vs. Human Measures The performance gap between Moses and SIGN may stem from the difference in their optimization criteria.", 
        "201": "Moses aims to optimize the BLEU score and given the overall lexical similarity between the original tweets and their interpretations, it therefore tends to keep them identical.", 
        "202": "SIGN, in contrast, targets sentiment words and changes them frequently.", 
        "203": "Consequently, we do not observe substantial differences between the algorithms in the automatic measures that are mostly based on ngram differences between the source and the interpretation.", 
        "204": "Likewise, the human fluency measure that accounts for the readability of the interpretation is not seriously affected by the translation process.", 
        "205": "When it comes to the human adequacy and sentiment measures, which account for the understanding of the tweet\u2019s meaning, SIGN reveals its power and demonstrates much better performance compared to Moses.", 
        "206": "To further understand the relationship between the automatic and the human based measures we computed the Pearson correlations for each pair of (automatic, human) measures.", 
        "207": "We observe that all correlation values are low (up to 0.12 for fluency, 0.13-0.18 for sentiment and 0.19-0.24 for adequacy).", 
        "208": "Moreover, for fluency the correlation values are insignificant (using a correlation significance t-test with p = 0.05).", 
        "209": "We believe this indicates that these automatic measures do not provide appropriate evaluation for our task.", 
        "210": "Designing automatic measures is hence left for future research.", 
        "211": "Sarcasm Interpretation as Sentiment Based Monolingual MT: Strengths and Weaknesses The SIGN models\u2019 strength is revealed when interpreting sarcastic tweets with strong sentiment words, transforming expressions such as \u201dAudits are a blast to do #sarcasm\u201d and \u201dBeing stuck in an airport is fun #sarcasm\u201d into \u201dAudits are a bummer to do\u201d and \u201dBeing stuck in an airport is boring\u201d, respectively.", 
        "212": "Even when there are no words of strong sentiment, the MT component of SIGN still performs well, interpreting tweets such as \u201dthe Cavs aren\u2019t getting any calls, this is new #sarcasm\u201d into \u201dthe Cavs aren\u2019t getting any calls, as usuall\u201d.", 
        "213": "The SIGN models perform well even in cases where there are several sentiment words but not all of them require change.", 
        "214": "For example, for the sarcastic tweet \u201dConstantly being irritated, anxious and depressed is a great feeling!", 
        "215": "#sarcasm\u201d, SIGN-context produces the adequate interpretation: \u201dConstantly being irritated, anxious and depressed is a terrible feeling\u201d.", 
        "216": "Future research directions rise from cases in which the SIGN models left the tweet unchanged.", 
        "217": "One prominent set of examples consists of tweets that require world knowledge for correct interpretation.", 
        "218": "Consider the tweet \u201dCan you imagine if Lebron had help?", 
        "219": "#sarcasm\u201d.", 
        "220": "The model requires knowledge of who Lebron is and what kind of help he needs in order to fully understand and interpret the sarcasm.", 
        "221": "In practice the SIGN models leave this tweet untouched.", 
        "222": "Another set of examples consists of tweets that lack an explicit sentiment word, for example, the tweet \u201dClear example they made of Sharapova then, ey?", 
        "223": "#sarcasm\u201d.", 
        "224": "While for a human reader it is apparent that the author means a clear example was not made of Sharapova, the lack of strong sentiment words results in all SIGN models leaving this tweet uninterpreted.", 
        "225": "Finally, tweets that present sentiment in phrases or slang words are particularly challenging for our approach which relies on the identification and clustering of sentiment words.", 
        "226": "Consider, for example, the following two cases: (a) the sarcastic tweet \u201dCan\u2019t wait until tomorrow #sarcasm\u201d, where the positive sentiment is expressed in the phrase can\u2019t wait; and (b) the sarcastic tweet \u201danother shooting?", 
        "227": "yeah we totally need to make guns easier for people to get #sarcasm\u201d, where the word totally receives a strong sentiment despite its normal use in language.", 
        "228": "While we believe that identifying the role of can\u2019t wait and of totally in the sentiment of the above tweets can be a key to properly interpreting them, our approach that relies on a sentiment word lexicon is challenged by such cases.", 
        "229": "Summary We presented a first attempt to approach the problem of sarcasm interpretation.", 
        "230": "Our major contributions are:\n\u2022 Construction of a dataset, first of its kind, that consists of 3000 tweets each augmented with five non-sarcastic interpretations generated by human experts.", 
        "231": "\u2022 Discussion of the proper evaluation in our task.", 
        "232": "We proposed a battery of human measures and compared their performance to the accepted measures in related fields such as machine translation.", 
        "233": "\u2022 An algorithmic approach: sentiment based monolingual machine translation.", 
        "234": "We demonstrated the strength of our approach and pointed on cases that are currently beyond its reach.", 
        "235": "Several challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner.", 
        "236": "These include the design of appropriate automatic evaluation measures as well as improving the algorithmic approach so that it can take world knowledge into account and deal with cases where the sentiment of the input tweet is not expressed with a clear sentiment words.", 
        "237": "We are releasing our dataset with its sarcasm interpretation guidelines, the code of the SIGN algorithms, and the output of the various algorithms considered in this paper (https://github.", 
        "238": "com/Lotemp/SarcasmSIGN).", 
        "239": "We hope this new resource will help researchers make further progress on this new task."
    }, 
    "document_id": "P17-1155.pdf.json"
}
