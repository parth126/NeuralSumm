{
    "abstract_sentences": {
        "1": "We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al.", 
        "2": "(2016b).", 
        "3": "While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion.", 
        "4": "These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively.", 
        "5": "These improve LSTMs in both transfer learning and supervised settings.", 
        "6": "We also introduce a new recurrent architecture, the GATED RECURRENT AVERAGING NETWORK, that is inspired by averaging and LSTMs while outperforming them both.", 
        "7": "We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.", 
        "8": "1"
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2078\u20132088 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1190\nWe consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al.", 
        "2": "(2016b).", 
        "3": "While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion.", 
        "4": "These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively.", 
        "5": "These improve LSTMs in both transfer learning and supervised settings.", 
        "6": "We also introduce a new recurrent architecture, the GATED RECURRENT AVERAGING NETWORK, that is inspired by averaging and LSTMs while outperforming them both.", 
        "7": "We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.", 
        "8": "1  1 Introduction  Modeling sentential compositionality is a fundamental aspect of natural language semantics.", 
        "9": "Researchers have proposed a broad range of compositional functional architectures (Mitchell and Lapata, 2008; Socher et al., 2011; Kalchbrenner et al., 2014) and evaluated them on a large variety of applications.", 
        "10": "Our goal is to learn a generalpurpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) (Agirre et al., 2012) and can also serve as a useful initialization for downstream tasks.", 
        "11": "We wish to learn this embedding function\n1Trained models and code are available at http:// ttic.uchicago.edu/\u02dcwieting.", 
        "12": "such that sentences with high semantic similarity have high cosine similarity in the embedding space.", 
        "13": "In particular, we focus on the setting of Wieting et al.", 
        "14": "(2016b), in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks.", 
        "15": "Surprisingly, Wieting et al.", 
        "16": "found that simple embedding functions\u2014those based on averaging word vectors\u2014outperform more powerful architectures based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).", 
        "17": "In this paper, we revisit their experimental setting and present several techniques that together improve the performance of the LSTM to be superior to word averaging.", 
        "18": "We first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database (PPDB; Ganitkevitch et al., 2013), we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia (Coster and Kauchak, 2011).", 
        "19": "Even though this data was intended for use by text simplification systems, we find it to be efficient and effective for learning sentence embeddings, outperforming much larger sets of examples from PPDB.", 
        "20": "We then show how we can modify and regularize the LSTM to further improve its performance.", 
        "21": "The main modification is to simply average the hidden states instead of using the final one.", 
        "22": "For regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence.", 
        "23": "We find that these techniques help in the transfer learning setting and on two supervised semantic similarity datasets as well.", 
        "24": "Further gains are obtained on the supervised tasks by initializing with our models from the transfer setting.", 
        "25": "Inspired by the strong performance of both averaging and LSTMs, we introduce a novel recurrent neural network architecture which we call\n2078\nthe GATED RECURRENT AVERAGING NETWORK (GRAN).", 
        "26": "The GRAN outperforms averaging and the LSTM in both the transfer and supervised learning settings, forming a promising new recurrent architecture for semantic modeling.", 
        "27": "2 Related Work  Modeling sentential compositionality has received a great deal of attention in recent years.", 
        "28": "A comprehensive survey is beyond the scope of this paper, but we mention popular functional families: neural bag-of-words models (Kalchbrenner et al., 2014), deep averaging networks (DANs) (Iyyer et al., 2015), recursive neural networks using syntactic parses (Socher et al., 2011, 2012, 2013; I\u0307rsoy and Cardie, 2014), convolutional neural networks (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014), and recurrent neural networks using long short-term memory (Tai et al., 2015; Ling et al., 2015; Liu et al., 2015).", 
        "29": "Simple operations based on vector addition and multiplication typically serve as strong baselines (Mitchell and Lapata, 2008, 2010; Blacoe and Lapata, 2012).", 
        "30": "Most work cited above uses a supervised learning framework, so the composition function is learned discriminatively for a particular task.", 
        "31": "In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences.", 
        "32": "Several others have pursued this goal (Socher et al., 2011; Le and Mikolov, 2014; Pham et al., 2015; Kiros et al., 2015; Hill et al., 2016; Arora et al., 2017; Pagliardini et al., 2017), though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity.", 
        "33": "An exception is the work of Wieting et al.", 
        "34": "(2016b).", 
        "35": "We closely follow their experimental setup and directly address some outstanding questions in their experimental results.", 
        "36": "Here we briefly summarize their main findings and their attempts at explaining them.", 
        "37": "They made the surprising discovery that word averaging outperforms LSTMs by a wide margin in the transfer learning setting.", 
        "38": "They proposed several hypotheses for why this occurs.", 
        "39": "They first considered that the LSTM was unable to adapt to the differences in sequence length between phrases in training and sentences in test.", 
        "40": "This was ruled out by showing that neither model showed any strong correlation between sequence length and performance on the test data.", 
        "41": "They next examined whether the LSTM was\noverfitting on the training data, but then showed that both models achieve similar values of the training objective and similar performance on indomain held-out test sets.", 
        "42": "Lastly, they considered whether their hyperparameters were inadequately tuned, but extensive hyperparameter tuning did not change the story.", 
        "43": "Therefore, the reason for the performance gap, and how to correct it, was left as an open problem.", 
        "44": "This paper takes steps toward addressing that problem.", 
        "45": "3 Models and Training    3.1 Models  Our goal is to embed a word sequence s into a fixed-length vector.", 
        "46": "We focus on three compositional models in this paper, all of which use words as the smallest unit of compositionality.", 
        "47": "We denote the tth word in s as st, and we denote its word embedding by xt.", 
        "48": "Our first two models have been well-studied in prior work, so we describe them briefly.", 
        "49": "The first, which we call AVG, simply averages the embeddings xt of all words in s. The only parameters learned in this model are those in the word embeddings themselves, which are stored in the word embedding matrix Ww.", 
        "50": "This model was found by Wieting et al.", 
        "51": "(2016b) to perform very strongly for semantic similarity tasks.", 
        "52": "Our second model uses a long short-term memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to embed s. We use the LSTM variant from Gers et al.", 
        "53": "(2003) including its \u201cpeephole\u201d connections.", 
        "54": "We consider two ways to obtain a sentence embedding from the LSTM.", 
        "55": "The first uses the final hidden vector, which we denote h\u22121.", 
        "56": "The second, denoted LSTMAVG, averages all hidden vectors of the LSTM.", 
        "57": "In both variants, the learnable parameters include both the LSTM parameters Wc and the word embeddings Ww.", 
        "58": "Inspired by the success of the two models above, we propose a third model, which we call the GATED RECURRENT AVERAGING NETWORK (GRAN).", 
        "59": "The GATED RECURRENT AVERAGING NETWORK combines the benefits of AVG and LSTMs.", 
        "60": "In fact it reduces to AVG if the output of the gate is all ones.", 
        "61": "We first use an LSTM to generate a hidden vector, ht, for each word st in s. Then we use ht to compute a gate that will be elementwise-multiplied with xt, resulting in a new, gated hidden vector at for each step t:\nat = xt \u03c3(Wxxt +Whht + b) (1)\nwhere Wx and Wh are parameter matrices, b is a parameter vector, and \u03c3 is the elementwise logistic sigmoid function.", 
        "62": "After all at have been generated for a sentence, they are averaged to produce the embedding for that sentence.", 
        "63": "This model includes as learnable parameters those of the LSTM, the word embeddings, and the additional parameters in Eq.", 
        "64": "(1).", 
        "65": "For both the LSTM and GRAN models, we use Wc to denote the \u201ccompositional\u201d parameters, i.e., all parameters other than the word embeddings.", 
        "66": "The motivation for the GRAN is that we are contextualizing the word embeddings prior to averaging.", 
        "67": "The gate can be seen as an attention, attending to the prior context of the sentence.2\nWe also experiment with four other variations of this model, though they generally were more complex and showed inferior performance.", 
        "68": "In the first, GRAN-2, the gate is applied to ht (rather than xt) to produce at, and then these at are averaged as before.", 
        "69": "GRAN-3 and GRAN-4 use two gates: one applied to xt and one applied to at\u22121.", 
        "70": "We tried two different ways of computing these gates: for each gate i, \u03c3(Wxixt+Whiht+bi) (GRAN-3) or \u03c3(Wxixt +Whiht +Waiat\u22121 + bi) (GRAN-4).", 
        "71": "The sum of these two terms comprised at.", 
        "72": "In this model, the last average hidden state, a\u22121, was used as the sentence embedding after dividing it by the length of the sequence.", 
        "73": "In these models, we are additionally keeping a running average of the embeddings that is being modified by the context at every time step.", 
        "74": "In GRAN-4, this running average is also considered when producing the contextualized word embedding.", 
        "75": "Lastly, we experimented with a fifth GRAN, GRAN-5, in which we use two gates, calculated by \u03c3(Wxixt +Whiht + bi) for each gate i.", 
        "76": "The first is applied to xt and the second is applied to ht.", 
        "77": "The output of these gates is then summed.", 
        "78": "Therefore GRAN-5 can be reduced to either wordaveraging or averaging LSTM states, depending on the behavior of the gates.", 
        "79": "If the first gate is all ones and the second all zeros throughout the sequence, the model is equivalent to wordaveraging.", 
        "80": "Conversely, if the first gate is all zeros and the second is all ones throughout the sequence, the model is equivalent to averaging the\n2We tried a variant of this model without the gate.", 
        "81": "We obtain at from f(Wxxt+Whht+b), where f is a nonlinearity, tuned over tanh and ReLU.", 
        "82": "The performance of the model is significantly worse than the GRAN in all experiments.", 
        "83": "LSTM states.", 
        "84": "Further analysis of these models is included in Section 4.", 
        "85": "3.2 Training  We follow the training procedure of Wieting et al.", 
        "86": "(2015) and Wieting et al.", 
        "87": "(2016b), described below.", 
        "88": "The training data consists of a set S of phrase or sentence pairs \u3008s1, s2\u3009 from either the Paraphrase Database (PPDB; Ganitkevitch et al., 2013) or the aligned Wikipedia sentences (Coster and Kauchak, 2011) where s1 and s2 are assumed to be paraphrases.", 
        "89": "We optimize a margin-based loss:\nmin Wc,Ww\n1\n|S|\n( \u2211\n\u3008s1,s2\u3009\u2208S max(0, \u03b4 \u2212 cos(g(s1), g(s2))\n+ cos(g(s1), g(t1))) + max(0, \u03b4 \u2212 cos(g(s1), g(s2))\n+ cos(g(s2), g(t2))) ) + \u03bbc \u2016Wc\u20162 + \u03bbw \u2016Wwinitial \u2212Ww\u20162\n(2)\nwhere g is the model in use (e.g., AVG or LSTM), \u03b4 is the margin, \u03bbc and \u03bbw are regularization parameters, Wwinitial is the initial word embedding matrix, and t1 and t2 are carefully-selected negative examples taken from a mini-batch during optimization.", 
        "90": "The intuition is that we want the two phrases to be more similar to each other (cos(g(s1), g(s2))) than either is to their respective negative examples t1 and t2, by a margin of at least \u03b4.", 
        "91": "3.2.1 Selecting Negative Examples  To select t1 and t2 in Eq.", 
        "92": "(2), we simply choose the most similar phrase in some set of phrases (other than those in the given phrase pair).", 
        "93": "For simplicity we use the mini-batch for this set, but it could be a different set.", 
        "94": "That is, we choose t1 for a given \u3008s1, s2\u3009 as follows:\nt1 = argmax t:\u3008t,\u00b7\u3009\u2208Sb\\{\u3008s1,s2\u3009} cos(g(s1), g(t))\nwhere Sb \u2286 S is the current mini-batch.", 
        "95": "That is, we want to choose a negative example ti that is similar to si according to the current model.", 
        "96": "The downside is that we may occasionally choose a phrase ti that is actually a true paraphrase of si.", 
        "97": "4 Experiments  Our experiments are designed to address the empirical question posed by Wieting et al.", 
        "98": "(2016b): why do LSTMs underperform AVG for transfer\nlearning?", 
        "99": "In Sections 4.1.2-4.2, we make progress on this question by presenting methods that bridge the gap between the two models in the transfer setting.", 
        "100": "We then apply these same techniques to improve performance in the supervised setting, described in Section 4.3.", 
        "101": "In both settings we also evaluate our novel GRAN architecture, finding it to consistently outperform both AVG and the LSTM.", 
        "102": "4.1 Transfer Learning    4.1.1 Datasets and Tasks  We train on large sets of noisy paraphrase pairs and evaluate on a diverse set of 22 textual similarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015.", 
        "103": "We also evaluate on the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 SICK Semantic Relatedness task (Marelli et al., 2014).", 
        "104": "Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent.", 
        "105": "We report the average Pearson\u2019s r over these 22 sentence similarity tasks.", 
        "106": "Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others.", 
        "107": "Further details are provided in the official task descriptions (Agirre et al., 2012, 2013, 2014, 2015).", 
        "108": "4.1.2 Experiments with Data Sources  We first investigate how different sources of training data affect the results.", 
        "109": "We try two data sources.", 
        "110": "The first is phrase pairs from the Paraphrase Database (PPDB).", 
        "111": "PPDB comes in different sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones.", 
        "112": "The pairs in PPDB are sorted by a confidence measure and so the smaller sets contain higher precision paraphrases.", 
        "113": "PPDB is derived automatically from naturally-occurring bilingual text, and versions of PPDB have been released for many languages without the need for any manual annotation (Ganitkevitch and Callison-Burch, 2014).", 
        "114": "The second source of data is a set of sentence pairs automatically extracted from Simple English Wikipedia and English Wikipedia articles by Coster and Kauchak (2011).", 
        "115": "This data was extracted for developing text simplification\nsystems, where each instance pairs a simple and complex sentence representing approximately the same information.", 
        "116": "Though the data was obtained for simplification, we use it as a source of training data for learning paraphrastic sentence embeddings.", 
        "117": "The dataset, which we call SimpWiki, consists of 167,689 sentence pairs.", 
        "118": "To ensure a fair comparison, we select a sample of pairs from PPDB XL such that the number of tokens is approximately the same as the number of tokens in the SimpWiki sentences.3\nWe use PARAGRAM-SL999 embeddings (Wieting et al., 2015) to initialize the word embedding matrix (Ww) for all models.", 
        "119": "For all experiments, we fix the mini-batch size to 100, and \u03bbc to 0.", 
        "120": "We tune the margin \u03b4 over {0.4, 0.6, 0.8} and \u03bbw over {10\u22124, 10\u22125, 10\u22126, 10\u22127, 10\u22128, 0}.", 
        "121": "We train AVG for 7 epochs, and the LSTM for 3, since it converges much faster and does not benefit from 7 epochs.", 
        "122": "For optimization we use Adam (Kingma and Ba, 2015) with a learning rate of 0.001.", 
        "123": "We use the 2016 STS tasks (Agirre et al., 2016) for model selection, where we average the Pearson\u2019s r over its 5 datasets.", 
        "124": "We refer to this type of model selection as test.", 
        "125": "For evaluation, we report the average Pearson\u2019s r over the 22 other sentence similarity tasks.", 
        "126": "The results are shown in Table 1.", 
        "127": "We first note that, when training on PPDB, we find the same result as Wieting et al.", 
        "128": "(2016b): AVG outperforms the LSTM by more than 13 points.", 
        "129": "However, when training both on sentence pairs, the gap shrinks to about 9 points.", 
        "130": "It appears that part of the inferior performance for the LSTM in prior work was due to training on phrase pairs rather than on sentence pairs.", 
        "131": "The AVG model also benefits from training on sentences, but not nearly as much as the LSTM.4\n3The PPDB data consists of 1,341,188 phrase pairs and contains 3 more tokens than the SimpWiki data.", 
        "132": "4We experimented with adding EOS tags at the end of training and test sentences, SOS tags at the start of train-\nOur hypothesis explaining this result is that in PPDB, the phrase pairs are short fragments of text which are not necessarily constituents or phrases in any syntactic sense.", 
        "133": "Therefore, the sentences in the STS test sets are quite different from the fragments seen during training.", 
        "134": "We hypothesize that while word-averaging is relatively unaffected by this difference, the recurrent models are much more sensitive to overall characteristics of the word sequences, and the difference between train and test matters much more.", 
        "135": "These results also suggest that the SimpWiki data, even though it was developed for text simplification, may be useful for other researchers working on semantic textual similarity tasks.", 
        "136": "4.1.3 Experiments with LSTM Variations  We next compare LSTM and LSTMAVG.", 
        "137": "The latter consists of averaging the hidden vectors of the LSTM rather than using the final hidden vector as in prior work (Wieting et al., 2016b).", 
        "138": "We hypothesize that the LSTM may put more emphasis on the words at the end of the sentence than those at the beginning.", 
        "139": "By averaging the hidden states, the impact of all words in the sequence is better taken into account.", 
        "140": "Averaging also makes the LSTM more like AVG, which we know to perform strongly in this setting.", 
        "141": "The results on AVG and the LSTM models are shown in Table 1.", 
        "142": "When training on PPDB, moving from LSTM to LSTMAVG improves performance by 10 points, closing most of the gap with AVG.", 
        "143": "We also find that LSTMAVG improves by moving from PPDB to SimpWiki, though in both cases it still lags behind AVG.", 
        "144": "4.2 Experiments with Regularization  We next experiment with various forms of regularization.", 
        "145": "Previous work (Wieting et al., 2016b,a) only used L2 regularization.", 
        "146": "Wieting et al.", 
        "147": "(2016b) also regularized the word embeddings back to their initial values.", 
        "148": "Here we use L2 regularization\ning and test sentences, adding both, and adding neither.", 
        "149": "We treated adding these tags as hyperparameters and tuned over these four settings along with the other hyperparameters in the original experiment.", 
        "150": "Interestingly, we found that adding these tags, especially EOS, had a large effect on the LSTM when training on SimpWiki, improving performance by 6 points.", 
        "151": "When training on PPDB, adding EOS tags only improved performance by 1.6 points.", 
        "152": "The addition of the tags had a smaller effect on LSTMAVG.", 
        "153": "Adding EOS tags improved performance by 0.3 points on SimpWiki and adding SOS tags on PPDB improved performance by 0.9 points.", 
        "154": "as well as several additional regularization methods we describe below.", 
        "155": "We try two forms of dropout.", 
        "156": "The first is just standard dropout (Srivastava et al., 2014) on the word embeddings.", 
        "157": "The second is \u201cword dropout\u201d, which drops out entire word embeddings with some probability (Iyyer et al., 2015).", 
        "158": "We also experiment with scrambling the inputs.", 
        "159": "For a given mini-batch, we go through each sentence pair and, with some probability, we shuffle the words in each sentence in the pair.", 
        "160": "When scrambling a sentence pair, we always shuffle both sentences in the pair.", 
        "161": "We do this before selecting negative examples for the mini-batch.", 
        "162": "The motivation for scrambling is to make it more difficult for the LSTM to memorize the sequences in the training data, forcing it to focus more on the identities of the words and less on word order.", 
        "163": "Hence it will be expected to behave more like the word averaging model.5\nWe also experiment with combining scrambling and dropout.", 
        "164": "In this setting, we tune over scrambling with either word dropout or dropout.", 
        "165": "The settings for these experiments are largely the same as those of the previous section with the exception that we tune \u03bbw over a smaller set of values: {10\u22125, 0}.", 
        "166": "When using L2 regularization, we tune \u03bbc over {10\u22123, 10\u22124, 10\u22125, 10\u22126}.", 
        "167": "When using dropout, we tune the dropout rate over {0.2, 0.4, 0.6}.", 
        "168": "When using scrambling, we tune the scrambling rate over {0.25, 0.5, 0.75}.", 
        "169": "We also include a bidirectional model (\u201cBi\u201d) for both LSTMAVG and the GATED RECURRENT AVERAGING NETWORK.", 
        "170": "We tune over two ways to combine the forward and backward hidden states; the first simply adds them together and the second uses a single feedforward layer with a tanh activation.", 
        "171": "We try two approaches for model selection.", 
        "172": "The first, test , is the same as was done in Section 4.1.2, where we use the average Pearson\u2019s r on the 5 2016 STS datasets.", 
        "173": "The second tunes based on the average Pearson\u2019s r of all 22 datasets in our evaluation.", 
        "174": "We refer to this as oracle.", 
        "175": "The results are shown in Table 2.", 
        "176": "They show that dropping entire word embeddings and scram-\n5We also tried some variations on scrambling that did not yield significant improvements: scrambling after obtaining the negative examples, partially scrambling by performing n swaps where n comes from a Poisson distribution with a tunable \u03bb, and scrambling individual sentences with some probability instead of always scrambling both in the pair.", 
        "177": "bling input sequences is very effective in improving the result of the LSTM, while neither type of dropout improves AVG.", 
        "178": "Moreover, averaging the hidden states of the LSTM is the most effective modification to the LSTM in improving performance.", 
        "179": "All of these modifications can be combined to significantly improve the LSTM, finally allowing it to overtake AVG.", 
        "180": "In Table 3, we compare the various GRAN architectures.", 
        "181": "We find that the GRAN provides a small improvement over the best LSTM configuration, possibly because of its similarity to AVG.", 
        "182": "It also outperforms the other GRAN models, despite being the simplest.", 
        "183": "In Table 4, we show results on all individual STS evaluation datasets after using STS 2016 for model selection (unidirectional models only).", 
        "184": "The LSTMAVG and GATED RECURRENT AVERAGING NETWORK are more closely correlated in performance, in terms of Spearman\u2019s \u03c1 and Pearson\u2019r r, than either is to AVG.", 
        "185": "But they do differ significantly in some datasets, most notably in those comparing machine translation output with its ref-\nerence.", 
        "186": "Interestingly, both the LSTMAVG and GATED RECURRENT AVERAGING NETWORK significantly outperform AVG in the datasets focused on comparing glosses like OnWN and FNWN.", 
        "187": "Upon examination, we found that these datasets, especially 2013 OnWN, contain examples of low similarity with high word overlap.", 
        "188": "For example, the pair \u3008the act of preserving or protecting something., the act of decreasing or reducing something.\u3009 from 2013 OnWN has a gold similarity score of 0.4.", 
        "189": "It appears that AVG was fooled by the high amount of word overlap in such pairs, while the other two models were better able to recognize the semantic differences.", 
        "190": "4.3 Supervised Text Similarity  We also investigate if these techniques can improve LSTM performance on supervised semantic textual similarity tasks.", 
        "191": "We evaluate on two supervised datasets.", 
        "192": "For the first, we start with the 20 SemEval STS datasets from 2012-2015 and then use 40% of each dataset for training, 10% for validation, and the remaining 50% for testing.", 
        "193": "There are 4,481 examples in training, 1,207 in validation, and 6,060 in the test set.", 
        "194": "The second is the SICK 2014 dataset, using its standard training, validation, and test sets.", 
        "195": "There are 4,500 sentence pairs\nin the training set, 500 in the development set, and 4,927 in the test set.", 
        "196": "The SICK task is an easier learning problem since the training examples are all drawn from the same distribution, and they are mostly shorter and use simpler language.", 
        "197": "As these are supervised tasks, the sentence pairs in the training set contain manually-annotated semantic similarity scores.", 
        "198": "We minimize the loss function6 from Tai et al.", 
        "199": "(2015).", 
        "200": "Given a score for a sentence pair in the range [1,K], where K is an integer, with sentence representations hL and hR, and model parameters \u03b8, they first compute:\nh\u00d7 = hL hR, h+ = |hL \u2212 hR|, hs = \u03c3 ( W (\u00d7)h\u00d7 +W (+)h+ + b(h) ) ,\np\u0302\u03b8 = softmax ( W (p)hs + b (p) ) ,\ny\u0302 = rT p\u0302\u03b8,\nwhere rT = [1 2 .", 
        "201": ".", 
        "202": ".", 
        "203": "K].", 
        "204": "They then define a sparse target distribution p that satisfies y = rT p:\npi =    y \u2212 byc, i = byc+ 1 byc \u2212 y + 1, i = byc 0 otherwise\nfor 1 \u2264 i \u2264 K. Then they use the following loss, the regularized KL-divergence between p and p\u0302\u03b8:\nJ(\u03b8) = 1\nm\nm\u2211\nk=1\nKL ( p(k) \u2225\u2225\u2225 p\u0302(k)\u03b8 ) ,\nwhere m is the number of training pairs.", 
        "205": "We experiment with the LSTM, LSTMAVG, and AVG models with dropout, word dropout, and scrambling tuning over the same hyperparameter as in Section 4.2.", 
        "206": "We again regularize the word embeddings back to their initial state, tuning \u03bbw over {10\u22125, 0}.", 
        "207": "We used the validation set for each respective dataset for model selection.", 
        "208": "The results are shown in Table 5.", 
        "209": "The GATED RECURRENT AVERAGING NETWORK has the best performance on both datasets.", 
        "210": "Dropout helps the word-averaging model in the STS task, unlike in the transfer learning setting.", 
        "211": "The LSTM benefits slightly from dropout, scrambling, and averaging on their own individually with the exception of word dropout on both datasets and averaging on the SICK dataset.", 
        "212": "However, when combined, these modifications are able to significantly\n6This objective function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error.", 
        "213": "improve the performance of the LSTM, bringing it much closer in performance to AVG.", 
        "214": "This experiment indicates that these modifications when training LSTMs are beneficial outside the transfer learning setting, and can potentially be used to improve performance for the broad range of problems that use LSTMs to model sentences.", 
        "215": "In Table 6 we compare the various GRAN architectures under the same settings as the previous experiment.", 
        "216": "We find that the GRAN still has the best overall performance.", 
        "217": "We also experiment with initializing the supervised models using our pretrained sentence model parameters, for the AVG model (no regularization), LSTMAVG (dropout, scrambling), and GATED RECURRENT AVERAGING NETWORK (dropout, scrambling) models from Table 2 and Table 3.", 
        "218": "We both initialize and then regularize back to these initial values, referring to this setting as \u201cuniversal\u201d.7\n7In these experiments, we tuned \u03bbw over {10, 1, 10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126, 10\u22127, 10\u22128, 0}\nThe results are shown in Table 8.", 
        "219": "Initializing and regularizing to the pretrained models significantly improves the performance for all three models, justifying our claim that these models serve a dual purpose: they can be used a black box semantic similarity function, and they possess rich knowledge that can be used to improve the performance of downstream tasks.", 
        "220": "5 Analysis    5.1 Error Analysis  We analyze the predictions of AVG and the recurrent networks, represented by LSTMAVG, on the 20 STS datasets.", 
        "221": "We choose LSTMAVG as it correlates slightly less strongly with AVG than the GRAN on the results over all SemEval datasets used for evaluation.", 
        "222": "We scale the models\u2019 cosine similarities to lie within [0, 5], then compare the predicted similarities of LSTMAVG and AVG to the gold similarities.", 
        "223": "We analyzed instances in which each model would tend to overestimate or underestimate the gold similarity relative to the other.", 
        "224": "These are illustrated in Table 7.", 
        "225": "We find that AVG tends to overestimate the semantic similarity of a sentence pair, relative to LSTMAVG, when the two sentences have a lot of\nand \u03bbc over {10, 1, 10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126, 0}.", 
        "226": "word or synonym overlap, but have either important differences in key semantic roles or where one sentence has significantly more content than the other.", 
        "227": "These phenomena are shown in examples 1 and 2 in Table 7.", 
        "228": "Conversely, AVG tends to underestimate similarity when there are one-word-tomultiword paraphrases between the two sentences as shown in examples 3 and 4.", 
        "229": "LSTMAVG tends to overestimate similarity when the two inputs have similar sequences of syntactic categories, but the meanings of the sentences are different (examples 5, 6, and 7).", 
        "230": "Instances of LSTMAVG underestimating the similarity relative to AVG are relatively rare, and those that we found did not have any systematic patterns.", 
        "231": "5.2 GRAN Gate Analysis  We also investigate what is learned by the gating function of the GATED RECURRENT AVERAGING NETWORK.", 
        "232": "We are interested to see whether its estimates of importance correlate with those of traditional syntactic and (shallow) semantic analysis.", 
        "233": "We use the oracle trained GATED RECURRENT AVERAGING NETWORK from Table 3 and calculate the L1 norm of the gate after embedding 10,000 sentences from English Wikipedia.8 We also automatically tag and parse these sentences using the Stanford dependency parser (Manning et al., 2014).", 
        "234": "We then compute the average gate L1 norms for particular part-of-speech tags, dependency arc labels, and their conjunction.", 
        "235": "Table 9 shows the highest/lowest average norm tags and dependency labels.", 
        "236": "The network prefers nouns, especially proper nouns, as well as cardinal numbers, which is sensible as these are among the most discriminative features of a sentence.", 
        "237": "Analyzing the dependency relations, we find\n8We selected only sentences of less than or equal to 15 tokens to ensure more accurate parsing.", 
        "238": "that nouns in the object position tend to have higher weight than nouns in the subject position.", 
        "239": "This may relate to topic and focus; the object may be more likely to be the \u201cnew\u201d information related by the sentence, which would then make it more likely to be matched by the other sentence in the paraphrase pair.", 
        "240": "We find that the weights of adjectives depend on their position in the sentence, as shown in Table 10.", 
        "241": "The highest norms appear when an adjective is an xcomp, acomp, or root; this typically means it is residing in an object-like position in its clause.", 
        "242": "Adjectives that modify a noun (amod) have\nmedium weight, and those that modify another adjective or verb (advmod) have low weight.", 
        "243": "Lastly, we analyze words tagged as VBG, a highly ambiguous tag that can serve many syntactic roles in a sentence.", 
        "244": "As shown in Table 11, we find that when they are used to modify a noun (amod) or in the object position of a clause (xcomp, pcomp) they have high weight.", 
        "245": "Medium weight appears when used in verb phrases (root, vmod) and low weight when used as prepositions or auxiliary verbs (prep, auxpass).", 
        "246": "6 Conclusion  We showed how to modify and regularize LSTMs to improve their performance for learning paraphrastic sentence embeddings in both transfer and supervised settings.", 
        "247": "We also introduced a new recurrent network, the GATED RECURRENT AVERAGING NETWORK, that improves upon both AVG and LSTMs for these tasks, and we release our code and trained models.", 
        "248": "Furthermore, we analyzed the different errors produced by AVG and the recurrent methods and found that the recurrent methods were learning composition that wasn\u2019t being captured by AVG.", 
        "249": "We also investigated the GRAN in order to better understand the compositional phenomena it was learning by analyzing the L1 norm of its gate over various inputs.", 
        "250": "Future work will explore additional data sources, including from aligning different translations of novels (Barzilay and McKeown, 2001), aligning new articles of the same topic (Dolan et al., 2004), or even possibly using machine translation systems to translate bilingual text into paraphrastic sentence pairs.", 
        "251": "Our new techniques, combined with the promise of new data sources, offer a great deal of potential for improved universal paraphrastic sentence embeddings.", 
        "252": "Acknowledgments  We thank the anonymous reviewers for their valuable comments.", 
        "253": "This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.", 
        "254": "We thank the developers of Theano (Theano Development Team, 2016) and NVIDIA Corporation for donating GPUs used in this research."
    }, 
    "document_id": "P17-1190.pdf.json"
}
