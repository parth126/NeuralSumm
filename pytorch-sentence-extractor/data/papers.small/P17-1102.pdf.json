{
    "abstract_sentences": {
        "1": "The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery.", 
        "2": "One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document\u2019s content and can facilitate fast information processing.", 
        "3": "In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word\u2019s occurrences into a biased PageRank.", 
        "4": "Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task.", 
        "5": "Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09%."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1105\u20131115 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1102  1 Introduction  The current Scholarly Web contains many millions of scientific documents.", 
        "2": "For example, Google Scholar is estimated to have more than 100 million documents.", 
        "3": "On one hand, these rapidly-growing scholarly document collections offer benefits for knowledge discovery, and on the other hand, finding useful information has become very challenging.", 
        "4": "Keyphrases associated with a document typically provide a high-level topic description of the document and can allow for efficient information processing.", 
        "5": "In addition, keyphrases are shown to be rich sources of information in many natural language processing and information retrieval tasks such as scientific paper summarization, classification, recommendation, clustering, and search (Abu-Jbara and Radev, 2011; Qazvinian et al.,\n2010; Jones and Staveley, 1999; Zha, 2002; Zhang et al., 2004; Hammouda et al., 2005).", 
        "6": "Due to their importance, many approaches to keyphrase extraction have been proposed in the literature along two lines of research: supervised and unsupervised (Hasan and Ng, 2014, 2010).", 
        "7": "In the supervised line of research, keyphrase extraction is formulated as a binary classification problem, where candidate phrases are classified as either positive (i.e., keyphrases) or negative (i.e., non-keyphrases) (Frank et al., 1999; Hulth, 2003).", 
        "8": "Various feature sets and classification algorithms yield different extraction systems.", 
        "9": "For example, Frank et al.", 
        "10": "(1999) developed a system that extracts two features for each candidate phrase, i.e., the tf-idf of the phrase and its distance from the beginning of the target document, and uses them as input to Na\u0131\u0308ve Bayes classifiers.", 
        "11": "Although supervised approaches typically perform better than unsupervised approaches (Kim et al., 2013), the requirement for large human-annotated corpora for each field of study has led to significant attention towards the design of unsupervised approaches.", 
        "12": "In the unsupervised line of research, keyphrase extraction is formulated as a ranking problem with graph-based ranking techniques being considered state-of-the-art (Hasan and Ng, 2014).", 
        "13": "These graph-based techniques construct a word graph from each target document, such that nodes correspond to words and edges correspond to word association patterns.", 
        "14": "Nodes are then ranked using graph centrality measures such as PageRank (Mihalcea and Tarau, 2004; Liu et al., 2010) or HITS (Litvak and Last, 2008), and the top ranked phrases are returned as keyphrases.", 
        "15": "Since their introduction, many graph-based extensions have been proposed, which aim at modeling various types of information.", 
        "16": "For example, Wan and Xiao (2008) proposed a model that incorporates a local\n1105\nFactorizing Personalized Markov Chains for Next-Basket Recommendation by Steffen Rendle, Christoph Freudenthaler and Lars Schmidt-Thieme Recommender systems are an important component of many websites.", 
        "17": "Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC).", 
        "18": "MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences.", 
        "19": "[...] we present a method bringing both approaches together.", 
        "20": "Our method is based on personalized transition graphs over underlying Markov chains.", 
        "21": "[...] our factorized personalized MC (FPMC) model subsumes both a common Markov chain and the normal matrix factorization model.", 
        "22": "[...] we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data.", 
        "23": "[...]\nAuthor-input keyphrases: Basket Recommendation, Markov Chain, Matrix Factorization\nFigure 1: The title and abstract of a WWW paper by Rendle et al.", 
        "24": "(2010) and the author-input keyphrases for the paper.", 
        "25": "Red bold phrases represent the gold-standard keyphrases for the document.", 
        "26": "neighborhood of the target document corresponding to its textually-similar documents, computed using the cosine similarity between the tf-idf vectors of documents.", 
        "27": "Liu et al.", 
        "28": "(2010) assumed a mixture of topics over documents and proposed to use topic models to decompose these topics in order to select keyphrases from all major topics.", 
        "29": "Keyphrases are then ranked by aggregating the topic-specific scores obtained from several topicbiased PageRanks.", 
        "30": "We posit that other information can be leveraged that has the potential to improve unsupervised keyphrase extraction.", 
        "31": "For example, in a scholarly domain, keyphrases generally occur on positions very close to the beginning of a document and occur frequently.", 
        "32": "Figure 1 shows an anecdotal example illustrating this behavior using the 2010 best paper award winner in the World Wide Web conference.", 
        "33": "The author input keyphrases are marked with red bold in the figure.", 
        "34": "Notice in this example the high frequency of the keyphrase \u201cMarkov chain\u201d that occurs very early in the document (even from its title).", 
        "35": "Hence, can we design an effective unsupervised approach to keyphrase extraction by jointly exploiting words\u2019 position information and their frequency in documents?", 
        "36": "We specifically address this question using research papers as a case study.", 
        "37": "The result of this extraction task will aid indexing of documents in digital libraries, and hence, will lead to improved organization, search, retrieval, and recommendation of scientific documents.", 
        "38": "The importance of keyphrase extraction from research papers is also emphasized by the SemEval Shared Tasks on this topic from 20171 and 2010 (Kim et al., 2010).", 
        "39": "Our contributions are as follows:\n1http://alt.qcri.org/semeval2017/task10/\n\u2022 We propose an unsupervised graph-based model, called PositionRank, that incorporates information from all positions of a word\u2019s occurrences into a biased PageRank to score keywords that are later used to score and rank keyphrases in research papers.", 
        "40": "\u2022 We show that PositionRank that aggregates information from all positions of a word\u2019s occurrences performs better than a model that uses only the first position of a word.", 
        "41": "\u2022 We experimentally evaluate PositionRank on three datasets of research papers and show statistically significant improvements over PageRank-based models that do not take into account word positions, as well as over strong baselines for keyphrase extraction.", 
        "42": "The rest of the paper is organized as follows.", 
        "43": "We summarize related work in the next section.", 
        "44": "PositionRank is described in Section 3.", 
        "45": "We then present the datasets of research papers, and our experiments and results in Section 4.", 
        "46": "Finally, we conclude the paper in Section 5.", 
        "47": "2 Related Work  Many supervised and unsupervised approaches to keyphrase extraction have been proposed in the literature (Hasan and Ng, 2014).", 
        "48": "Supervised approaches use annotated documents with \u201ccorrect\u201d keyphrases to train classifiers for discriminating keyphrases from nonkeyphrases for a document.", 
        "49": "KEA (Frank et al., 1999) and GenEx (Turney, 2000) are two representative supervised approaches with the most important features being the frequency and the position of a phrase in a target document.", 
        "50": "Hulth\n(2003) used a combination of lexical and syntactic features such as the collection frequency and the part-of-speech tag of a phrase in conjunction with a bagging technique.", 
        "51": "Nguyen and Kan (2007) extended KEA to include features such as the distribution of candidate phrases in different sections of a research paper, and the acronym status of a phrase.", 
        "52": "In a different work, Medelyan et al.", 
        "53": "(2009) extended KEA to integrate information from Wikipedia.", 
        "54": "Lopez and Romary (2010) used bagged decision trees learned from a combination of features including structural features (e.g., the presence of a phrase in particular sections of a document) and lexical features (e.g., the presence of a candidate phrase in WordNet or Wikipedia).", 
        "55": "Chuang et al.", 
        "56": "(2012) proposed a model that incorporates a set of statistical and linguistic features (e.g., tf-idf, BM25, part-of-speech filters) for identifying descriptive terms in a text.", 
        "57": "Caragea et al.", 
        "58": "(2014a) designed features based on information available in a document network (such as a citation network) and used them with traditional features in a supervised framework.", 
        "59": "In unsupervised approaches, various measures such as tf-idf and topic proportions are used to score words, which are later aggregated to obtain scores for phrases (Barker and Cornacchia, 2000; Zhang et al., 2007; Liu et al., 2009).", 
        "60": "The ranking based on tf-idf has been shown to work well in practice (Hasan and Ng, 2014, 2010), despite its simplicity.", 
        "61": "Graph-based ranking methods and centrality measures are considered stateof-the-art for unsupervised keyphrase extraction.", 
        "62": "Mihalcea and Tarau (2004) proposed TextRank for scoring keyphrases by applying PageRank on a word graph built from adjacent words within a document.", 
        "63": "Wan and Xiao (2008) extended TextRank to SingleRank by adding weighted edges between words that co-occur in a window of variable size w \u2265 2.", 
        "64": "Textually-similar neighboring documents are included in ExpandRank (Wan and Xiao, 2008) to compute more accurate word cooccurrence information.", 
        "65": "Gollapalli and Caragea (2014) extended ExpandRank to integrate information from citation networks where papers cite one another.", 
        "66": "Lahiri et al.", 
        "67": "(2014) extracted keyphrases from documents using various centrality measures such as node degree, clustering coefficient and closeness.", 
        "68": "Martinez-Romo et al.", 
        "69": "(2016) used information from WordNet to enrich the semantic relationships between the words in the graph.", 
        "70": "Several unsupervised approaches leverage word clustering techniques such as first grouping candidate words into topics and then, extracting one representative keyphrase from each topic (Liu et al., 2009; Bougouin et al., 2013).", 
        "71": "Liu et al.", 
        "72": "(2010) extended topic-biased PageRank (Haveliwala, 2003) to kephrase extraction.", 
        "73": "In particular, they decomposed a document into multiple topics, using topic models, and applied a separate topicbiased PageRank for each topic.", 
        "74": "The PageRank scores from each topic were then combined into a single score, using as weights the topic proportions returned by topic models for the document.", 
        "75": "The best performing keyphrase extraction system in SemEval 2010 (El-Beltagy and Rafea, 2010) used statistical observations such as term frequencies to filter out phrases that are unlikely to be keyphrases.", 
        "76": "More precisely, thresholding on the frequency of phrases is applied, where the thresholds are estimated from the data.", 
        "77": "The candidate phrases are then ranked using the tf-idf model in conjunction with a boosting factor which aims at reducing the bias towards single word terms.", 
        "78": "Danesh et al.", 
        "79": "(2015) computed an initial weight for each phrase based on a combination of statistical heuristics such as the tf-idf score and the first position of a phrase in a document.", 
        "80": "Phrases and their initial weights are then incorporated into a graph-based algorithm which produces the final ranking of keyphrase candidates.", 
        "81": "Le et al.", 
        "82": "(2016) showed that the extraction of keyphrases from a document can benefit from considering candidate phrases with part of speech tags other than nouns or adjectives.", 
        "83": "Adar and Datta (2015) extracted keyphrases by mining abbreviations from scientific literature and built a semantically hierarchical keyphrase database.", 
        "84": "Word embedding vectors were also employed to measure the relatedness between words in graph based models (Wang et al., 2014).", 
        "85": "Many of the above approaches, both supervised and unsupervised, are compared and analyzed in the ACL survey on keyphrase extraction by Hasan and Ng (2014).", 
        "86": "In contrast to the above approaches, we propose PositionRank, aimed at capturing both highly frequent words or phrases and their position in a document.", 
        "87": "Despite that the relative position of a word in a document is shown to be a very effective feature in supervised keyphrase extraction (Hulth, 2003; Zhang et al., 2007), to our knowledge, the position information has not been used before in unsupervised methods.", 
        "88": "The strong contribution of\nthis paper is the design of a position-biased PageRank model that successfully incorporates all positions of a word\u2019s occurrences, which is different from supervised models that use only the first position of a word.", 
        "89": "Our model assigns higher probabilities to words found early on in a document instead of using a uniform distribution over words.", 
        "90": "3 Proposed Model  In this section, we describe PositionRank, our fully unsupervised, graph-based model, that simultaneously incorporates the position of words and their frequency in a document to compute a biased PageRank score for each candidate word.", 
        "91": "Graph-based ranking algorithms such as PageRank (Page et al., 1998) measure the importance of a vertex within a graph by taking into account global information computed recursively from the entire graph.", 
        "92": "For each word, we compute a weight by aggregating information from all positions of the word\u2019s occurrences.", 
        "93": "This weight is then incorporated into a biased PageRank algorithm in order to assign a different \u201cpreference\u201d to each word.", 
        "94": "3.1 PositionRank  The PositionRank algorithm involves three essential steps: (1) the graph construction at word level; (2) the design of Position-Biased PageRank; and (3) the formation of candidate phrases.", 
        "95": "These steps are detailed below.", 
        "96": "3.1.1 Graph Construction  Let d be a target document for extracting keyphrases.", 
        "97": "We first apply the part-of-speech filter using the NLP Stanford toolkit and then select as candidate words only nouns and adjectives, similar to previous works (Mihalcea and Tarau, 2004; Wan and Xiao, 2008).", 
        "98": "We build a word graph G = (V,E) for d such that each unique word that passes the part-of-speech filter corresponds to a node in G. Two nodes vi and vj are connected by an edge (vi, vj) \u2208 E if the words corresponding to these nodes co-occur within a window of w contiguous tokens in the content of d. The weight of an edge (vi, vj) \u2208 E is computed based on the co-occurrence count of the two words within a window ofw successive tokens in d. Note that the graph can be constructed both directed and undirected.", 
        "99": "However, Mihalcea and Tarau (2004) showed that the type of graph used to represent the text does not significantly influence the per-\nformance of keyphrase extraction.", 
        "100": "Hence, in this work, we build undirected graphs.", 
        "101": "3.1.2 Position-Biased PageRank  Formally, letG be an undirected graph constructed as above and let M be its adjacency matrix.", 
        "102": "An element mij \u2208 M is set to the weight of edge (vi, vj) if there exist an edge between nodes vi and vj , and is set to 0 otherwise.", 
        "103": "The PageRank score of a node vi is recursively computed by summing the normalized scores of nodes vj , which are linked to vi (as explained below).", 
        "104": "Let S denote the vector of PageRank scores, for all vi \u2208 V .", 
        "105": "The initial values of S are set to 1|V | .", 
        "106": "The PageRank score of each node at step t+1, can then be computed recursively using:\nS(t+ 1) = M\u0303 \u00b7 S(t) (1)\nwhere M\u0303 is the normalized form of matrixM with m\u0303ij \u2208 M\u0303 defined as:\nm\u0303ij =\n{ mij/ \u2211|V | j=1mij if \u2211|V | j=1mij 6= 0\n0 otherwise\nThe PageRank computation can be seen as a Markov Chain process in which nodes represent states and the links between them are the transitions.", 
        "107": "By recursively applying Eq.", 
        "108": "(1), we obtain the principal eigenvector, which represents the stationary probability distribution of each state, in our case of each node (Manning et al., 2008).", 
        "109": "To ensure that the PageRank (or the random walk) does not get stuck into cycles of the graph, a damping factor \u03b1 is added to allow the \u201cteleport\u201d operation to another node in the graph.", 
        "110": "Hence, the computation of S becomes:\nS = \u03b1 \u00b7 M\u0303 \u00b7 S + (1\u2212 \u03b1) \u00b7 p\u0303 (2)\nwhere S is the principal eigenvector and p\u0303 is a vector of length |V | with all elements 1|V | .", 
        "111": "The vector p\u0303 indicates that, being in a node vi, the random walk can jump to any other node in the graph with equal probability.", 
        "112": "By biasing p\u0303, the random walk would prefer nodes that have higher probability in the graph (Haveliwala, 2003).", 
        "113": "The idea of PositionRank is to assign larger weights (or probabilities) to words that are found early in a document and are frequent.", 
        "114": "Specifically, we want to assign a higher probability to a word found on the 2nd position as compared to a word\nfound on the 50th position in the same document.", 
        "115": "We weigh each candidate word with its inverse position in the document before any filters are applied.", 
        "116": "If the same word appears multiple times in the target document, then we sum all its position weights.", 
        "117": "For example, if a word is found on the following positions: 2nd, 5th and 10th, its weight is: 12 + 1 5 + 1 10 = 4 5 = 0.8.", 
        "118": "Summing up the position weights for a given word aims to grant more confidence to frequently occurring words by taking into account the position weight of each occurrence.", 
        "119": "Then, the vector p\u0303 is set to the normalized weights for each candidate word as follows: p\u0303 = [\np1 p1+p2+...+p|V | , p2p1+p2+...+p|V | , ..., p|V | p1+p2+...+p|V |\n]\nThe PageRank score of a vertex vi, i.e., S(vi), can be obtained in an algebraic way by recursively computing the following equation:\nS(vi) = (1\u2212 \u03b1) \u00b7 p\u0303i + \u03b1 \u00b7 \u2211\nvj\u2208Adj(vi)\nwji O(vj) S(vj)\nwhere O(vj) = \u2211\nvk\u2208Adj(vj)wjk and p\u0303i is the weight found in the vector p\u0303 for vertex vi.", 
        "120": "In our experiments, the words\u2019 PageRank scores are recursively computed until the difference between two consecutive iterations is less than 0.001 or a number of 100 iterations is reached.", 
        "121": "3.1.3 Forming Candidate Phrases  Candidate words that have contiguous positions in a document are concatenated into phrases.", 
        "122": "We consider noun phrases that match the regular expression (adjective)*(noun)+, of length up to three, (i.e., unigrams, bigrams, and trigrams).", 
        "123": "Finally, phrases are scored by using the sum of scores of individual words that comprise the phrase (Wan and Xiao, 2008).", 
        "124": "The top-scoring phrases are output as predictions (i.e., the predicted keyphrases for the document).", 
        "125": "4 Experiments and Results    4.1 Datasets and Evaluation Metrics  In order to evaluate the performance of PositionRank, we carried out experiments on three datasets.", 
        "126": "The first and second datasets were made available by Gollapalli and Caragea (2014).2 These datasets are compiled from the CiteSeerX digital library (Giles et al., 1998) and consist of\n2http://www.cse.unt.edu/\u223cccaragea/keyphrases.html\nresearch papers from the ACM Conference on Knowledge Discovery and Data Mining (KDD) and the World Wide Web Conference (WWW).", 
        "127": "The third dataset was made available by Nguyen and Kan (2007) and consist of research papers from various disciplines.", 
        "128": "In experiments, we use the title and abstract of each paper to extract keyphrases.", 
        "129": "The author-input keyphrases are used as gold-standard for evaluation.", 
        "130": "All three datasets are summarized in Table 1, which shows the number of papers in each dataset, the total number of keyphrases (Kp), the average number of keyphrases per document (AvgKp), and a brief insight into the length and number of available keyphrases.", 
        "131": "Evaluation Metrics.", 
        "132": "We use mean reciprocal rank (MRR) curves to illustrate our experimental findings.", 
        "133": "MRR gives the averaged ranking of the first correct prediction and is defined as:\nMRR = 1|D| \u2211 d\u2208D 1 rd\nwhere D is the collection of documents and rd is the rank at which the first correct keyphrase of document d was found.", 
        "134": "We also summarize the results in terms of Precision, Recall, and F1score in a table to contrast PositionRank with previous models since these metrics are widely used in previous works (Hulth, 2003; Wan and Xiao, 2008; Mihalcea and Tarau, 2004; Hasan and Ng, 2014).", 
        "135": "To compute \u201cperformance@k\u201d (such as MRR@k), we examine the top-k predictions (with k ranging from 1 to 10).", 
        "136": "We use average k to refer to the average number of keyphrases for a particular dataset as listed in Table 1.", 
        "137": "For example, average k = 5 for the WWW dataset.", 
        "138": "For comparison purposes, we used Porter Stemmer to reduce both predicted and gold keyphrases to a base form.", 
        "139": "4.2 Results and Discussion  Our experiments are organized around several questions, which are discussed below.", 
        "140": "How sensitive is PositionRank to its parameters?", 
        "141": "One parameter of our model that can influence its performance is the window size w, which determines how edges are added between candidate words in the graph.", 
        "142": "We experimented with values of w ranging from 2 to 10 in steps of 1 and chose several configurations for illustration.", 
        "143": "Figure 2 shows the MRR curves of PositionRank for different values of w, on all three datasets.", 
        "144": "As can be seen from the figure, the performance of our model does not change significantly as w changes.", 
        "145": "In addition to the window size, our model has one more parameter, i.e., the damping factor \u03b1.", 
        "146": "In order to understand its influence on the performance of PositionRank, we experimented with several values of \u03b1, e.g., 0.75, 0.8, 0.85, 0.9, and did not find significant differences in the performance of PositionRank (results not shown due to highly overlapping curves).", 
        "147": "Hence, in Equation 2, we set \u03b1 = 0.85 as in (Haveliwala, 2003).", 
        "148": "What is the impact of aggregating information from all positions of a word over using a word\u2019s first position only?", 
        "149": "In this experiment, we analyze the influence that position-weighted frequent words in a document would have on the performance of PositionRank.", 
        "150": "Specifically, we compare the performance of the model that aggregates information from all positions of a word\u2019s occurrences, referred as PositionRank - full model with that of the model that uses only the first position of a word, referred as PositionRank - fp.", 
        "151": "In the example from the previous section, a word occurring on positions 2nd, 5th, and 10th will have a weight of 12 + 1 5 + 1 10 = 4 5 = 0.8 in the full model, and a weight of 12 = 0.5 in the first position (fp) model.", 
        "152": "Note that the weights of words are normalized before they are used in the biased PageRank.", 
        "153": "Figure 3 shows the results of this experiment in terms of MRR for the top k predicted keyphrases, with k from 1 to 10, for all datasets, KDD, WWW, and Nguyen.", 
        "154": "As we can see from the figure, the performance of PositionRank - full model consistently outperforms its counterpart that uses the first position only, on all datasets.", 
        "155": "We can conclude\nfrom this experiment that aggregating information from all occurrences of a word acts as an important component in PositionRank.", 
        "156": "Hence, we use PositionRank - full model for further comparisons.", 
        "157": "How well does position information aid in unsupervised keyphrase extraction from research papers?", 
        "158": "In this experiment, we compare our position-biased PageRank model (PositionRank) with two PageRank-based models, TextRank and SingleRank, that do not make use of the position information.", 
        "159": "In TextRank, an undirected graph is built for each target paper, so that nodes correspond to words and edges are drawn between two words that occur next to each other in text, i.e., the window sizew is 2.", 
        "160": "SingleRank extends TextRank by adding edges between two words that co-occur in a window of w \u2265 2 contiguous words in text.", 
        "161": "Figure 4 shows the MRR curves comparing PositionRank with TextRank and SingleRank.", 
        "162": "As can be seen from the figure, PositionRank substantially outperforms both TextRank and SingleRank on all three datasets, illustrating that the words\u2019 positions contain significant hints that aid the keyphrase extraction task.", 
        "163": "PositionRank can successfully harness this information in an unsupervised setting to obtain good improvements in the extraction performance.", 
        "164": "For example, PositionRank that uses information from all positions of a word\u2019s occurrences yields improvements in MRR@average k of 17.46% for KDD, 20.18% for WWW, and 17.03% for Nguyen over SingleRank.", 
        "165": "How does PositionRank compare with other existing state-of-the-art methods?", 
        "166": "In Figure 5, we\ncompare PositionRank with several strong baselines: TF-IDF, ExpandRank, and TopicalPageRank (TPR) (Hasan and Ng, 2014; Wan and Xiao, 2008; Liu et al., 2010).", 
        "167": "We selected these baselines based on the ACL survey on keyphrase extraction by Hasan and Ng (2014).", 
        "168": "In TF-IDF, we calculate the tf score of each candidate word in the target document, whereas the idf component is estimated from all three datasets.", 
        "169": "In ExpandRank, we build an undirected graph from each paper and its local textual neighborhood and calculate the candidate words\u2019 importance scores using PageRank.", 
        "170": "We performed experiments with various numbers of textually-similar neighbors and present the best results for each dataset.", 
        "171": "In TPR, we build an undirected graph using information from the target paper.", 
        "172": "We then perform topic decomposition of the target document using topic models to infer the topic distribution of a document and to compute the probability of words in these topics.", 
        "173": "Last, we calculate the candidate words\u2019 importance scores by aggregating the scores from several topic-biased PageRanks (one PageRank per topic).", 
        "174": "We used the implementation of topic models from Mallet.3 To train the topic\n3http://mallet.cs.umass.edu/\nmodel, we used a subset of about 45, 000 paper abstracts extracted from the CiteSeerx scholarly big dataset introduced by Caragea et al.", 
        "175": "(2014b).", 
        "176": "For all models, the score of a phrase is obtained by summing the score of the constituent words in the phrase.", 
        "177": "From Figure 5, we can see that PositionRank achieves a significant increase in MRR over the baselines, on all datasets.", 
        "178": "For example, the highest relative improvement in MRR@average k for this experiment is as high as 29.09% achieved on the Nguyen collection.", 
        "179": "Among all models compared in Figure 5, ExpandRank is clearly the best performing baseline, while TPR achieves the lowest MRR values, on all datasets.", 
        "180": "4.3 Overall Performance  As already mentioned, prior works on keyphrase extraction report results also in terms of precision (P), recall (R), and F1-score (F1) (Hulth, 2003; Hasan and Ng, 2010; Liu et al., 2010; Wan and Xiao, 2008).", 
        "181": "Consistent with these works, in Table 2, we show the results of the comparison of PositionRank with all baselines, in terms of P, R and F1 for top k = 2, 4, 6, 8 predicted keyphrases, on all three datasets.", 
        "182": "As can be seen from the ta-\nble, PositionRank outperforms all baselines, on all datasets.", 
        "183": "For example, on WWW at top 6 predicted keyphrases, PositionRank achieves an F1score of 12.1% as compared to 11.2% achieved by ExpandRank and 10.7% achieved by both TFIDF and TPR.", 
        "184": "From the table, we can also see that ExpandRank is generally the best performing baseline on all datasets.", 
        "185": "However, it is interesting to note that, unlike PositionRank that uses information only from the target paper, ExpandRank adds external information from a textually-similar neighborhood of the target paper, and hence, is computationally more expensive.", 
        "186": "PositionRank-first position only (fp) typically performs worse than PositionRank-full model, but it still outperforms the baseline methods for most top k predicted keyphrases, on all datasets.", 
        "187": "For example, on Nguyen at top 4, PositionRank-fp achieves an F1-score of 10.5% compared to the\nbest baseline (TF-IDF in this case), which reaches only a score of 9.6%.", 
        "188": "A striking observation is that PositionRank outperforms TPR on all datasets.", 
        "189": "Compared with our model, TPR is a very complex model, which uses topic models to learn topics of words and infer the topic proportion of documents.", 
        "190": "Additionally, TPR has more parameters (e.g., the number of topics) that need to be tuned separately for each dataset.", 
        "191": "PositionRank is much less complex, it does not require an additional dataset (e.g., to train a topic model) and its performance is better than that of TPR.", 
        "192": "TF-IDF and ExpandRank are the best performing baselines, on all datasets, KDD, WWW, and Nguyen.", 
        "193": "For example, on KDD at k = 4, TF-IDF and ExpandRank yield an F1-score of 9.4% and 10.1%, respectively, compared with 8.4%, 9.0% and 8.9% achieved by TextRank, SingleRank and TPR, respectively.", 
        "194": "Geographically0.274 Focused0.134 Collaborative0.142 Crawling0.165 by Weizheng Gao, Hyun Chul Lee and Yingbo Miao A collaborative0.142 crawler0.165 is a group0.025 of crawling0.165 nodes0.033, in which each crawling0.165 node0.033 is responsible0.012 for a specific0.010 portion0.010 of the web0.015.", 
        "195": "We study the problem0.007 of collecting0.011 geographically0.274 aware0.006 pages0.018 using collaborative0.142 crawling0.165 strategies0.017.", 
        "196": "We first propose several collaborative0.142 crawling0.165 strategies0.017 for the geographically0.274 focused0.134 crawling0.165, whose goal0.004 is to collect web0.015 pages0.018 about specified0.010 geographic0.274 locations0.003 by considering features0.005 like URL0.006 address0.005 of page0.018 [...] More precisely, features0.005 like URL0.006 address0.005 of page0.018 and extended0.004 anchor0.004 text0.004 of link0.004 are shown to yield the best overall performance0.003 for the geographically0.274 focused0.134 crawling0.165.", 
        "197": "Author-input keyphrases: collaborative crawling, geographically focused crawling, geographic entities\nFigure 6: The title and abstract of a WWW paper by Gao et al.", 
        "198": "(2006) and the author-input keyphrases for the paper.", 
        "199": "Bold dark red phrases represent predicted keyphrases for the document.", 
        "200": "With a paired t-test on our results, we found that the improvements in MRR, precision, recall, and F1-score for PositionRank are statistically significant (p-values < 0.05).", 
        "201": "4.4 Anecdotal Evidence  We show anecdotal evidence using a paper by Gao et al.", 
        "202": "(2006) that is part of the Nguyen dataset.", 
        "203": "Figure 6 shows the title and abstract of this paper together with the author-input keyphrases.", 
        "204": "We marked in bold dark red the candidate phrases that are predicted as keyphrases by our proposed model (PositionRank), in black the words that are selected as candidate phrases and in gray the words that are filtered out based on their part-ofspeech tags or the stopwords list being used.", 
        "205": "We show the probability (or weight) of each candidate word in its upper right corner.", 
        "206": "These weights are computed based on both the word\u2019s position and its frequency in the text.", 
        "207": "Note that our model uses these weights to bias the PageRank algorithm to prefer specific nodes in the graph.", 
        "208": "As we can see from the figure, component words of author\u2019s keyphrases such as: \u201ccollaborative,\u201d \u201ccrawling,\u201d \u201cfocused,\u201d and \u201cgeographically\u201d are assigned the highest scores while candidates such as \u201cperformance,\u201d \u201canchor,\u201d or \u201cfeatures\u201d are assigned very low weights, making them less likely to be chosen as keyphrases.", 
        "209": "5 Conclusion and Future Work  We proposed a novel unsupervised graph-based algorithm, called PositionRank, which incorporates both the position of words and their frequency\nin a document into a biased PageRank.", 
        "210": "To our knowledge, we are the first to integrate the position information in novel ways in unsupervised keyphrase extraction.", 
        "211": "Specifically, unlike supervised approaches that use only the first position information, we showed that modeling the entire distribution of positions for a word outperforms models that use only the first position.", 
        "212": "Our experiments on three datasets of research papers show that our proposed model achieves better results than strong baselines, with relative improvements in performance as high as 29.09%.", 
        "213": "In the future, it would be interesting to explore the performance of PositionRank on other types of documents, e.g., web pages and emails.", 
        "214": "Acknowledgments  We are grateful to Dr. C. Lee Giles for the CiteSeerX data that we used to create our KDD and WWW datasets as well as to train the topic models.", 
        "215": "We very much thank our anonymous reviewers for their constructive comments and feedback.", 
        "216": "This research was supported by the NSF award #1423337 to Cornelia Caragea.", 
        "217": "Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF."
    }, 
    "document_id": "P17-1102.pdf.json"
}
