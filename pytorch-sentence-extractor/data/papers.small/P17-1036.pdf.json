{
    "abstract_sentences": {
        "1": "Aspect extraction is an important and challenging task in aspect-based sentiment analysis.", 
        "2": "Existing works tend to apply variants of topic models on this task.", 
        "3": "While fairly successful, these methods usually do not produce highly coherent aspects.", 
        "4": "In this paper, we present a novel neural approach with the aim of discovering coherent aspects.", 
        "5": "The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings.", 
        "6": "Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space.", 
        "7": "In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects.", 
        "8": "Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 388\u2013397 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1036  1 Introduction  Aspect extraction is one of the key tasks in sentiment analysis.", 
        "2": "It aims to extract entity aspects on which opinions have been expressed (Hu and Liu, 2004; Liu, 2012).", 
        "3": "For example, in the sentence \u201cThe beef was tender and melted in my mouth\u201d, the aspect term is \u201cbeef\u201d.", 
        "4": "Two sub-tasks are performed in aspect extraction: (1) extracting all aspect terms (e.g., \u201cbeef\u201d) from a review corpus, (2) clustering aspect terms with similar meaning into categories where each category represents a single\naspect (e.g., cluster \u201cbeef\u201d, \u201cpork\u201d, \u201cpasta\u201d, and \u201ctomato\u201d into one aspect food).", 
        "5": "Previous works for aspect extraction can be categorized into three approaches: rule-based, supervised, and unsupervised.", 
        "6": "Rule-based methods usually do not group extracted aspect terms into categories.", 
        "7": "Supervised learning requires data annotation and suffers from domain adaptation problems.", 
        "8": "Unsupervised methods are adopted to avoid reliance on labeled data needed for supervised learning.", 
        "9": "In recent years, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Titov and McDonald, 2008; Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012) have become the dominant unsupervised approach for aspect extraction.", 
        "10": "LDA models the corpus as a mixture of topics (aspects), and topics as distributions over word types.", 
        "11": "While the mixture of aspects discovered by LDA-based models may describe a corpus fairly well, we find that the individual aspects inferred are of poor quality \u2013 aspects often consist of unrelated or loosely-related concepts.", 
        "12": "This may substantially reduce users\u2019 confidence in using such automated systems.", 
        "13": "There could be two primary reasons for the poor quality.", 
        "14": "Conventional LDA models do not directly encode word co-occurrence statistics which are the primary source of information to preserve topic coherence (Mimno et al., 2011).", 
        "15": "They implicitly capture such patterns by modeling word generation from the document level, assuming that each word is generated independently.", 
        "16": "Furthermore, LDA-based models need to estimate a distribution of topics for each document.", 
        "17": "Review documents tend to be short, thus making the estimation of topic distributions more difficult.", 
        "18": "In this work, we present a novel neural approach to tackle the weaknesses of LDA-based methods.", 
        "19": "We start with neural word embeddings that al-\n388\nready map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et al., 2013).", 
        "20": "We then filter the word embeddings within a sentence using an attention mechanism (Bahdanau et al., 2015) and use the filtered words to construct aspect embeddings.", 
        "21": "The training process for aspect embeddings is analogous to autoencoders, where we use dimension reduction to extract the common factors among embedded sentences and reconstruct each sentence through a linear combination of aspect embeddings.", 
        "22": "The attention mechanism deemphasizes words that are not part of any aspect, allowing the model to focus on aspect words.", 
        "23": "We call our proposed model Attention-based Aspect Extraction (ABAE).", 
        "24": "In contrast to LDA-based models, our proposed method explicitly encodes word-occurrence statistics into word embeddings, uses dimension reduction to extract the most important aspects in the review corpus, and uses an attention mechanism to remove irrelevant words to further improve coherence of the aspects.", 
        "25": "We have conducted extensive experiments on large review data sets.", 
        "26": "The results show that ABAE is effective in discovering meaningful and coherent aspects.", 
        "27": "It substantially outperforms baseline methods on multiple evaluation tasks.", 
        "28": "In addition, ABAE is intuitive and structurally simple.", 
        "29": "It can also easily scale to a large amount of training data.", 
        "30": "Therefore, it is a promising alternative to LDA-based methods proposed previously.", 
        "31": "2 Related Work  The problem of aspect extraction has been well studied in the past decade.", 
        "32": "Initially, methods were mainly based on manually defined rules.", 
        "33": "Hu and Liu (2004) proposed to extract different product features through finding frequent nouns and noun phrases.", 
        "34": "They also extracted opinion terms by finding the synonyms and antonyms of opinion seed words through WordNet.", 
        "35": "Following this, a number of methods have been proposed based on frequent item mining and dependency information to extract product aspects (Zhuang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011).", 
        "36": "These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns.", 
        "37": "Supervised learning approaches generally model aspect extraction as a standard sequence\nlabeling problem.", 
        "38": "Jin and Ho (2009) and Li et al.", 
        "39": "(2010) proposed to use hidden Markov models (HMM) and conditional random fields (CRF), respectively with a set of manually-extracted features.", 
        "40": "More recently, different neural models (Yin et al., 2016; Wang et al., 2016) were proposed to automatically learn features for CRF-based aspect extraction.", 
        "41": "Rule-based models are usually not refined enough to categorize the extracted aspect terms.", 
        "42": "On the other hand, supervised learning requires large amounts of labeled data for training purposes.", 
        "43": "Unsupervised approaches, especially topic models, have been proposed subsequently to avoid reliance on labeled data.", 
        "44": "Generally, the outputs of those models are word distributions or rankings for each aspect.", 
        "45": "Aspects are naturally obtained without separately performing extraction and categorization.", 
        "46": "Most existing works (Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012; Chen et al., 2014) are based on variants and extensions of LDA (Blei et al., 2003).", 
        "47": "Recently, Wang et al.", 
        "48": "(2015) proposed a restricted Boltzmann machine (RBM)-based model to simultaneously extract aspects and relevant sentiments of a given review sentence, treating aspects and sentiments as separate hidden variables in RBM.", 
        "49": "However, the RBM-based model proposed in (Wang et al., 2015) relies on a substantial amount of prior knowledge such as part-of-speech (POS) tagging and sentiment lexicons.", 
        "50": "A biterm topic model (BTM) that generates co-occurring word pairs was proposed in (Yan et al., 2013).", 
        "51": "We experimentally compare ABAE and BTM on multiple tasks in this paper.", 
        "52": "Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Hermann et al., 2015).", 
        "53": "Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task.", 
        "54": "Unlike previous works, in this paper, we apply attention to an unsupervised neural model.", 
        "55": "Our experimental results demonstrate its effectiveness under an unsupervised setting for aspect extraction.", 
        "56": "3 Model Description  We describe the Attention-based Aspect Extraction (ABAE) model in this section.", 
        "57": "The ultimate goal is to learn a set of aspect embeddings, where each aspect can be interpreted by looking at the nearest words (representative words) in the embedding space.", 
        "58": "We begin by associating each word w in our vocabulary with a feature vector ew \u2208 Rd.", 
        "59": "We use word embeddings for the feature vectors as word embeddings are designed to map words that often co-occur in a context to points that are close by in the embedding space (Mikolov et al., 2013).", 
        "60": "The feature vectors associated with the words correspond to the rows of a word embedding matrix E \u2208 RV\u00d7d, where V is the vocabulary size.", 
        "61": "We want to learn embeddings of aspects, where aspects share the same embedding space with words.", 
        "62": "This requires an aspect embedding matrix T \u2208 RK\u00d7d, where K, the number of aspects defined, is much smaller than V .", 
        "63": "The aspect embeddings are used to approximate the aspect words in the vocabulary, where the aspect words are filtered through an attention mechanism.", 
        "64": "Each input sample to ABAE is a list of indexes for words in a review sentence.", 
        "65": "Given such an input, two steps are performed as shown in Figure 1.", 
        "66": "First, we filter away non-aspect words by down-weighting them using an attention mechanism, and construct a sentence embedding zs from weighted word embeddings.", 
        "67": "Then, we try to reconstruct the sentence embedding as a linear combination of aspect embeddings from T. This process of dimension reduction and reconstruction, where ABAE aims to transform sentence embeddings of the filtered sentences (zs) into their reconstructions (rs) with the least possible amount of distortion, preserves most of the information of the aspect words in the K embedded aspects.", 
        "68": "We next describe the process in detail.", 
        "69": "3.1 Sentence Embedding with Attention Mechanism  We construct a vector representation zs for each input sentence s in the first step.", 
        "70": "In general, we want the vector representation to capture the most relevant information with regards to the aspect (topic) of the sentence.", 
        "71": "We define the sentence embedding zs as the weighted summation of word embeddings ewi , i = 1, ..., n corresponding to the\nword indexes in the sentence.", 
        "72": "zs =\nn\u2211\ni=1\naiewi .", 
        "73": "(1)\nFor each word wi in the sentence, we compute a positive weight ai which can be interpreted as the probability that wi is the right word to focus on in order to capture the main topic of the sentence.", 
        "74": "The weight ai is computed by an attention model, which is conditioned on the embedding of the word ewi as well as the global context of the sentence:\nai = exp(di)\u2211n j=1 exp(dj)\n(2)\ndi = e > wi \u00b7M \u00b7 ys (3)\nys = 1\nn\nn\u2211\ni=1\newi (4)\nwhere ys is simply the average of the word embeddings, which we believe captures the global context of the sentence.", 
        "75": "M \u2208 Rd\u00d7d is a matrix mapping between the global context embedding ys and the word embedding ew and is learned as part of the training process.", 
        "76": "We can think of the attention mechanism as a two-step process.", 
        "77": "Given a sentence, we first construct its representation by averaging all the word representations.", 
        "78": "Then the weight of a word is assigned by considering two things.", 
        "79": "First, we filter the word through the transformation M which is able to capture the relevance of the word to the K aspects.", 
        "80": "Then we capture the relevance of the filtered word to the sentence by taking the inner product of the filtered word to the global context ys.", 
        "81": "3.2 Sentence Reconstruction with Aspect Embeddings  We have obtained the sentence embedding.", 
        "82": "Now we describe how to compute the reconstruction of the sentence embedding.", 
        "83": "As shown in Figure 1, the reconstruction process consists of two steps of transitions, which is similar to an autoencoder.", 
        "84": "Intuitively, we can think of the reconstruction as a linear combination of aspect embeddings from T:\nrs = T > \u00b7 pt (5)\nwhere rs is the reconstructed vector representation, pt is the weight vector overK aspect embeddings, where each weight represents the probability that the input sentence belongs to the related aspect.", 
        "85": "pt can simply be obtained by reducing zs from d dimensions to K dimensions and then applying a softmax non-linearity that yields normalized non-negative weights:\npt = softmax (W \u00b7 zs + b) (6)\nwhere W, the weighted matrix parameter, and b, the bias vector, are learned as part of the training process.", 
        "86": "3.3 Training Objective  ABAE is trained to minimize the reconstruction error.", 
        "87": "We adopted the contrastive max-margin objective function used in previous work (Weston et al., 2011; Socher et al., 2014; Iyyer et al., 2016).", 
        "88": "For each input sentence, we randomly sample m sentences from our training data as negative samples.", 
        "89": "We represent each negative sample as ni which is computed by averaging its word embeddings.", 
        "90": "Our objective is to make the reconstructed embedding rs similar to the target sentence embedding zs while different from those negative samples.", 
        "91": "Therefore, the unregularized objective J is formulated as a hinge loss that maximize the inner product between rs and zs and simultaneously minimize the inner product between rs and the negative samples:\nJ(\u03b8) = \u2211\ns\u2208D\nm\u2211\ni=1\nmax(0, 1\u2212 rszs + rsni) (7)\nwhere D represents the training data set and \u03b8 = {E,T,M,W,b} represents the model parameters.", 
        "92": "3.4 Regularization Term  We hope to learn vector representations of the most representative aspects for a review dataset.", 
        "93": "However, the aspect embedding matrix T may suffer from redundancy problems during training.", 
        "94": "To ensure the diversity of the resulting aspect embeddings, we add a regularization term to the objective function J to encourage the uniqueness of each aspect embedding:\nU(\u03b8) = \u2016Tn \u00b7T>n \u2212 I\u2016 (8)\nwhere I is the identity matrix, and Tn is T with each row normalized to have length 1.", 
        "95": "Any nondiagonal element tij(i 6= j) in the matrix Tn \u00b7T>n corresponds to the dot product of two different aspect embeddings.", 
        "96": "U reaches its minimum value when the dot product between any two different aspect embeddings is zero.", 
        "97": "Thus the regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors.", 
        "98": "Our final objective function L is obtained by adding J and U :\nL(\u03b8) = J(\u03b8) + \u03bbU(\u03b8) (9)\nwhere \u03bb is a hyperparameter that controls the weight of the regularization term.", 
        "99": "4 Experimental Setup    4.1 Datasets  We evaluate our method on two real-word datasets.", 
        "100": "The detailed statistics of the datasets are summarized in Table 1.", 
        "101": "(1) Citysearch corpus: This is a restaurant review corpus widely used by previous works (Ganu et al., 2009; Brody and Elhadad, 2010; Zhao et al., 2010), which contains over 50,000 restaurant reviews from Citysearch New York.", 
        "102": "Ganu et al.", 
        "103": "(2009) also provided a subset of 3,400 sentences from the corpus with manually labeled aspects.", 
        "104": "These annotated sentences are used for evaluation of aspect identification.", 
        "105": "There are six manually defined aspect labels: Food, Staff, Ambience, Price, Anecdotes, and Miscellaneous.", 
        "106": "(2) BeerAdvocate: This is a beer review corpus introduced in (McAuley et al., 2012), containing over 1.5 million reviews.", 
        "107": "A subset of 1,000 reviews, corresponding to 9,245 sentences, are annotated with five aspect labels: Feel, Look, Smell, Taste, and Overall.", 
        "108": "4.2 Baseline Methods  To validate the performance of ABAE, we compare it against a number of baselines:\n(1) LocLDA (Brody and Elhadad, 2010): This method uses a standard implementation of LDA.", 
        "109": "In order to prevent the inference of global topics and direct the model towards rateable aspects, each sentence is treated as a separate document.", 
        "110": "(2) k-means: We initialize the aspect matrix T by using the k-means centroids of the word embeddings.", 
        "111": "To show the power of ABAE, we compare its performance with using the kmeans centroids directly.", 
        "112": "(3) SAS (Mukherjee and Liu, 2012): This is a hybrid topic model that jointly discovers both aspects and aspect-specific opinions.", 
        "113": "This model has been shown to be competitive among topic models in discovering meaningful aspects (Mukherjee and Liu, 2012; Wang et al., 2015).", 
        "114": "(4) BTM (Yan et al., 2013): This is a biterm topic model that is specially designed for short texts such as texts from social media and review sites.", 
        "115": "The major advantage of BTM over conventional LDA models is that it alleviates the problem of data sparsity in short documents by directly modeling the generation of unordered word-pair co-occurrences (biterms) over the corpus.", 
        "116": "It has been shown to perform better than conventional LDA models in discovering coherent topics.", 
        "117": "4.3 Experimental Settings  Review corpora are preprocessed by removing punctuation symbols, stop words, and words appearing less than 10 times.", 
        "118": "For LocLDA, we use the open-source implementation GibbsLDA++1 and for BTM, we use the implementation released by (Yan et al., 2013)2.", 
        "119": "We tune the hyperparameters of all topic model baselines on a held-out set\n1http://gibbslda.sourceforge.net 2http://code.google.com/p/btm/\nwith grid search using the topic coherence metric to be introduced later in Eq 10: for LocLDA, the Dirichlet priors \u03b1 = 0.05 and \u03b2 = 0.1; for SAS and BTM, \u03b1 = 50/K and \u03b2 = 0.1.", 
        "120": "We run 1,000 iterations of Gibbs sampling for all topic models.", 
        "121": "For the ABAE model, we initialize the word embedding matrix E with word vectors trained by word2vec with negative sampling on each dataset, setting the embedding size to 200, window size to 10, and negative sample size to 5.", 
        "122": "The parameters we use for training word embeddings are standard with no specific tuning to our data.", 
        "123": "We also initialize the aspect embedding matrix T with the centroids of clusters resulting from running k-means on word embeddings.", 
        "124": "Other parameters are initialized randomly.", 
        "125": "During the training process, we fix the word embedding matrix E and optimize other parameters using Adam (Kingma and Ba, 2014) with learning rate 0.001 for 15 epochs and batch size of 50.", 
        "126": "We set the number of negative samples per input sample m to 20, and the orthogonality penalty weight \u03bb to 1 by tuning the hyperparameters on a held-out set with grid search.", 
        "127": "The results reported for all models are the average over 10 runs.", 
        "128": "Following (Brody and Elhadad, 2010; Zhao et al., 2010), we set the number of aspects for the restaurant corpus to 14.", 
        "129": "We experimented with different number of aspects from 10 to 20 for the beer corpus.", 
        "130": "The results showed no major difference, so we also set it to 14.", 
        "131": "As in previous work (Brody and Elhadad, 2010; Zhao et al., 2010), we manually mapped each inferred aspect to one of the gold-standard aspects according to its top ranked representative words.", 
        "132": "In ABAE, representative words of an aspect can be found by looking at its nearest words in the embedding space using cosine as the similarity metric.", 
        "133": "5 Evaluation and Results  We describe the evaluation tasks and report the experimental results in this section.", 
        "134": "We evaluate ABAE on two criteria:\n\u2022 Is it able to find meaningful and semantically coherent aspects?", 
        "135": "\u2022 Is it able to improve aspect identification performance on real-world review datasets?", 
        "136": "5.1 Aspect Quality Evaluation  Table 2 presents all 14 aspects inferred by ABAE for the restaurant domain.", 
        "137": "Compared to gold-\nstandard labels, the inferred aspects are more finegrained.", 
        "138": "For example, it can distinguish main dishes from desserts, and drinks from food.", 
        "139": "5.1.1 Coherence Score  In order to objectively measure the quality of aspects, we use coherence score as a metric which has been shown to correlate well with human judgment (Mimno et al., 2011).", 
        "140": "Given an aspect z and a set of top N words of z, Sz = {wz1, ..., wzN}, the coherence score is calculated as follows:\nC(z;Sz) = N\u2211\nn=2\nn\u22121\u2211\nl=1\nlog D2(w\nz n, w z l ) + 1\nD1(wzl ) (10)\nwhere D1(w) is the document frequency of word w and D2(w1, w2) is the co-document frequency of words w1 and w2.", 
        "141": "A higher coherence score indicates a better aspect interpretability, i.e., more meaningful and semantically coherent.", 
        "142": "Figure 2 shows the average coherence score of each model which is computed as 1 K \u2211K k=1C(zk;S\nzk) on both the restaurant domain and beer domain.", 
        "143": "From the results, we make the following observations: (1) ABAE outperforms previous models for all ranked buckets.", 
        "144": "(2) BTM performs slightly better than LocLDA and SAS.", 
        "145": "This may be because BTM directly models the generation of biterms, while conventional LDA just implicitly captures such patterns by modeling word generation from the document level.", 
        "146": "(3) It is interesting to note that performing k-means on the word embeddings is sufficient to perform better than all topic model baselines, including BTM.", 
        "147": "This indicates that neural word embedding is a better model for capturing co-occurrence than LDA, even for BTM which specifically models the generation of co-occurring word pairs.", 
        "148": "5.1.2 User Evaluation  As we want to discover a set of aspects that the human user finds agreeable, it is also necessary\nto carry out user evaluation directly.", 
        "149": "Following the experimental setting in (Chen et al., 2014), we recruited three human judges.", 
        "150": "Each aspect is labeled as coherent if the majority of judges assess that most of its top 50 terms coherently represent a product aspect.", 
        "151": "The numbers of coherent aspects discovered by each model are shown in Table 3.", 
        "152": "ABAE discovers the most number of coherent aspects compared with other models.", 
        "153": "For a coherent aspect, each of its top terms is labeled as correct if and only if the majority of judges assess that it reflects the related aspect.", 
        "154": "We adopt precision@n (or p@n) to evaluate the results, which was also used in (Mukherjee and Liu, 2012; Chen et al., 2014).", 
        "155": "Figure 3 shows the average p@n results over all coherent aspects for each domain.", 
        "156": "We can see that the user evaluation results correlate well with the coherence scores shown in Figure 2, where ABAE substantially outperforms all other models for all ranked buckets, especially for large values of n.  5.2 Aspect Identification  We evaluate the performance of sentence-level aspect identification on both domains using the annotated sentences shown in Table 1.", 
        "157": "The evaluation criterion is to judge how well the predictions match the true labels, measured by precision, recall, and F1 scores.", 
        "158": "The results4 are shown in Table 4 and Table 5.", 
        "159": "Given a review sentence, ABAE first assigns an inferred aspect label which corresponds to the highest weight in pt calculated as shown in Equation 6 .", 
        "160": "And we then assign the gold-standard label to the sentence according to the mapping between inferred aspects and gold-standard labels.", 
        "161": "3k-means assigns a sentence an inferred aspect whose embedding is the closest to the averaged word embeddings of the sentence.", 
        "162": "4Note that the values of P/R/F1 reported are the average over 10 runs (except some values taken from published results in Table 4).", 
        "163": "Thus the F1 values cannot be computed directly from corresponding P/R values\nFor the restaurant domain, we follow the experimental settings of previous work (Brody and Elhadad, 2010; Zhao et al., 2010; Wang et al., 2015) to make our results comparable.", 
        "164": "To do that, (1) we only used the single-label sentences for evaluation to avoid ambiguity (about 83% of labeled sentences have a single label), and (2) we only evaluated on three major aspects, namely Food, Staff, and Ambience.", 
        "165": "The other aspects do not show clear patterns in either word usage or writing style, which makes these aspects very hard for even humans to identify.", 
        "166": "Besides the baseline models, we also compare the results with other published models, including MaxEnt-LDA (ME-LDA) (Zhao et al., 2010) and SERBM (Wang et al., 2015).", 
        "167": "SERBM has reported state-of-the-art results for aspect identification on the restaurant corpus to date.", 
        "168": "However, SERBM relies on a substantial amount of prior knowledge.", 
        "169": "We make the following observations from Table 4: (1) ABAE outperforms all other models on F1 score for aspects Staff and Ambience.", 
        "170": "(2) The F1 score of ABAE for Food is worse than SERBM while its precision is very high.", 
        "171": "We analyzed the errors and found that most of the sentences we failed to recognize as Food are general descriptions without specific food words appearing.", 
        "172": "For example, the true label for the sentence \u201cThe food is prepared quickly and efficiently.\u201d is Food.", 
        "173": "ABAE assigns Staff to it as the highly focused words according to the attention mechanism are quickly and efficiently which are more related to Staff.", 
        "174": "In fact, although this sentence contains the word food, we think it is a rather general description of service.", 
        "175": "(3) ABAE substantially outperforms k-means for this task although both methods perform well for extracting coherent aspects as shown in Figure 2 and Figure 3.", 
        "176": "This shows the power brought by the attention mechanism, which is able to capture the main topic of a sentence by only focusing on aspect-related words.", 
        "177": "For the beer domain, in addition to the five goldstandard aspect labels, we also combined Taste and Smell to form a single aspect \u2013 Taste+Smell.", 
        "178": "This is because these two aspects are very similar\nand many words can be used to describe both aspects.", 
        "179": "For example, the words spicy, bitter, fresh, sweet, etc.", 
        "180": "are top ranked representative words in both aspects, which makes it very hard even for humans to distinguish them.", 
        "181": "Since Taste and Smell are highly correlated and difficult to separate in real life, a natural way to evaluate is to treat them as a single aspect.", 
        "182": "We can see from Table 5 that due to the issue described above, all models perform poorly on Taste and Smell.", 
        "183": "ABAE outperforms previous models in F1 scores on all aspects except for Taste.", 
        "184": "The results demonstrate the capability of ABAE in identifying separable aspects.", 
        "185": "5.3 Validating the Effectiveness of Attention Model  Figure 4 shows the weights of words assigned by the attention model for some example sentences.", 
        "186": "As we can see, the weights learned by the model correspond very strongly with human intuition.", 
        "187": "In order to evaluate how attention model affects the overall performance of ABAE, we conduct experiments to compare ABAE and ABAE\u2212 on aspect identification, where ABAE\u2212 denotes the model in which the attention layer is switched off and sentence embedding is calculated by averaging its word embeddings: zs = 1n \u2211n i=1 ewi .", 
        "188": "The results on the restaurant domain are shown in Table 6.", 
        "189": "ABAE achieves substantially higher precision and recall on all aspects compared with\nABAE\u2212, which demonstrates the effectiveness of the attention mechanism.", 
        "190": "6 Conclusion  We have presented ABAE, a simple yet effective neural attention model for aspect extraction.", 
        "191": "In contrast to LDA models, ABAE explicitly captures word co-occurrence patterns and overcomes the problem of data sparsity present in review corpora.", 
        "192": "Our experimental results demonstrated that ABAE not only learns substantially higher quality aspects, but also more effectively captures the aspects of reviews than previous methods.", 
        "193": "To the best of our knowledge, we are the first to propose an unsupervised neural approach for aspect extraction.", 
        "194": "ABAE is intuitive and structurally simple, and also scales up well.", 
        "195": "All these benefits make it a promising alternative to LDA-based methods in practice.", 
        "196": "Acknowledgements  This research is partially funded by the Economic Development Board and the National Research Foundation of Singapore."
    }, 
    "document_id": "P17-1036.pdf.json"
}
