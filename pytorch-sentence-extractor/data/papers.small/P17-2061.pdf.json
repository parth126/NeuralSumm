{
    "abstract_sentences": {
        "1": "In this paper, we propose a novel domain adaptation method named \u201cmixed fine tuning\u201d for neural machine translation (NMT).", 
        "2": "We combine two existing approaches namely fine tuning and multi domain NMT.", 
        "3": "We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-ofdomain corpora.", 
        "4": "All corpora are augmented with artificial tags to indicate specific domains.", 
        "5": "We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385\u2013391 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2061  1 Introduction  One of the most attractive features of neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems.", 
        "2": "However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora.", 
        "3": "In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016).", 
        "4": "Domain adaptation has been shown to be effective for low resource NMT.", 
        "5": "The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016).", 
        "6": "However, fine tuning\n\u2217This work was done when the first author was a researcher of Japan Science and Technology Agency.", 
        "7": "tends to overfit quickly due to the small size of the in-domain data.", 
        "8": "On the other hand, multi domain NMT (Kobus et al., 2016) involves training a single NMT model for multiple domains.", 
        "9": "This method adds tags \u201c<2domain>\u201d to the source sentences in the parallel corpora to indicate domains without any modifications to the NMT system architecture.", 
        "10": "However, this method has not been studied for domain adaptation in particular.", 
        "11": "Motivated by these two lines of studies, we propose a new domain adaptation method called \u201cmixed fine tuning,\u201d where we first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus that is a mix of the in-domain and out-of-domain corpora.", 
        "12": "Fine tuning on the mixed corpus instead of the indomain corpus can address the overfitting problem.", 
        "13": "All corpora are augmented with artificial tags to indicate specific domains.", 
        "14": "We tried two different corpora settings on two different language pairs:\n\u2022 Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015).", 
        "15": "\u2022 Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor).", 
        "16": "The parallel corpus of the latter domain was automatically extracted (Chu et al., 2016a).", 
        "17": "We observed that \u201cmixed fine tuning\u201d works significantly better than methods that use fine tuning\n385\nand domain tag based approaches separately.", 
        "18": "Our contributions are twofold:\n\u2022 We propose a novel method that combines the best of existing approaches and show that it is effective.", 
        "19": "\u2022 To the best of our knowledge this is the first work on an empirical comparison of various domain adaptation methods.", 
        "20": "2 Related Work  Fine tuning has also been explored for various NLP tasks using neural networks such as sentiment analysis and paraphrase detection (Mou et al., 2016).", 
        "21": "Tag based NMT has also been shown to be effective for control the politeness of translations (Sennrich et al., 2016a) and multilingual NMT (Johnson et al., 2016).", 
        "22": "Besides fine tuning and multi domain NMT using tags, another direction of domain adaptation for NMT is using in-domain monolingual data.", 
        "23": "Either training an in-domain recurrent neural network (RNN) language model for the NMT decoder (Gu\u0308lc\u0327ehre et al., 2015) or generating synthetic data by back translating target in-domain monolingual data (Sennrich et al., 2016b) have been studied.", 
        "24": "3 Methods for Comparison  All the methods that we compare are simple and do not need any modifications to the NMT system.", 
        "25": "3.1 Fine Tuning  Fine tuning is the conventional way for domain adaptation, and thus serves as a baseline in this study.", 
        "26": "In this method, we first train an NMT system on a resource rich out-of-domain corpus till convergence, and then fine tune its parameters on a resource poor in-domain corpus (Figure 1).", 
        "27": "3.2 Multi Domain  The multi domain method is originally motivated by (Sennrich et al., 2016a), which uses tags to\ncontrol the politeness of NMT translations.", 
        "28": "The overview of this method is shown in the dotted section in Figure 2.", 
        "29": "In this method, we simply concatenate the corpora of multiple domains with two small modifications:\n\u2022 Appending the domain tag \u201c<2domain>\u201d to the source sentences of the respective corpora.1 This primes the NMT decoder to generate sentences for the specific domain.", 
        "30": "\u2022 Oversampling the smaller corpus so that the training procedure pays equal attention to each domain.", 
        "31": "We can further fine tune the multi domain model on the in-domain data, which is named as \u201cmulti domain + fine tuning.\u201d  3.3 Mixed Fine Tuning  The proposed mixed fine tuning method is a combination of the above methods (shown in Figure 2).", 
        "32": "The training procedure is as follows:\n1.", 
        "33": "Train an NMT model on out-of-domain data till convergence.", 
        "34": "2.", 
        "35": "Resume training the NMT model from step 1 on a mix of in-domain and out-of-domain data (by oversampling the in-domain data) till convergence.", 
        "36": "By default, we utilize domain tags, but we also consider settings where we do not use them (i.e., \u201cw/o tags\u201d).", 
        "37": "We can further fine tune the model from step 2 on the in-domain data, which is named as \u201cmixed fine tuning + fine tuning.\u201d\nNote that in the \u201cfine tuning\u201d method, the vocabulary obtained from the out-of-domain data is used for the in-domain data; while for the \u201cmulti domain\u201d and \u201cmixed fine tuning\u201d methods, we use a vocabulary obtained from the mixed in-domain and out-of-domain data for all the training stages.", 
        "38": "Regarding development data, for fine tuning, an out-of-domain development set is first used for training the out-of-domain NMT model, then an in-domain development set is used for fine tuning; For multi-domain, a mix of in-domain and out-ofdomain development sets are used; For mixed fine tuning, an out-of-domain development set is first used for training the out-of-domain NMT model, then a mix of in-domain and out-of-domain development sets are used for mixed fine tuning.", 
        "39": "1We verified the effectiveness of the domain tags by comparing against a setting that does not use them, see the \u201cw/o tags\u201d settings in Tables 1 and 2.", 
        "40": "4 Experimental Settings  We conducted NMT domain adaptation experiments in two different settings as follows:  4.1 High Quality In-domain Corpus Setting  Chinese-to-English translation was the focus of the high quality in-domain corpus setting.", 
        "41": "We utilized the resource rich patent out-of-domain data to augment the resource poor spoken language indomain data.", 
        "42": "The patent domain MT was conducted on the Chinese-English subtask (NTCIRCE) of the patent MT task at the NTCIR-10 workshop2 (Goto et al., 2013).", 
        "43": "The NTCIR-CE task uses 1M, 2k, and 2k sentences for training, development, and testing, respectively.", 
        "44": "The spoken domain MT was conducted on the Chinese-English subtask (IWSLT-CE) of the TED talk MT task at the IWSLT 2015 workshop (Cettolo et al., 2015).", 
        "45": "The IWSLT-CE task contains 209,491 sentences for training.", 
        "46": "We used the dev 2010 set for development, containing 887 sentences.", 
        "47": "We evaluated all methods on the 2010, 2011, 2012, and 2013 test sets, containing 1570, 1245, 1397, and 1261 sentences, respectively.", 
        "48": "4.2 Low Quality In-domain Corpus Setting  Chinese-to-Japanese translation was the focus of the low quality in-domain corpus setting.", 
        "49": "We utilized the resource rich scientific out-of-domain data to augment the resource poor Wikipedia (essentially open) in-domain data.", 
        "50": "The scientific domain MT was conducted on the Chinese-Japanese paper excerpt corpus (ASPEC-CJ)3 (Nakazawa et al., 2016), which is one subtask of the workshop on Asian translation (WAT)4 (Nakazawa et al.,\n2http://ntcir.nii.ac.jp/PatentMT-2/ 3http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 4http://orchid.kuee.kyoto-u.ac.jp/WAT/\n2015).", 
        "51": "The ASPEC-CJ task uses 672315, 2090, and 2107 sentences for training, development, and testing, respectively.", 
        "52": "The Wikipedia domain task was conducted on a Chinese-Japanese corpus automatically extracted from Wikipedia (WIKI-CJ) (Chu et al., 2016a) using the ASPEC-CJ corpus as a seed.", 
        "53": "The WIKI-CJ task contains 136013, 198, and 198 sentences for training, development, and testing, respectively.", 
        "54": "4.3 MT Systems  For NMT, we used the KyotoNMT system5 (Cromieres et al., 2016).", 
        "55": "The NMT settings were the same as (Cromieres et al., 2016) except that we used a vocabulary size of 32k for all the experiments, and did not ensemble independently trained parameters.", 
        "56": "The sizes of the source and target vocabularies, the source and target side embeddings, the hidden states, the attention mechanism hidden states, and the deep softmax output with a 2-maxout layer were set to 32,000, 620, 1000, 1000, and 500, respectively.", 
        "57": "We used 2- layer LSTMs for both the source and target sides.", 
        "58": "ADAM was used as the learning algorithm, with a dropout rate of 20% for the inter-layer dropout, and L2 regularization with a weight decay coefficient of 1e-6.", 
        "59": "The mini batch size was 64, and sentences longer than 80 tokens were discarded.", 
        "60": "We early stopped the training process when we observed that the BLEU score of the development set converges.", 
        "61": "For testing, we ensembled the three parameters of the best development loss, the best development BLEU, and the final parameters in a single training run.", 
        "62": "Beam size was set to 100.", 
        "63": "The maximum length of the translation was set to 2, and 1.5 times of the source sentences for Chineseto-English, and Chinese-to-Japanese, respectively.", 
        "64": "5https://github.com/fabiencro/knmt\nFor performance comparison, we also conducted experiments on phrase based SMT (PBSMT).", 
        "65": "We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments.", 
        "66": "For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively.", 
        "67": "In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment.", 
        "68": "For both MT systems, we preprocessed the data as follows.", 
        "69": "For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b).", 
        "70": "For English, we tokenized and lowercased the sentences using the tokenizer.perl script in Moses.", 
        "71": "Japanese was segmented using JUMAN9 (Kurohashi et al., 1994).", 
        "72": "For NMT, we further split the words into subwords using byte pair encoding (BPE) (Sennrich et al., 2016c), which has been shown to be effective for the rare word problem in NMT.", 
        "73": "Another motivation of using sub-words is making the different domains share more vocabulary, which is important especially for the resource poor domain.", 
        "74": "For the Chinese-to-English tasks, we trained two BPE models on the Chinese and English vocabularies, respectively.", 
        "75": "For the Chinese-to-Japanese tasks, we trained a joint BPE model on both of the Chinese and Japanese vocabularies, because Chinese and Japanese could share some vocabularies of Chinese characters.", 
        "76": "The number of merge operations was set to 30k for all the tasks.", 
        "77": "6https://github.com/kpu/kenlm/ 7http://code.google.com/p/giza-pp 8https://bitbucket.org/msmoshen/kyotomorph-beta 9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN  5 Results  Tables 1 and 2 show the translation results on the Chinese-to-English and Chinese-to-Japanese tasks, respectively.", 
        "78": "The entries with SMT and NMT are the PBSMT and NMT systems, respectively; others are the different methods described in Section 3.", 
        "79": "In both tables, the numbers in bold indicate the best system and all systems that were not significantly different from the best system.", 
        "80": "The significance tests were performed using the bootstrap resampling method (Koehn, 2004) at p < 0.05.", 
        "81": "We can see that without domain adaptation, the SMT systems perform significantly better than the NMT system on the resource poor domains, i.e., IWSLT-CE and WIKI-CJ; while on the resource rich domains, i.e., NTCIR-CE and ASPECCJ, NMT outperforms SMT.", 
        "82": "Directly using the SMT/NMT models trained on the out-of-domain data to translate the in-domain data shows bad performance.", 
        "83": "With our proposed \u201cMixed fine tuning\u201d domain adaptation method, NMT significantly outperforms SMT on the in-domain tasks.", 
        "84": "Comparing different domain adaptation methods, \u201cMixed fine tuning\u201d shows the best perfor-\nmance.", 
        "85": "We believe the reason for this is that \u201cMixed fine tuning\u201d can address the over-fitting problem of \u201cFine tuning.\u201d We observed that both fine-tuning and mixed fine-tuning tends to converge after 1 epoch of training, and thus we early stopped training soon after 1 epoch.", 
        "86": "After 1 epoch of training, fine-tuning overfitted very quickly, while mixed fine-tuning did not overfit.", 
        "87": "In addition, \u201cMixed fine tuning\u201d does not worsen the quality of out-of-domain translations, while \u201cFine tuning\u201d and \u201cMulti domain\u201d do.", 
        "88": "One shortcoming of \u201cMixed fine tuning\u201d is that compared to \u201cFine tuning,\u201d it took a longer time for the fine tuning process, as the time until convergence is essentially proportional to the size of the data used for fine tuning.", 
        "89": "Note that training as long as \u201cMixed fine tuning\u201d is not helpful for \u201cFine tuning\u201d due to overfitting.", 
        "90": "\u201cMulti domain\u201d performs either as well as (IWSLT-CE) or worse than (WIKI-CJ) \u201cFine tuning,\u201d but \u201cMixed fine tuning\u201d performs either significantly better than (IWSLT-CE) or is comparable to (WIKI-CJ) \u201cFine tuning.\u201d We believe the performance difference between the two tasks is due to their unique characteristics.", 
        "91": "As WIKI-CJ data is of relatively poorer quality, mixing it with out-of-domain data does not have the same level of positive effects as those obtained by the IWSLTCE data.", 
        "92": "The domain tags are helpful for both \u201cMulti domain\u201d and \u201cMixed fine tuning.\u201d Essentially, further fine tuning on in-domain data does not help for both \u201cMulti domain\u201d and \u201cMixed fine tuning.\u201d We believe that there are two reasons for this.", 
        "93": "Firstly, the \u201cMulti domain\u201d and \u201cMixed fine tuning\u201d methods already utilize the in-domain data used for fine tuning.", 
        "94": "Secondly, fine tuning on the small in-domain data overfits very quickly.", 
        "95": "Actually, we observed that adding fine-tuning on top of both \u201cMulti domain\u201d and \u201cMixed fine tuning\u201d overfits at the beginning of training.", 
        "96": "Mixed fine tuning performs significantly better on the out-domain NTCIR-CE test set without tags as compared to with tags (39.67 v.s.", 
        "97": "37.01).", 
        "98": "We believe the reason for this is that without tags the IWSLT-CE in-domain data can contribute more to the out-of-domain NTCIR-CE data.", 
        "99": "With tags, the NMT training tends to learn a model that pays equal attention to each domain.", 
        "100": "Without tags, the NMT training pays more attention to the NTCIRCE data as it contains much longer sentences, al-\nthough we oversampled the IWSLT-CE data.", 
        "101": "As the IWSLT-CE data is TED talks, there could be some vocabulary and content overlaps between the IWSLT-CE the NTCIR-CE data, and thus appending the IWSLT-CE data to the NTCIR-CE data can benefit for the NTCIR-CE translation.", 
        "102": "In the case of WIKI-CJ and ASPEC-CJ, due to the low quality of WIKI-CJ, appending WIKI-CJ to ASPECCJ does not improve the ASPEC-CJ translation.", 
        "103": "6 Conclusion  In this paper, we proposed a novel domain adaptation method named \u201cmixed fine tuning\u201d for NMT.", 
        "104": "We empirically compared our proposed method against fine tuning and multi domain methods, and have shown that it is effective but is sensitive to the quality of the in-domain data used.", 
        "105": "The presented methods are language and domain independent, and thus we believe that the general observations also hold on other languages and domains.", 
        "106": "Furthermore, we believe the contribution in this paper can be helpful for domain adaptation of other NN based natural language processing tasks.", 
        "107": "In the future, we plan to incorporate an RNN model into our architecture to leverage abundant in-domain monolingual corpora.", 
        "108": "We also plan on exploring the effects of synthetic data by back translating large in-domain monolingual corpora.", 
        "109": "Acknowledgments  This work was partly supported by JSPS and DST under the Japan-India Science Cooperative Program.", 
        "110": "We are very appreciated to Prof. Daisuke Kawahara, Dr. Toshiaki Nakazawa, and Dr. Fabien Cromieres for helping improving the writing quality of this paper.", 
        "111": "We also thank the anonymous reviewers for their insightful comments."
    }, 
    "document_id": "P17-2061.pdf.json"
}
