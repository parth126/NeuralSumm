{
    "abstract_sentences": {
        "1": "Vector space representations of words capture many aspects of word similarity, but such methods tend to produce vector spaces in which antonyms (as well as synonyms) are close to each other.", 
        "2": "For spectral clustering using such word embeddings, words are points in a vector space where synonyms are linked with positive weights, while antonyms are linked with negative weights.", 
        "3": "We present a new signed spectral normalized graph cut algorithm, signed clustering, that overlays existing thesauri upon distributionally derived vector representations of words, so that antonym relationships between word pairs are represented by negative weights.", 
        "4": "Our signed clustering algorithm produces clusters of words that simultaneously capture distributional and synonym relations.", 
        "5": "By using randomized spectral decomposition (Halko et al., 2011) and sparse matrices, our method is both fast and scalable.", 
        "6": "We validate our clusters using datasets containing human judgments of word pair similarities and show the benefit of using our word clusters for sentiment prediction."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 939\u2013949 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1087  1 Introduction  In distributional vector representations, opposite relations are not fully captured.", 
        "2": "Take, for example, words such as \u201cgreat\u201d and \u201cawful\u201d that can appear with similar frequency in the same sentence structure: \u201cJohn had a great meeting\u201d and \u201cJohn had an awful day.\u201d Word embeddings, which are successful in a wide array of NLP tasks (Turney et al., 2010; Dhillon et al., 2015), fail to capture this antonymy because they follow the distributional hypothesis that similar words are used in similar\ncontexts (Harris, 1954), thus assigning small cosine or euclidean distances between the vector representations of \u201cgreat\u201d and \u201cawful\u201d.", 
        "3": "While vector space models (Turney et al., 2010) such as word2vec (Mikolov et al., 2013), Global vectors (GloVe) (Pennington et al., 2014), or Eigenwords (Dhillon et al., 2015) capture relatedness, they do not adequately encode synonymy and semantic similarity (Mohammad et al., 2013; Scheible et al., 2013).", 
        "4": "Our goal is to create clusters of synonyms or semantically equivalent words and linguistically motivated unified constructs.", 
        "5": "Signed graphs, which are graphs with negative edge weights, were first introduced by Cartwright and Harary (1956).", 
        "6": "However, signed graph clustering for multiclass normalized cuts (K-clusters) has been largely unexplored until recently.", 
        "7": "We present a novel theory and method that extends multiclass normalized cuts (K-cluster) of Yu and Shi (2003) to signed graphs (Gallier, 2016)1 and the work of Kunegis et al.", 
        "8": "(2010) to K-clustering.", 
        "9": "This extension allows the incorporation of knowledge base information, positive and negatively weighted links (see figure 2.1).", 
        "10": "Negative edges serve as repellent or opposite relationships between nodes.", 
        "11": "Our signed spectral normalized graph cut algorithm (henceforth, signed clustering) builds negative edge relations into graph embeddings using similarity structure in vector spaces.", 
        "12": "It takes as input an initial set of vectors and edge relations, and hence is easy to combine with any word embedding method.", 
        "13": "This paper formally improves on the discrete optimization problem of Yu and Shi (2003).", 
        "14": "Signed clustering gives better clusters than spectral clustering (Shi and Malik, 2000) of word embeddings, and it has better coverage and is more robust than thesaurus look-up.", 
        "15": "This is because the-\n1Gallier (2016) is a full theoretical exposition of our methods with proofs on arXiv.", 
        "16": "939\nsauri erroneously give equal weight to rare senses of a word \u2013 for example, \u201crich\u201d as a rarely used synonym of \u201cabsurd\u201d.", 
        "17": "Also, the overlap between thesauri is small, due to their manual creation.", 
        "18": "Lin (1998) found 17.8397% overlap between synonym sets from Roget\u2019s Thesaurus and WordNet 1.5.", 
        "19": "We find similarly small overlap between all three thesauri tested.", 
        "20": "We evaluate our clusters using SimLex-999 (Hill et al., 2014) and SimVerb-3500 (Gerz et al., 2016) as a ground truth for our cluster evaluation.", 
        "21": "Finally, we test our method on the sentiment analysis task.", 
        "22": "Overall, signed spectral clustering can augment methods using signed information and has broad application for many fields.", 
        "23": "Our main contributions are: the novel extension of signed clustering to the multiclass (K-cluster), and the application of this method to create semantic word clusters that are agnostic to vector space representations and thesauri.", 
        "24": "1.1 Related Work  Semantic word cluster and distributional thesauri have been well studied in the NLP literature (Lin, 1998; Curran, 2004).", 
        "25": "Recently there has been a line of research on incorporating synonyms and antonyms into word embeddings.", 
        "26": "Our approach is very much in the line of Vlachos et al.", 
        "27": "(2009).", 
        "28": "However, they explicitly made verb clusters using Dirichlet Process Mixture Models and must-link / cannot-link clustering.", 
        "29": "Furthermore, they note that cannot-link clustering does not improve performance whereas our signed clustering antonyms are key.", 
        "30": "Most recent models either attempt to make richer contexts, in order to find semantic similarity, or overlay thesaurus information in a supervised or semi-supervised manner.", 
        "31": "One line of active research is post processing the word vector embedding by transforming the space using a single or multi-relational objective (Yih et al., 2012; Tang et al., 2014; Chang et al., 2013; Tang et al., 2014; Zhang et al., 2014; Faruqui et al., 2015; Mrks\u030cic\u0301 et al., 2016).", 
        "32": "Alternatively, there are methods to modify the objective function for generating the word embeddings (Ono et al., 2015; Pham et al., 2015; Schwartz et al., 2015).", 
        "33": "Our approach differs from the aforementioned methods in that we created word clusters using the antonym relationships as negative links.", 
        "34": "Unlike\nthe previous approaches using semi-supervised methods, we incorporated the thesauri as a knowledge base.", 
        "35": "Similar to word vector retrofitting and counter-fitting methods described in Faruqui et al.", 
        "36": "(2015) and Mrks\u030cic\u0301 et al.", 
        "37": "(2016), our signed clustering method uses existing vector representations to create word clusters.", 
        "38": "To our knowledge, this work is the first theoretical foundation of multiclass signed normalized cuts.2 Zass and Shashua (2005) solved multiclass cluster from another approach, by relaxing the orthogonality assumption and focusing instead on the non-negativity constraint.", 
        "39": "This led to a doubly stochastic optimization problem.", 
        "40": "Negative edges are handled by a constrained hyperparameter.", 
        "41": "Hou (2005) used positive degrees of nodes in the degree matrix of a signed graph with weights (-1, 0, 1), which was advanced by Kolluri et al.", 
        "42": "(2004) and Kunegis et al.", 
        "43": "(2010) using absolute values of weights in the degree matrix.", 
        "44": "Interestingly, Chiang et al.", 
        "45": "(2014) presented a theoretical foundation for edge sign prediction and a recursive clustering approach.", 
        "46": "Mercado et al.", 
        "47": "(2016) found that using the geometric mean of the graph Laplacian improves performance.", 
        "48": "Wang et al.", 
        "49": "(2016) used semi-supervised polarity induction (Rao and Ravichandran, 2009) to create clusters of words with similar valence and arousal.", 
        "50": "Must-link and cannot-link soft spectral clustering (Rangapuram and Hein, 2012) share similarities with our method, particularly in the limit where there are no must-link edges present.", 
        "51": "Both must-link and cannot-link clustering as well as polarity induction differ in optimization method.", 
        "52": "Our method is significantly faster due to the use of randomized SVD (Halko et al., 2011) and can thus be applied to large scale NLP problems.", 
        "53": "We developed a novel theory and algorithm that extends the clustering of Shi and Malik (2000) and Yu and Shi (2003) to the multiclass signed graph case.", 
        "54": "2 Signed Graph Cluster Estimation    2.1 Signed Normalized Cut  Weighted graphs for which the weight matrix is a symmetric matrix in which negative and positive entries are allowed are called signed graphs.", 
        "55": "2The full exposition by Gallier (2016) is available on arXiv.", 
        "56": "Such graphs (with weights (\u22121, 0,+1)) were introduced as early as 1953 by (Harary, 1953), to model social relations involving disliking, indifference, and liking.", 
        "57": "The problem of clustering the nodes of a signed graph arises naturally as a generalization of the clustering problem for weighted graphs.", 
        "58": "Figure 1 shows a signed graph of word similarities with a thesaurus overlay.", 
        "59": "Gallier\n(2016) extends normalized cuts to signed graphs in order to incorporate antonym information into word clusters.", 
        "60": "Definition 2.1.", 
        "61": "A weighted graph is a pair G = (V,W ), where V = {v1, .", 
        "62": ".", 
        "63": ".", 
        "64": ", vm} is a set of nodes or vertices, and W is a symmetric matrix called the weight matrix, such that wi j \u2265 0 for all i, j \u2208 {1, .", 
        "65": ".", 
        "66": ".", 
        "67": ",m}, and wi i = 0 for i = 1, .", 
        "68": ".", 
        "69": ".", 
        "70": ",m. We say that a set {vi, vj} is an edge iff wi j > 0.", 
        "71": "The corresponding (undirected) graph (V,E) with E = {{vi, vj} | wi j > 0}, is called the underlying graph of G.\nGiven a signed graph G = (V,W ) (where W is a symmetric matrix with zero diagonal entries), the underlying graph of G is the graph with node set V and set of (undirected) edges E = {{vi, vj} | wij 6= 0}.", 
        "72": "If (V,W ) is a signed graph, where W is an m \u00d7 m symmetric matrix with zero diagonal entries and with the other entries wij \u2208 R arbitrary, for any node vi \u2208 V , the signed degree of vi is defined as\ndi = d(vi) = m\u2211\nj=1\n|wij |,\nand the signed degree matrix D as\nD = diag(d(v1), .", 
        "73": ".", 
        "74": ".", 
        "75": ", d(vm)).", 
        "76": "For any subset A of the set of nodes V , let\nvol(A) = \u2211\nvi\u2208A di =\n\u2211\nvi\u2208A\nm\u2211\nj=1\n|wij |.", 
        "77": "For any two subsets A and B of V and AC which is the complement of A, define links+(A,B), links\u2212(A,B), and cut(A,AC) by\nlinks+(A,B) = \u2211\nvi\u2208A,vj\u2208B wij>0\nwij\nlinks\u2212(A,B) = \u2211\nvi\u2208A,vj\u2208B wij<0\n\u2212wij\ncut(A,AC) = \u2211\nvi\u2208A,vj\u2208AC wij 6=0\n|wij |.", 
        "78": "Then, the signed Laplacian L is defined by\nL = D \u2212W,\nand its normalized version Lsym by\nLsym = D \u22121/2 LD \u22121/2 = I \u2212D\u22121/2WD\u22121/2.", 
        "79": "Kunegis et al.", 
        "80": "(2010) showed that L is positive semidefinite.", 
        "81": "For a graph without isolated vertices, we have d(vi) > 0 for i = 1, .", 
        "82": ".", 
        "83": ".", 
        "84": ",m, so D\n\u22121/2 is well defined.", 
        "85": "Given a partition of V into K clusters (A1, .", 
        "86": ".", 
        "87": ".", 
        "88": ", AK), if we represent the jth block of this partition by a vector Xj such that\nXji = { aj if vi \u2208 Aj 0 if vi /\u2208 Aj ,\nfor some aj 6= 0.", 
        "89": "For illustration, suppose m = 5 and A1 = {v1, v3} then (X1)> = [a1, 0, a1, 0, 0].", 
        "90": "Definition 2.2.", 
        "91": "The signed normalized cut sNcut(A1, .", 
        "92": ".", 
        "93": ".", 
        "94": ", AK) of the partition (A1, ..., AK) is defined as\nsNcut(A1, .", 
        "95": ".", 
        "96": ".", 
        "97": ",AK) =\nK\u2211\nj=1\ncut(Aj , A C j ) + 2links \u2212(Aj , Aj)\nvol(Aj) .", 
        "98": "It should be noted that this formulation differs significantly from Kunegis et al.", 
        "99": "(2010) and even more so from must-link / cannot-link clustering.", 
        "100": "Observe that minimizing sNcut(A1, .", 
        "101": ".", 
        "102": ".", 
        "103": ", AK) minimizes the number of positive and negative edges between clusters and also the number of negative edges within clusters.", 
        "104": "Removing the term links\u2212(Aj , Aj) reduces sNcut to normalized cuts.", 
        "105": "A linear algebraic formulation is\nsNcut(A1, .", 
        "106": ".", 
        "107": ".", 
        "108": ", AK) = K\u2211\nj=1\n(Xj)>LXj (Xj)>DXj .", 
        "109": "where X is the N \u00d7K matrix whose jth column is Xj .", 
        "110": "2.2 Optimization Problem  We now formulate K-way clustering of a graph using normalized cuts.", 
        "111": "If we let X = {\n[X1 .", 
        "112": ".", 
        "113": ".", 
        "114": "XK ] | Xj = aj(xj1, .", 
        "115": ".", 
        "116": ".", 
        "117": ", xjN ),\nxji \u2208 {1, 0}, aj \u2208 R, Xj 6= 0 }\nour solution set is\nK = { X \u2208 X | (Xi)>DXj = 0,\n1 \u2264 i, j \u2264 K, i 6= j } .", 
        "118": "The resulting optimization problem is\nminimize\nK\u2211\nj=1\n(Xj)>LXj (Xj)>DXj\nsubject to (Xi)>DXj = 0,\n1 \u2264 i, j \u2264 K, i 6= j, X \u2208 X .", 
        "119": "The problem can be reformulated to an equiva-\nlent optimization problem:\nminimize tr(X>LX)\nsubject to X>DX = I, X \u2208 X .", 
        "120": "We then form a relaxation of the above problem,\ndropping the condition that X \u2208 X , giving Relaxed Problem\nminimize tr(Y >D \u22121/2 LD \u22121/2 Y ) subject to Y >Y = I.", 
        "121": "The minimum of the relaxed problem is achieved by the K unit eigenvectors associated with the smallest eigenvalues of Lsym.", 
        "122": "2.3 Finding an Approximate Discrete Solution  Given a solution Z of the relaxed problem, we look for pairs (X,Q) with X \u2208 X and where Q is aK\u00d7K matrix with nonzero and pairwise orthogonal columns, with \u2016X\u2016F = \u2016Z\u2016F , that minimize\n\u03d5(X,Q) = \u2016X \u2212 ZQ\u2016F .", 
        "123": "Here, \u2016A\u2016F is the Frobenius norm of A.", 
        "124": "This nonlinear optimization problem involves two unknown matrices X and Q.", 
        "125": "To solve the relaxed problem, we proceed by alternating between minimizing \u03d5(X,Q) = \u2016X \u2212 ZQ\u2016F with respect to X holding Q fixed (step 5 in algorithm 1), and minimizing \u03d5(X,Q) with respect to Q holding X fixed (steps 6 and 7 in algorithm 1).", 
        "126": "This second stage in which X is held fixed has been studied, but it is still a hard problem for which no closed-form solution is known.", 
        "127": "Hence we divide the problem into steps 6 and 7 for which the solution is known.", 
        "128": "Since Q is of the form Q = R\u039b whereR \u2208 O(K) and \u039b is a diagonal invertible matrix, we minimize \u2016X \u2212 ZR\u039b\u2016F .", 
        "129": "The matrix R\u039b is not a minimizer of \u2016X \u2212 ZR\u039b\u2016F in general, but it is an improvement on R alone, and both stages can be solved quite easily.", 
        "130": "In step 6 the problem reduces to minimizing \u22122tr(Q>Z>X); that is, maximizing tr(Q>Z>X).", 
        "131": "Algorithm 1 Signed Clustering 1: Input: W the weight matrix (without isolated nodes),\nK the number of clusters, and termination threshold .", 
        "132": "2: Using theD the degree matrix, and the signed Laplacian\nL, compute Lsym the signed normalized Laplacian.", 
        "133": "3: Initialize \u039b = I , X = D \u2212 1\n2U where U is the matrix of the eigenvectors corresponding to the K smallest eigenvalues of Lsym.", 
        "134": "3 4: while \u2016X \u2212 ZR\u039b\u2016F > do 5: Minimize \u2016X \u2212 ZR\u039b\u2016F with respect to X holding Q fixed.", 
        "135": "6: Fix X , Z, and \u039b, find R \u2208 O(K) that minimizes \u2016X \u2212 ZR\u039b\u2016F .", 
        "136": "7: Fix X , Z, and R, find a diagonal invertible matrix \u039b that minimizes \u2016X \u2212 ZR\u039b\u2016F .", 
        "137": "8: end while 9: Find the discrete solution X\u2217 by choosing the\nlargest entry xij on row i set xij = 1 and all other xij = 0 for row i.", 
        "138": "10: Output: X\u2217.", 
        "139": "Steps 3 through 10 may be replaced by standard Kmeans clustering.", 
        "140": "It should also be noted that by\nremoving the solution requirement that Xj 6= 0, the algorithm can find k \u2264 K clusters.", 
        "141": "3 Similarity Calculation  The main input to the spectral signed clustering algorithm is the similarity matrixW , which overlays both the distributional properties and thesaurus information.", 
        "142": "Following Belkin and Niyogi (2003), we chose the heat kernel based on the Euclidean distance between word vector representations as our similarity metric, such that\nWij =    0 if e\u2212 \u2016wi\u2212wj\u20162 \u03c3 <\ne\u2212 \u2016wi\u2212wj\u20162 \u03c3 otherwise .", 
        "143": "where \u03c3 and are hyperparameters found using grid search (see Supplemental material for more detail).", 
        "144": "We represented the thesaurus as two matrices where\nT synij = { 1 if words i and j are synonyms 0 otherwise .", 
        "145": "and\nT antij = { \u22121 if words i and j are antonyms 0 otherwise .", 
        "146": "T syn is the synonym graph and T ant is the antonym graph.", 
        "147": "The signed graph can then be written in matrix form as W\u0302 = \u03b3W +\u03b2antT ant W+\u03b2synT syn W , where computes Hadamard product (element-wise multiplication).", 
        "148": "The parameters \u03b3, \u03b2syn, and \u03b2ant are tuned to the data target dataset using cross validation.", 
        "149": "The reader should note that \u03c3 and are not found using a target dataset, but instead using cross validation and grid search to minimize the number of negative edges within clusters and the number of disconnected components in the cluster.", 
        "150": "4 Evaluation Metrics  We evaluated the clusters using both intrinsic and extrinsic methods.", 
        "151": "For intrinsic evaluation, we used thesaurus information for two novel metrics: 1) the number of negative edges (NNE) within the clusters, which in our semantic clusters is the number of antonyms in the same cluster, and 2) the number of disconnected components (NDC) in the synonym graph, so the number of groups of words\nthat are not connected by a synonym relation in the thesaurus.", 
        "152": "The NDC thus has the disadvantage that it is a function of the thesaurus coverage.", 
        "153": "Our third intrinsic measure uses a gold standard designed to measure how well we capture word similarity: Semantically similar words should be in the same cluster and semantically dissimilar words should not.", 
        "154": "For extrinsic evaluation, as descibed below, we measure how much our clusters help to identify text polarity.", 
        "155": "We also compare multiple word embeddings and thesauri to demonstrate the stability of our method.", 
        "156": "5 Experiments with Synthetic Data  In order to evaluate our signed graph clustering method, we first focused on intrinsic measures of cluster quality in synthetic data.", 
        "157": "To do so, we created random signed graphs with the same proportion of positive and negative edges as in our real dataset.", 
        "158": "Figure 2 demonstrates that the number of\nnegative edges within a cluster is minimized using our clustering algorithm on simulated data.", 
        "159": "As the number of clusters becomes large, the number of disconnected components, which includes clusters of size one, consistently increases.", 
        "160": "Determining the optimal cluster size and similarity parameters requires making a trade off between NDC and NNE.", 
        "161": "For example, in figure 2 the optimal cluster size is 20.", 
        "162": "One can see that as the number of clusters increases NNE goes to zero, but the number of disconnected components becomes the number of vertices.", 
        "163": "In the extreme case all clusters contain one vertex.", 
        "164": "K-means, also shown in figure 2, does not optimize NNE.", 
        "165": "6 Experimental Setup    6.1 Word Embeddings  We used four different word embedding methods for evaluation: Skip-gram vectors (word2vec) (Mikolov et al., 2013), Global vectors (GloVe) (Pennington et al., 2014), Eigenwords (Dhillon et al., 2015), and Global Context (GloCon) (Huang et al., 2012); however, we only report the results for word2vec, which is the most popular word embedding (see the supplemental material for other embeddings).", 
        "166": "We used word2vec 300 dimensional embeddings which were trained on several billion words of English: the Gigaword and the English discussion forum data gathered as part of BOLT.", 
        "167": "Tokenization was performed using CMU\u2019s Twokenize.4  6.2 Thesauri  Several thesauri were used in order to test the robustness including Roget\u2019s Thesaurus (Roget, 1852), the Microsoft Word English (MS Word) thesaurus from Samsonovic et al.", 
        "168": "(2010) and WordNet 3.0 (Miller, 1995).", 
        "169": "We chose a subset of 5108 words for the training dataset, which had high overlap between various sources.", 
        "170": "Changes to the training dataset had minimal effects on the optimal parameters.", 
        "171": "Within the training dataset, each of the thesauri had roughly 3700 antonym pairs; combined they had 6680.", 
        "172": "However, the number of distinct connected components varied, with Roget\u2019s Thesaurus having the fewest (629), and MS Word Thesaurus (1162) and WordNet (2449) having the most.", 
        "173": "These ratios were consistent across the full dataset.", 
        "174": "6.3 Gold Standard SimLex-999 And  SimVerb-3500\nFollowing the analysis of Vlachos et al.", 
        "175": "(2009), we threshold the semantically similar datasets to find word pairs which should or should not belong to the same cluster.", 
        "176": "As ground truth, we extracted 120 semantically similar words from SimLex-999 with a similarity score greater than 8 out of 10.", 
        "177": "SimLex-999 is a gold standard resource for semantic similarity, not relatedness, based on ratings by human annotators.", 
        "178": "Our 120 pair subset of SimLex-999 has multiple parts-of-speech including Noun-Noun pairs, VerbVerb pairs and Adjective-Adjective pairs.", 
        "179": "Within\n4https://github.com/brendano/ ark-tweet-nlp\nSimVerb-3500, we used a subset of 318 semantically similar verb pairs.", 
        "180": "The community is attempting to define better gold standards; however, currently these are the best datasets that we are aware of.", 
        "181": "We tried to use WordNet, Roget, and the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) as a gold standard, but manual inspection as well as empirical results showed that none of the automatically generated datasets were a sufficient gold standard.", 
        "182": "Possibly the symmetric pattern of (Schwartz et al., 2015) would have been sufficient; we did not have time to validate this.", 
        "183": "6.4 Stanford Sentiment Treebank  We also evaluated our clusters by using them as features for predicting sentiment, using sentiment treebank 5 (Socher et al., 2013) with coarsegrained labels on phrases and sentences from movie review excerpts.", 
        "184": "This dataset is widely used for the evaluation of sentiment analysis.", 
        "185": "We used the standard partition of the treebank into training (6920), development (872), and test (1821) sets.", 
        "186": "7 Cluster Evaluation  Table 1 shows the four most-associated words with \u201caccept\u201d using different methods.", 
        "187": "We now turn to quantitative measures of word similarity and synonym cluster quality.", 
        "188": "7.1 Comparison with K-means and Normalized Cuts  In order to assess the model we tested (1) Kmeans, (2) normalized cuts without thesaurus, and (3) signed normalized cuts.", 
        "189": "As a baseline, we created clusters using K-means on the original word2vec vector representations where the number of K clusters was set to 750.", 
        "190": "Table 2 shows the relative ratios of the different clustering methods of with respect to antonym pair inclusion and the number of disconnected components within the clusters.", 
        "191": "For both methods, over twenty percent of the clusters contain antonym pairs even though the median cluster size is six.", 
        "192": "Signed clustering radically reduced the number of antonyms within clusters compared to the other methods.", 
        "193": "5http://nlp.stanford.edu/sentiment/ treebank.html  8 Empirical Results  Tables 3 and 5 present our main result.", 
        "194": "When using our signed clustering method with similar words, as labeled by SimLex-999 and SimVerb3500, our clustering accuracy increased by 5% on both SimLex-999 and SimVerb-3000.", 
        "195": "Furthermore, by combining the thesauri lookup with our clustering, we achieved almost perfect accuracy (96%).", 
        "196": "Table 5 shows the sentiment analysis task performance.", 
        "197": "Our method outperforms all methods with similar complexity; however, we did not reach state-of-the-art results when compared to much more complex models which also use a richer dataset.", 
        "198": "8.1 Evaluation Using Word Similarity Datasets  In a perfect setting, all word pairs rated highly similar by human annotators would be in the same cluster, and all words which were rated dissimilar would be in different clusters.", 
        "199": "Since our clustering algorithm produced sets of words, we used this evaluation instead of the more commonly reported correlations.", 
        "200": "In table 3 we show the results of the evaluation with SimLex-999.", 
        "201": "Combining thesaurus lookup and word2vec+CombThes clusters, labeled as Lookup + SC(W2V), yielded an accuracy of 0.96 (5 errors).", 
        "202": "Note that clusters using word2vec with normalized cuts does not improve accuracy.", 
        "203": "The MSW thesaurus has much lower coverage, but 100 % accuracy, which is why when\ncombined with the signed clustering the performance is 0.95.", 
        "204": "In table 3 we state the proportion of clusters containing dissimilar words as a sanity check for cluster size.", 
        "205": "(See supplemental material for full cluster size optimization information.)", 
        "206": "Another important result is that the verb accuracy yielded the largest accuracy gains, consistent with the results of Schwartz et al.", 
        "207": "(2015).", 
        "208": "Table 4 clearly shows that the overall performance of all methods is lower for verb similarity.", 
        "209": "However, the improvement using both signed clustering as well as thesaurus look is also larger.", 
        "210": "8.2 Sentiment Analysis  We trained an l2-norm regularized logistic regression (Friedman et al., 2001) and simultaneously \u03b3, \u03b2syn, and \u03b2ant using our word clusters in order to predict the coarse-grained sentiment at the sentence level.", 
        "211": "The \u03b3 and \u03b2 parameters were found using a portion of the data where we iteratively switch between the logistic regression and the parameters, holding each fixed.", 
        "212": "However, hyperparameters \u03c3 and , and the number of clusters\nK were optimized minimizing error using grid search.", 
        "213": "We compared our model against existing models: Naive Bayes with bag of words (NB) (Socher et al., 2013), sentence word embedding averages (VecAvg), retrofitted sentence word embeddings (RVecAvg) (Faruqui et al., 2015) that incorporate thesaurus information, simple recurrent neural networks (RNN), and two baselines of normalized cuts and signed normalized cuts using only thesaurus information.", 
        "214": "While the state-of-the art Convolutional Neural Network (CNN) (Kim, 2014) is at 0.881, our model performs quite well with much less information and complexity.", 
        "215": "Table 5 shows that signed clustering outperforms the baselines of Naive Bayes, normalized cuts, and signed cuts using just thesaurus information.", 
        "216": "Furthermore, we outperform comparable models, including retrofitting, which has thesaurus information, and the recurrent neural network, which has access to domain specific context information.", 
        "217": "Signed clustering using only thesaurus information (SC(Thes)) performed significantly worse than all other methods.", 
        "218": "This was largely due to low coverage; rare words such as \u201cWOW\u201d and \u201c??", 
        "219": "?\u201d are not covered.", 
        "220": "As expected, because normalized cut clusters include antonyms, the method performs worse than others.", 
        "221": "Nonetheless the improvement from 0.79 to 0.836 is quite drastic.", 
        "222": "9 Conclusion  We developed a novel theory for signed normalized cuts and an algorithm for finding their discrete solution.", 
        "223": "We showed that we can find su-\nperior semantically similar clusters which do not require new word embeddings but simply overlay thesaurus information on preexisting ones.", 
        "224": "The clusters are general and can be used with many out-of-the-box word embeddings.", 
        "225": "By accounting for antonym relationships, our algorithm greatly outperforms simple normalized cuts.", 
        "226": "Finally, we examined our clustering method on the sentiment analysis task from Socher et al.", 
        "227": "(2013) sentiment treebank dataset and showed that it improved performance versus comparable models.", 
        "228": "Our automatically generated clusters give better coverage than manually constructed thesauri.", 
        "229": "Our signed spectral clustering method allows us to incorporate the knowledge contained in these thesauri without modifying the word embeddings themselves.", 
        "230": "We further showed that use of the thesauri can be tuned to the task at hand.", 
        "231": "Our signed spectral clustering method could be applied to a broad range of NLP tasks, such as prediction of social group clustering, identification of personal versus non-personal verbs, and analyses of clusters which capture positive, negative, and objective emotional content.", 
        "232": "It could also be used to explore multi-view relationships, such as aligning synonym clusters across multiple languages.", 
        "233": "Another possibility is to use thesauri and word vector representations together with word sense disambiguation to generate semantically similar clusters for multiple senses of words.", 
        "234": "Furthermore, signed spectral clustering has broader applications such as cellular biology, social networking, and electricity networks.", 
        "235": "Finally, we plan to extend the hard signed clustering presented here to probabilistic soft clustering."
    }, 
    "document_id": "P17-1087.pdf.json"
}
