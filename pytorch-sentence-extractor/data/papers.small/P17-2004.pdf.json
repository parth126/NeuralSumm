{
    "abstract_sentences": {
        "1": "MT evaluation metrics are tested for correlation with human judgments either at the sentenceor the corpus-level.", 
        "2": "Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only.", 
        "3": "We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized.", 
        "4": "To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment.", 
        "5": "Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than\u2013and on average outperforms\u2013 both models on both objectives."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20\u201325 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2004\nMT evaluation metrics are tested for correlation with human judgments either at the sentence- or the corpus-level.", 
        "2": "Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only.", 
        "3": "We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized.", 
        "4": "To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment.", 
        "5": "Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than\u2013and on average outperforms\u2013 both models on both objectives.", 
        "6": "1 Introduction  Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made.", 
        "7": "Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojevic\u0301 and Sima\u2019an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc.", 
        "8": "Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004).", 
        "9": "Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments.", 
        "10": "Because the most widely available type of human judgments are relative ranking (RR) judgments, the main machine learning method used for training the metrics were based on the learningto-rank framework (Li, 2011).", 
        "11": "While the effectiveness of this framework for training evaluation metrics has been confirmed many times, e.g., (Ye et al., 2007; Duh, 2008; Stanojevic\u0301 and Sima\u2019an, 2014; Ma et al., 2016), so far there is no prior work exploring alternative objective functions for training learning-to-rank models.", 
        "12": "Without exception, all existing learning-to-rank models are trained to rank sentences while completely ignoring the corpora judgments, likely because human judgments come in the form of sentence rankings.", 
        "13": "It might seem that sentence and corpus level tasks are very similar but that is not the case.", 
        "14": "Empirically it has been shown that many metrics that perform well on the sentence level do not perform well on the corpus level and vice versa.", 
        "15": "By training to rank sentences the model does not necessarily learn to give scores that are well scaled, but only to give higher scores to better translations.", 
        "16": "Training for the corpus level score would force the metric to give well scaled scores on the sentence level.", 
        "17": "Human judgments of sentences can be aggregated in different ways to hypothesize human judgments of full corpora.", 
        "18": "However, this fact has not been used so far to train learning-to-rank models that are good for ranking different corpora.", 
        "19": "This work fills-in this gap by exploring the merits of different objective functions that take corpus level judgments into consideration.", 
        "20": "We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences.", 
        "21": "This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method.", 
        "22": "To tackle this prob-\n20\nlem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously.", 
        "23": "This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models.", 
        "24": "2 Models  All the models that we define have one basic function in common, we call it a forward(\u00b7) function, that maps the features of any sentence to a single real number.", 
        "25": "That function can be any differentiable function including multi-layer neural networks as in (Ma et al., 2016), but here we will stick with the standard linear model:\nforward(\u03c6) = \u03c6Tw + b\nHere \u03c6 is a vector with feature values of a sentence, w is a weight vector and b is a bias term.", 
        "26": "Usually in training we would like to process a mini-batch of feature vectors \u03a6, where \u03a6 is a matrix in which each column is a feature vector of individual sentence in the mini-batch or in the corpus.", 
        "27": "By using broadcasting we can rewrite the previous definition of the forward(\u00b7) function as:\nforward(\u03a6) = \u03a6Tw + b\nNow we can define the score of a sentence as a sigmoid function applied over the output of the forward(\u00b7) function because we want to get a score between 0 and 1:\nsentScore(\u03c6) = \u03c3(forward(\u03c6))\nAs the corpus level score we will use just the average of sentence level scores:\ncorpScore(\u03a6) = 1\nm\n\u2211 sentScore(\u03a6)\nwhere m is the number of sentences in the corpus.", 
        "28": "Next we present several objective functions that are illustrated by the computation graph in Figure 1.", 
        "29": "2.1 Training for Sentence Level Accuracy  Here we use the training objective very similar to BEER (Stanojevic\u0301 and Sima\u2019an, 2014) which is a learning-to-rank framework that finds a separating hyper-plane between \u201cgood\u201d and \u201cbad\u201d translations.", 
        "30": "Unlike BEER, we use a max-margin objective instead of logistic regression.", 
        "31": "For each mini-batch we randomly select m human relative ranking pairwise judgments and after extracting features for all the sentences taking part in these judgments we put features in two matrices \u03a6swin and \u03a6slos.", 
        "32": "These matrices are structured in such a way that for judgment i the column i in \u03a6swin contains the features of the \u201cgood\u201d translation in the judgment and the column i in \u03a6slos the features of the \u201cbad\u201d translation.", 
        "33": "We would like to maximize the average margin that would separate sentence level scores of pairs of translations in each judgment.", 
        "34": "Because the squashing sigmoid function does not influence the ranking we can directly optimize on the unsquashed forward pass and require that the margin between \u201cgood\u201d and \u201cbad\u201d translation is at least 1:\n\u2206sent = forward(\u03a6swin)\u2212 forward(\u03a6slos)\nLossSent = 1\nm\n\u2211 max(0, 1\u2212\u2206sent)  2.2 Training for Corpus Level Accuracy  At the corpus level we would like to do a similar thing as on the sentence level: maximize the distance between the scores of \u201cgood\u201d and \u201cbad\u201d corpora.", 
        "35": "In this case we have additional information that is not present on the sentence level: we know not only which corpus is (according to humans) better, but also by how much it is better.", 
        "36": "For\nthat we can use one of the heuristics such as the Expected Wins (Koehn, 2012).", 
        "37": "We can use this information to guide the learning model by how much it should separate the scores of two corpora.", 
        "38": "For doing this we use an approach similar to Max-Margin Markov Networks (Taskar et al., 2003) where for each training instance we dynamically scale the margin that should be enforced.", 
        "39": "We want the margin between the scores \u2206corp to be at least as big as the margin between the human scores \u2206human assigned to these systems.", 
        "40": "In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices \u03a6cwin and \u03a6clos for which we have a human comparison.", 
        "41": "The corpus level loss function is given by:\n\u2206corp = corpScore(\u03a6cwin)\u2212 corpScore(\u03a6clos) LossCorp = max(0,\u2206human \u2212\u2206corp)  2.3 Training Jointly for Sentence and Corpus Level Accuracy  In this model we optimize both objectives jointly in the style of multi-task learning (Caruana, 1997).", 
        "42": "Here we employ the simplest approach of just tasking the interpolation of the previously introduced loss functions.", 
        "43": "LossJoint = \u03b1 \u00b7 LossSent + (1\u2212 \u03b1) \u00b7 LossCorp\nThe interpolation is controlled by the hyperparameter \u03b1 which could in principle be tuned for good performance, but here we just fix it to 0.5 to give both objectives equal importance.", 
        "44": "2.4 Feature Functions  The feature functions that are used are reimplementation of many (but not all) feature functions of BEER.", 
        "45": "Because the point of this paper is about the exploration of different objective functions we did not try to experiment with more complex feature functions based on paraphrasing, function words or permutation trees.", 
        "46": "We use just simple precision, recall and 3 types of F-score (with \u03b2 parameters 1, 2 and 0.5) over different \u201cpieces\u201d of translation:\n\u2022 character n-grams of orders 1,2,3,4 and 5 \u2022 word n-grams of orders 1,2,3 and 4 \u2022 skip-bigrams of maximum skip 2 and \u221e\n(similar to ROUGE-S2 and ROUGE-S* (Lin and Och, 2004))\nOne final feature deals with length-disbalance.", 
        "47": "If the length of the system and reference translation are a and b respectively then this feature is computed as max(a,b)\u2212min(a,b)min(a,b) .", 
        "48": "It is computed both for word and character length.", 
        "49": "3 Experiments  Experiments are conducted on WMT13 (Macha\u0301c\u030cek and Bojar, 2013), WMT14 (Machacek and Bojar, 2014) and WMT16 (Bojar et al., 2016) datasets which were used as training, validation and testing datasets respectively.", 
        "50": "All of the models are implemented using TensorFlow1 and trained with L2 regularization \u03bb = 0.001 and ADAM optimizer with learning rate 0.001.", 
        "51": "The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison.", 
        "52": "Each model is trained for 200 epochs out of which the one performing best on the validation set for the objective function being optimized is used during the test time.", 
        "53": "We show the results for the relative ranking (RR) judgments correlation in Table 1.", 
        "54": "For all language pairs that are of the form en-X we show it under the column X and for all the language pairs that have English on the target side we present their average under the column en.", 
        "55": "RR corpus vs. sentence objective The corpusobjective is better than the sentence-objective for both corpus and sentence level RR judgments on 5 out of 7 languages and also on average correlation.", 
        "56": "RR joint vs. single-objectives Training for the joint objective improves even more on both levels of RR correlation and outperforms both singleobjective models on average and on 4 out of 7 languages.", 
        "57": "Making confident conclusions from these results is difficult because, to the best of our knowledge, there is no principled way of measuring statistical significance on the RR judgments.", 
        "58": "That is why we also tested on direct assessment (DA) judgments available from WMT16.", 
        "59": "On DA we can measure statistical significance on the sentence level using Williams test (Graham et al., 2015) and on the corpus level using combination of hybrid-supersampling and Williams test (Graham and Liu, 2016).", 
        "60": "The results of correlation with human judgment are for sentence and corpus level are shown in Table 2.", 
        "61": "1https://www.tensorflow.org/\nDA corpus vs. other objectives On DA judgments the results for corpus level objective are completely different than on the RR judgments.", 
        "62": "On DA judgments the corpus-objective model is significantly outperformed on both levels and on all languages by both of the other objectives.", 
        "63": "This shows that gambling on one objective function (being that sentence or corpus level objective) could give unpredictable results.", 
        "64": "This is precisely the motivation for creating the joint model with multi-objective training.", 
        "65": "DA joint vs. single objectives By choosing to jointly optimize both objectives we get a much more stable model that performs well both on DA and RR judgments and on both levels of judgment.", 
        "66": "On the DA sentence level, the joint model was not outperformed by any other model and on 3 out of 7 language pairs it significantly outperforms both alternative objectives.", 
        "67": "On the corpus level results are\na bit mixed, but still joint objective outperforms both other models on 4 out of 7 language pairs and also it gives higher correlation on average.", 
        "68": "4 Conclusion  In this work we found that altering the objective function for training MT metrics can have radical effects on performance.", 
        "69": "Also the effects of the objective functions can sometimes be unexpected: the sentence objective might not be good for sentence level correlation (in case of RR judgments) and the corpus objective might not be good for corpus level correlation (in case of DA judgments).", 
        "70": "The difference among objectives is better explained by different types of human judgments: the corpus objective is better for RR while sentence objective is better for DA judgments.", 
        "71": "Finally, the best results are achieved by training for both objectives at the same time.", 
        "72": "This gives\nan evaluation metric that is far more stable in its performance over all methods of meta-evaluation.", 
        "73": "Acknowledgments  This work is supported by NWO VICI grant nr.", 
        "74": "277-89-002, DatAptor project STW grant nr.", 
        "75": "12271 and QT21 project H2020 nr.", 
        "76": "645452."
    }, 
    "document_id": "P17-2004.pdf.json"
}
