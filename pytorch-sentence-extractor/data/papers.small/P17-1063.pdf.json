{
    "abstract_sentences": {
        "1": "The referring expressions (REs) produced by a natural language generation (NLG) system can be misunderstood by the hearer, even when they are semantically correct.", 
        "2": "In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them.", 
        "3": "We present an algorithm for generating corrective REs that use contrastive focus (\u201cno, the BLUE button\u201d) to emphasize the information the hearer most likely misunderstood.", 
        "4": "We show empirically that these contrastive REs are preferred over REs without contrast marking."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 678\u2013687 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1063  1 Introduction  Interactive natural language generation (NLG) systems face the task of detecting when they have been misunderstood, and reacting appropriately to fix the problem.", 
        "2": "For instance, even when the system generated a semantically correct referring expression (RE), the user may still misunderstand it, i.e.", 
        "3": "resolve it to a different object from the one the system intended.", 
        "4": "In an interactive setting, such as a dialogue system or a pedestrian navigation system, the system can try to detect such misunderstandings \u2013 e.g.", 
        "5": "by predicting what the hearer understood from their behavior (Engonopoulos et al., 2013) \u2013 and to produce further utterances which resolve the misunderstanding and get the hearer to identify the intended object after all.", 
        "6": "When humans correct their own REs, they routinely employ contrastive focus (Rooth, 1992; Krifka, 2008) to clarify the relationship to the original RE.", 
        "7": "Say that we originally described an object b as \u201cthe blue button\u201d, but the hearer approaches a button b\u2032 which is green, thus providing evidence that they misunderstood the RE to mean b\u2032.", 
        "8": "In this\ncase, we would like to say \u201cno, the BLUE button\u201d, with the contrastive focus realized by an appropriate pitch accent on \u201cBLUE\u201d.", 
        "9": "This utterance alerts the hearer to the fact that they misunderstood the original RE; it reiterates the information from the original RE; and it marks the attribute \u201cblue\u201d as a salient difference between b\u2032 and the object the original RE was intended to describe.", 
        "10": "In this paper, we describe an algorithm for generating REs with contrastive focus.", 
        "11": "We start from the modeling assumption that misunderstandings arise because the RE rs the system uttered was corrupted by a noisy channel into an RE ru which the user \u201cheard\u201d and then resolved correctly; in the example above, we assume the user literally heard \u201cthe green button\u201d.", 
        "12": "We compute this (hypothetical) RE ru as the RE which refers to b\u2032 and has the lowest edit distance from rs.", 
        "13": "Based on this, we mark the contrastive words in rs, i.e.", 
        "14": "we transform \u201cthe blue button\u201d into \u201cthe BLUE button\u201d.", 
        "15": "We evaluate our system empirically on REs from the GIVE Challenge (Koller et al., 2010) and the TUNA Challenge (van der Sluis et al., 2007), and show that the contrastive REs generated by our system are preferred over a number of baselines.", 
        "16": "The paper is structured as follows.", 
        "17": "We first review related work in Section 2 and define the problem of generating contrastive REs in Section 3.", 
        "18": "Section 4 sketches the general architecture for RE generation on which our system is based.", 
        "19": "In Section 5, we present the corruption model and show how to use it to reconstruct ru.", 
        "20": "Section 6 describes how we use this information to generate contrastive markup in rs, and in Section 7 we evaluate our approach.", 
        "21": "2 Related Work  The notion of focus has been extensively studied in the literature on theoretical semantics and prag-\n678\nmatics, see e.g.", 
        "22": "Krifka (2008) and Rooth (1997) for overview papers.", 
        "23": "Krifka follows Rooth (1992) in taking focus as \u201cindicat(ing) the presence of alternatives that are relevant for the interpretation of linguistic expressions\u201d; focus then establishes a contrast between an object and these alternatives.", 
        "24": "Bornkessel and Schlesewsky (2006) find that corrective focus can even override syntactic requirements, on the basis of \u201cits extraordinarily high communicative saliency\u201d.", 
        "25": "This literature is purely theoretical; we offer an algorithm for automatically generating contrastive focus.", 
        "26": "In speech, focus is typically marked through intonation and pitch accents (Levelt, 1993; Pierrehumbert and Hirschberg, 1990; Steube, 2001), while concepts that can be taken for granted are deaccented and/or deleted.", 
        "27": "Developing systems which realize precise pitch contours for focus in text-to-speech settings is an ongoing research effort.", 
        "28": "We therefore realize focus in written language in this paper, by capitalizing the focused word.", 
        "29": "We also experiment with deletion of background words.", 
        "30": "There is substantial previous work on interactive systems that detect and respond to misunderstandings.", 
        "31": "Misu et al.", 
        "32": "(2014) present an error analysis of an in-car dialogue system which shows that more than half the errors can only be resolved through further clarification dialogues, as opposed to better sensors and/or databases; that is, by improved handling of misunderstandings.", 
        "33": "Engonopoulos et al.", 
        "34": "(2013) detect misunderstandings of REs in interactive NLG through the use of a statistical model.", 
        "35": "Their model also predicts the object to which a misunderstood RE was incorrectly resolved.", 
        "36": "Moving from misunderstanding detection to error correction, Zarrie\u00df and Schlangen (2016) present an interactive NLG algorithm which is capable of referring in installments, in that it can generate multiple REs that are designed to correct misunderstandings of earlier REs to the same object.", 
        "37": "The interactive NLG system developed by Akkersdijk et al.", 
        "38": "(2011) generates both reflective and anticipative feedback based on what a user does and sees.", 
        "39": "Their error detection and correction strategy distinguishes a fixed set of possible situations where feedback is necessary, and defines custom, hard-coded RE generation sub-strategies for each one.", 
        "40": "None of these systems generate REs marked for focus.", 
        "41": "We are aware of two items of previous work that\naddress the generation of contrastive REs directly.", 
        "42": "Milosavljevic and Dale (1996) outline strategies for generating clarificatory comparisons in encyclopedic descriptions.", 
        "43": "Their surface realizer can generate contrastive REs, but the attributes that receive contrastive focus have to be specified by hand.", 
        "44": "Krahmer and Theune (2002) extend the Incremental Algorithm (Dale and Reiter, 1995) so it can mark attributes as contrastive.", 
        "45": "This is a fully automatic algorithm for contrastive REs, but it inherits all the limitations of the Incremental Algorithm, such as its reliance on a fixed attribute order.", 
        "46": "Neither of these two approaches evaluates the quality of the contrastive REs it generates.", 
        "47": "Finally, some work has addressed the issue of generating texts that realize the discourse relation contrast.", 
        "48": "For instance, Howcroft et al.", 
        "49": "(2013) show how to choose contrastive discourse connectives (but, while, .", 
        "50": ".", 
        "51": ". )", 
        "52": "when generating restaurant descriptions, thus increasing human ratings for naturalness.", 
        "53": "Unlike their work, the research presented in this paper is not about discourse relations, but about assigning focus in contrastive REs.", 
        "54": "3 Interactive NLG  We start by introducing the problem of generating corrective REs in an interactive NLG setting.", 
        "55": "We use examples from the GIVE Challenge (Koller et al., 2010) throughout the paper; however, the algorithm itself is domain-independent.", 
        "56": "GIVE is a shared task in which an NLG system (the instruction giver, IG) must guide a human user (the instruction follower, IF) through a virtual 3D environment.", 
        "57": "The IF needs to open a safe and steal a trophy by clicking on a number of buttons in the right order without triggering alarms.", 
        "58": "The job of the NLG system is to generate natural-language instructions which guide the IF to complete this task successfully.", 
        "59": "The generation of REs has a central place in the GIVE Challenge because the system frequently needs to identify buttons in the virtual environment to the IF.", 
        "60": "Figure 1 shows a screenshot of a GIVE game in progress; here b1 and b4 are blue buttons, b2 and b3 are yellow buttons, and w1 is a window.", 
        "61": "If the next button the IF needs to press is b4 \u2013 the intended object, os \u2013 then one good RE for b4 would be \u201cthe blue button below the window\u201d, and the system should utter:\n(1) Press the blue button below the window.", 
        "62": "After uttering this sentence, the system can\ntrack the IF\u2019s behavior to see whether the IF has understood the RE correctly.", 
        "63": "If the wrong button is pressed, or if a model of IF\u2019s behavior suggests that they are about to press the wrong button (Engonopoulos et al., 2013), the original RE has been misunderstood.", 
        "64": "However, the system still gets a second chance, since it can utter a corrective RE, with the goal of identifying b4 to the IF after all.", 
        "65": "Examples include simply repeating the original RE, or generating a completely new RE from scratch.", 
        "66": "The system can also explicitly take into account which part of the original RE the IF misunderstood.", 
        "67": "If it has reason to believe that the IF resolved the RE to b3, it could say:\n(2) No, the BLUE button below the window.", 
        "68": "This use of contrastive focus distinguishes the attributes the IF misunderstood (blue) from those that they understood correctly (below the window), and thus makes it easier for the IF to resolve the misunderstanding.", 
        "69": "In speech, contrastive focus would be realized with a pitch accent; we approximate this accent in written language by capitalizing the focused word.", 
        "70": "We call an RE that uses contrastive focus to highlight the difference between the misunderstood and the intended object, a contrastive RE.", 
        "71": "The aim of this paper is to present an algorithm for computing contrastive REs.", 
        "72": "4 Generating Referring Expressions  While we make no assumptions on how the original RE rs was generated, our algorithm for reconstructing the corrupted RE ru requires an RE generation algorithm that can represent all semantically correct REs for a given object compactly in a chart.", 
        "73": "Here we sketch the RE generation of Engonopoulos and Koller (2014), which satisfies this requirement.", 
        "74": "This algorithm assumes a synchronous grammar which relates strings with the sets of objects they refer to.", 
        "75": "Strings and their referent sets are constructed in parallel from lexicon entries and grammar rules; each grammar rule specifies how the referent set of the parent is determined from those of the children.", 
        "76": "For the scene in Figure 1, we assume lexicon entries which express, among other things, that the word \u201cblue\u201d denotes the set {b1, b4} and the word \u201cbelow\u201d denotes the relation {(w1, b1), (w1, b2), (b3, w1), (b4, w1)}.", 
        "77": "We combine these lexicons entries using rules such as\n\u201cN\u2192 button() |button |{b1, b2, b3, b4}\u201d which generates the string \u201cbutton\u201d and asso-\nciates it with the set of all buttons or\n\u201cN\u2192 N1(N,PP) |w1 \u2022 w2 |R1 \u2229R2\u201d which states that a phrase of type noun can be combined with a prepositional phrase and their denotations will be intersected.", 
        "78": "Using these rules we can determine that \u201cthe window\u201d denotes {w1}, that \u201cbelow the window\u201d can refer to {b3, b4} and that \u201cblue button below the window\u201d uniquely refers to {b4}.", 
        "79": "The syntax tree in Fig.", 
        "80": "2 represents a complete derivation of an RE for {b4}.", 
        "81": "The algorithm of Engonopoulos and Koller computes a chart which represents the set of all possible REs for a given set of input objects, such as {b4}, according to the grammar.", 
        "82": "This is done by building a chart containing all derivations of the grammar which correspond to the desired set.", 
        "83": "They represent this chart as a finite tree automaton (Comon et al., 2007).", 
        "84": "Here we simply write the chart as a Context-Free Grammar.", 
        "85": "The strings produced by this Context-Free Grammar are then exactly the REs for the intended object.", 
        "86": "For example, the syntax tree in Fig.", 
        "87": "2 is generated by the parse chart for the set {b4}.", 
        "88": "Its nonterminal symbols consist of three parts: a syntactic category\n(given by the synchronous grammar), the referent for which an RE is currently being constructed, and the set of objects to which the entire subtree refers.", 
        "89": "The grammar may include recursion and therefore allow for an infinite set of possible REs.", 
        "90": "If it is weighted, one can use the Viterbi algorithm to compute the best RE from the chart.", 
        "91": "5 Listener Hypotheses and Edit Distance    5.1 Corruption model  Now let us say that the system has generated and uttered an RE rs with the intention of referring to the object os, but it has then found that the IF has misunderstood the RE and resolved it to another object, ou (see Fig.", 
        "92": "3).", 
        "93": "We assume for the purposes of this paper that such a misunderstanding arises because rs was corrupted by a noisy channel when it was transmitted to the IF, and the IF \u201cheard\u201d a different RE, ru.", 
        "94": "We further assume that the IF then resolved ru correctly, i.e.", 
        "95": "the corruption in the transmission is the only source of misunderstandings.", 
        "96": "In reality, there are of course many other reasons why the IF might misunderstand rs, such as lack of attention, discrepancies in the lexicon or the world model of the IG and IF, and so on.", 
        "97": "We make a simplifying assumption in order to make the misunderstanding explicit at the level of the RE strings, while still permitting meaningful corrections for a large class of misunderstandings.", 
        "98": "An NLG system that builds upon this idea in order to generate a corrective RE has access to the values of os, rs and ou; but it needs to infer the most likely corrupted RE ru.", 
        "99": "To do this, we model the corruption using the edit operations used for the familiar Levenshtein edit distance (Mohri, 2003) over the alphabet \u03a3: Sa, substitution of a word with a symbol a \u2208 \u03a3; D, deletion of a word; Ia, insertion of the symbol a \u2208 \u03a3; or K, keeping the word.", 
        "100": "The noisy channel passes\nover each word in rs and applies either D, K or one of the S operations to it.", 
        "101": "It may also apply I operations before or after a word.", 
        "102": "We call any sequence s of edit operations that could apply to rs an edit sequence for rs.", 
        "103": "An example for an edit sequence which corrupts rs = \u201cthe blue button below the window\u201d into ru = \u201cthe yellow button above the window\u201d is shown in Figure 4.", 
        "104": "The same ru could also have been generated by the edit operation sequence K Syellow K Sabove KK, and there is generally a large number of edit sequences that could transform between any two REs.", 
        "105": "If an edit sequence s maps x to y, we write apply(s, x) = y.", 
        "106": "We can now define a probability distribution P (s | rs) over edit sequences s that the noisy channel might apply to the string rs, as follows:\nP (s | rs) = 1\nZ\n\u220f si\u2208s exp(\u2212c(si)),\nwhere c(si) is a cost for using the edit operation si.", 
        "107": "We set c(K) = 0, and for any a in our alphabet we set c(Sa) = c(Ia) = c(D) = C, for some fixed C > 0.", 
        "108": "Z is a normalizing constant which is independent of s and ensures that the probabilities sum to 1.", 
        "109": "It is finite for sufficiently high values of C, because no sequence for rs can ever contain more K, S and D operations than there are words in rs, and the total weight of sequences generated by adding more and more I operations will converge.", 
        "110": "Finally, let L be the set of referring expressions that the IF would resolve to ou, i.e.", 
        "111": "the set of candidates for ru.", 
        "112": "Then the most probable edit sequence for rs which generates an ru \u2208 L is given by\ns\u2217 = arg max s : apply(s,rs)\u2208L P (s | rs)\n= arg mins \u2211 si\u2208s c(si),\ni.e.", 
        "113": "s\u2217 is the edit sequence that maps rs to an RE in L with minimal cost.", 
        "114": "We will assume that s\u2217 is the edit sequence that corrupted rs, i.e.", 
        "115": "that ru = apply(s\u2217, rs).", 
        "116": "5.2 Finding the most likely corruption  It remains to compute s\u2217; we will then show in Section 6 how it can be used to generate a corrective RE.", 
        "117": "Attempting to find s\u2217 by enumeration is impractical, as the set of edit sequences for a given rs and ru may be large and the set of possible ru for a given ou may be infinite.", 
        "118": "Instead\nwe will use the algorithm from Section 4 to compute a chart for all the possible REs for ou, represented as a context-free grammar G whose language L = L(G) consists of these REs.", 
        "119": "We will then intersect it with a finite-state automaton which keeps track of the edit costs, obtaining a second context-free grammar G\u2032.", 
        "120": "These operations can be performed efficiently, and s\u2217 can be read off of the minimum-cost syntax tree of G\u2032.", 
        "121": "Edit automaton.", 
        "122": "The possible edit sequences for a given rs can be represented compactly in the form of a weighted finite-state automaton F (rs) (Mohri, 2003).", 
        "123": "Each run of the automaton on a string w corresponds to a specific edit sequence that transforms rs intow, and the sum of transition weights of the run is the cost of that edit sequence.", 
        "124": "We call F (rs) the edit automaton.", 
        "125": "It has a state qi for every position i in rs; the start state is q0 and the final state is q|rs|.", 
        "126": "For each i, it has a \u201ckeep\u201d transition from qi to qi+1 that reads the word at position i with cost 0.", 
        "127": "In addition, there are transitions from qi to qi+1 with cost C that read any symbol in \u03a3 (for substitution) and ones that read the empty string (for deletion).", 
        "128": "Finally, there is a loop with cost C from each qi to itself and for any symbol in \u03a3, implementing insertion.", 
        "129": "An example automaton for rs = \u201cthe blue button below the window\u201d is shown in Figure 5.", 
        "130": "The transitions are written in the form \u3008word in w : associated cost\u3009.", 
        "131": "Note that every path through the edit transducer corresponds to a specific edit sequence s, and the sum of the costs along the path corresponds to \u2212 logP (s | rs)\u2212 logZ.", 
        "132": "Combining G and F (rs).", 
        "133": "Now we can combine G with F (rs) to obtain G\u2032, by intersecting them using the Bar-Hillel construction (Bar-Hillel et al., 1961; Hopcroft and Ullman, 1979).", 
        "134": "For the purposes of our presentation we assume that G is in Chomsky Normal Form, i.e.", 
        "135": "all rules have the form A \u2192 a, where a is a word, or A \u2192 B C, where both symbols on the right hand side are nonterminals.", 
        "136": "The resulting grammar G\u2032 uses nonterminal symbols of the form Nb,A,\u3008qi,qk\u3009, where\nb, A are as in Section 4, and qi, qk indicate that the string derived by this nonterminal was generated by editing the substring of rs from position i to k.\nLet Nb,A \u2192 a be a production rule of G with a word a on the right-hand side; as explained above, b is the object to which the subtree should refer, and A is the set of objects to which the subtree actually might refer.", 
        "137": "Let t = qi \u2192 \u3008a:c\u3009qk be a transition in F (rs), where q, q\u2032 are states of F (rs) and c is the edit cost.", 
        "138": "From these two, we create a context-free rule Nb,A,\u3008qi,qk\u3009 \u2192 a with weight c and add it to G\u2032.", 
        "139": "If k = i + 1, these rules represent K and S operations; if k = i, they represent insertions.", 
        "140": "Now let Nb,A \u2192 Xb1,A1 Yb2,A2 be a binary rule in G, and let qi, qj , qk be states of F (rs) with i \u2264 j \u2264 k. We then add a rule Nb,A,\u3008qi,qk\u3009 \u2192 Xb1,A1,\u3008qi,qj\u3009 Yb2,A2,\u3008qj ,qk\u3009 to G\n\u2032.", 
        "141": "These rules are assigned weight 0, as they only combine words according to the grammar structure of G and do not encode any edit operations.", 
        "142": "Finally, we deal with deletion.", 
        "143": "Let Nb,A be a nonterminal symbol in G and let qh, qi, qj , qk be states of F (rs) with h \u2264 i \u2264 j \u2264 k. We then add a rule Nb,A,\u3008qh,qk\u3009 \u2192 Nb,A,\u3008qi,qj\u3009 to G\u2032.", 
        "144": "This rule deletes the substrings from positions h to i and j to k from rs; thus we assign it the cost ((i\u2212 h) + (k \u2212 j))C, i.e.", 
        "145": "the cost of the corresponding transitions.", 
        "146": "If the start symbol of G is Sb,A, then the start symbol of G\u2032 is Sb,A,\u3008q0,q|rs|\u3009.", 
        "147": "This construction intersects the languages of G and F (rs), but because F (rs) accepts all strings over the alphabet, the languages of G\u2032 and G will be the same (namely, all REs for ou).", 
        "148": "However, the weights in G\u2032 are inherited from F (rs); thus the weight of each RE in L(G\u2032) is the edit cost from rs.", 
        "149": "Example.", 
        "150": "Fig.", 
        "151": "6 shows an example tree for the G\u2032 we obtain from the automaton in Fig.", 
        "152": "5.", 
        "153": "We can read the string w = \u201cthe yellow button above the window\u201d off of the leaves; by construction, this is an RE for ou.", 
        "154": "Furthermore, we can reconstruct the edit sequence that maps from rs to w from the rules of G\u2032 that\nwere used to derive w. We can see that \u201cyellow\u201d was created by an insertion because the two states of F (rs) in the preterminal symbol just above it are the same.", 
        "155": "If the two states are different, then the word was either substituted (\u201cabove\u201d, if the rule had weight C) or kept (\u201cthe\u201d, if the rule had weight 0).", 
        "156": "By contrast, unary rules indicate deletions, in that they make \u201cprogress\u201d in rs without adding new words to w.\nWe can compute the minimal-cost tree ofG\u2032 using the Viterbi algorithm.", 
        "157": "Thus, to summarize, we can calculate s\u2217 from the intersection of a contextfree grammar G representing the REs to ou with the automaton F (rs) representing the edit distance to rs.", 
        "158": "From this, we obtain ru = apply(s\u2217, rs).", 
        "159": "This is efficient in practice.", 
        "160": "6 Generating Contrastive REs    6.1 Contrastive focus  We are now ready to generate a contrastive RE from rs and s\u2217.", 
        "161": "We assign focus to the words in rs which were changed by the corruption \u2013 that is, the ones to which s\u2217 applied Substitute or Delete operations.", 
        "162": "For instance, the edit sequence in Fig.", 
        "163": "6 deleted \u201cblue\u201d and substituted \u201cbelow\u201d with \u201cabove\u201d.", 
        "164": "Thus, we mark these words with focus, and obtain the contrastive RE \u201cthe BLUE button BELOW the window\u201d.", 
        "165": "We call this strategy Emphasis, and write rsE for the RE obtained\nby applying the Emphasis strategy to the RE rs.", 
        "166": "6.2 Shortening  We also investigate a second strategy, which generates more succinct contrastive REs than the Emphasis strategy.", 
        "167": "Most research on RE generation (e.g.", 
        "168": "Dale and Reiter (1995)) has assumed that hearers should prefer succinct REs, which in particular do not violate the Maxim of Quantity (Grice, 1975).", 
        "169": "When we utter a contrastive RE, the user has previously heard the RE rs, so some of the information in rsE is redundant.", 
        "170": "Thus we might obtain a more succinct, and possibly better, RE by dropping such redundant information from the RE.", 
        "171": "For the grammars we consider here, rsE often combines an NP and a PP, e.g.", 
        "172": "\u201c[blue button]NP [below the window]PP \u201d.", 
        "173": "If errors occur only in one of these constituents, then it might be sufficient to generate a contrastive RE using only that constituent.", 
        "174": "We call this strategy Shortening and define it as follows.", 
        "175": "If all the words that are emphasized in rsE are in the NP, the Shortening RE is \u201cthe\u201d plus the NP, with emphasis as in rsE .", 
        "176": "So if rs is \u201cthe [blue button] [above the window]\u201d and s\u2217 = K SyellowKKKK, corresponding to a rsE of \u201cthe [BLUE button] [above the window]\u201d, then the RE would be \u201cthe [BLUE button]\u201d.", 
        "177": "If all the emphasis in rsE is in the PP, we use\n\u201cthe one\u201d plus the PP and again capitalize as in rs\nE .", 
        "178": "So if we have s\u2217 = KKK SbelowKK, where rsE is \u201cthe [blue button] [ABOVE the window]\u201d, we obtain \u201cthe one [ABOVE the window].\u201d If there is no PP or if rsE emphasizes words in both the NP and the PP, then we just use rsE .", 
        "179": "7 Evaluation  To test whether our algorithm for contrastive REs assigns contrastive focus correctly, we evaluated it against several baselines in crowdsourced pairwise comparison overhearer experiments.", 
        "180": "Like Bu\u00df et al.", 
        "181": "(2010), we opted for an overhearer experiment to focus our evaluation on the effects of contrastive feedback, as opposed to the challenges presented by the navigational and timing aspects of a fully interactive system.", 
        "182": "7.1 Domains and stimuli  We created the stimuli for our experiments from two different domains.", 
        "183": "We performed a first experiment with scenes from the GIVE Challenge, while a second experiment replaced these scenes with stimuli from the \u201cPeople\u201d domain of the TUNA Reference Corpus (van der Sluis et al., 2007).", 
        "184": "This corpus consists of photographs of men annotated with nine attributes, such as whether the\nWe wanted our player to select the person circled in green:\nSo we told them: the light haired old man in a suit looking straight.", 
        "185": "But they selected the person circled in red instead.", 
        "186": "Which correction is better for this scene?", 
        "187": "person has a beard, a tie, or is looking straight.", 
        "188": "Six of these attributes were included in the corpus to better reflect human RE generation strategies.", 
        "189": "Many human-generated REs in the corpus are overspecific, in that they contain attributes that are not necessary to make the RE semantically unique.", 
        "190": "We chose the GIVE environment in order to test REs referring both to attributes of an object, i.e.", 
        "191": "color, and to its spatial relation to other visible objects in the scene.", 
        "192": "The TUNA Corpus was chosen as a more challenging domain, due to the greater number of available properties for each object on a scene.", 
        "193": "Each experimental subject was presented with screenshots containing a marked object and an RE.", 
        "194": "Subjects were told that we had previously referred to the marked object with the given RE, but an (imaginary) player misunderstood this RE and selected a different object, shown in a second screenshot.", 
        "195": "They were then asked to select which one of two corrections they considered better, where \u201cbetter\u201d was intentionally left unspecific.", 
        "196": "Figs.", 
        "197": "7 and 8 show examples for each domain.", 
        "198": "The full set of stimuli is available as supplementary material.", 
        "199": "To maintain annotation quality in our crowdsourcing setting, we designed test items with a\nclearly incorrect answer, such as REs referring to the wrong target or a nonexistent one.", 
        "200": "These test items were randomly interspersed with the real stimuli, and only subjects with a perfect score on the test items were taken into account.", 
        "201": "Experimental subjects were asked to rate up to 12 comparisons, shown in groups of 3 scenes at a time, and were automatically disqualified if they evaluated any individual scene in less than 10 seconds.", 
        "202": "The order in which the pairs of strategies were shown was randomized, to avoid effects related to the order in which they were presented on screen.", 
        "203": "7.2 Experiment 1  Our first experiment tested four strategies against each other.", 
        "204": "Each experimental subject was presented with two screenshots of 3D scenes with a marked object and an RE (see Fig.", 
        "205": "7 for an example).", 
        "206": "Each subject was shown a total of 12 scenes, selected at random from 16 test scenes.", 
        "207": "We collected 10 judgments for each possible combination of GIVE scene and pair of strategies, yielding a total of 943 judgements from 142 subjects after removing fake answers.", 
        "208": "We compared the Emphasis and Shortening strategies from Section 6 against two baselines.", 
        "209": "The Repeat strategy simply presented rs as a \u201ccontrastive\u201d RE, without any capitalization.", 
        "210": "Comparisons to Repeat test the hypothesis that subjects prefer explicit contrastive focus.", 
        "211": "The Random strategy randomly capitalized adjectives, adverbs, and/or prepositions that were not capitalized by the Emphasis strategy.", 
        "212": "Comparisons to Random verify that any preference for Emphasis is not only due to the presence of contrastive focus, but also because our method identifies precisely where that focus should be.", 
        "213": "Table 1a shows the results of all pairwise comparisons.", 
        "214": "For each row strategy StratR and each column strategy StratC , the table value corresponds to (#StratR pref.", 
        "215": "over StratC)\u2212(#StratC pref.", 
        "216": "over StratR)\n(# tests between StratR and StratC)\nSignificance levels are taken from a two-tailed binomial test over the counts of preferences for each strategy.", 
        "217": "We find a significant preference for the Emphasis strategy over all others, providing evidence that our algorithm assigns contrastive focus to the right words in the corrective RE.", 
        "218": "While the Shortening strategy is numerically preferred over both baselines, the difference is not significant, and it is significantly worse than\nthe Emphasis strategy.", 
        "219": "This is surprising, given our initial assumption that listeners prefer succinct REs.", 
        "220": "It is possible that a different strategy for shortening contrastive REs would work better; this bears further study.", 
        "221": "7.3 Experiment 2  In our second experiment, we paired the Emphasis, Repeat, and Random strategies against each other, this time evaluating each strategy in the TUNA people domain.", 
        "222": "Due to its poor performance in Experiment 1, which was confirmed in pilot experiments for Experiment 2, the Shortening strategy was not included.", 
        "223": "The experimental setup for the TUNA domain used 3x4 grids of pictures of people chosen at random from the TUNA Challenge, as shown in Fig.", 
        "224": "8.", 
        "225": "We generated 8 such grids, along with REs ranging from two to five attributes and requiring one or two attributes to establish the correct contrast.", 
        "226": "The larger visual size of objects in the the TUNA scenes allowed us to mark both os and ou in a single picture without excessive clutter.", 
        "227": "The REs for Experiment 2 were designed to only include attributes from the referred objects, but no information about its position in relation to other objects.", 
        "228": "The benefit is twofold: we avoid taxing our subjects\u2019 memory with extremely long REs, and we ensure that the overall length of the second set of REs is comparable to those in the previous experiment.", 
        "229": "We obtained 240 judgements from 65 subjects (after removing fake answers).", 
        "230": "Table 1b shows the results of all pairwise comparisons.", 
        "231": "We find that even in the presence of a larger number of attributes, our algorithm assigns contrastive focus to the correct words of the RE.", 
        "232": "7.4 Discussion  Our experiments confirm that the strategy for computing contrastive REs presented in this paper works in practice.", 
        "233": "This validates the corruption model, which approximates semantic mismatches between what the speaker said and what the listener understood as differences at the level of words in strings.", 
        "234": "Obviously, this model is still an approximation, and we will test its limits in future work.", 
        "235": "We find that users generally prefer REs with an emphasis over simple repetitions.", 
        "236": "In the more challenging scenes of the TUNA corpus, users even have a significant preference of Random over\nRepeat, although this makes no semantic sense.", 
        "237": "This preference may be due to the fact that emphasizing anything at least publically acknowledges the presence of a misunderstanding that requires correction.", 
        "238": "It will be interesting to explore whether this preference holds up in an interactive setting, rather than an overhearer experiment, where listeners will have to act upon the corrective REs.", 
        "239": "The poor performance of the Shortening strategy is a surprising negative result.", 
        "240": "We would expect a shorter RE to always be preferred, following the Gricean Maxim of Quantity (Grice, 1975).", 
        "241": "This may because our particular Shortening strategy can be improved, or it may be because listeners interpret the shortened REs not with respect to the original instructions, but rather with respect to a \u201crefreshed\u201d context (as observed, for instance, in Gotzner et al.", 
        "242": "(2016)).", 
        "243": "In this case the shortened REs would not be unique with respect to the refreshed, wider context.", 
        "244": "8 Conclusion  In this paper, we have presented an algorithm for generating contrastive feedback for a hearer who has misunderstood a referring expression.", 
        "245": "Our technique is based on modeling likely user misunderstandings and then attempting to give feedback that contrasts with the most probable incorrect understanding.", 
        "246": "Our experiments show that this technique accurately predicts which words to mark as focused in a contrastive RE.", 
        "247": "In future work, we will complement the overhearer experiment presented here with an end-toend evaluation in an interactive NLG setting.", 
        "248": "This will allow us to further investigate the quality of the correction strategies and refine the Shortening strategy.", 
        "249": "It will also give us the opportunity to investigate empirically the limits of the corruption model.", 
        "250": "Furthermore, we could use this data to refine the costs c(D), c(Ia) etc.", 
        "251": "for the edit operations, possibly assigning different costs to different edit operations.", 
        "252": "Finally, it would be interesting to combine our algorithm with a speech synthesis system.", 
        "253": "In this way, we will be able to express focus with actual pitch accents, in contrast to the typographic approximation we made here."
    }, 
    "document_id": "P17-1063.pdf.json"
}
