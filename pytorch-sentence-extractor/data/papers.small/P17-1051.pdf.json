{
    "abstract_sentences": {
        "1": "In this paper we present a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes.", 
        "2": "This framework is the first to consider vocabulary-wide syntactico-semantic information for this task.", 
        "3": "We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality.", 
        "4": "We validate our algorithm across different datasets and languages and present new state-of-the-art"
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 552\u2013561 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1051  1 Introduction  Morpheme segmentation is a core natural language processing (NLP) task used as an integral component in related-fields such as information retrieval (IR) (Zieman and Bleich, 1997; Kurimo et al., 2007), automatic speech recognition (ASR) (Bilmes and Kirchhoff, 2003; Kurimo et al., 2006), and machine translation (MT) (Lee, 2004; Virpioja et al., 2007).", 
        "2": "Most previous works have relied solely on orthographic features (Harris, 1970; Goldsmith, 2000; Creutz and Lagus, 2002, 2005, 2007), neglecting the underlying semantic information.", 
        "3": "This has led to an over-segmentation of words because a change of the surface form pattern is a necessary but insufficient indication of a morphological change.", 
        "4": "For example, the surface form of \u201cfreshman\u201d, hints that it should be segmented to \u201cfresh-man\u201d, although \u201cfreshman\u201d does not describe semantically the compositional meaning of \u201cfresh\u201d and \u201cman\u201d.", 
        "5": "To compensate for this lack of semantic knowledge, previous works (Schone and Jurafsky,\n2000; Baroni et al., 2002; Narasimhan et al., 2015) have incorporated semantic knowledge locally by checking the semantic relatedness of possibly morphologically related pair of words.", 
        "6": "Narasimhan et al.", 
        "7": "(2015) check for semantic relatedness using cosine similarity in word representations (Mikolov et al., 2013a; Pennington et al., 2014).", 
        "8": "A limitation of such an approach is the inherent \u201csample noise\u201d in specific word representations (exacerbated in the case of rare words).", 
        "9": "Moreover, limitation to local comparison enforces modeling morphological relations via semantic relatedness, although it has been shown that difference vectors model morphological relations more accurately (Mikolov et al., 2013b).", 
        "10": "To address this issue, we introduce a new framework (MORSE), the first to bring semantics into morpheme segmentation both on a local and a vocabulary-wide level.", 
        "11": "That is, when checking for the morphological relation between two words, we not only check for the semantic relatedness of the pair at hand (local), but also check if the difference vectors of pairs showing similar orthographic change are consistent (vocabulary-wide).", 
        "12": "In summary, MORSE clusters pairs of words which only vary by an affix; for example, pairs such as (\u201cquick\u201d, \u201cquickly\u201d) and (\u201chopeful\u201d, \u201chopefully\u201d) get clustered together.", 
        "13": "To verify the cluster of a specific affix from a semantic corpuswide standpoint, we check for the consistency of the difference vectors (Mikolov et al., 2013b).", 
        "14": "To evaluate it from an orthographic corpus-wide perspective, we check for the size of each cluster of an affix.", 
        "15": "To evaluate each pair in a cluster locally from a semantic standpoint, we check if a pair of words in a valid affix cluster are morphologically related by checking if its difference vector is consistent with other members in the cluster and if the words in the pair are semantically related (i.e.", 
        "16": "close in the vector space).", 
        "17": "The reason for local\n552\nevaluations is exemplified by (\u201con\u201d,\u201conly\u201d) which belongs to the cluster of a valid affix (\u201cly\u201d), although they are not (obviously) morphologically related.", 
        "18": "We would expect such a pair to fail the last two local evaluation methods.", 
        "19": "Our proposed segmentation algorithm is evaluated using benchmarking datasets from the Morpho Challenge (MC) for multiple languages and a newly introduced dataset for English which compensates for lack of discriminating capabilities in the MC dataset.", 
        "20": "Experiments reveal that our proposed framework not only outperforms the widely used approach, but also performs better than published state-of-the-art results.", 
        "21": "The central contribution of this work is a novel framework that performs morpheme segmentation resulting in new state-of-the-art results.", 
        "22": "To the best of our knowledge this is the first unsupervised approach to consider the vocabulary-wide semantic knowledge of words and their affixes in addition to relying on their surface forms.", 
        "23": "Moreover we point out the deficiencies in the MC datasets with respect to the compositionality of morphemes and introduce our own dataset free of these deficiencies.", 
        "24": "2 Related Work  Extensive work has been done in morphology learning, with tasks such as morphological analysis (Baayen et al., 1993), morphological reinflection (Cotterell et al., 2016), and morpheme segmentation.", 
        "25": "Given the less complex nature of morpheme segmentation in comparison to the other tasks, most systems developed for morpheme segmentation have been unsupervised or minimally supervised (mostly for parameter tuning).", 
        "26": "Unsupervised morpheme segmentation traces back to (Harris, 1970), which falls under the framework of Letter Successor Variety (LSV) which builds on the hypothesis that predictability of successor letters is high within morphemes and low otherwise.", 
        "27": "The most dominant pieces of work on unsupervised morpheme segmentation, Morfessor (Creutz and Lagus, 2002, 2005, 2007) and Linguistica (Goldsmith, 2000) adopt the Minimum Description Length (MDL) principle (Rissanen, 1998): they aim to minimize describing the lexicon of morphs as well as minimizing the description of an input corpus.", 
        "28": "Morfessor has a widely used API and has inspired a large body of following work (Kohonen et al., 2010; Gro\u0308nroos\net al., 2014).", 
        "29": "The unsupervised original implementation was later adapted (Kohonen et al., 2010; Gro\u0308nroos et al., 2014) to allow for minimal supervision.", 
        "30": "Another work on minimally supervised morpheme segmentation is (Sirts and Goldwater, 2013) which relies on Adaptor Grammars (AGs) (Johnson et al., 2006).", 
        "31": "AGs learn latent tree structures over an input corpus using a nonparametric Bayesian model (Sirts and Goldwater, 2013).", 
        "32": "(Lafferty et al., 2001) use Conditional Random Fields (CRF) for morpheme segmentation.", 
        "33": "In this supervised method, the morpheme segmentation task is modeled as a sequence-to-sequence learning problem, whereby the sequence of labels defines the boundaries of morphemes (Ruokolainen et al., 2013, 2014).", 
        "34": "In contrast to the previously mentioned generative approaches of MDL and AG, this method takes a discriminative approach and allows for the inclusion of a larger set of features.", 
        "35": "In this approach, CRF learns a conditional probability of a segmentation given a word (Ruokolainen et al., 2013, 2014).", 
        "36": "All these morpheme segmenters rely solely on orthographic features of morphemes.", 
        "37": "Semantics were initially introduced to morpheme segmenters by (Schone and Jurafsky, 2000), using LSA to generate word representations and then evaluate if two words are morphologically related based on semantic relatedness, as well as deterministic orthographic methods.", 
        "38": "Similarly, (Baroni et al., 2002) use edit distance and mutual information as metrics for semantic and orthographic validity of a morphological relation between two words.", 
        "39": "Recent work in (Narasimhan et al., 2015), inspired by the log-linear model in (Poon et al., 2009) incorporates semantic relatedness into the model via word representations.", 
        "40": "Other systems such as (U\u0308stu\u0308n and Can, 2016) rely solely on evaluating two words from a semantic standpoint by the use of a twolayer neural network.", 
        "41": "MORSE introduces semantic information into its morpheme segmenters via distributed word representations while also relying on orthographic features.", 
        "42": "Inspired by the work of (Soricut and Och, 2015), instead of merely evaluating semantic relatedness, we are the first to evaluate the morphological relationship via the difference vector of morphologically related words.", 
        "43": "Comparing the difference vectors of multiple pairs across the corpus following the same morphological relation, gives\nMORSE a vocabulary-wide evaluation of morphological relations learned.", 
        "44": "3 System  The key limitation of previous frameworks that rely solely on orthographic features is the resulting over-segmentation.", 
        "45": "As an example, MDLbased frameworks segment \u201csing\u201d to \u201cs-ing\u201d due to the high frequency of the morphemes: \u201cs\u201d and \u201cing\u201d.", 
        "46": "Our framework combines semantic relatedness with orthographic relatedness to eliminate such error.", 
        "47": "For the example mentioned, MORSE validates morphemes such as \u201cs\u201d and \u201cing\u201d from an orthographic perspective, yet invalidates the relation between \u201cs\u201d and \u201csing\u201d from a local and vocabulary-wide semantic perspective.", 
        "48": "Hence, MORSE will segment \u201cjumping\u201d as \u201cjump-ing\u201d, and perform no segmentations on \u201csing\u201d.", 
        "49": "To bring in semantic understanding into MORSE, we rely on word representations (Mikolov et al., 2013a; Pennington et al., 2014).", 
        "50": "These word representations capture the semantics of the vocabulary through statistics over the context in which they appear.", 
        "51": "Moreover, morphosyntactic regularities have been shown over these word representations, whereby pairs of words sharing the same relationship exhibit equivalent difference vectors (Mikolov et al., 2013b).", 
        "52": "For example, it is expected in the vector space of word representations that ~wjumping \u00b4 ~wjump \u00ab ~wplaying \u00b4 ~wplay, but ~wsing \u00b4 ~ws ff ~wplaying \u00b4 ~wplay.", 
        "53": "As a high level description, we first learn all possible affix transformations (morphological rules) in the language from pairs of words from an orthographic standpoint.", 
        "54": "For example, the pair (\u201cjump\u201d, \u201cjumping\u201d) corresponds to the valid affix transformation \u03c6 suffix\u00dd\u00dd\u00dd\u00d1 \u201cing\u201d (where \u03c6 represents the empty string), and the pair (\u201cslow\u201d, \u201cslogan\u201d) corresponds to the invalid rule \u201cw\u201d suffix\u00dd\u00dd\u00dd\u00d1 \u201cgan\u201d.", 
        "55": "Then we invalidate the rules, such as \u201cw\u201d suffix\u00dd\u00dd\u00dd\u00d1 \u201cgan\u201d, that do not conform to the linear relation in the vector space.", 
        "56": "We also invalidate pairs of words which, due to randomness, are orthographically related via a valid rule although they are not morphologically related, such as (\u201con\u201d, \u201conly\u201d).", 
        "57": "Now we formalize the objects we learn in MORSE and the scores (orthographic and semantic) used for validation.", 
        "58": "This constitutes the training stage.", 
        "59": "Finally, we formalize the inference stage, where we use these objects and scores to perform morpheme segmentation.", 
        "60": "3.1 Training Stage  Objects:\n\u2022 Rule set R made of all possible affix transformations in a language.", 
        "61": "R is populated via the following definition: Rsuffix = {aff1 suffix\u00dd\u00dd\u00dd\u00d1 aff2: D (w1, w2) P V2, stem(w1) = stem(w2), w1 = stem(w1) + aff1, w2 = stem(w2) + aff2}, Rprefix is defined similarly for prefixes, and R = Rsuffix Y Rprefix.", 
        "62": "An example R would be equal to {\u03c6 suffix\u00dd\u00dd\u00dd\u00d1 \u201cly\u201d, \u03c6 prefix\u00dd\u00dd\u00dd\u00d1 \u201cun\u201d, \u201cing\u201d suffix\u00dd\u00dd\u00dd\u00d1 \u201ced\u201d,.", 
        "63": ".", 
        "64": ".", 
        "65": "}.", 
        "66": "\u2022 Support set SSr for a rule r P R consists of all pairs of words related via r on a surface level.", 
        "67": "SSr is populated via the following definition: SSr = {(w1, w2): w1, w2 P V, w1\nr\u00dd\u00d1 w2}.", 
        "68": "An example support set of the rule \u201cing\u201d suffix\u00dd\u00dd\u00dd\u00d1 \u201ced\u201d would be {(\u201cplaying\u201d, \u201cplayed\u201d), (\u201ccrafting\u201d, \u201ccrafted\u201d),.", 
        "69": ".", 
        "70": ".}.", 
        "71": "Scores:\n\u2022 scorer orth(r) is a vocabulary-wide orthographic confidence score for rule r P R. It reflects the validity of an affix transformation in a language from an orthographic perspective.", 
        "72": "This score is evaluated as scorer orth(r) = |SSr|.", 
        "73": "\u2022 scorer sem(r) is a vocabulary-wide seman-\ntic confidence score for rule r P R. It reflects the validity of an affix transformation in a language from a semantic perspective.", 
        "74": "This score is evaluated as: scorer sem(r) = |clusterr|/|SSr|2 where clusterr = {((w1, w2), (w3, w4)): (w1, w2), (w3, w4) P SSr, ~w1 \u00b4 ~w2 \u00ab ~w3 \u00b4 ~w4 }.", 
        "75": "We consider ~w1 \u00b4 ~w2 \u00ab ~w3 \u00b4 ~w4 if cos(~w4, ~w2 \u00b4 ~w1 ` ~w3) \u0105 0.1.", 
        "76": "\u2022 scorew sem((w1, w2) P SSr) is a vocabularywide semantic confidence score for a pair of words (w1, w2).", 
        "77": "The pair of words is related via r on an orthographic level, but the score reflects the validity of the morphological relation via r on a semantic level.", 
        "78": "This score is evaluated as: scorew sem((w1, w2) P SSr) = |{(w3, w4): (w3, w4) P SSr, ~w1 \u00b4 ~w2 \u00ab ~w3\u00b4 ~w4}|/|SSr|.", 
        "79": "In other words, it is the fraction of pairs of words in the support set that exhibit a similar linear relation as (w1, w2) in the vector space.", 
        "80": "\u2022 scoreloc sem((w1, w2) P SSr) is a local semantic confidence score for a pair of words (w1, w2).", 
        "81": "The pair of words is related via r on an orthographic level, but the score reflects the semantic relatedness between the pair.", 
        "82": "The score is evaluated as: scoreloc sem((w1, w2) P SSr) = cos(~w1, ~w2).", 
        "83": "3.2 Inference Stage  In this stage we perform morpheme segmentation using the knowledge gained from the first stage.", 
        "84": "We begin with some notation: let Radd = {r : r P R, r = aff1\nr\u00dd\u00d1 aff2, aff1 = \u03c6, aff2 \u2030 \u03c6 }, Rrep = {r : r P R, r = aff1 r\u00dd\u00d1 aff2, aff1 \u2030 \u03c6, aff2 \u2030 \u03c6 }.", 
        "85": "In other words, we divide the rules to those where an affix is added (Radd) and to those where an affix is replaced (Rrep).", 
        "86": "Given a word w to segment, we search for r\u02da, the solution to the following optimization problem1.", 
        "87": "The search space is limited to the rules that include w in their support set, a fairly small search space and the corresponding computation readily tractable:\nmax r\n\u00ff\nt1\nscoret1ppw1, wq P SSrq ` \u00ff\nt2\nscoret2prq\ns. t. r P Radd scorer semprq \u0105 tr sem scorer orthprq \u0105 tr orth scorew semppw1, wq P SSrq \u0105 tw sem scoreloc semppw1, wq P SSrq \u0105 tloc sem\nWhere t1 = {w sem, loc sem}, t2 = {r sem, r orth}, and tr sem, tr orth, tw sem, tloc sem are hyperparameters of the system.", 
        "88": "Now given r\u02da = \u03c6 suffix\u00dd\u00dd\u00dd\u00d1 suf, w1 is defined as w1 r\n\u02da\u00dd\u00d1 w. Thus the algorithm segments w \u00d1 w1-suf.", 
        "89": "We treat prefixes similarly.", 
        "90": "Next, the algorithm iterates over w1.", 
        "91": "Figure 1 shows the segmentation process of the word \u201cunhealthy\u201d based on the sequentially retrieved r\u02da.", 
        "92": "The reason we restrict our rule set to Radd in the optimization problem is to avoid rules such as \u201cer\u201d suffix\u00dd\u00dd\u00dd\u00d1 \u201cing\u201d like in (\u201cplayer\u201d, \u201cplaying\u201d) leading to false segmentations such as \u201cplaying\u201d \u00d1 \u201cplayering\u201d.", 
        "93": "Yet we cannot completely restrict our search to Radd due to rules such as \u201cy\u201d \u00d1 \u201cies\u201d in words like (\u201csky\u201d, \u201cskies\u201d).", 
        "94": "To be able to segment words such as \u201cskies\u201d, we\u2019d have to consider rules in Rrep\n1r and w uniquely identify w1, and thus the search space is defined only over r.\nbut only after searching in Radd.", 
        "95": "Thus if the first optimization problem was unfeasible, we repeat it while replacing Radd with Rrep.", 
        "96": "The program terminates when both optimization problems are infeasible.", 
        "97": "4 Experiments  We conduct a variety of experiments to assess the performance of MORSE, and compare it with prior works.", 
        "98": "First, the performance is assessed intrinsically on the task of morpheme segmentation and against the most widely used morpheme segmenter: Morfessor 2.0.", 
        "99": "We evaluate the performance across three languages of varying morphology levels: English, Turkish, Finnish, with Finnish being the richest in morphology and English being the poorest.", 
        "100": "Second, we show the inadequacies of benchmarking gold datasets for this task and describe a new dataset that we create to address the inadequacy.", 
        "101": "Third, in order to highlight the effect of including semantic information, we compare MORSE against Morfessor on a set of words which should not be segmented from a semantic perspective although orthographically they seem to be segmentable (such as \u201cfreshman\u201d).", 
        "102": "In all of our experiments (unless specified otherwise), we report precision and recall (and corresponding F1 scores) with locations of morpheme boundaries being considered positives and the rest of the locations considered negatives.", 
        "103": "It should be noted that we disregard starting and ending positions of words, since they form trivial boundaries (Virpioja et al., 2011).", 
        "104": "4.1 Setup  Both systems, Morfessor and MORSE, were trained on the same monolingual corpus: Wikipedia2 (as of September 20, 2016) to control for affecting factors within the experiment.", 
        "105": "For each language considered, the respective Wikipedia dump was preprocessed using an available code3.", 
        "106": "We use Word2Vec (Mikolov\n2https://dumps.wikimedia.org 3https://github.com/bwbaugh/\nwikipedia-extractor\net al., 2013a) to train word representations of 300 dimensions and based on a context window of size 5.", 
        "107": "Also, for computational efficiency, MORSE was limited to a vocabulary of size 1M, a restriction not enforced on Morfessor.", 
        "108": "MORSE\u2019s hyperparameters are tuned based on a tuning set of gold morpheme segmentations.", 
        "109": "We have publicly released the source code of a pretrained MORSE4 as described in this paper.", 
        "110": "4.2 Morpho Challenge Dataset  As our first intrinsic experiment, we consider the Morpho Challenge (MC) gold segmentations available online5.", 
        "111": "For every language, two datasets are supplied: training and development.", 
        "112": "For the purpose of our experiments, all systems use the development dataset as a test dataset, and the training dataset is used for tuning MORSE\u2019s hyperparameters.", 
        "113": "MC dataset sizes are reported in Table 1.", 
        "114": "4.3 Semantically Driven Dataset  There are a variety of weaknesses in the MC dataset, specifically related to whether the segmentation is semantically appropriate or not.", 
        "115": "We introduce a new semantically driven dataset (SD17) for morpheme segmentation along with the methodology used for creation; this new dataset is publicly available in the canonical6 and non-canonical7 versions (Cotterell and Vieira, 2016).", 
        "116": "Non-compositional segmentation: One of the key requirements of morpheme segmentation is the compositionality of the meaning of the word from the meaning of its morphemes.", 
        "117": "This requirement is violated on multiple occasions in the MC dataset.", 
        "118": "One example from Table 2 is segmenting the word \u201cbusiness\u201d into \u201cbusi-ness\u201d, which falsely assumes that \u201cbusiness\u201d means the act of being busy.", 
        "119": "Such a segmentation might be consistent with the historic origin of the word, but with\n4https://goo.gl/w4r7vP 5http://research.ics.aalto.fi/events/\nmorphochallenge2010 6https://goo.gl/MgKfG1 7https://goo.gl/0vTXVt\nradical semantic changes over time, the segmentation no longer semantically represents the compositionality of the words\u2019 components (Wijaya and Yeniterzi, 2011).", 
        "120": "Not only does such a weakness contribute to false segmentations, but it also favors segmentation methods following the MDL principle.", 
        "121": "Trivial instances: The second weakness in the MC dataset is due to abundance of trivial instances.", 
        "122": "These instances lack discriminating capability since all methods can easily predict them (Baker, 2001).", 
        "123": "These instances are comprised of genetive cases (such as teacher\u2019s) as well as hyphenated words (such as turning-point).", 
        "124": "For genetive cases, segmenting at the apostrophe leads to perfect precision and recall, and thus such instances are deemed trivial.", 
        "125": "In the case of hyphenated words, segmenting at the hyphen is a correct segmentation with a very high probability.", 
        "126": "In the MC tuning dataset, in 43 times out of 46, the hyphen was a correct indication of segmentation.", 
        "127": "Other issues exist in the Morpho Challenge dataset although less abundantly.", 
        "128": "There are instances of wrong segmentations possibly due to human error.", 
        "129": "One example of such instance is \u201cturning-point\u201d segmented to \u201cturning - point\u201d instead of \u201cturn ing - point\u201d.", 
        "130": "Another issue, which is hard to avoid, is ambiguity of segmentation boundaries.", 
        "131": "Take for example the word \u201cstrafed\u201d, the segmentations \u201cstraf-ed\u201d and \u201cstrafe-d\u201d are equally justified.", 
        "132": "In such situations, the MC dataset favors complete affixes rather than complete lemmas.", 
        "133": "This also favors MDL-based segmenters.", 
        "134": "We note that the MC dataset also provides segmentations in a canonical version such as \u201cstrafe-ed\u201d, yet for the sake of a fair comparison with Morfessor and all previously evaluated systems on the MC dataset, we consider only the former version of segmentations.", 
        "135": "Due to these reasons, we create a new dataset SD17 for English gold morpheme segmentations with compositionality guiding the annotations.", 
        "136": "We select 2000 words randomly from the 10K most frequent words in the English Wikipedia dump and have them annotated by two proficient English speakers.", 
        "137": "The segmentation criterion was to segment the word to the largest extent possible while preserving its compositionality from the segments.", 
        "138": "The inter-annotator agreement reached 91% on a word level.", 
        "139": "Based on post annotation discussions, annotators agreed on 99% of the words, and words not agreed on were eliminated along with words containing non-alpha characters to avoid trivial instances.", 
        "140": "SD17 is used to evaluate the performance of both Morfessor and MORSE.", 
        "141": "We claim that the performance on SD17 is a better indication of the performance of a morpheme segmenter.", 
        "142": "By the use of SD17 we expect to gain insights on the extent to which morpheme segmentation is a function of semantics in addition to orthography.", 
        "143": "4.4 Handling Compositionality  We have hypothesized that following the MDL principle (such as Morfessor) leads to oversegmentation.", 
        "144": "This over-segmentation happens specifically when the meaning of the word does not follow from the meaning of its morphemes.", 
        "145": "Examples include words such as \u201cred head\u201d, \u201cduck face\u201d, \u201chow ever\u201d, \u201cs ing\u201d.", 
        "146": "A subset of these words are defined by linguists as exocentric compounds (Bauer, 2008).", 
        "147": "MORSE does not suffer from this issue owing to its use of a semantic model.", 
        "148": "We use a collection of 100 English words which appear to be segmentable but actually are not (example: \u201chowever\u201d).", 
        "149": "Such a collection will highlight a system\u2019s capability of distinguishing frequent letter sequences from the semantic contribution of this letter sequence in a word.", 
        "150": "We make this collection publicly available8.", 
        "151": "8https://goo.gl/EFbacj  5 Results  We compare MORSE with Morfessor, and place the performance alongside the state-of-the-art published results.", 
        "152": "5.1 Morpho Challenge Dataset  As demonstrated in Table 3, MORSE performs better than Mofessor on English and Turkish, and worse on Finnish.", 
        "153": "Considering English first, using MORSE instead of Morfessor, resulted in a 6% absolute increase in F1 scores.", 
        "154": "This supports our claim for the need of semantic cues in morpheme segmentation, and also validates the method used in this paper.", 
        "155": "Since English is a less systematic language in terms of the orthographic structure of words, semantic cues are of greater need, and hence a system which relies on semantic cues is expected to perform better; indeed this is the case.", 
        "156": "Similarly, MORSE performs better on Turkish with a 7% absolute margin in terms of F1 score.", 
        "157": "On the other hand, Morfessor surpasses MORSE in performance on Finnish by a large margin as well, especially in terms of recall.", 
        "158": "5.1.1 Discussion  We hypothesize that the richness of morphology in Finnish led to suboptimal performance of MORSE.", 
        "159": "This is because richness in morphology leads to word level sparsity which directly leads to: (1) Degradation of quality of word representations (2) Increased vocabulary size exacerbating the issue of limited vocabulary (recall MORSE was limited to a vocabulary of 1M).", 
        "160": "In a language with productive morphology, limiting its vocabulary results in a lower chance of finding morphologically related word pairs.", 
        "161": "This negatively im-\npacts the training stage of MORSE which relies on the availability of such pairs.", 
        "162": "In order to detect the suffix \u201cly\u201d from the word \u201ccheerfully\u201d MORSE needs to come across \u201ccheerful\u201d as well.", 
        "163": "Coming across \u201ccheerful\u201d is now a lower probability event due to high sparsity.", 
        "164": "This is not as much of an issue for Morfessor under the MDL principle, since it might detect \u201cly\u201d just by coming across multiple words ending with \u201cly\u201d even without encountering the base forms of those words.", 
        "165": "We show how the detection of rules is affected by considering the number of candidate rules detected as well as the number of candidate morphologically related word pairs detected.", 
        "166": "As shown in Table 4, the number of detected candidate rules and candidate related words decreases with the increase in morphology in a language.", 
        "167": "This confirms our hypothesis; we note that this issue can be directly attributed to the limited vocabulary size in MORSE.", 
        "168": "With the increase in processing power, and thus larger vocabulary coverage, MORSE is expected to perform better.", 
        "169": "5.2 Semantically Driven Dataset  The performance of MORSE and Morfessor on SD17 is shown in Table 5.", 
        "170": "The use of MC data (which does not adhere to the compositionality principle) to tune MORSE to be evaluated on SD17 (which does adhere to the compositionality principle) is not optimal.", 
        "171": "Thus, we evaluate MORSE on SD17 using 5-fold cross validation, where 80% of the dataset is used to tune and 20% is used to evaluate.", 
        "172": "Precision, Recall, and F1 scores are averaged and reported in Table 5 using the label MORSE-CV.", 
        "173": "Based on the results in Table 5, we make the following observations.", 
        "174": "Comparing MORSE-CV to MORSE reflects the fundamental difference between SD17 and MC datasets.", 
        "175": "Knowing the basis of construction of SD17 and the fundamental weaknesses in MC datasets, we attribute the performance increase to the lack of compositionality in MC dataset.", 
        "176": "Comparing MORSE-CV to Morfessor, we observe a significant jump in performance (an increase of 24%).", 
        "177": "In comparison, the increase on the MC dataset (6%) shows that the Morpho Challenge dataset underestimates the performance gap between Morfessor and MORSE due its inherent weaknesses.", 
        "178": "Since MORSE is equipped with the capability to retrieve full morphemes even when not present\nin full orthographically, a capability that Morfessor lacks, we evaluated both systems on the canonical version of SD17.", 
        "179": "The results are reported in Table 6.", 
        "180": "We notice that evaluating on the canonical form of SD17 gives a further edge for MORSE over Morfessor.", 
        "181": "For evaluation on the canonical version of SD17, we switch to morpheme-level evaluation instead of boundary-level as a more suitable method for Morfessor.", 
        "182": "Morpheme-level evaluation is distinguished from boundary-level evaluation in that we evaluate the detection of morphemes instead of the boundary locations in the segmented word.", 
        "183": "We next compare MORSE against published state-of-the-art results9.", 
        "184": "As one can see in Table 7 MORSE significantly performs better than published state-of-the-art results, most notably (Narasimhan et al., 2015) referred to as LLSM in the Table.", 
        "185": "Comparison is also made against the top results in the latest Morpho Challenge: Morfessor S+W and Morfessor S+W+L (Kohonen et al., 2010), and Base Inference (Lignos, 2010).", 
        "186": "5.3 Handling Compositionality  We compare the performance of MORSE and Morfessor on a set of words made up of morphemes which don\u2019t compose the meaning of the word.", 
        "187": "Since all the boundaries in this dataset are negative, to evaluate both systems (with MORSE tuned on SD17), we only report the number of segments generated.", 
        "188": "The more segments a system generates, the worse is its performance.", 
        "189": "We find that MORSE generates 7 false morphemes whereas Morfessor generates 43 false morphemes.", 
        "190": "This shows MORSE\u2019s robustness to such examples through its semantic knowledge and validates our claim that Morfessor oversegments on such examples.", 
        "191": "6 Discussion  One of the benefits of MORSE against other frameworks such as MDL is its ability to identify the lemma within the segmentation.", 
        "192": "The lemma would be the last non-segmented word in the iterative process of segmentation.", 
        "193": "Hence, an advantage of our framework is its easy adaptability into a lemmatizer and even a stemmer.", 
        "194": "Another key aspect which is not present in some of the competitive systems is the need for a small tuning dataset.", 
        "195": "This is a point in favor of completely unsupervised systems such as Morfessor.", 
        "196": "On the other hand, these hyperparameters could allow for flexibility.", 
        "197": "Figure 2 shows how precision and recall changes as a function of the hyperparameter selection10.", 
        "198": "As one would expect, increasing the hyperparameters, in general, leads\n9The five published state-of-the-art results are on different datasets\n10Only a subset of the hyperparameters is used for display purposes\nto a stricter search space and thus increases precision and decreases recall.", 
        "199": "Putting these results in perspective, the user of MORSE is given the capability of controlling for precision and recall based on the needs of the downstream task.", 
        "200": "Moreover, to check for the level of dependency of MORSE on a set of gold morpheme segmentations for tuning, we check for the variation in performance with respect to size of tuning data.", 
        "201": "For the purpose of this experiment we take an 80- 20 split of SD17 and vary the size of the tuning set.", 
        "202": "We notice that the performance (81.90% F1) reaches a steady state at 20% (\u00ab 300 gold segmentations) of the tuning data.", 
        "203": "This reflects the minimal dependency on a tuning dataset.", 
        "204": "Regarding the training stage, homomorphs are treated as one rule and allomorphs are treated as separate rules.", 
        "205": "For example, (\u201ctall\u201d, \u201ctaller\u201d) and (\u201cfast\u201d, \u201cfaster\u201d) are wrongly considered to have the same morphological relation, besides (\u201ccat\u201d, \u201ccats\u201d) and (\u201cbutterfly\u201d, \u201cbutterflies\u201d) are wrongly considered to have different morphological relations.", 
        "206": "The separate clustering of the different forms of a homomorph leads to the underestimation of the respective orthographic scores.", 
        "207": "Moreover, the clustering of allomorphs together would lead to the underestimation of the semantic score of the rule as well as the underestimation of the vocabulary-wide semantic score of word pairs in the support set of this rule.", 
        "208": "This does not significantly affect the performance of MORSE, since the tuned thresholds are able to distinguish between the low scores of an invalid rule and the mediocre underestimated scores of allomorphs and homomorphs.", 
        "209": "As for the inference stage of MORSE, the greedy inference approach limits its performance.", 
        "210": "In other words, a wrong segmentation at the be-\nginning will propagate and result in consequent wrong segmentations.", 
        "211": "Also, MORSE\u2019s limitation to concatenative morphology decreases its efficacy on languages that include non-concatenative morphology.", 
        "212": "This opens the stage for further research on a more optimal inference stage and a more global modeling of orthographic morphological transformations.", 
        "213": "7 Conclusions and Future Work  In this paper, we have presented MORSE, a first morpheme segmenter to consider semantic structure at this scale (local and vocabulary-wide).", 
        "214": "We show its superiority over state-of-the-art algorithms using intrinsic evaluation on a variety of languages.", 
        "215": "We also pinpointed the weaknesses in current benchmarking datasets, and presented a new dataset free of these weaknesses.", 
        "216": "With a relative increase in performance reaching 24% absolute increase over Morfessor, this work proves the significance of semantic cues as well as validates a new state-of-the-art morpheme segmenter.", 
        "217": "For future work, we plan to address the limitations of MORSE: minimal supervision, greedy inference, and concatenative orthographic model.", 
        "218": "Moreover, we plan to computationally optimize the training stage for the sake of wider adoption by the community.", 
        "219": "Acknowledgements  This work is supported in part by IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizons Network."
    }, 
    "document_id": "P17-1051.pdf.json"
}
