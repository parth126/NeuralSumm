{
    "abstract_sentences": {
        "1": "In this paper, we study how to improve the domain adaptability of a deletion-based Long Short-Term Memory (LSTM) neural network model for sentence compression.", 
        "2": "We hypothesize that syntactic information helps in making such models more robust across domains.", 
        "3": "We propose two major changes to the model: using explicit syntactic features and introducing syntactic constraints through Integer Linear Programming (ILP).", 
        "4": "Our evaluation shows that the proposed model works better than the original model as well as a traditional non-neural-network-based model in a cross-domain setting."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1385\u20131393 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1127  1 Introduction  Sentence compression is the task of compressing long, verbose sentences into short, concise ones.", 
        "2": "It can be used as a component of a text summarization system.", 
        "3": "Figure 1 shows two example input sentences and the compressed sentences written by human.", 
        "4": "The task has been studied for almost two decades.", 
        "5": "Early work on this task mostly relies on syntactic information such as constituency-based parse trees to help decide what to prune from a sentence or how to re-write a sentence (Jing, 2000; Knight and Marcu, 2000).", 
        "6": "Recently, there has been much interest in applying neural network models to solve the problem, where little or no linguistic analysis is performed except for tokenization (Filippova et al., 2015; Rush et al., 2015; Chopra et al., 2016).", 
        "7": "Although neural network-based models have achieved good performance on this task recently, they tend to suffer from two problems: (1) They require a large amount of data for training.", 
        "8": "For example, Filippova et al.", 
        "9": "(2015) used close to two\nmillion sentence pairs to train an LSTM-based sentence compression model.", 
        "10": "Rush et al.", 
        "11": "(2015) used about four million title-article pairs from the Gigaword corpus (Napoles et al., 2012) as training data.", 
        "12": "Although it may be easy to automatically obtain such training data in some domains (e.g., the news domain), for many other domains, it is not possible to obtain such a large amount of training data.", 
        "13": "(2) These neural network models trained on data from one domain may not work well on out-of-domain data.", 
        "14": "For example, when we trained a standard neural sequence-to-sequence model1 on 3.8 million title-article pairs from the Gigaword corpus and applied it to both in-domain data and out-of-domain data, we found that the performance on in-domain data was good but the performance on out-of-domain data could be very\n1http://opennmt.net/\n1385\npoor.", 
        "15": "Two example compressed sentences by this trained model are shown in Figure 1 to illustrate the comparison between in-domain and out-ofdomain performance.", 
        "16": "The two limitations above imply that these neural network-based models may not be good at learning generalizable patterns, or in other words, they tend to overfit the training data.", 
        "17": "This is not surprising because these models do not explicitly use much syntactic information, which is more general than lexical information.", 
        "18": "In this paper, we aim to study how syntactic information can be incorporated into neural network models for sentence compression to improve their domain adaptability.", 
        "19": "We hope to train a model that performs well on both in-domain and out-ofdomain data.", 
        "20": "To this end, we extend the deletionbased LSTM model for sentence compression by Filippova et al.", 
        "21": "(2015).", 
        "22": "Although deletion-based sentence compression is not as flexible as abstractive sentence compression, we chose to work on deletion-based sentence compression for the following reason.", 
        "23": "Abstractive sentence compression allows new words to be used in a compressed sentence, i.e., words that do not occur in the input sentence.", 
        "24": "Oftentimes these new words serve as paraphrases of some words or phrases in the source sentence.", 
        "25": "But to generate such paraphrases, the model needs to have seen them in the training data.", 
        "26": "Because we are interested in a cross-domain setting, the paraphrases learned in one domain may not work well in another domain if the two domains have very different vocabularies.", 
        "27": "On the other hand, a deletion-based method does not face such a problem in a cross-domain setting.", 
        "28": "Specifically, we propose two major changes to the model by Filippova et al.", 
        "29": "(2015): (1) We explicitly introduce POS embeddings and dependency relation embeddings into the neural network model.", 
        "30": "(2) Inspired by a previous method (Clarke and Lapata, 2008), we formulate the final predictions as an Integer Linear Programming problem to incorporate constraints based on syntactic relations between words and expected lengths of the compressed sentences.", 
        "31": "In addition to the two major changes above, we also use bi-directional LSTM to include contextual information from both directions into the model.", 
        "32": "We evaluate our method using around 10,000 sentence pairs released by Filippova et al.", 
        "33": "(2015) and two other data sets representing out-of-\ndomain data.", 
        "34": "We test both in-domain and outof-domain performance.", 
        "35": "The experimental results showed that our proposed method can achieve competitive performance compared with the original method in the single-domain setting but with much less training data (around 8,000 sentence pairs for training instead of close to two million sentence pairs).", 
        "36": "In the cross-domain setting, our proposed method can clearly outperform the original method.", 
        "37": "We also compare our method with a traditional ILP-based method using syntactic structures of sentences but not based on neural networks (Clarke and Lapata, 2008).", 
        "38": "We find that our method can outperform this baseline for both in-domain and out-of-domain data.", 
        "39": "2 Method  In this section, we present our sentence compression method that is aimed at working in a crossdomain setting.", 
        "40": "2.1 Problem Definition  Recall that we focus on deletion-based sentence compression.", 
        "41": "Our problem setup is the same as that by Filippova et al.", 
        "42": "(2015).", 
        "43": "Let us use s = (w1, w2, .", 
        "44": ".", 
        "45": ".", 
        "46": ", wn) to denote an input sentence, which consists of a sequence of words.", 
        "47": "Here wi \u2208 V , where V is the vocabulary.", 
        "48": "We would like to delete some of the words in s to obtain a compressed sentence that still contains the most important information in s. To represent such a compressed sentence, we can use a sequence of binary labels y = (y1, y2, .", 
        "49": ".", 
        "50": ".", 
        "51": ", yn), where yi \u2208 {0, 1}.", 
        "52": "Here yi = 0 indicates that wi is deleted, and yi = 1 indicates that wi is retained.", 
        "53": "We assume that we have a set of training sentences and their corresponding deletion/retention labels, denoted as D = {(sj ,yj)}Nj=1.", 
        "54": "Our goal is to learn a sequence labeling model from D so that for any unseen sentence s we can predict its label sequence y and thus compress the sentence.", 
        "55": "2.2 Our Base Model  We first introduce our base model, which uses LSTM to perform sequence labeling.", 
        "56": "This base model is largely based on the model by Filippova et al.", 
        "57": "(2015) with some differences, which will be explained below.", 
        "58": "We assume that each word in the vocabulary has a d-dimensional embedding vector.", 
        "59": "For input sentence s, let us use (w1,w2, .", 
        "60": ".", 
        "61": ".", 
        "62": ",wn) to denote the\nsequence of the word embedding vectors, where wi \u2208 Rd.", 
        "63": "We use a standard bi-directional LSTM model to process these embedding vectors sequentially from both directions to obtain a sequence of hidden vectors (h1,h2, .", 
        "64": ".", 
        "65": ".", 
        "66": ",hn), where hi \u2208 Rh.", 
        "67": "We omit the details of the bi-LSTM and refer the interested readers to the work by Graves et al.", 
        "68": "(2013) for further explanation.", 
        "69": "Following Filippova et al.", 
        "70": "(2015), our bi-LSTM has three layers, as shown in Figure 2.", 
        "71": "We then use the hidden vectors to predict the label sequence.", 
        "72": "Specifically, label yi is predicted from hi as follows:\np(yi | hi) = softmax(Whi + b), (1)\nwhere W \u2208 R2\u00d7h and b \u2208 R2 are a weight matrix and a weight vector to be learned.", 
        "73": "There are some differences between our base model and the LSTM model by Filippova et al.", 
        "74": "(2015).", 
        "75": "(1) Filippova et al.", 
        "76": "(2015) first encoded the input sentence in its reverse order using the same LSTM before processing the sentence for sequence labeling.", 
        "77": "(2) Filippova et al.", 
        "78": "(2015) used only a single-directional LSTM while we use bi-LSTM to capture contextual information from both directions.", 
        "79": "(3) Although Filippova et al.", 
        "80": "(2015) did not use any syntactic information in their basic model, they introduced some features based on dependency parse trees in their advanced models.", 
        "81": "Here we follow their basic model because later we will introduce more explicit syntaxbased features.", 
        "82": "(4) Filippova et al.", 
        "83": "(2015) com-\nbined the predicted yi\u22121 with wi to help predict yi.", 
        "84": "This adds some dependency between consecutive labels.", 
        "85": "We do not do this because later we will introduce an ILP layer to introduce dependencies among labels.", 
        "86": "2.3 Incorporation of Syntactic Features  Note that in the base model that we presented above, there is no explicit use of any syntactic information such as the POS tags of the words or the parse tree structures of the sentences.", 
        "87": "Because we believe that syntactic information is important for learning a generalizable model for sentence compression, we would like to introduce syntactic features into our model.", 
        "88": "First, we perform part-of-speech tagging on the input sentences.", 
        "89": "For sentence s, let us use (t1, t2, .", 
        "90": ".", 
        "91": ".", 
        "92": ", tn) to denote the POS tags of the words inside, where ti \u2208 T and T is a POS tag set.", 
        "93": "We further assume that each t \u2208 T has an embedding vector (to be learned).", 
        "94": "Let us use (t1, t2, .", 
        "95": ".", 
        "96": ".", 
        "97": ", tn) (ti \u2208 Rp, p < |T |) to denote the sequence of POS embedding vectors of this sentence.", 
        "98": "We can then simply concatenate wi with ti as a new vector to be processed by the bi-LSTM model.", 
        "99": "Next, we perform dependency parsing on the input sentences.", 
        "100": "For each word wi in sentence s, let ri \u2208 R denote the dependency relation between wi and its parent word in the sentence, whereR is the set of all dependency relation types.", 
        "101": "We then assume that each r \u2208 R has an embedding vector (to be learned).", 
        "102": "Let (r1, r2, .", 
        "103": ".", 
        "104": ".", 
        "105": ", rn) (r \u2208 Rq, q < |R|) denote corresponding dependency embedding vectors of this sentence.", 
        "106": "We can also concatenate wi with ri and feed the new vector to the bi-LSTM model.", 
        "107": "In our model, we combine the word embedding, POS embedding and dependency embedding into a single vector to be processed by the bi-LSTM model:\nxi = wi \u2295 ti \u2295 ri, \u2212\u2192 h i = LSTM\u2212\u2192\u0398 ( \u2212\u2192 h i\u22121,xi), \u2190\u2212 h i = LSTM\u2190\u2212\u0398 ( \u2190\u2212 h i+1,xi),\nhi = \u2212\u2192 h i \u2295 \u2190\u2212 h i,\nwhere \u2295 represents concatenation of vectors, and\u2212\u2192 \u0398 and \u2190\u2212 \u0398 are parameters of the bi-LSTM model.", 
        "108": "The complete model is shown in Figure 2.", 
        "109": "2.4 Global Inference through ILP  Although the method above has explicitly incorporated some syntactic information into the biLSTM model, the syntactic information is used in a soft manner through the learned model weights.", 
        "110": "We hypothesize that there are also hard constraints we can impose on the compressed sentences.", 
        "111": "For example, the method above as well as the original method by Filippova et al.", 
        "112": "(2015) cannot impose any length constraint on the compressed sentences.", 
        "113": "This is because the labels y1, y2, .", 
        "114": ".", 
        "115": ".", 
        "116": ", yn are not jointly predicted.", 
        "117": "We propose to use Integer Linear Programming (ILP) to find an optimal combination of the labels y1, y2, .", 
        "118": ".", 
        "119": ".", 
        "120": ", yn for a sentence, subject to some constraints.", 
        "121": "Specifically, the ILP problem consists of two parts: the objective function, and the constraints.", 
        "122": "The Objective Function Recall that the trained bi-LSTM model above produces a probability distribution for each label yi, as defined in Eqn.", 
        "123": "(1).", 
        "124": "Let us use \u03b1i to denote the probability of yi = 1 as estimated by the bi-LSTM model.", 
        "125": "Intuitively, we would like to set yi to 1 if \u03b1i is large.", 
        "126": "Besides the probability estimated by the biLSTM model, here we also consider the depth of the word wi in the dependency parse tree of the sentence.", 
        "127": "Intuitively, a word closer to the root of the tree is more likely to be retained.", 
        "128": "In order to incorporate this observation, we define dep(wi) to be the depth of the word wi in the dependency parse tree of the sentence.", 
        "129": "The root node of the tree has a depth of 0, an immediate child of the root has a depth of 1, and so on.", 
        "130": "For example, the dependency parse tree of an example sentence together with the depth of each word is shown in Figure 3.", 
        "131": "We can see that some of the words that are deleted according to the ground truth have a relatively larger depth, such as the first \u201cshe\u201d (with a depth of 4) and the word \u201cflexible\u201d (with a depth of 5).", 
        "132": "Based on these considerations, we define the objective function to be the following:\nmax\nn\u2211\ni=1\nyi(\u03b1i \u2212 \u03bb \u00b7 dep(wi)), (2)\nwhere \u03bb is a positive parameter to be manually set, and yi is the same as defined before, which is either 0 or 1 to indicate whether wi is deleted or not.", 
        "133": "Constraints\nWe further introduce some constraints to capture tow considerations.", 
        "134": "The first consideration is related to the syntactic structure of a sentence, and the second consideration is related to the length of the compressed sentence.", 
        "135": "Some of the constraints are inspired by Clarke and Lapata (2008).", 
        "136": "Our constraints are listed below: (1) No missing parent: Generally, we believe that if a word is retained in the compressed sentence, its parent in the dependency parse tree should also be retained.", 
        "137": "(2) No missing child: For some dependency relations such as nsubj, if the parent word is retained, it makes sense to also keep the child word; otherwise the sentence may become ungrammatical.", 
        "138": "(3) Max length: Since we are trying to compress a sentence, we may need to impose a minimum compression rate.", 
        "139": "This could be achieved by setting a maximum value of the sum of yi.", 
        "140": "(4) Min length: We observe that the original model sometimes produces very short compressed sentences.", 
        "141": "We therefore believe that it is also important to maintain a mininum length of the compressed sentence.", 
        "142": "This can be achieved by setting a minimum value of the sum of yi.", 
        "143": "Formally, the constraints are listed as follows:\nn\u2211\ni=1\nyi <= \u03b2n,\nn\u2211\ni=1\nyi >= \u03b3n,\n\u2200yi : yi \u2264 ypi , \u2200ri \u2208 T \u2032 : yi \u2265 ypi ,\nwhere wpi is the parent word of wi in the dependency parse tree, ri is the dependency relation type between wi and wpi , and T \u2032 is a set of dependency relations for which the child word is often retained when the parent word is retained in the compressed sentence.", 
        "144": "The set T \u2032 is derived as follows.", 
        "145": "For each dependency relation type, based on the training data, we compute the conditional probability of the child word being retained given that the parent word is retained.", 
        "146": "If this probability is higher than 90%, we include this dependency relation type in T \u2032.", 
        "147": "3 Experiments    3.1 Datasets and Experiment Settings  Because we are mostly interested in a crossdomain setting where the model is trained on one domain and test on a different domain, we need data from different domains for our evaluation.", 
        "148": "Here we use three datasets.", 
        "149": "Google News: The first dataset contains 10,000 sentence pairs collected and released by Filippova et al.", 
        "150": "(2015)2.", 
        "151": "The sentences were automatically acquired from the web through Google News using a method introduced by Filippova and Altun (2013).", 
        "152": "The news articles were from 2013 and 2014.", 
        "153": "BNC News: The second dataset contains around 1,500 sentence pairs collected by Clarke and Lapata (2008)3.", 
        "154": "The sentences were from British National Corpus (BNC) and the American News Text corpus before 2008.", 
        "155": "Research Papers: The last dataset contains 100 sentences taken from 10 randomly selected papers published at the ACL conference in 2016.", 
        "156": "For Google News and BNC News, we have the ground truth compressed sentences, which are deletion-based compressions, i.e., subsequences of the original sentences.", 
        "157": "For Research Papers, we use it only for manual evaluation in terms of readability and informativeness, as we will explain below.", 
        "158": "We evaluate three settings of our method: BiLSTM: In this setting, we use only the base biLSTM model without incorporating any syntactic feature.", 
        "159": "BiLSTM+SynFeat: In this setting, we combine word embeddings with POS embeddings and de-\n2Available at http://storage.googleapis.", 
        "160": "com/sentencecomp/compression-data.json.", 
        "161": "3Available at http://jamesclarke.net/ research/resources/.", 
        "162": "pendency embeddings as input to the bi-LSTM model and use the predictions of y from the biLSTM model.", 
        "163": "BiLSTM+SynFeat+ILP: In this setting, on top of BiLSTM+SynFeat, we solve the ILP problem as described in Section 2.4 to predict the final label sequence y.", 
        "164": "In the experiments, our model was trained using the Adam (Kingma and Ba, 2015) algorithm with a learning rate initialized at 0.001.", 
        "165": "The dimension of the hidden layers of bi-LSTM is 100.", 
        "166": "Word embeddings are initialized from GloVe 100- dimensional pre-trained embeddings (Pennington et al., 2014).", 
        "167": "POS and dependency embeddings are randomly initialized with 40-dimensional vectors.", 
        "168": "The embeddings are all updated during training.", 
        "169": "Dropping probability for dropout layers between stacked LSTM layers is 0.5.", 
        "170": "The batch size is set as 30.", 
        "171": "For the ILP part, \u03bb is set to 0.5, \u03b2 and \u03b3 are turned by the validation data and finally they are set to 0.7 and 0.2, respectively.", 
        "172": "We utilize an open source ILP solver4 in our method.", 
        "173": "We compare our methods with a few baselines: LSTM: This is the basic LSTM-based deletion method proposed by (Filippova et al., 2015).", 
        "174": "We report both the performance they achieved using close to two million training sentence pairs and the performance of our re-implementation of their model trained on the 8,000 sentence pairs.", 
        "175": "LSTM+: This is advanced version of the model proposed by Filippova et al.", 
        "176": "(2015), where the authors incorporated some dependency parse tree information into the LSTM model and used the prediction on the previous word to help the prediction on the current word.", 
        "177": "Traditional ILP: This is the ILP-based method proposed by Clarke and Lapata (2008).", 
        "178": "This method does not use neural network models and\n4gnu.org/software/glpk\nis an unsupervised method that relies heavily on the syntactic structures of the input sentences5.", 
        "179": "Abstractive seq2seq: This is an abstractive sequence-to-sequence model trained on 3.8 million Gigaword title-article pairs as described in Section 1.", 
        "180": "3.2 Automatic Evaluation  With the two datasets Google News and BNC News that have the ground truth compressed sentences, we can perform automatic evaluation.", 
        "181": "We first split the Google News dataset into a training set, a validation set and a test set.", 
        "182": "We took the first 1,000 sentence pairs from Google News as the test set, following the same practice as Filippova et al.", 
        "183": "(2015).", 
        "184": "We then use 8,000 of the remaining sentence pairs for training and the other 1,000 sentence pairs for validation.", 
        "185": "For the NBC News dataset, we use it only as a test set, applying the sentence compression models trained from the 8,000 sentence pairs from Google News.", 
        "186": "We use the ground truth compressed sentences to compute accuracy and F1 scores.", 
        "187": "Accuracy is defined as the percentage of tokens for which the predicted label yi is correct.", 
        "188": "F1 scores are derived from precision and recall values, where precision is defined as the percentage of retained words that overlap with the ground truth, and recall is defined as the percentage of words in the ground truth compressed sentences that overlap with the generated compressed sentences.", 
        "189": "We report both in-domain performance and cross-domain performance in Table 1.", 
        "190": "From the table, we have the following observations: (1) For the abstractive sequence-to-sequence model, it was trained on the Gigaword data, so for both Google News and NBC News, the performance shown is cross-domain performance.", 
        "191": "We can see that indeed this abstractive method performed poorly in cross-domain settings.", 
        "192": "(2) In the in-domain setting, with the same amount of training data (8,000), our BiLSTM method with syntactic features (BiLSTM+SynFeat and BiLSTM+SynFeat+ILP) performs similarly to or better than the LSTM+ method proposed by Filippova et al.", 
        "193": "(2015), in terms of both F1 and accuracy.", 
        "194": "This shows that our method is comparable to the LSTM+ method in the in-domain setting.", 
        "195": "(3) In the in-domain setting, even compared with the\n5We use an open source implementation: https:// github.com/cnap/sentence-compression.", 
        "196": "performance of LSTM+ trained on 2 million sentence pairs, our method trained on 8,000 sentence pairs does not perform substantially worse.", 
        "197": "(4) In the out-of-domain setting, our BiLSTM+SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM+ methods.", 
        "198": "This shows that by incorporating more syntactic features, our methods learn a sentence compression model that is less domain-dependent.", 
        "199": "(5) The Traditional ILP method also works better than the LSTM and LSTM+ methods in the out-of-domain setting.", 
        "200": "This is probably because the Traditional ILP method relies heavily on syntax, which is less domain-dependent compared with lexical patterns.", 
        "201": "But the Traditional ILP method performs worse in the in-domain setting than both the LSTM and LSTM+ methods and our methods.", 
        "202": "Overall, Table 1 shows that our proposed method combines both the strength of neural network models in the in-domain setting and the strength of the syntax-based methods in the crossdomain setting.", 
        "203": "Therefore, our method works reasonably well for both in-domain and out-ofdomain data.", 
        "204": "We also notice that on Google News, adding the ILP layer decreased the sentence compression performance.", 
        "205": "After some analysis, we think the reason is that some of the constraints used in the ILP layer have led to less deletion but the ground truth compressed sentences in the Google News data tend to be shorter compared with those in the NBC News data.", 
        "206": "We also conduct additional experiments to see the effect of the training data size on our meth-\nods and the LSTM+ method.", 
        "207": "Figure 4 shows the F1 scores on the in-domain Google News data and the out-of-domain NBC News data when we train the models using different amounts of sentence pairs.", 
        "208": "We can see that in the in-domain setting, our method does not have any advantage over the LSTM+ method.", 
        "209": "But in the cross-domain setting, our method that uses ILP to impose syntax-based constraints clearly performs better than LSTM+ when the amount of training data is relatively small.", 
        "210": "3.3 Manual Evaluation  The evaluation above does not look at the readability of the compressed sentences.", 
        "211": "In order to evaluate whether sentences generated by our method are readable, we adopt the manual evaluation procedure by Filippova et al.", 
        "212": "(2015) to compare our method with LSTM+ and Traditional ILP in terms of readability and informativeness.", 
        "213": "We asked two raters to score a randomly selected set of 100 sentences from the Research Papers dataset.", 
        "214": "The compressed sentences were randomly ordered and presented to the human raters to avoid any bias.", 
        "215": "The raters were asked to score the sentences on a five-point scale in terms of both readability and informativeness.", 
        "216": "We show the average scores of the three methods we compare in Table 3.", 
        "217": "We can see that our BiLSTM+SynFeat+ILP method clearly outperforms the two baseline methods in the manual evaluation.", 
        "218": "We also show a small sample of input sentences from the Research Papers dataset and the automatically compressed sentences by different methods in Table 2.", 
        "219": "As we can see from the table, a gen-\neral weakness of the LSTM+ method is that the compressed sentences may not be grammatical.", 
        "220": "In comparison, our method does better in terms of preserving grammaticality.", 
        "221": "4 Related Work  Sentence compression can be seen as sentencelevel summarization.", 
        "222": "Similar to document summarization, sentence compression methods can be divided into extractive compression and abstractive compression methods, based on whether words in the compressed sentence all come from the source sentence.", 
        "223": "In this paper, we focus on deletion-based sentence compression, which is a spacial case of extractive sentence compression.", 
        "224": "An early work on sentence compression was done by Jing (2000), who proposed to use several resources to decide whether a phrase in a sentence should be removed or not.", 
        "225": "Knight and Marcu (2000) proposed to apply a noisy-channel model from machine translation to the sentence compression task, but their model encountered the problem that many SCFG rules have unreliable probability estimates with inadequate data.", 
        "226": "Galley and McKeown (2007) tried to solve this problem by utilizing parent annotation, Markovization and lexicalization, which have all been shown to improve the quality of the rule probability estimates.", 
        "227": "Cohn and Lapata (2007) formulated sentence compression as a tree-to-tree rewrite problem.", 
        "228": "They utilized a synchronous tree substitution grammar (STSG) to license the space of all possible rewrites.", 
        "229": "Each rule has a weight learned from the training data.", 
        "230": "For prediction, an algorithm was used to search for the best scoring compression using the gram-\nmar rules.", 
        "231": "Besides, Cohn and Lapata (2008) extended this model to abstractive sentence compression, which includes substitution, reordering and insertion.", 
        "232": "McDonald (2006) proposed a graphbased sentence compression method.", 
        "233": "The general idea is that each word pair in the original sentence has a score.", 
        "234": "The task then becomes how to find a compressed sentence with a length limit according word pair scores.", 
        "235": "Their method is similar to graphbased dependency parsing.", 
        "236": "Clarke and Lapata (2008) first used an ILP framework for sentence compression.", 
        "237": "In the paper, the author put forward three models.", 
        "238": "The first model is a language model reformulated by ILP.", 
        "239": "As the first model treats all the words equally, the second model uses a corpus to learn an importance score for each word and then incorporates it in the ILP model.", 
        "240": "The Last model, which is based on (McDonald, 2006), replaces the decoder with an ILP model and adds many linguistic constraints such as dependency parsing compared with the previous two ILP models.", 
        "241": "Filippova and Strube (2008) represented sentences with dependency parse trees and an ILPbased method was used to decide whether the dependencies were preserved or not.", 
        "242": "Different from most previous work that treats sentence extrac-\ntion and sentence compression separately, BergKirkpatrick et al.", 
        "243": "(2011) jointly model the two processes in one ILP problem.", 
        "244": "Bigrams and subtrees are represented by some features, and feature are learned on some training data.", 
        "245": "The ILP problem maximizes the coverage of weighted bigrams and deleted subtrees of the summary.", 
        "246": "In recent years, neural network models, especially sequence-to-sequence models, have been applied to sentence compression.", 
        "247": "Our work is based on the deletion-based LSTM model for sentence compression by Filippova et al.", 
        "248": "(2015).", 
        "249": "There has also been much interest in applying sequence-to-sequence models for abstractive sentence compression (Rush et al., 2015; Chopra et al., 2016).", 
        "250": "As we pointed out in Section 1, in a cross-domain setting, abstractive sentence compression may not be suitable.", 
        "251": "5 Conclusions  In this paper, we studied how to modify an LSTM model for deletion-based sentence compression so that the model works well in a cross-domain setting.", 
        "252": "We hypothesized that incorporation of syntactic information into the training of the LSTM model would help.", 
        "253": "We thus proposed two ways to incorporate syntactic information, one through directly adding POS tag embeddings and dependency type embeddings, and the other through the objective function and constraints of an Integer Linear Programming (ILP) model.", 
        "254": "The experiments showed that our proposed bi-LSTM model with syntactic features and an ILP layer works\nwell in both in-domain and cross-domain settings.", 
        "255": "In comparison, the original LSTM model does not work well in the cross-domain setting, and a traditional ILP method does not work well in the in-domain setting.", 
        "256": "Therefore, our proposed method is relatively more robust than these baselines.", 
        "257": "We also manually evaluated the compressed sentences generated by our method and found that the method works better than the baselines in terms of both readability and informativeness.", 
        "258": "Acknowledgment  This work is supported by DSO grant DSOCL15223.", 
        "259": "The work was conducted during the first author\u2019s visit to the Singapore Management University."
    }, 
    "document_id": "P17-1127.pdf.json"
}
