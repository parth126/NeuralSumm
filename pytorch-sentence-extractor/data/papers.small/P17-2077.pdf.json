{
    "abstract_sentences": {
        "1": "This paper focuses on the task of noisy label aggregation in social media, where users with different social or culture backgrounds may annotate invalid or malicious tags for documents.", 
        "2": "To aggregate noisy labels at a small cost, a network framework is proposed by calculating the matching degree of a document\u2019s topics and the annotators\u2019 meta-data.", 
        "3": "Unlike using the back-propagation algorithm, a probabilistic inference approach is adopted to estimate network parameters.", 
        "4": "Finally, a new simulation method is designed for validating the effectiveness of the proposed framework in aggregating noisy labels."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 484\u2013490 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2077  1 Introduction  Social media allows users to share their views, opinions, emotion tendencies, and other personal information online.", 
        "2": "It is quite valuable to analyze and predict user opinions from these materials (Wang and Pal, 2015), in which supervised learning is one of the effective paradigms (Xu et al., 2015).", 
        "3": "However, the performance of a supervised learning algorithm relies heavily on the quality of training labels (Song et al., 2015).", 
        "4": "In social media, many training data are collected via simple heuristic rules or online crowdsourcing systems, such as Amazon\u2019s Mechanical Turk (www.mturk.com) which allows multiple labelers to annotate the same object (Zhang et al., 2013).", 
        "5": "Due to the lack\n\u2217The corresponding author.", 
        "6": "of quality control, it can be hard for a model to reconcile such noise in training labels.", 
        "7": "This study aims to aggregate noisy labels by matching annotators and documents.", 
        "8": "Unlike other noisy label aggregation and integration tasks (or algorithms), such as Learning to Rank (LtR) and integrating crowdsourced labels which rely on accurate instance sources (Ustinovskiy et al., 2016) or confidence scores (Oyama et al., 2013), we only need features that can be obtained with a small cost (i.e., topics).", 
        "9": "Compared with acquiring accurate instance sources or confidence scores, which is very hard, extracting topics can be done conveniently by many existing topic models.", 
        "10": "Note that label noise is not always random, as adversarial noisemay occur in real-world environments when a malicious agent is permitted to select labels for certain instances (Auer and Cesa-Bianchi, 1998).", 
        "11": "For example, a fake annotator is purchased to promote defective goods by giving high ratings.", 
        "12": "Noisy labels in such a manner are extremely difficult to be handled (Nicholson et al., 2015).", 
        "13": "To validate the effectiveness of aggregating the aforementioned noisy labels, we propose to design a new simulation method in Section 4.", 
        "14": "2 Related Work  To aggregate or refine noisy labels, several approaches have been proposed recently.", 
        "15": "Whitehill et al.", 
        "16": "(Whitehill et al., 2009) explored a probabilistic model to combine labels from both human labelers and automatic classifiers in image classification.", 
        "17": "Raykar et al.", 
        "18": "(Raykar et al., 2010) used a Bayesian approach for supervised learning over\n484\nnoisy labels from multiple annotators.", 
        "19": "Oyama et al.", 
        "20": "(Oyama et al., 2013) proposed to integrate labels of crowdsourcing workers using their confidence scores.", 
        "21": "Song et al.", 
        "22": "(Song et al., 2015) developed a single-label refinement algorithm to adjust noisy and missing labels.", 
        "23": "Ustinovskiy et al.", 
        "24": "(Ustinovskiy et al., 2016) proposed an optimization framework via remapping and reweighting methods to solve the problem of LtR with the existence of noisy labels.", 
        "25": "Different from the previous study that modeled the difficulties of instances and the user\u2019s authority (Whitehill et al., 2009), we target at integrating multiple labels for each instance by estimating the matching degree of documents and annotators.", 
        "26": "Consequently, our work is applicable to aggregating individual sentiment labels in social media, where users under various scenarios (e.g., character and preference) may express invalid or noisy sentiments to different topics.", 
        "27": "3 Noisy Label Aggregation Framework    3.1 Problem Definition  The problem of noisy label aggregation is defined as follows: Given N documents (instances) annotated byM users (annotators) over C kinds of labels, we generate D topics by existing unsupervised topic models.", 
        "28": "Let T \u2208 RN\u00d7D be topics of all instances, where the i-th row of T (i.e., Ti) is the topic distribution of document i, and the size of Ti (i.e., |Ti|) isD.", 
        "29": "Let F \u2208 RM\u00d7U be features (e.g., age and gender) of all annotators, where Fj is the feature distribution of user j and |Fj | = U .", 
        "30": "To model different dimensions of document topics (D) and annotator features (U ) jointly, we map Ti and Fj to K latent factors denoted as Si and Aj , i.e., |Si| = |Aj | = K.\nTo estimate the ground truth label Zi, we propose a novel network framework via aggregating the observable labels Vi, as shown in Fig.", 
        "31": "1.", 
        "32": "In our framework, the correctness of Vij depends on whether annotator j matches document i.", 
        "33": "3.2 Detailed Steps  Topic Extraction (TE): For document features, it is rough to use tf or tf-idf since they ignore the versatility of semantics among various contexts.", 
        "34": "Without considering the semantic units called topics, the accurate category of each document may be hard to access (Song et al., 2016).", 
        "35": "Short messages (e.g., tweets) are prevalent in social media, which differ from normal documents insofar as the number of words is fewer and most words only occur once in each instance.", 
        "36": "To extract topics from such a sparse word space, we employ the Biterm Topic Model (BTM) by breaking each document into biterms and leveraging the information of the whole corpus (Yan et al., 2013).", 
        "37": "Fully-connected Operation (FcO): There can be a large difference between dimensions of document topics and annotator features, so we need convert T and F to the same latent space.", 
        "38": "This step conducts linear transformation by introducing fully-connected weights WT \u2208 RD\u00d7K and WF \u2208 RU\u00d7K , as follows: S = TWT and A = FWF.", 
        "39": "The values of S and A are proportional to the label correctness probability.", 
        "40": "Since more cohesive topics may indicate that the document\u2019s category is more concentrated and can be correctly annotated by more users, the topic distribution embeds key information on the document factors S. To map T to S well, we propose the concept of topic entropy that acts as the constraint factor, by calculating the centralization of each document\u2019s topics: H(di) = \u2212\u2211Dz=1 p(tz|di) logD ( p(tz|di) ) , where p(tz|di) is the probability of the z-th topic conditioned to document i, and D constrains the values ranging from 0 to 1.", 
        "41": "The lower H(di), the higher the concentration of topics and the label correctness for document i.", 
        "42": "We thus infer the relationship between Si and H(di) as ||Si||2 \u221d 1/H(di), where ||Si||2 is the Euclidean norm of Si.", 
        "43": "Matching Degree Calculation (MDC): This step calculates the matching degree of document i and annotator j, which is denoted as gij by the similarity/distance between latent factors Si and Aj .", 
        "44": "Intuitively, a basketball enthusiast j matches\nclose to a document i that contains the \u201cbasketball\u201d topic, which indicates that the \u201cmatching degree\u201d of i and j is high with a large similarity.", 
        "45": "The inner product is used here, and it can be replaced by distance measures.", 
        "46": "Weight Transformation (WT): We employ transformation to distinguish different scores effectively.", 
        "47": "The activation function is sigmoid (softmax) or tanh.", 
        "48": "Since most document labels are assumed to be discrete independent variables, we encode Vij as a binary vector.", 
        "49": "The higher gij of a label, the closer it is to the ground truth.", 
        "50": "Namely, we should weight these labels in such a way that if a label has high gij , its weight will be increased; meanwhile, other labels should be punished.", 
        "51": "For sigmoid and tanh, the punishment is 1 \u2212 wij and \u2212wij , respectively.", 
        "52": "Take four labels, the transformation weight wij and Vij = (1, 0, 0, 0) as an example, the label weight via sigmoid is V newij = (wij , 1\u2212 wij , 1\u2212 wij , 1\u2212 wij).", 
        "53": "Label Weighting (LW) and One-max Pooling: The final step is to output by integrating weighted labels, where the multiplicative combination is used in aggregation, and the output is the maximum one of aggregated labels ZiC .", 
        "54": "3.3 Parameter Estimation  Since training labels may contain noise, it is inaccurate to employ the back-propagation method which uses the error between predicted and training labels as feedback for parameter estimation.", 
        "55": "Thus, we turn the estimation of model parameters WT and WF into a probabilistic problem.", 
        "56": "The graphical representation is illustrated in Fig.", 
        "57": "2.", 
        "58": "Firstly, we define W = {WT,WF} for simplicity.", 
        "59": "Secondly, the parameter distribution is determined by the Maximum A Posteriori (MAP) principal: W\u2217 = arg maxW Pr(W|V,T,F) = arg maxW \u2211 Z Pr(Z)Pr(W|V,T,F,Z).", 
        "60": "Finally, the following Expectation Maximization (EM) algorithm is used to estimate W\u2217.", 
        "61": "Initialization: We first initialize W randomly.", 
        "62": "The prior of ground truth Z can be set to 1/C or the frequency of each observable label.", 
        "63": "Expectation (E): We then compute the expectation of the joint log-likelihood of observable and hidden variables given W (i.e., the Q function), as follows: Q(W) = E[lnPr(V,Z,T,F|W)] = E[lnPr(V|Z,T,F,W)]+E[lnPr(Z,T,F|W)].", 
        "64": "Maximization (M): According to the Q function, the maximum likelihood of hidden variables is estimated by the gradient ascent method.", 
        "65": "Alternation: The above E and M steps are alternately performed until the likelihood converges.", 
        "66": "4 Experiments    4.1 Datasets and Baselines  As sentiment and emotion detection are widely studied in social media analysis (Wang and Pal, 2015), we test model performance based on the Stanford Twitter Sentiment (STS) and the International Survey on Emotion Antecedents and Reactions (ISEAR) corpus.", 
        "67": "The original STS dataset (Go et al., 2009) contains 1.6 million tweets that were automatically labeled as positive or negative using emoticons as labels, in which 80K (5%) randomly selected tweets were used to speed up the training process, 16K (1%) randomly selected tweets were used as the validation set, and 359 tweets were manually annotated as the testing set (dos Santos and Gatti, 2014).", 
        "68": "ISEAR is composed of 7, 666 sentences annotated by 1, 096 participants with different culture backgrounds (Scherer and Wallbott, 1994).", 
        "69": "These participants completed questionnaires about their 34 kinds of personal information (e.g., age, gender, city, country, and religion), as well as their experiences and reactions over seven emotions.", 
        "70": "For the ISEAR corpus, we randomly selected 60% of sentences as the training set, 20% as the validation set, and the remaining 20% as the testing set.", 
        "71": "We use the following models for comparison: Majority Voting (MV) (Sheng et al., 2008), Maximum Likelihood Estimator (MLE) (Raykar et al., 2010), and Generative model of Labels, Abilities and Difficulties (GLAD) (Whitehill et al., 2009).", 
        "72": "The baselines of MV and MLE are implemented by following (Sheng et al., 2008; Raykar et al., 2010), and GLAD is run by the software that is available in public at (Whitehill et al., 2009).", 
        "73": "We\nalso implement the multivariate version of GLAD, called MGLAD as the baseline for the ISEAR corpus with seven emotions.", 
        "74": "Although there are some more recent models on label aggregation (Oyama et al., 2013) or refinement (Song et al., 2015; Ustinovskiy et al., 2016), they either require additional features like users\u2019 reported confidence scores, or are only suitable to a corpus with one label for each document.", 
        "75": "To compare sentiment and emotion classification performance using the aggregated labels for training, we further apply the above noisy label aggregation models to a linear Support Vector Machine (SVM) with squared hinge loss (Chang and Lin, 2011).", 
        "76": "As shown in the existing studies with refined labels, the linear SVM performed well on sentiment classification of reviews (Pang et al., 2002) and tweets (Vo and Zhang, 2015).", 
        "77": "4.2 Experimental Design  To evaluate the performance of noisy label aggregation models, each instance should be annotated by multiple users.", 
        "78": "Unlike previous studies which introduced a parameter to disturb ground truth labels (Sheng et al., 2008) or employed online crowdsourcing systems (Whitehill et al., 2009; Raykar et al., 2010) to generate noisy annotations, we design a new simulation approach by following the process of Profile Injection Attack in Collaborative Recommender Systems (Williams and Mobasher, 2006).", 
        "79": "This is because the existing methods can not assign multiple labels to each instance, or are difficult to generate virtual users and access their information (e.g., age and gender).", 
        "80": "In particular, the following steps have been performed.", 
        "81": "First, we generate virtual users with different features, making them the neighbors of existing (actual) annotators.", 
        "82": "For each dimension of the actual annotators\u2019 features, we take the mean value if the attribute is continuous.", 
        "83": "For discrete attributes, we randomly select one type from the existing attribute values.", 
        "84": "If the dataset has no user features, we set it as a unit vector.", 
        "85": "Second, we generate document annotating vectors for virtual users.", 
        "86": "Each annotating vector is composed of three parts: annotating for filler instances (IF ), which is a set of randomly chosen filler instances drawn from the whole dataset, untagged instances (I\u2205), and the target instance (it).", 
        "87": "The purpose of setting IF and I\u2205 is to make the virtual user looks like an ordinary annotator.", 
        "88": "We\nselect three simulation types from Profile Injection Attack (Williams and Mobasher, 2006), i.e., random, average, and love/hate.", 
        "89": "In the random method, the label for each instance i \u2208 IF is drawn from a normal distribution around the annotations across the whole dataset, and the probability of labeling correctly to i is 1/C.", 
        "90": "The corresponding probabilities are 0.5 and 1 for the average and love/hate methods, respectively.", 
        "91": "In all these methods, the annotation for it is randomly selected from wrong labels.", 
        "92": "We tune the number of topics D and annotator features U by performing a grid search over all D and U values, with D \u2208 {2, 3, 4, ..., 10} on both datasets, U = 34 on ISEAR, and U \u2208 {1, 10, 100, 500, 1000} on STS that contains user ID only.", 
        "93": "The value of K is set to the maximum of D and U .", 
        "94": "Based on the performance on the validation set, we set D = 6, U = 1000,K = 1000 for STS, andD = 2, U = 34,K = 34 for ISEAR.", 
        "95": "For the sum of |IF | and |it| (i.e., attack size) for each virtual user, we set it as the mean number of annotations in actual users.", 
        "96": "The sum of selecting it in each simulation is called the profile size, and the percentage of the profile size is denoted as o.", 
        "97": "Following the previous criterion of choosing the noise rate (Auer and Cesa-Bianchi, 1998), we set o \u2208 {0.05, 0.1, 0.2, 0.5}.", 
        "98": "According to (Ustinovskiy et al., 2016), each target instance except for those in IF is annotated by three users.", 
        "99": "Thus, the number of virtual users is set to 2oN .", 
        "100": "We set the parameter values of MV, MLE, and M/GLAD according to (Sheng et al., 2008; Raykar et al., 2010; Whitehill et al., 2009), and apply the grid search method to obtain the optimal parameters for SVM.", 
        "101": "4.3 Results and Analysis  Firstly, we evaluate the noisy label aggregation performance of different models by comparing the proportion of estimated labels which match the actual categories (i.e., accuracy).", 
        "102": "The results are shown in Fig.", 
        "103": "3, which indicates that our model performs the best under various conditions.", 
        "104": "From the aspect of simulation methods, the accuracy of the random one is the lowest and the Love/Hate one is the highest, which is consistent to the correctly labeling probability for each method.", 
        "105": "The results of the random and average ones over STS are similar, because C = 2 on STS.", 
        "106": "Particularly, our model performs better than\nbaselines in aggregating noisy labels, especially when the noise scale becomes large.", 
        "107": "For instance, our model achieves 85% and 57% accuracies on STS and ISEAR when using the random method and o = 0.5, which indicates that our model has higher capability of recognizing adversarial noise (it).", 
        "108": "In the random method, we can also observe that the performance differences are more significant on ISEAR than STS.", 
        "109": "This is because ISEAR has more elaborate, i.e., 34 kinds of observable user information, which validates the joint influence of users and documents on noisy label aggregation.", 
        "110": "To evaluate the performance differences statistically, we use the 12 groups of results over all methods and o values based on the conventional significance level (i.e., p value) of 0.05.", 
        "111": "The p values of t-tests between our model and MV, M/GLAD, MLE are 0.0087, 0.0009, 0.0067 over STS, and 0.0535, 0.1037, 0.0007 over ISEAR, which indicates that the performance differences between our model and baselines are statistically significant on both datasets, except for MV and MGLAD in the love/hate method over ISEAR.", 
        "112": "The reason may be that each virtual user annotates around seven instances on ISEAR, and only one label is incorrect for the love/hate method, which makes the simple MV perform competitively.", 
        "113": "Secondly, we compare the classification perfor-\nmance of SVM using labels from different noisy label aggregation models for training.", 
        "114": "The accuracies are shown in Fig.", 
        "115": "4, in which dotted lines represent results on benchmark datasets without conducting the Profile Injection Attack process.", 
        "116": "Compared to other methods, the performance of SVM based on the aggregated labels from our model is almost closer to that of SVM using benchmark datasets.", 
        "117": "For the average method and o = 0.2 over STS, we can observe that SVM in conjunction with our model performs even better than that on the benchmark dataset.", 
        "118": "This is because emoticons are used as annotations for STS, which may introduce errors to the original labels.", 
        "119": "5 Conclusions  In this paper, we proposed a network framework for noisy label aggregation by calculating the matching degree of documents and annotators.", 
        "120": "Experiments using a new simulation method of generating noisy labels validated the effectiveness of the proposed framework.", 
        "121": "As our model is linear in feature transformation, it is flexible to handle large-scale datasets.", 
        "122": "In the future, we plan to compare the model performance using different topic models, improve our model by exploiting the feedback of a small proportion of refined labels, and recruit actual participants to provide noisy labels.", 
        "123": "Acknowledgments  The authors are thankful to the reviewers for their constructive comments and suggestions on this paper.", 
        "124": "The work described in this paper was supported by the National Natural Science Foundation of China (61502545), a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (UGC/FDS11/E03/16), the Start-Up Research Grant (RG 37/2016-2017R), and the Internal Research Grant (RG 66/2016- 2017) of The Education University of Hong Kong.", 
        "125": "B Noisy Label Aggregation Algorithm  In our method of noisy label aggregation as shown in Algorithm 1, the cost of calculating S and A by FcO (line 6) is linear to the number of\nAlgorithm 1 Noisy Label Aggregation Input:\nV: Observable labels; F: Features of users; \u03c9: Words of documents; \u03b4: Threshold of convergence.", 
        "126": "Output: Aggregated labels.", 
        "127": "1: T\u2190 TE(\u03c9); 2: Initialize parameter W randomly; 3: Q\u2190 0; 4: repeat 5: lastQ\u2190 Q; 6: {S,A} \u2190 FcO(W, T, F); 7: for each i \u2208 [1, N ] do 8: for each j \u2208 [1,M ] do 9: gij = MDC(Si,Aj); 10: V newij = WT (gij , Vij , sigmoid); 11: end for 12: ZiC = LW(Vnewi ); 13: end for 14: Q\u2190 E-Step(ZiC); 15: W\u2190M-Step(Q, W); 16: until |Q - lastQ| < \u03b4; 17: return Zi, i.e., the maximum one of ZiC .", 
        "128": "instances, i.e., O(NDK), and the total number of users, i.e., O(MUK), respectively.", 
        "129": "Before the EM iteration (lines 7 to 13), it takes O(NM(K + C)) to weigh all labels V. For each iteration of EM (lines 14 to 15), the optimization with stochastic gradient descent takesO(NMC+NK+MK) when each user annotates all documents.", 
        "130": "Assume that our algorithm converges after t iterations (t < 10 in our experiments), the overall time complexity is O(NM(K + C)t), which is linear to the numbers of instances and users.", 
        "131": "C Gradient Derivation  Given the estimated value of ZiC , the Q function can be calculated byQ(W) = \u2211 ij ZiC lnV new ij + const.", 
        "132": "Since the vector V newij has two possible values when using sigmoid (i.e., wij and 1 \u2212 wij), the gradient of lnV newij on parameter W i,k T is (Vij\u2212wij)Ajk, i.e., [wij(1\u2212wij)]/wijAjk and [\u2212wij(1\u2212wij)]/(1\u2212wij)Ajk, respectively.", 
        "133": "Then, the gradient of Q on parameter W i,kT can be derived as \u2202Q/\u2202W i,kT = \u2211 j ZiC(Vij \u2212 wij)Ajk.", 
        "134": "Similarly, the gradient of Q on parameterW j,kF is given by \u2202Q/\u2202W j,kF = \u2211 i ZiC(Vij \u2212 wij)Sik."
    }, 
    "document_id": "P17-2077.pdf.json"
}
