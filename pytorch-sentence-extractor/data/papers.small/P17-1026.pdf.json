{
    "abstract_sentences": {
        "1": "We propose a new A* CCG parsing model in which the probability of a tree is decomposed into factors of CCG categories and its syntactic dependencies both defined on bi-directional LSTMs.", 
        "2": "Our factored model allows the precomputation of all probabilities and runs very efficiently, while modeling sentence structures explicitly via dependencies.", 
        "3": "Our model achieves the stateof-the-art results on English and Japanese CCG parsing.1"
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 277\u2013287 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1026  1 Introduction  Supertagging in lexicalized grammar parsing is known as almost parsing (Bangalore and Joshi, 1999), in that each supertag is syntactically informative and most ambiguities are resolved once a correct supertag is assigned to every word.", 
        "2": "Recently this property is effectively exploited in A* Combinatory Categorial Grammar (CCG; Steedman (2000)) parsing (Lewis and Steedman, 2014; Lewis et al., 2016), in which the probability of a CCG tree y on a sentence x of length N is the product of the probabilities of supertags (categories) ci (locally factored model):\nP (y|x) = \u220f\ni\u2208[1,N ] Ptag(ci|x).", 
        "3": "(1)\nBy not modeling every combinatory rule in a derivation, this formulation enables us to employ efficient A* search (see Section 2), which finds the most probable supertag sequence that can build a well-formed CCG tree.", 
        "4": "Although much ambiguity is resolved with this supertagging, some ambiguity still remains.", 
        "5": "Figure 1 shows an example, where the two CCG\n1 Our software and the pretrained models are available at: https://github.com/masashi-y/depccg.", 
        "6": "parses are derived from the same supertags.", 
        "7": "Lewis et al.\u2019s approach to this problem is resorting to some deterministic rule.", 
        "8": "For example, Lewis et al.", 
        "9": "(2016) employ the attach low heuristics, which is motivated by the right-branching tendency of English, and always prioritizes (b) for this type of ambiguity.", 
        "10": "Though for English it empirically works well, an obvious limitation is that it does not always derive the correct parse; consider a phrase \u201ca house in Paris with a garden\u201d, for which the correct parse has the structure corresponding to (a) instead.", 
        "11": "In this paper, we provide a way to resolve these remaining ambiguities under the locally factored model, by explicitly modeling bilexical dependencies as shown in Figure 1.", 
        "12": "Our joint model is still locally factored so that an efficient A* search can be applied.", 
        "13": "The key idea is to predict the head of every word independently as in Eq.", 
        "14": "1 with a strong unigram model, for which we utilize the scoring model in the recent successful graph-based dependency parsing on LSTMs (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).", 
        "15": "Specif-\n277\nically, we extend the bi-directional LSTM (biLSTM) architecture of Lewis et al.", 
        "16": "(2016) predicting the supertag of a word to predict the head of it at the same time with a bilinear transformation.", 
        "17": "The importance of modeling structures beyond supertags is demonstrated in the performance gain in Lee et al.", 
        "18": "(2016), which adds a recursive component to the model of Eq.", 
        "19": "1.", 
        "20": "Unfortunately, this formulation loses the efficiency of the original one since it needs to compute a recursive neural network every time it searches for a new node.", 
        "21": "Our model does not resort to the recursive networks while modeling tree structures via dependencies.", 
        "22": "We also extend the tri-training method of Lewis et al.", 
        "23": "(2016) to learn our model with dependencies from unlabeled data.", 
        "24": "On English CCGbank test data, our model with this technique achieves 88.8% and 94.0% in terms of labeled and unlabeled F1, which mark the best scores so far.", 
        "25": "Besides English, we provide experiments on Japanese CCG parsing.", 
        "26": "Japanese employs freer word order dominated by the case markers and a deterministic rule such as the attach low method may not work well.", 
        "27": "We show that this is actually the case; our method outperforms the simple application of Lewis et al.", 
        "28": "(2016) in a large margin, 10.0 points in terms of clause dependency accuracy.", 
        "29": "2 Background  Our work is built on A* CCG parsing (Section 2.1), which we extend in Section 3 with a head prediction model on bi-LSTMs (Section 2.2).", 
        "30": "2.1 Supertag-factored A* CCG Parsing  CCG has a nice property that since every category is highly informative about attachment decisions, assigning it to every word (supertagging) resolves most of its syntactic structure.", 
        "31": "Lewis and Steedman (2014) utilize this characteristics of the grammar.", 
        "32": "Let a CCG tree y be a list of categories \u27e8c1, .", 
        "33": ".", 
        "34": ".", 
        "35": ", cN \u27e9 and a derivation on it.", 
        "36": "Their model looks for the most probable y given a sentence x of length N from the set Y (x) of possible CCG trees under the model of Eq.", 
        "37": "1:\ny\u0302 = arg max y\u2208Y (x)\n\u2211\ni\u2208[1,N ] logPtag(ci|x).", 
        "38": "Since this score is factored into each supertag, they call the model a supertag-factored model.", 
        "39": "Exact inference of this problem is possible by A* parsing (Klein and D. Manning, 2003), which uses the following two scores on a chart:\nb(Ci,j) = \u2211\nck\u2208ci,j logPtag(ck|x),\na(Ci,j) = \u2211\nk\u2208[1,N ]\\[i,j] max ck logPtag(ck|x),\nwhere Ci,j is a chart item called an edge, which abstracts parses spanning interval [i, j] rooted by category C. The chart maps each edge to the derivation with the highest score, i.e., the Viterbi parse for Ci,j .", 
        "40": "ci,j is the sequence of categories on such Viterbi parse, and thus b is called the Viterbi inside score, while a is the approximation (upper bound) of the Viterbi outside score.", 
        "41": "A* parsing is a kind of CKY chart parsing augmented with an agenda, a priority queue that keeps the edges to be explored.", 
        "42": "At every step it pops the edge e with the highest priority b(e) + a(e) and inserts that into the chart, and enqueue any edges that can be built by combining e with other edges in the chart.", 
        "43": "The algorithm terminates when an edge C1,N is popped from the agenda.", 
        "44": "A* search for this model is quite efficient because both b and a can be obtained from the unigram category distribution on every word, which can be precomputed before search.", 
        "45": "The heuristics a gives an upper bound on the true Viterbi outside score (i.e., admissible).", 
        "46": "Along with this the condition that the inside score never increases by expansion (monotonicity) guarantees that the first found derivation on C1,N is always optimal.", 
        "47": "a(Ci,j) matches the true outside score if the onebest category assignments on the outside words (argmaxck logPtag(ck|x)) can comprise a wellformed tree with Ci,j , which is generally not true.", 
        "48": "Scoring model For modeling Ptag, Lewis and Steedman (2014) use a log-linear model with features from a fixed window context.", 
        "49": "Lewis et al.", 
        "50": "(2016) extend this with bi-LSTMs, which encode the complete sentence and capture the long range syntactic information.", 
        "51": "We base our model on this bi-LSTM architecture, and extend it to modeling a head word at the same time.", 
        "52": "Attachment ambiguity In A* search, an edge with the highest priority b+a is searched first, but as shown in Figure 1 the same categories (with the same priority) may sometimes derive more than\none tree.", 
        "53": "In Lewis and Steedman (2014), they prioritize the parse with longer dependencies, which they judge with a conversion rule from a CCG tree to a dependency tree (Section 4).", 
        "54": "Lewis et al.", 
        "55": "(2016) employ another heuristics prioritizing low attachments of constituencies, but inevitably these heuristics cannot be flawless in any situations.", 
        "56": "We provide a simple solution to this problem by explicitly modeling bilexical dependencies.", 
        "57": "2.2 Bi-LSTM Dependency Parsing  For modeling dependencies, we borrow the idea from the recent graph-based neural dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) in which each dependency arc is scored directly on the outputs of bi-LSTMs.", 
        "58": "Though the model is first-order, bi-LSTMs enable conditioning on the entire sentence and lead to the state-of-the-art performance.", 
        "59": "Note that this mechanism is similar to modeling of the supertag distribution discussed above, in that for each word the distribution of the head choice is unigram and can be precomputed.", 
        "60": "As we will see this keeps our joint model still locally factored and A* search tractable.", 
        "61": "For score calculation, we use an extended bilinear transformation by Dozat and Manning (2016) that models the prior headness of each token as well, which they call biaffine.", 
        "62": "3 Proposed Method    3.1 A* parsing with Supertag and Dependency Factored Model  We define a CCG tree y for a sentence x = \u27e8xi, .", 
        "63": ".", 
        "64": ".", 
        "65": ", xN \u27e9 as a triplet of a list of CCG categories c = \u27e8c1, .", 
        "66": ".", 
        "67": ".", 
        "68": ", cN \u27e9, dependencies h = \u27e8h1, .", 
        "69": ".", 
        "70": ".", 
        "71": ", hN \u27e9, and the derivation, where hi is the head index of xi.", 
        "72": "Our model is defined as follows:\nP (y|x) = \u220f\ni\u2208[1,N ] Ptag(ci|x)\n\u220f\ni\u2208[1,N ] Pdep(hi|x).", 
        "73": "(2)\nThe added term Pdep is a unigram distribution of the head choice.", 
        "74": "A* search is still tractable under this model.", 
        "75": "The search problem is changed as:\ny\u0302 = arg max y\u2208Y (x)\n( \u2211\ni\u2208[1,N ] logPtag(ci|x)\n+ \u2211\ni\u2208[1,N ] logPdep(hi|x)\n) ,\nand the inside score is given by:\nb(Ci,j) = \u2211\nck\u2208ci,j logPtag(ck|x) (3)\n+ \u2211\nk\u2208[i,j]\\{root(hCi,j)} logPdep(hk|x),\nwhere hCi,j is a dependency subtree for the Viterbi parse on Ci,j and root(h) returns the root index.", 
        "76": "We exclude the head score for the subtree root token since it cannot be resolved inside [i, j].", 
        "77": "This causes the mismatch between the goal inside score b(C1,N ) and the true model score (log of Eq.", 
        "78": "2), which we adjust by adding a special unary rule that is always applied to the popped goal edge C1,N .", 
        "79": "We can calculate the dependency terms in Eq.", 
        "80": "3 on the fly when expanding the chart.", 
        "81": "Let the currently popped edge be Ai,k, which will be combined with Bk,j into Ci,j .", 
        "82": "The key observation is that only one dependency arc (between root(hAi,k) and root(hBk,j)) is resolved at every combination (see Figure 2).", 
        "83": "For every rule C \u2192 A B we can define the head direction (see Section 4) and Pdep is obtained accordingly.", 
        "84": "For example, when the right child B becomes the head, b(Ci,j) = b(Ai,k) + b(Bk,j) + logPdep(hl = m|x), where l = root(hAi,k) andm = root(h B k,j) (l < m).", 
        "85": "The Viterbi outside score is changed as:\na(Ci,j) = \u2211\nk\u2208[1,N ]\\[i,j] max ck logPtag(ck|x)\n+ \u2211\nk\u2208L max hk logPdep(hk|x),\nwhere L = [1, N ] \\ [k\u2032|k\u2032 \u2208 [i, j], root(hCi,j) \u0338= k\u2032].", 
        "86": "We regard root(hCi,j) as an outside word since its head is undefined yet.", 
        "87": "For every outside word we independently assign the weight of its argmax\nhead, which may not comprise a well-formed dependency tree.", 
        "88": "We initialize the agenda by adding an item for every supertag C and word xi with the score a(Ci,i) = \u2211 k\u2208I\\{i}max logPtag(ck|x) +\u2211\nk\u2208I max logPdep(hk|x).", 
        "89": "Note that the dependency component of it is the same for every word.", 
        "90": "3.2 Network Architecture  Following Lewis et al.", 
        "91": "(2016) and Dozat and Manning (2016), we model Ptag and Pdep using biLSTMs for exploiting the entire sentence to capture the long range phenomena.", 
        "92": "See Figure 3 for the overall network architecture, where Ptag and Pdep share the common bi-LSTM hidden vectors.", 
        "93": "First we map every word xi to their hidden vector ri with bi-LSTMs.", 
        "94": "The input to the LSTMs is word embeddings, which we describe in Section 6.", 
        "95": "We add special start and end tokens to each sentence with the trainable parameters following Lewis et al.", 
        "96": "(2016).", 
        "97": "For Pdep, we use the biaffine transformation in Dozat and Manning (2016):\ngdepi = MLP dep child(ri), gdephi = MLP dep head(rhi), Pdep(hi|x) (4) \u221d exp((gdepi )TWdepg dep hi +wdepg dep hi ),\nwhere MLP is a multilayered perceptron.", 
        "98": "Though Lewis et al.", 
        "99": "(2016) simply use an MLP for mapping ri to Ptag, we additionally utilize the hidden vector of the most probable head hi = argmaxh\u2032i Pdep(h \u2032 i|x), and apply ri and rhi to a bilinear function:2\ngtagi = MLP tag child(ri), gtaghi = MLP tag head(rhi), (5)\n\u2113 = (gtagi ) TUtagg tag hi +Wtag [ gtagi gtaghi ] + btag, Ptag(ci|x) \u221d exp(\u2113c),\nwhere Utag is a third order tensor.", 
        "100": "As in Lewis et al.", 
        "101": "these values can be precomputed before search, which makes our A* parsing quite efficient.", 
        "102": "4 CCG to Dependency Conversion  Nowwe describe our conversion rules from a CCG tree to a dependency one, which we use in two pur-\n2 This is inspired by the formulation of label prediction in Dozat and Manning (2016), which performs the best among other settings that remove or reverse the dependence between the head model and the supertag model.", 
        "103": "poses: 1) creation of the training data for the dependency component of our model; and 2) extraction of a dependency arc at each combinatory rule during A* search (Section 3.1).", 
        "104": "Lewis and Steedman (2014) describe one way to extract dependencies from a CCG tree (LEWISRULE).", 
        "105": "Below in addition to this we describe two simpler alternatives (HEADFIRST and HEADFINAL), and see the effects on parsing performance in our experiments (Section 6).", 
        "106": "See Figure 4 for the overview.", 
        "107": "LEWISRULE This is the same as the conversion rule in Lewis and Steedman (2014).", 
        "108": "As shown in Figure 4c the output looks a familiar English dependency tree.", 
        "109": "For forward application and (generalized) forward composition, we define the head to be the left argument of the combinatory rule, unless it matches either X/X or X/(X\\Y ), in which case the right argument is the head.", 
        "110": "For example, on \u201cBlack Monday\u201d in Figure 4a we choose Monday as the head of Black.", 
        "111": "For the backward rules, the conversions are defined as the reverse of the corresponding forward rules.", 
        "112": "For other rules, RemovePunctuation (rp) chooses the non punctuation argument as the head, while Conjunction (\u03a6) chooses the right argument.3\n3When applying LEWISRULE to Japanese, we ignore the feature values in determining the head argument, which we find often leads to a more natural dependency structure.", 
        "113": "For example, in \u201ctabe ta\u201d (eat PAST), the category of auxiliary verb \u201cta\u201d is Sf1\\Sf2 with f1 \u0338= f2, and thus Sf1 \u0338= Sf2 .", 
        "114": "We choose \u201ctabe\u201d as the head in this case by removing the feature values, which makes the category X\\X .", 
        "115": "One issue when applying this method for obtaining the training data is that due to the mismatch between the rule set of our CCG parser, for which we follow Lewis and Steedman (2014), and the grammar in English CCGbank (Hockenmaier and Steedman, 2007) we cannot extract dependencies from some of annotated CCG trees.4 For this reason, we instead obtain the training data for this method from the original dependency annotations on CCGbank.", 
        "116": "Fortunately the dependency annotations of CCGbank matches LEWISRULE above in most cases and thus they can be a good approximation to it.", 
        "117": "HEADFINAL Among SOV languages, Japanese is known as a strictly head final language, meaning that the head of every word always follows it.", 
        "118": "Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2002) has exploited this property explicitly by only allowing left-toright dependency arcs.", 
        "119": "Inspired by this tradition, we try a simple HEADFINAL rule in Japanese CCG parsing, in which we always select the right argument as the head.", 
        "120": "For example we obtain the head final dependency tree in Figure 4e from the Japanese CCG tree in Figure 4b.", 
        "121": "HEADFIRST We apply the similar idea as HEADFINAL into English.", 
        "122": "Since English has the opposite, SVO word order, we define the simple \u201chead first\u201d rule, in which the left argument always becomes the head (Figure 4d).", 
        "123": "4 For example, the combinatory rules in Lewis and Steedman (2014) do not contain Nconj \u2192 N N in CCGbank.", 
        "124": "Another difficulty is that in English CCGbank the name of each combinatory rule is not annotated explicitly.", 
        "125": "Though this conversion may look odd at first sight it also has some advantages over LEWISRULE.", 
        "126": "First, since the model with LEWISRULE is trained on the CCGbank dependencies, at inference, occasionally the two components Pdep and Ptag cause some conflicts on their predictions.", 
        "127": "For example, the true Viterbi parse may have a lower score in terms of dependencies, in which case the parser slows down and may degrade the accuracy.", 
        "128": "HEADFIRST, in contract, does not suffer from such conflicts.", 
        "129": "Second, by fixing the direction of arcs, the prediction of heads becomes easier, meaning that the dependency predictions become more reliable.", 
        "130": "Later we show that this is in fact the case for existing dependency parsers (see Section 5), and in practice, we find that this simple conversion rule leads to the higher parsing scores than LEWISRULE on English (Section 6).", 
        "131": "5 Tri-training  We extend the existing tri-training method to our models and apply it to our English parsers.", 
        "132": "Tri-training is one of the semi-supervised methods, in which the outputs of two parsers on unlabeled data are intersected to create (silver) new training data.", 
        "133": "This method is successfully applied to dependency parsing (Weiss et al., 2015) and CCG supertagging (Lewis et al., 2016).", 
        "134": "We simply combine the two previous approaches.", 
        "135": "Lewis et al.", 
        "136": "(2016) obtain their silver data annotated with the high quality supertags.", 
        "137": "Since they make this data publicly available 5, we obtain our silver data by assigning dependency\n5https://github.com/uwnlp/taggerflow\nstructures on top of them.6\nWe train two very different dependency parsers from the training data extracted from CCGbank Section 02-21.", 
        "138": "This training data differs depending on our dependency conversion strategies (Section 4).", 
        "139": "For LEWISRULE, we extract the original dependency annotations of CCGbank.", 
        "140": "For HEADFIRST, we extract the head first dependencies from the CCG trees.", 
        "141": "Note that we cannot annotate dependency labels so we assign a dummy \u201cnone\u201d label to every arc.", 
        "142": "The first parser is graph-based RBGParser (Lei et al., 2014) with the default settings except that we train an unlabeled parser and use word embeddings of Turian et al.", 
        "143": "(2010).", 
        "144": "The second parser is transition-based lstm-parser (Dyer et al., 2015) with the default parameters.", 
        "145": "On the development set (Section 00), with LEWISRULE dependencies RBGParser shows 93.8% unlabeled attachment score while that of lstm-parser is 92.5% using gold POS tags.", 
        "146": "Interestingly, the parsers with HEADFIRST dependencies achieve higher scores: 94.9% by RBGParser and 94.6% by lstm-parser, suggesting that HEADFIRST dependencies are easier to parse.", 
        "147": "For both dependencies, we obtain more than 1.7 million sentences on which two parsers agree.", 
        "148": "Following Lewis et al.", 
        "149": "(2016), we include 15 copies of CCGbank training set when using these silver data.", 
        "150": "Also to make effects of the tri-train samples smaller we multiply their loss by 0.4.", 
        "151": "6 Experiments  We perform experiments on English and Japanese CCGbanks.", 
        "152": "6.1 English Experimental Settings  We follow the standard data splits and use Sections 02-21 for training, Section 00 for development, and Section 23 for final evaluation.", 
        "153": "We report labeled and unlabeled F1 of the extracted CCG semantic dependencies obtained using generate program supplied with C&C parser.", 
        "154": "For our models, we adopt the pruning strategies in Lewis and Steedman (2014) and allow at most 50 categories per word, use a variable-width beam with \u03b2 = 0.00001, and utilize a tag dictionary, which maps frequent words to the possible\n6We annotate POS tags on this data using Stanford POS tagger (Toutanova et al., 2003).", 
        "155": "supertags7.", 
        "156": "Unless otherwise stated, we only allow normal form parses (Eisner, 1996; Hockenmaier and Bisk, 2010), choosing the same subset of the constraints as Lewis and Steedman (2014).", 
        "157": "We use as word representation the concatenation of word vectors initialized to GloVe8 (Pennington et al., 2014), and randomly initialized prefix and suffix vectors of the length 1 to 4, which is inspired by Lewis et al.", 
        "158": "(2016).", 
        "159": "All affixes appearing less than two times in the training data are mapped to \u201cUNK\u201d.", 
        "160": "Other model configurations are: 4-layer biLSTMs with left and right 300-dimensional LSTMs, 1-layer 100-dimensional MLPs with ELU non-linearity (Clevert et al., 2015) for all MLP depchild, MLP dep head, MLP tag child and MLP tag head, and the Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.9, L2 norm (1e\u22126), and learning rate decay with the ratio 0.75 for every 2,500 iteration starting from 2e\u22123, which is shown to be effective for training the biaffine parser (Dozat and Manning, 2016).", 
        "161": "6.2 Japanese Experimental Settings  We follow the default train/dev/test splits of Japanese CCGbank (Uematsu et al., 2013).", 
        "162": "For the baselines, we use an existing shift-reduce CCG parser implemented in an NLP tool Jigg9 (Noji and Miyao, 2016), and our implementation of the supertag-factored model using bi-LSTMs.", 
        "163": "For Japanese, we use as word representation the concatenation of word vectors initialized to Japanese Wikipedia Entity Vector10, and 100- dimensional vectors computed from randomly initialized 50-dimensional character embeddings through convolution (dos Santos and Zadrozny, 2014).", 
        "164": "We do not use affix vectors as affixes are less informative in Japanese.", 
        "165": "All characters appearing less than two times are mapped to \u201cUNK\u201d.", 
        "166": "We use the same parameter settings as English for bi-LSTMs, MLPs, and optimization.", 
        "167": "One issue in Japanese experiments is evaluation.", 
        "168": "The Japanese CCGbank is encoded in a different format than the English bank, and no standalone script for extracting semantic dependencies is available yet.", 
        "169": "For this reason, we evaluate the parser outputs by converting them to bunsetsu\n7We use the same tag dictionary provided with their biLSTM model.", 
        "170": "8http://nlp.stanford.edu/projects/ glove/\n9https://github.com/mynlp/jigg 10http://www.cl.ecei.tohoku.ac.jp/\n\u02dcm-suzuki/jawiki_vector/\ndependencies, the syntactic representation ordinary used in Japanese NLP (Kudo andMatsumoto, 2002).", 
        "171": "Given a CCG tree, we obtain this by first segment a sentence into bunsetsu (chunks) using CaboCha11 and extract dependencies that cross a bunsetsu boundary after obtaining the word-level, head final dependencies as in Figure 4b.", 
        "172": "For example, the sentence in Figure 4e is segmented as \u201cBoku wa | eigo wo | hanashi tai\u201d, from which we extract two dependencies (Boku wa) \u2190 (hanashi tai) and (eigo wo) \u2190 (hanashi tai).", 
        "173": "We perform this conversion for both gold and output CCG trees and calculate the (unlabeled) attachment accuracy.", 
        "174": "Though this is imperfect, it can detect important parse errors such as attachment errors and thus can be a good proxy for the performance as a CCG parser.", 
        "175": "6.3 English Parsing Results  Effect of Dependency We first see how the dependency components added in our model affect the performance.", 
        "176": "Table 1 shows the results on the development set with the several configurations, in which \u201cw/o dep\u201d means discarding the depen-\n11http://taku910.github.io/cabocha/\ndency terms of the model and applying the attach low heuristics (Section 1) instead (i.e., a supertagfactored model; Section 2.1).", 
        "177": "We can see that for both LEWISRULE and HEADFIRST, adding dependency terms improves the performance.", 
        "178": "Choice of Dependency Conversion Rule To our surprise, our simple HEADFIRST strategy always leads to better results than the linguistically motivated LEWISRULE.", 
        "179": "The absolute improvements by tri-training are equally large (about 1.0 points), suggesting that our model with dependencies can also benefit from the silver data.", 
        "180": "Excluding Normal Form Constraints One advantage of HEADFIRST is that the direction of arcs is always right, making the structures simpler and more parsable (Section 5).", 
        "181": "From another viewpoint, this fixed direction means that the constituent structure behind a (head first) dependency tree is unique.", 
        "182": "Since the constituent structures of CCGbank trees basically follow the normal form (NF), we hypothesize that the model learned with HEADFIRST has an ability to force the outputs in NF automatically.", 
        "183": "We summarize the results without the NF constraints in Table 2, which shows that the above argument is correct; the number of violating NF rules on the outputs of HEADFIRST is much smaller than that of LEWISRULE (89 vs. 283).", 
        "184": "Interestingly the scores of HEADFIRST slightly increase from the models with NF (e.g., 86.8 vs. 86.6 for CCGbank), suggesting that the NF constraints hinder the search of HEADFIRST models occasionally.", 
        "185": "Results on Test Set Parsing results on the test set (Section 23) are shown in Table 3, where we compare our best performing HEADFIRST dependency model without NF constraints with the several existing parsers.", 
        "186": "In the CCGbank experi-\nment, our parser shows the better result than all the baseline parsers except C&C with an LSTM supertagger (Vaswani et al., 2016).", 
        "187": "Our parser outperforms EasySRL by 0.5% and our reimplementation of that parser (EasySRL reimpl) by 0.9% in terms of labeled F1.", 
        "188": "In the tri-training experiment, our parser shows much increased performance of 88.8% labeled F1 and 94.0% unlabeled F1, outperforming the current state-of-theart neuralccg (Lee et al., 2016) that uses recursive neural networks by 0.1 point and 0.3 point in terms of labeled and unlabeled F1.", 
        "189": "This is the best reported F1 in English CCG parsing.", 
        "190": "Efficiency Comparison We compare the efficiency of our parser with neuralccg and EasySRL reimpl.12 The results are shown in Table 4.", 
        "191": "For the overall speed (the third row), our parser is faster than neuralccg although lags behind EasySRL reimpl.", 
        "192": "Inspecting the details, our supertagger runs slower than those of neuralccg and EasySRL reimpl, while in A* search our parser processes over 7 times more sentences than neuralccg.", 
        "193": "The delay in supertagging can be attributed to several factors, in particular the differences in network architectures including the number of biLSTM layers (4 vs. 2) and the use of bilinear transformation instead of linear one.", 
        "194": "There are also many implementation differences in our parser (C++A* parser with neural network model implemented with Chainer (Tokui et al., 2015)) and neuralccg (Java parser with C++ TensorFlow (Abadi et al., 2015) supertagger and recursive neural model in C++ DyNet (Neubig et al., 2017)).", 
        "195": "6.4 Japanese Parsing Result  We show the results of the Japanese parsing experiment in Table 5.", 
        "196": "The simple application of Lewis\n12This experiment is performed on a laptop with 4-thread 2.0 GHz CPU.", 
        "197": "et al.", 
        "198": "(2016) (Supertag model) is not effective for Japanese, showing the lowest attachment score of 81.5%.", 
        "199": "We observe a performance boost with our method, especially with HEADFINAL dependencies, which outperforms the baseline shift-reduce parser by 1.1 points on category assignments and 4.0 points on bunsetsu dependencies.", 
        "200": "The degraded results of the simple application of the supertag-factored model can be attributed to the fact that the structure of a Japanese sentence is still highly ambiguous given the supertags (Figure 5).", 
        "201": "This is particularly the case in constructions where phrasal adverbial/adnominal modifiers (with the supertag S/S) are involved.", 
        "202": "The result suggests the importance of modeling dependencies in some languages, at least Japanese.", 
        "203": "7 Related Work  There is some past work that utilizes dependencies in lexicalized grammar parsing, which we review briefly here.", 
        "204": "For Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag (1994)), there are studies to use the predicted dependency structure to improve HPSG parsing accuracy.", 
        "205": "Sagae et al.", 
        "206": "(2007) use dependencies to constrain the form of the output tree.", 
        "207": "As in our method, for every rule (schema) application they define which child becomes the head and impose a soft constraint that these dependencies agree with the output of the dependency parser.", 
        "208": "Our method is different\nin that we do not use the one-best dependency structure alone, but rather we search for a CCG tree that is optimal in terms of dependencies and CCG supertags.", 
        "209": "Zhang et al.", 
        "210": "(2010) use the syntactic dependencies in a different way, and show that dependency-based features are useful for predicting HPSG supertags.", 
        "211": "In the CCG parsing literature, some work optimizes a dependency model, instead of supertags or a derivation (Clark and Curran, 2007; Xu et al., 2014).", 
        "212": "This approach is reasonable given that the objective matches the evaluation metric.", 
        "213": "Instead of modeling dependencies alone, our method finds a CCG derivation that has a higher dependency score.", 
        "214": "Lewis et al.", 
        "215": "(2015) present a joint model of CCG parsing and semantic role labeling (SRL), which is closely related to our approach.", 
        "216": "They map each CCG semantic dependency to an SRL relation, for which they give the A* upper bound by the score from a predicate to the most probable argument.", 
        "217": "Our approach is similar; the largest difference is that we instead model syntactic dependencies from each token to its head, and this is the key to our success.", 
        "218": "Since dependency parsing can be formulated as independent head selections similar to tagging, we can build the entire model on LSTMs to exploit features from the whole sentence.", 
        "219": "This formulation is not straightforward in the case of multi-headed semantic dependencies in their model.", 
        "220": "8 Conclusion  We have presented a new A* CCG parsing method, in which the probability of a CCG tree is decomposed into local factors of the CCG categories and its dependency structure.", 
        "221": "By explicitly modeling the dependency structure, we do not require any deterministic heuristics to resolve attachment ambiguities, and keep the model locally factored so that all the probabilities can be precomputed before running the search.", 
        "222": "Our parser efficiently finds the optimal parse and achieves the state-of-the-art performance in both English and Japanese parsing.", 
        "223": "Acknowledgments  We are grateful to Mike Lewis for answering our questions and your Github repository from which we learned many things.", 
        "224": "We also thank Yuichiro Sawai for the faster LSTM implementation.", 
        "225": "This work was in part supported by JSPS\nKAKENHI Grant Number 16H06981, and also by JST CREST Grant Number JPMJCR1301."
    }, 
    "document_id": "P17-1026.pdf.json"
}
