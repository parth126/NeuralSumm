{
    "abstract_sentences": {
        "1": "Language models are typically applied at the sentence level, without access to the broader document context.", 
        "2": "We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence.", 
        "3": "Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model.", 
        "4": "Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 355\u2013365 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1033  1 Introduction  Topic models provide a powerful tool for extracting the macro-level content structure of a document collection in the form of the latent topics (usually in the form of multinomial distributions over terms), with a plethora of applications in NLP (Hall et al., 2008; Newman et al., 2010a; Wang and McCallum, 2006).", 
        "2": "A myriad of variants of the classical LDA method (Blei et al., 2003) have been proposed, including recent work on neural topic models (Cao et al., 2015; Wan et al., 2012; Larochelle and Lauly, 2012; Hinton and Salakhutdinov, 2009).", 
        "3": "Separately, language models have long been a foundational component of any NLP task involving generation or textual normalisation of a noisy input (including speech, OCR and the processing of social media text).", 
        "4": "The primary purpose of a language model is to predict the probability of a\nspan of text, traditionally at the sentence level, under the assumption that sentences are independent of one another, although recent work has started using broader local context such as the preceding sentences (Wang and Cho, 2016; Ji et al., 2016).", 
        "5": "In this paper, we combine the benefits of a topic model and language model in proposing a topically-driven language model, whereby we jointly learn topics and word sequence information.", 
        "6": "This allows us to both sensitise the predictions of the language model to the larger document narrative using topics, and to generate topics which are better sensitised to local context and are hence more coherent and interpretable.", 
        "7": "Our model has two components: a language model and a topic model.", 
        "8": "We implement both components using neural networks, and train them jointly by treating each component as a sub-task in a multi-task learning setting.", 
        "9": "We show that our model is superior to other language models that leverage additional context, and that the generated topics are potentially more coherent than LDA topics.", 
        "10": "The architecture of the model provides an extra dimensionality of topic interpretability, in supporting the generation of sentences from a topic (or mix of topics).", 
        "11": "It is also highly flexible, in its ability to be supervised and incorporate side information, which we show to further improve language model performance.", 
        "12": "An open source implementation of our model is available at: https://github.com/jhlau/ topically-driven-language-model.", 
        "13": "2 Related Work  Griffiths et al.", 
        "14": "(2004) propose a model that learns topics and word dependencies using a Bayesian framework.", 
        "15": "Word generation is driven by either LDA or an HMM.", 
        "16": "For LDA, a word is generated based on a sampled topic in the document.", 
        "17": "For the\n355\nHMM, a word is conditioned on previous words.", 
        "18": "A key difference over our model is that their language model is driven by an HMM, which uses a fixed window and is therefore unable to track longrange dependencies.", 
        "19": "Cao et al.", 
        "20": "(2015) relate the topic model view of documents and words \u2014 documents having a multinomial distribution over topics and topics having a multinomial distributional over words \u2014 from a neural network perspective by embedding these relationships in differentiable functions.", 
        "21": "With that, the model lost the stochasticity and Bayesian inference of LDA but gained non-linear complex representations.", 
        "22": "The authors further propose extensions to the model to do supervised learning where document labels are given.", 
        "23": "Wang and Cho (2016) and Ji et al.", 
        "24": "(2016) relax the sentence independence assumption in language modelling, and use preceeding sentences as additional context.", 
        "25": "By treating words in preceeding sentences as a bag of words, Wang and Cho (2016) use an attentional mechanism to focus on these words when predicting the next word.", 
        "26": "The authors show that the incorporation of additional\ncontext helps language models.", 
        "27": "3 Architecture  The architecture of the proposed topically-driven language model (henceforth \u201ctdlm\u201d) is illustrated in Figure 1.", 
        "28": "There are two components in tdlm: a language model and a topic model.", 
        "29": "The language model is designed to capture word relations in sentences, while the topic model learns topical information in documents.", 
        "30": "The topic model works like an auto-encoder, where it is given the document words as input and optimised to predict them.", 
        "31": "The topic model takes in word embeddings of a document and generates a document vector using a convolutional network.", 
        "32": "Given the document vector, we associate it with the topics via an attention scheme to compute a weighted mean of topic vectors, which is then used to predict a word in the document.", 
        "33": "The language model is a standard LSTM language model (Hochreiter and Schmidhuber, 1997; Mikolov et al., 2010), but it incorporates the weighted topic vector generated by the topic model to predict succeeding words.", 
        "34": "Marrying the language and topic models allows the language model to be topically driven, i.e.", 
        "35": "it models not just word contexts but also the document context where the sentence occurs, in the form of topics.", 
        "36": "3.1 Topic Model Component  Let xi \u2208 Re be the e-dimensional word vector for the i-th word in the document.", 
        "37": "A document of n words is represented as a concatenation of its word vectors:\nx1:n = x1 \u2295 x2 \u2295 ...\u2295 xn\nwhere \u2295 denotes the concatenation operator.", 
        "38": "We use a number of convolutional filters to process the word vectors, but for clarity we will explain the network with one filter.", 
        "39": "Let wv \u2208 Reh be a convolutional filter which we apply to a window of hwords to generate a feature.", 
        "40": "A feature ci for a window of words xi:i+h\u22121 is given as follows:\nci = I(w \u1d40 vxi:i+h\u22121 + bv)\nwhere bv is a bias term and I is the identity function.1 A feature map c is a collection of features computed from all windows of words:\nc = [c1, c2, ..., cn\u2212h+1]\nwhere c \u2208 Rn\u2212h+1.", 
        "41": "To capture the most salient features in c, we apply a max-over-time pooling operation (Collobert et al., 2011), yielding a scalar:\nd = max i ci\nIn the case where we use a filters, we have d \u2208 Ra, and this constitutes the vector representation of the document generated by the convolutional and max-over-time pooling network.", 
        "42": "The topic vectors are stored in two lookup tables A \u2208 Rk\u00d7a (input vector) and B \u2208 Rk\u00d7b (output vector), where k is the number of topics, and a and b are the dimensions of the topic vectors.", 
        "43": "To align the document vector d with the topics, we compute an attention vector which is used to\n1A non-linear function is typically used here, but preliminary experiments suggest that the identity function works best for tdlm.", 
        "44": "compute a document-topic representation:2\np = softmax(Ad) (1)\ns = B\u1d40p (2)\nwhere p \u2208 Rk and s \u2208 Rb.", 
        "45": "Intuitively, s is a weighted mean of topic vectors, with the weighting given by the attention p. This is inspired by the generative process of LDA, whereby documents are defined as having a multinomial distribution over topics.", 
        "46": "Finally s is connected to a dense layer with softmax output to predict each word in the document, where each word is generated independently as a unigram bag-of-words, and the model is optimised using categorical cross-entropy loss.", 
        "47": "In practice, to improve efficiency we compute loss for predicting a sequence of m1 words in the document, where m1 is a hyper-parameter.", 
        "48": "3.2 Language Model Component  The language model is implemented using LSTM units (Hochreiter and Schmidhuber, 1997):\nit = \u03c3(Wivt +Uiht\u22121 + bi)\nft = \u03c3(Wfvt +Ufht\u22121 + bf )\not = \u03c3(Wovt +Uoht\u22121 + bi)\nc\u0302t = tanh(Wcvt +Ucht\u22121 + bc)\nct = ft ct\u22121 + it c\u0302t ht = ot tanh(ct)\nwhere denotes element-wise product; it, ft, ot are the input, forget and output activations respectively at time step t; and vt, ht and ct are the input word embedding, LSTM hidden state, and cell state, respectively.", 
        "49": "Hereinafter W, U and b are used to refer to the model parameters.", 
        "50": "Traditionally, a language model operates at the sentence level, predicting the next word given its history of words in the sentence.", 
        "51": "The language model of tdlm incorporates topical information by assimilating the document-topic representation (s) with the hidden output of the LSTM (ht) at each time step t. To prevent tdlm from memorising the next word via the topic model network, we exclude the current sentence from the document context.", 
        "52": "2The attention mechanism was inspired by memory networks (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Tran et al., 2016).", 
        "53": "We explored various attention styles (including traditional schemes which use one vector for a topic), but found this approach to work best.", 
        "54": "We use a gating unit similar to a GRU (Cho et al., 2014; Chung et al., 2014) to allow tdlm to learn the degree of influence of topical information on the language model:\nzt = \u03c3(Wzs+Uzht + bz)\nrt = \u03c3(Wrs+Urht + br)\nh\u0302t = tanh(Whs+Uh(rt ht) + bh) h\u2032t = (1\u2212 zt) ht + zt h\u0302t\n(3)\nwhere zt and rt are the update and reset gate activations respectively at timestep t. The new hidden state h\u2032t is connected to a dense layer with linear transformation and softmax output to predict the next word, and the model is optimised using standard categorical cross-entropy loss.", 
        "55": "3.3 Training and Regularisation  tdlm is trained using minibatches and SGD.3 For the language model, a minibatch consists of a batch of sentences, while for the topic model it is a batch of documents (each predicting a sequence of m1 words).", 
        "56": "We treat the language and topic models as subtasks in a multi-task learning setting, and train them jointly using categorical cross-entropy loss.", 
        "57": "Most parameters in the topic model are shared by the language model, as illustrated by their scopes (dotted lines) in Figure 1.", 
        "58": "Hyper-parameters of tdlm are detailed in Table 1.", 
        "59": "Word embeddings for the topic model and language model components are not shared, although their dimensions are the same (e).4 For m1, m2 and m3, sequences/documents shorter than these thresholds are padded.", 
        "60": "Sentences longer than m2 are broken into multiple sequences, and documents longer than m3 are truncated.", 
        "61": "Optimal hyper-parameter settings are tuned using the development set; the presented values are used for experiments in Sections 4 and 5.", 
        "62": "To regularise tdlm, we use dropout regularisation (Srivastava et al., 2014).", 
        "63": "We apply dropout to d and s in the topic model, and to the input word embedding and hidden output of the LSTM in the language model (Pham et al., 2013; Zaremba et al., 2014).", 
        "64": "4 Language Model Evaluation  We use standard language model perplexity as the evaluation metric.", 
        "65": "In terms of dataset, we use doc-\n3We use Adam as the optimiser (Kingma and Ba, 2014).", 
        "66": "4Word embeddings are updated during training.", 
        "67": "ument collections from 3 sources: APNEWS, IMDB and BNC.", 
        "68": "APNEWS is a collection of Associated Press5 news articles from 2009 to 2016.", 
        "69": "IMDB is a set of movie reviews collected by Maas et al.", 
        "70": "(2011).", 
        "71": "BNC is the written portion of the British National Corpus (BNC Consortium, 2007), which contains excerpts from journals, books, letters, essays, memoranda, news and other types of text.", 
        "72": "For APNEWS and BNC, we randomly sub-sample a set of documents for our experiments.", 
        "73": "For preprocessing, we tokenise words and sentences using Stanford CoreNLP (Klein and Manning, 2003).", 
        "74": "We lowercase all word tokens, filter word types that occur less than 10 times, and exclude the top 0.1% most frequent word types.6 We additionally remove stopwords for the topic model document context.7 All datasets are partitioned into training, development and test sets; preprocessed dataset statistics are presented in Table 2.", 
        "75": "We tune hyper-parameters of tdlm based on development set language model perplexity.", 
        "76": "In general, we find that optimal settings are fairly robust across collections, with the exception of m3, as document length is collection dependent; optimal hyper-parameter values are given in Table 1.", 
        "77": "In terms of LSTM size, we explore 2 settings: a small model with 1 LSTM layer and 600 hidden units, and a large model with 2 layers and 900 hidden units.8 For the topic number, we experiment with 50, 100 and 150 topics.", 
        "78": "Word embeddings are pre-trained 300-dimension word2vec Google News vectors.9\nFor comparison, we compare tdlm with:10\nvanilla-lstm: A standard LSTM language model, using the same tdlm hyper-parameters where applicable.", 
        "79": "This is the baseline model.", 
        "80": "lclm: A larger context language model that incorporates context from preceding sentences (Wang and Cho, 2016), by treating the preceding sentence as a bag of words, and using an\n5https://www.ap.org/en-gb/.", 
        "81": "6For the topic model, we remove word tokens that correspond to these filtered word types; for the language model we represent them as \u3008unk\u3009 tokens (as for unseen words in test).", 
        "82": "7We use Mallet\u2019s stopword list: https://github.", 
        "83": "com/mimno/Mallet/tree/master/stoplists.", 
        "84": "8Multi-layer LSTMs are vanilla stacked LSTMs without skip connections (Gers and Schmidhuber, 2000) or depthgating (Yao et al., 2015).", 
        "85": "9https://code.google.com/archive/p/ word2vec/.", 
        "86": "10Note that all models use the same pre-trained word2vec vectors.", 
        "87": "attentional mechanism when predicting the next word.", 
        "88": "An additional hyper-parameter in lclm is the number of preceeding sentences to incorporate, which we tune based on a development set (to 4 sentences in each case).", 
        "89": "All other hyperparameters (such as nbatch , e, nepoch , k2) are the same as tdlm.", 
        "90": "lstm+lda: A standard LSTM language model that incorporates LDA topic information.", 
        "91": "We first train an LDA model (Blei et al., 2003; Griffiths and Steyvers, 2004) to learn 50/100/150 topics for APNEWS, IMDB and BNC.11 For a document, the LSTM incorporates the LDA topic distribution (q) by concatenating it with the output hidden state (ht) to predict the next word (i.e.", 
        "92": "h\u2032t = ht \u2295 q).", 
        "93": "That is, it incorporates topical information into the language model, but unlike tdlm the language model and topic model are trained separately.", 
        "94": "We present language model perplexity performance in Table 3.", 
        "95": "All models outperform the baseline vanilla-lstm, with tdlm performing the\n11Based on Gibbs sampling; \u03b1 = 0.1, \u03b2 = 0.01.\nbest across all collections.", 
        "96": "lclm is competitive over the BNC, although the superiority of tdlm for the other collections is substantial.", 
        "97": "lstm+lda performs relatively well over APNEWS and IMDB, but very poorly over BNC.", 
        "98": "The strong performance of tdlm over lclm suggests that compressing document context into topics benefits language modelling more than using extra context words directly.12 Overall, our results show that topical information can help language modelling and that joint inference of topic and language model produces the best results.", 
        "99": "5 Topic Model Evaluation  We saw that tdlm performs well as a language model, but it is also a topic model, and like LDA it produces: (1) a probability distribution over topics for each document (Equation (1)); and (2) a probability distribution over word types for each topic.", 
        "100": "12The context size of lclm (4 sentences) is technically smaller than tdlm (full document), however, note that increasing the context size does not benefit lclm, as the context size of 4 gives the best performance.", 
        "101": "Recall that s is a weighted mean of topic vectors for a document (Equation (2)).", 
        "102": "Generating the vocabulary distribution for a particular topic is therefore trivial: we can do so by treating s as having maximum weight (1.0) for the topic of interest, and no weight (0.0) for all other topics.", 
        "103": "Let Bt denote the topic output vector for the t-th topic.", 
        "104": "To generate the multinomial distribution over word types for the t-th topic, we replace s with Bt before computing the softmax over the vocabulary.", 
        "105": "Topic models are traditionally evaluated using model perplexity.", 
        "106": "There are various ways to estimate test perplexity (Wallach et al., 2009), but Chang et al.", 
        "107": "(2009) show that perplexity does not correlate with the coherence of the generated topics.", 
        "108": "Newman et al.", 
        "109": "(2010b); Mimno et al.", 
        "110": "(2011); Aletras and Stevenson (2013) propose automatic approaches to computing topic coherence, and Lau et al.", 
        "111": "(2014) summarises these methods to understand their differences.", 
        "112": "We propose using automatic topic coherence as a means to evaluate the topic model aspect of tdlm.", 
        "113": "Following Lau et al.", 
        "114": "(2014), we compute topic coherence using normalised PMI (\u201cNPMI\u201d) scores.", 
        "115": "Given the top-n words of a topic, coherence is computed based on the sum of pair-\nwise NPMI scores between topic words, where the word probabilities used in the NPMI calculation are based on co-occurrence statistics mined from English Wikipedia with a sliding window (Newman et al., 2010b; Lau et al., 2014).13\nBased on the findings of Lau and Baldwin (2016), we average topic coherence over the top5/10/15/20 topic words.", 
        "116": "To aggregate topic coherence scores for a model, we calculate the mean coherence over topics.", 
        "117": "In terms of datasets, we use the same document collections (APNEWS, IMDB and BNC) as the language model experiments (Section 4).", 
        "118": "We use the same hyper-parameter settings for tdlm and do not tune them.", 
        "119": "For comparison, we use the following topic models:\nlda: We use a LDA model as a baseline topic model.", 
        "120": "We use the same LDA models as were used to learn topic distributions for lstm+lda (Section 4).", 
        "121": "13We use this toolkit to compute topic coherence: https://github.com/jhlau/topic_ interpretability.", 
        "122": "ntm: ntm is a neural topic model proposed by Cao et al.", 
        "123": "(2015).", 
        "124": "The document-topic and topicword multinomials are expressed from a neural network perspective using differentiable functions.", 
        "125": "Model hyper-parameters are tuned using development loss.", 
        "126": "Topic model performance is presented in Table 4.", 
        "127": "There are two models of tdlm (tdlm-small and tdlm-large), which specify the size of its LSTM model (1 layer+600 hidden vs. 2 layers+900 hidden; see Section 4).", 
        "128": "tdlm achieves encouraging results: it has the best performance over APNEWS, and is competitive over IMDB.", 
        "129": "lda, however, produces more coherent topics over BNC.", 
        "130": "Interestingly, coherence appears to increase as the topic number increases for lda, but the trend is less pronounced for tdlm.", 
        "131": "ntm performs the worst of the 3 topic models, and manual inspection reveals that topics are in general not very interpretable.", 
        "132": "Overall, the results suggest that tdlm topics are competitive: at best they are more coherent than lda topics, and at worst they are as good as lda topics.", 
        "133": "To better understand the spread of coherence scores and impact of outliers, we present box plots for all models (number of topics = 100) over the 3 domains in Figure 2.", 
        "134": "Across all domains, ntm has poor performance and larger spread of scores.", 
        "135": "The difference between lda and tdlm is small (tdlm > lda in APNEWS, but lda < tdlm in BNC), which is consistent with our previous observation that tdlm topics are competitive with lda topics.", 
        "136": "6 Extensions  One strength of tdlm is its flexibility, owing to it taking the form of a neural network.", 
        "137": "To showcase this flexibility, we explore two simple extensions of tdlm, where we: (1) build a supervised model using document labels (Section 6.1); and (2) incorporate additional document metadata (Section 6.2).", 
        "138": "6.1 Supervised Model  In datasets where document labels are known, supervised topic model extensions are designed to leverage the additional information to improve modelling quality.", 
        "139": "The supervised setting also has an additional advantage in that model evaluation is simpler, since models can be quantitatively assessed via classification accuracy.", 
        "140": "To incorporate supervised document labels, we treat document classification as another sub-task in tdlm.", 
        "141": "Given a document and its label, we feed the document through the topic model network to generate the document-topic representation s, and connect it to another dense layer with softmax output to generate the probability distribution over classes.", 
        "142": "During training, we have additional minibatches for the documents.", 
        "143": "We start the document classification training after the topic and language models have completed training in each epoch.", 
        "144": "We use 20NEWS in this experiment, which is a popular dataset for text classification.", 
        "145": "20NEWS is a collection of forum-like messages from 20 newsgroups categories.", 
        "146": "We use the \u201cbydate\u201d version of the dataset, where the train and test partition is separated by a specific date.", 
        "147": "We sample 2K documents from the training set to create the development set.", 
        "148": "For preprocessing we tokenise words and sentence using Stanford CoreNLP (Klein and Manning, 2003), and lowercase all words.", 
        "149": "As with previous experiments (Section 4) we additionally filter low/high frequency word types and stopwords.", 
        "150": "Preprocessed dataset statistics are presented in Table 5.", 
        "151": "For comparison, we use the same two topic\nTopic No.", 
        "152": "Metadata Coherence Perplexity\nmodels as in Section 5: ntm and lda.", 
        "153": "Both ntm and lda have natural supervised extensions (Cao et al., 2015; McAuliffe and Blei, 2008) for incorporating document labels.", 
        "154": "For this task, we tune the model hyper-parameters based on development accuracy.14 Classification accuracy for all models is presented in Table 6.", 
        "155": "We present tdlm results using only the small setting of LSTM (1 layer + 600 hidden), as we found there is little gain when using a larger LSTM.", 
        "156": "ntm performs very strongly, outperforming both lda and tdlm by a substantial margin.", 
        "157": "Comparing lda and tdlm, tdlm achieves better performance, especially when there is a smaller number of topics.", 
        "158": "Upon inspection of the topics we found that ntm topics are much less coherent than those of lda and tdlm, consistent with our observations from Section 5.", 
        "159": "14Most hyper-parameter values for tdlm are similar to those used in the language and topic model experiments; the only exceptions are: a = 80, b = 100, nepoch = 20, m3 = 150.", 
        "160": "The increase in parameters is unsurprising, as the additional supervision provides more constraint to the model.", 
        "161": "6.2 Incorporating Document Metadata  In APNEWS, each news article contains additional document metadata, including subject classification tags, such as \u201cGeneral News\u201d, \u201cAccidents and Disasters\u201d, and \u201cMilitary and Defense\u201d.", 
        "162": "We present an extension to incorporate document metadata in tdlm to demonstrate its flexibility in integrating this additional information.", 
        "163": "As some of the documents in our original APNEWS sample were missing tags, we re-sampled a set of APNEWS articles of the same size as our original, all of which have tags.", 
        "164": "In total, approximately 1500 unique tags can be found among the training articles.", 
        "165": "To incorporate these tags, we represent each of them as a learnable vector and concatenate it with the document vector before computing the attention distribution.", 
        "166": "Let zi \u2208 Rf denote the f -dimension vector for the i-th tag.", 
        "167": "For the j-th document, we sum up all tags associated with it:\ne =\nntags\u2211\ni=1\nI(i, j)zi\nwhere ntags is the total number of unique tags, and function I(i, j) returns 1 is the i-th tag is in the j-th document or 0 otherwise.", 
        "168": "We compute d as before (Section 3.1), and concatenate it with the summed tag vector: d\u2032 = d\u2295 e.\nWe train two versions of tdlm on the new APNEWS dataset: (1) the vanilla version that ignores the tag information; and (2) the extended version which incorporates tag information.15 We exper-\n15Model hyper-parameters are the same as the ones used in the language (Section 4) and topic model (Section 5) experiments.", 
        "169": "imented with a few values for the tag vector size (f ) and find that a small value works well; in the following experiments we use f = 5.", 
        "170": "We evaluate the models based on language model perplexity and topic model coherence, and present the results in Table 7.16\nIn terms of language model perplexity, we see a consistent improvement over different topic settings, suggesting that the incorporation of tags improves modelling.", 
        "171": "In terms of topic coherence, there is a small but encouraging improvement (with one exception).", 
        "172": "To investigate whether the vectors learnt for these tags are meaningful, we plot the top-14 most frequent tags in Figure 3.17 The plot seems reasonable: there are a few related tags that are close to each other, e.g.", 
        "173": "\u201cState government\u201d and \u201cGovernment and politics\u201d; \u201cCrime\u201d and \u201cViolent Crime\u201d; and \u201cSocial issues\u201d and \u201cSocial affairs\u201d.", 
        "174": "7 Discussion  Topics generated by topic models are typically interpreted by way of their top-N highest probability words.", 
        "175": "In tdlm, we can additionally generate sentences related to the topic, providing another way to understand the topics.", 
        "176": "To do this, we can constrain the topic vector for the language model to be the topic output vector of a particular topic (Equation (3)).", 
        "177": "We present 4 topics from a APNEWS model (k = 100; LSTM size = \u201clarge\u201d) and 3 randomly generated sentences conditioned on each\n16As the vanilla tdlm is trained on the new APNEWS dataset, the numbers are slightly different to those in Tables 3 and 4.", 
        "178": "17The 5-dimensional vectors are compressed using PCA.", 
        "179": "topic in Table 8.18 The generated sentences highlight the content of the topics, providing another interpretable aspect for the topics.", 
        "180": "These results also reinforce that the language model is driven by topics.", 
        "181": "8 Conclusion  We propose tdlm, a topically driven neural language model.", 
        "182": "tdlm has two components: a language model and a topic model, which are jointly trained using a neural network.", 
        "183": "We demonstrate that tdlm outperforms a state-of-the-art language model that incorporates larger context, and that its topics are potentially more coherent than LDA topics.", 
        "184": "We additionally propose simple extensions of tdlm to incorporate information such as document labels and metadata, and achieved encouraging results.", 
        "185": "Acknowledgments  We thank the anonymous reviewers for their insightful comments and valuable suggestions.", 
        "186": "This work was funded in part by the Australian Research Council."
    }, 
    "document_id": "P17-1033.pdf.json"
}
