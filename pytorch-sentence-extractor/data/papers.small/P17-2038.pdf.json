{
    "abstract_sentences": {
        "1": "Automatic identification of good arguments on a controversial topic has applications in civics and education, to name a few.", 
        "2": "While in the civics context it might be acceptable to create separate models for each topic, in the context of scoring of students\u2019 writing there is a preference for a single model that applies to all responses.", 
        "3": "Given that good arguments for one topic are likely to be irrelevant for another, is a single model for detecting good arguments a contradiction in terms?", 
        "4": "We investigate the extent to which it is possible to close the performance gap between topicspecific and across-topics models for identification of good arguments."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 244\u2013249 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2038  1 Introduction & Related Work  Argumentation is an important skill in higher education and the workplace; students are expected to show sound reasoning and use relevant evidence (Council of Chief State School Officers & National Governors Association, 2010).", 
        "2": "The increase in argumentative writing tasks, in both instructional and assessment contexts, results in a high demand for automated feedback on and scoring of arguments.", 
        "3": "Automated analysis of argumentative writing has mostly concentrated on argument structure \u2013 namely, presence of claims and premises, and relationships between them (Ghosh et al., 2016; Nguyen and Litman, 2016; Persing and Ng, 2016; Ong et al., 2014; Stab and Gurevych, 2014).", 
        "4": "Addressing the content of arguments in on-line debates, Habernal and Gurevych (2016) ranked arguments on the same topic by convincingness; they showed that convincingness can be automatically predicted, to an extent, in a cross-topics fashion, as\nthey trained their systems on 31 debates and tested on a new one.", 
        "5": "Swanson et al.", 
        "6": "(2015) reported that annotation of argument quality is challenging, with inter-annotator agreement (ICC) around 0.40.", 
        "7": "They also showed that automated acrosstopics prediction is very hard; for some topics, no effective prediction was achieved.", 
        "8": "Song et al.", 
        "9": "(2014) developed an annotation protocol for analyzing argument critiques in students\u2019 essays, drawing on the theory of argumentation schemes (Walton et al., 2008; Walton, 1996).", 
        "10": "According to this theory, different types of arguments invite specific types of critiques.", 
        "11": "For example, an argument from authority made in the prompt \u2013 According to X, Y is the case \u2013 avails critiques along the lines of whether X has the necessary knowledge and is an unbiased source of information about Y. Analyzing prompts used in an assessment of argument critique skills, Song et al.", 
        "12": "(2014) identified a number of common schemes, such as arguments from policy, sample, example, and used the argumentation schemes theory to specify what critiques would count as \u201cgood\u201d for arguments from the given scheme.", 
        "13": "Once a prompt is associated with a specific set of argumentation schemes, it follows that those critiques that count as good under one of the schemes used in the prompt would be considered as good critiques in essays responding to that prompt.", 
        "14": "The goal of the annotation was to identify all sentences in an essay that participate in making a good critique, according to the above definition.", 
        "15": "Every sentence in an essay is annotated with the label of the critique that it raises, or \u201cgeneric\u201d if none.", 
        "16": "In the current paper, we build upon this earlier work.", 
        "17": "In practical large-scale automated scoring contexts, new essay prompts are often introduced without rebuilding the scoring system, which is typically subject to a periodic release schedule.", 
        "18": "Therefore, the assumption that the system\n244\nwill have seen essays responding to each of the prompts it could encounter at deployment time is often unwarranted.", 
        "19": "Further, not only should a system be able to handle responses to an unseen prompt, it must do it gracefully, since a large disparity in the system\u2019s performance across different prompts might raise fairness concerns.", 
        "20": "Our practical goal is thus a development of a robust argument critique analysis system for essays.", 
        "21": "Our theoretical goal is the investigation of the extent that it is at all possible to capture aspects of argument content in a fashion that would generalize across various essay topics.", 
        "22": "2 Annotation  We used Song et al.", 
        "23": "(2014) annotation protocol, adapting as needed to cover additional argumentation schemes.", 
        "24": "Song et al.", 
        "25": "(2017) provides a detailed exposition of the argumentation-schemebased analysis of a number of prompts and of the annotation process.", 
        "26": "For the current study, we used a simplified version of the annotation where sentences are labeled as non-generic (namely, containing a good critique according to some argumentation scheme), or generic (all the rest of the sentences in the essay).", 
        "27": "The average interannotator agreement on the \u201cgeneric\u201d vs \u201cnongeneric\u201d sentence-level classification is k=0.67.", 
        "28": "The \u201cnon-generic\u201d category covers all sentences that raise a good critique; everything else is \u201cgeneric\u201d.", 
        "29": "The latter category thus includes, for example, sentences that rehash the argument in the prompt, provide critical but vague statements that cannot be clearly identified as a specific critique from our list (such as \u201cThe author should provide more information\u201d), provide specific critical statements that aren\u2019t valid arguments.", 
        "30": "For example, in response to the prompt that states that the new policy led to a 10% decrease in unemployment in four years, one writer argued that \u201cPeople who were unemployed could have died within the last four years and that is why there is a decrease.\u201d While trying to provide an alternative explanation to the putative effect of the policy is a reasonable move, this is not a valid argument, because it is exceedingly unlikely that unemployed people died in such disproportionate numbers to have such a big impact on the unemployment statistics.", 
        "31": "3 Data  For this study, we use a same-topic and an acrosstopics sets of college-level argument critique essays.", 
        "32": "The first is used to set the bar for the performance in the context where the training and the testing essays respond to the same prompt.", 
        "33": "The second is the main dataset focused on generalization across prompts.", 
        "34": "3.1 Same-topic  A total of 900 essays were annotated, 300 essays for each of 3 prompts.", 
        "35": "For each prompt, we train a model on 260 responses and test on 40.", 
        "36": "The training sets per prompt contain on average 2,700 sentences, of which 38% are classified as containing good argument critiques.", 
        "37": "Two of the three same-topic sets were used previously in Song et al.", 
        "38": "(2014).", 
        "39": "3.2 Across-topics  A total of 500 essays were annotated, 50 essays for each of 10 prompts.", 
        "40": "We perform 10-fold cross validation, training on 9 prompts and testing on the 10th, modeling a scenario of generalization to an unknown topic.", 
        "41": "There are, on average, 5,492 sentences available for training, of which 3,917 (42%) are classified as containing good critiques.", 
        "42": "4 How far do we get with pure content?", 
        "43": "Given that making a good critique is presumably mostly about saying the right things, we expect lexical models to perform well in sametopic context and badly in the across-topics one.", 
        "44": "We evaluated 1-3grams, 1-4grams, and 1-5grams models learned using a logistic regression classifier.", 
        "45": "Differences in performance tended to be in the third or fourth decimal digit; we therefore report results for 1-3grams only.", 
        "46": "Classification accuracies are shown in row 2 of Table 1, following the majority baseline (row 1), in both same-topic and across-topics scenarios.", 
        "47": "We show average performance (Av) as well as the worst performance (Min) on 3 prompts (same-topic) and on 10 prompts (across topics).", 
        "48": "We also evaluated models built using chi-square based feature selection (f.s.", 
        "49": "), eliminating all features with p value above 0.05 (row 3 in Table 1).", 
        "50": "For the same-topic context, lexical features perform at .738.", 
        "51": "As expected, lexical features are much less effective across topics, with average\nperformance of only .645.", 
        "52": "We observe substantial gaps of 9 (.738 vs .645) and 8 (.679 vs .604) accuracy points, for average and worst case, respectively, between same-topic and across-topics scenarios for ngram models.", 
        "53": "Feature selection is ineffective in both scenarios (compare rows 2 and 3 in Table 1).", 
        "54": "5 How far do we get with pure structure?", 
        "55": "An approach that is perhaps better aligned with the across-topics setting is to notice that in detailing one\u2019s arguments, one tends to utilize a specially structured discourse, and that discourse role could provide a clue to the argumentative function of a sentence, without reliance on what the sentence is actually saying (beyond discourse connectives that are used to help identify the discourse role).", 
        "56": "In particular, argumentative essays often have a fairly standard structure, where a general claim (or stance, or thesis) on the issue is introduced in the beginning of the essay, followed by a sequence of main points, each elaborated using supporting statements, and finally followed by a conclusion that often re-states the thesis and provides a high-level summary of the argument.", 
        "57": "We expect the \u201cmeat\u201d of the argument to occur mostly in the supporting statements that provide detailed exposition of the author\u2019s arguments.", 
        "58": "We use a discourse parser for argumentative essays (Burstein et al., 2003) to classify sentences into the following discourse units: Thesis, Background, MainPoint, Support, Conclusion, and Other.", 
        "59": "Row 4 (dr) in Table 1 shows the performance of this set of 6 binary features.", 
        "60": "Of the 6 features, Support and MainPoint have a positive weight (predict \u201cnongeneric\u201d), the rest predict \u201cgeneric\u201d.", 
        "61": "We further hypothesize that the position of a sentence inside a discourse segment might also provide some information: A sentence surrounded by Support sentences is likely to be in the middle of exposition of an argument, as opposed to the last Supporting sentence before the next Main Point that could be summary-like, leading up to a shift to a new topic.", 
        "62": "We therefore built two sets of transition features, one for all pairs of <previous sentence role,current sentence role> (such as <Thesis,Main Point> for a sentence that is classified as Main Point and follows a Thesis sentence), and the other \u2013 for all pairs of <current sentence role,next sentence role>.", 
        "63": "We also added BeginningOfEssay and EndOfEssay\ndiscourse tags to handle the first and the last sentences of the essay.", 
        "64": "Table 2 shows the weights for some of the features.", 
        "65": "We observe that the likelihood of the current Support sentence to carry argumentative content is higher if it follows another Support sentence (row 1) than if it follows a Main Point (row 2); if the Support sentence follows Thesis, it is actually not likely to contain argumentative content (perhaps it is more like a Main Point sentence than like a typical Support).", 
        "66": "Likewise, being followed by another Support sentence is a good sign (row 4), but being the last Support sentence before transitioning to a new Main Point has a much lower positive weight (row 5), and being the last Support before Conclusion has a still lower positive weight (row 6).", 
        "67": "Interestingly, while being the last Conclusion sentence in the essay strongly predicts \u201cgeneric\u201d (row 8), if the next sentence is still within the Conclusion segment, the prediction is actually slightly positive (row 7), suggesting that some authors rehash their arguments in substantial detail in concluding remarks, warranting a \u201cnon-generic\u201d designation.", 
        "68": "1The majority baseline for one prompt is below 50% because for that prompt the majority class is actually sentences that raise appropriate arguments, differently from the other 9 prompts.", 
        "69": "Table 1 shows the performance of the discourse role features (dr), the transition pairs using previous and next discourse roles (dr pn),2 and the combination of content and discourse (row 6).", 
        "70": "We observe that transforming the discourse role features into transitional features is effective.", 
        "71": "Second, the discourse role features are inferior to the content features for same-topic, while the opposite is true for the across-topics scenario.", 
        "72": "Discourse structure information does in fact get us quite far in the across-topics scenario, further than the lexical information on its own.", 
        "73": "Combining the two types of information further improves performance in across-topics scenario, and reduces the gap between across-topics and sametopic contexts to 4 points on average (.741 vs .700) and 1.5 points in worst case (.690 vs .674), for a combined discourse structure and content model.", 
        "74": "6 Can we do better?", 
        "75": "In an attempt to further improve across-topics performance, we generalized ngrams representations and adapted feature selection to reflect the acrosstopics dynamic more directly.", 
        "76": "6.1 Generalized ngrams  Suppose the prompt is arguing that some entity N should do some action V. While N and V might differ across prompts, critical sentences to the end that N should not do V are likely to occur across different prompts.", 
        "77": "In the current ngrams representation, N and V differ across prompts, and are unknown for a prompt that is unseen during training.", 
        "78": "We represent all content words (nouns, verbs, adjectives, adverbs, and cardinal numbers) in the prompt as their part-of-speech labels; we should be able to capture features such as \u201cshould not VB\u201d.", 
        "79": "Rows 7 and 8 in Table 1 show the 1- 3gr ppos model; it improves over 1-3gr in both the average and the worst cases, on its own and on top of the dr pn features, in the across-topics scenario.", 
        "80": "The improvement over dr pn+1-3gr is marginally significant (p<0.1, Wilcoxon SignedRanks 2-tailed test, n=10, W=33).", 
        "81": "The single strongest lexical predictor of a generic sentence is the first person singular pronoun I; such sentences are likely to express stance\n2Since argument critiques often span more than one sentence, we experimented with sequence labeling using Conditional Random Fields, but performance was not better than with logistic regression.", 
        "82": "(I think this is a good plan), or contain discoursemanagement expressions such as I will show that the author\u2019s arguments are flawed.", 
        "83": "Words such as assumptions, evidence, information, argument, statistics, idea, reasons all have negative weight, suggesting that they typically belong to generic sentences such as The author\u2019s argument lacks evidence that does not raise a specific critique.", 
        "84": "Lexical features for the positive class include modality as in might, perhaps, could, possible that, potential, necessarily, if a; negation (not, will not), as well as more specific lexica that point out, for example, outcomes of a policy (expensive, increase in, affected the, fails to).", 
        "85": "Positive features with prompt elements include NNS does not, NN do not, many NNS, NN NNS are, NNS who VBD, could have VBN, will not VB.", 
        "86": "6.2 Feature Selection  We experimented with three feature selection methods.", 
        "87": "(1) We selected features with p<0.05 using \u03c72 test (p0.05).", 
        "88": "(2) We selected features with p<0.05 for at least two out of the 9 training prompts, to find features that are likely to generalize across prompts (p0.05 2pr).", 
        "89": "(3) We selected features based on their mutual information with the label conditioned on values of the dr pn features, to encourage selection of features that augment, rather than repeat, the discourse information.", 
        "90": "We calculated for each training prompt, and took the 2nd highest of the 9 values.3 We selected features in the top 5% of this metric (mi5% 2pr).", 
        "91": "Table 3 shows the results.", 
        "92": "The p0.05 mechanism is ineffective; 0.05 2pr selection is better.", 
        "93": "The mi5% 2pr mechanism performs within .002 of the original, while reducing the number of features by two orders of magnitude.", 
        "94": "7 Benchmark  We also compared our best across-topics system (dr pn+1-3gr ppos) to the system described in\n3Thus, the feature has at least that much informaton beyond dr pn for at least two different prompts.", 
        "95": "Song et al.", 
        "96": "(2014).", 
        "97": "The Song et al.", 
        "98": "(2014) system uses the following features: length of the sentence, parts of speech, overlap of words in the sentence with the prompt, relative position of the sentence in the essay, 1-3gr, and 1-3gr in previous and next sentences.", 
        "99": "The performance is shown in row 9 in Table 1.", 
        "100": "Our improvement over Song et al.", 
        "101": "(2014) is statistically significant in the across-topics scenario (p<0.05, Wilcoxon Signed-Ranks test, 2- tailed, n=10, W=51).", 
        "102": "8 Conclusion  We presented experiments on classifying essay sentences as containing good argument critiques or not.", 
        "103": "While a good argument critique is a matter of content, we show that it is possible to build classifiers that are not prompt-specific, using discourse structure features and generalized lexical features that take into account reference to the text of the prompt to which the author is responding.", 
        "104": "Starting from a ngrams baseline where the performance gap between same-prompt and acrosstopics scenarios is 9 accuracy points on average (.738 vs .645) and 8 points in worst case (.679 vs .604), we close half the gap in average performance (.745 vs .706) and are down to only 1.5 point difference in worst case performance (.701 vs .686).", 
        "105": "This performance is preserved with only about 0.5% of the features, using a conditional mutual information criterion.", 
        "106": "The improvement in worst case performance is important for ensuring that the system does not exhibit large performance differences across different essay prompts used on the same test.", 
        "107": "We also show that our best system significantly improves over the state-of-art system for argument critique detection task on comparable essay data for the across-topics scenario."
    }, 
    "document_id": "P17-2038.pdf.json"
}
