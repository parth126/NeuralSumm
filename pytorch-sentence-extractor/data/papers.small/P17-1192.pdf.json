{
    "abstract_sentences": {
        "1": "We present a method for populating fine-grained classes (e.g., \u201c1950s American jazz musicians\u201d) with instances (e.g., Charles Mingus).", 
        "2": "While stateof-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head.", 
        "3": "An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2099\u20132109 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1192  1 Introduction  The majority of approaches (Snow et al., 2006; Shwartz et al., 2016) for extracting IsA relations from text rely on lexical patterns as the primary signal of whether an instance belongs to a class.", 
        "2": "For example, observing a pattern like \u201cX such as Y\u201d is a strong indication that Y (e.g., \u201cCharles Mingus\u201d) is an instance of class X (e.g., \u201cmusician\u201d) (Hearst, 1992).", 
        "3": "Methods based on these \u201cHearst patterns\u201d assume that class labels can be treated as atomic lexicalized units.", 
        "4": "This assumption has several significant weakness.", 
        "5": "First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text.", 
        "6": "The requirement is reasonable for class labels containing a single word, but in practice, there are many possible fine-grained classes: not only \u201cmusicians\u201d but also \u201c1950s American jazz musicians\u201d.", 
        "7": "The probability that a given label will appear in its entirety within one of the expected patterns is very low, even in large\n\u2217Contributed during an internship at Google.", 
        "8": "amounts of text.", 
        "9": "Second, when class labels are treated as though they cannot be decomposed, every class label must be modeled independently, even those containing overlapping words (\u201cAmerican jazz musician\u201d, \u201cFrench jazz musician\u201d).", 
        "10": "As a result, the number of meaning representations to be learned is exponential in the length of the class label, and quickly becomes intractable.", 
        "11": "Thus, compositional models of taxonomic relations are necessary for better language understanding.", 
        "12": "We introduce a compositional approach for reasoning about fine-grained class labels.", 
        "13": "Our approach is based on the notion from formal semantics, in which modifiers (\u201c1950s\u201d) correspond to properties that differentiate instances of a subclass (\u201c1950s musicians\u201d) from instances of the superclass (\u201cmusicians\u201d) (Heim and Kratzer, 1998).", 
        "14": "Our method consists of two stages: interpreting each modifier relative to the head (\u201cmusicians active during 1950s\u201d), and using the interpretations to identify instances of the class from text (Figure 1).", 
        "15": "Our main contributions are: 1) a compositional method for IsA extraction, which in-\n2099\nvolves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic \u201cmeaning\u201d to a phrase, and reasoning about that phrase in a truth-theoretic context.", 
        "16": "2 Related Work  Noun Phrase Interpretation.", 
        "17": "Compound noun phrases (\u201cjazz musician\u201d) communicate implicit semantic relations between modifiers and the head.", 
        "18": "Many efforts to provide semantic interpretations of such phrases rely on matching the compound to pre-defined patterns or semantic ontologies (Fares et al., 2015; O\u0301 Se\u0301aghdha and Copestake, 2007; Tratz and Hovy, 2010; Surtani and Paul, 2015; Choi et al., 2015).", 
        "19": "Recently, interpretations may take the form of arbitrary natural language predicates (Hendrickx et al., 2013).", 
        "20": "Most approaches are supervised, comparing unseen noun compounds to the most similar phrase seen in training (Wijaya and Gianfortoni, 2011; Nulty and Costello, 2013; Van de Cruys et al., 2013).", 
        "21": "Other unsupervised approaches apply information extraction techniques to paraphrase noun compounds (Kim and Nakov, 2011; Xavier and Strube de Lima, 2014; Pas\u0327ca, 2015).", 
        "22": "They focus exclusively on providing good paraphrases for an input noun compound.", 
        "23": "To our knowledge, ours is the first attempt to use these interpretations for the downstream task of IsA relation extraction.", 
        "24": "IsA Relation Extraction.", 
        "25": "Most efforts to acquire taxonomic relations from text build on the seminal work of Hearst (1992), which observes that certain textual patterns\u2013e.g., \u201cX and other Y\u201d\u2013are high-precision indicators of whether X is a member of class Y.", 
        "26": "Recent work focuses on learning such patterns automatically from corpora (Snow et al., 2006; Shwartz et al., 2016).", 
        "27": "These IsA extraction techniques provide a key step for the more general task of knowledge base population.", 
        "28": "The \u201cuniversal schema\u201d approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2017), which infers relations using matrix factorization, often includes Hearst patterns as input features.", 
        "29": "Graphical (Bansal et al., 2014)\nand joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes.", 
        "30": "A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014).", 
        "31": "These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text.", 
        "32": "This requirement limits the ability to handle arbitrarily fine-grained classes.", 
        "33": "Our work addresses this limitation by modeling fine-grained class labels compositionally.", 
        "34": "Thus the proposed method can combine evidence from multiple sentences, and can perform IsA extraction without requiring any example instances of a given class.1\nTaxonomy Construction.", 
        "35": "Previous work on the construction of a taxonomy of IsA relations (Flati et al., 2014; de Melo and Weikum, 2010; Kozareva and Hovy, 2010; Ponzetto and Strube, 2007; Ponzetto and Navigli, 2009) considers that task to be different than extracting a flat set of IsA relations from text in practice.", 
        "36": "Challenges specific to taxonomy construction include overall concept positioning and how to discover whether concepts are unrelated, subordinated or parallel to each other (Kozareva and Hovy, 2010); the need to refine and enrich the taxonomy (Flati et al., 2014); the difficulty in adding relevant IsA relations towards the top of the taxonomy (Ponzetto and Navigli, 2009); eliminating cycles and inconsistencies (Ponzetto and Navigli, 2009; Kozareva and Hovy, 2010).", 
        "37": "For practical purposes, these challenges are irrelevant when extracting flat IsA relations.", 
        "38": "Whereas Flati et al.", 
        "39": "(2014); Bizer et al.", 
        "40": "(2009); de Melo and Weikum (2010); Nastase and Strube (2013); Ponzetto and Strube (2007); Ponzetto and Navigli (2009); Hoffart et al.", 
        "41": "(2013) rely on data within human-curated resources, our work operates over unstructured text.", 
        "42": "Resources constructed in Bizer et al.", 
        "43": "(2009); Nastase and Strube (2013); Hoffart et al.", 
        "44": "(2013) contain not just a taxonomy of IsA relations,\n1Pasupat and Liang (2014) also focuses on zero-shot IsA extraction, but exploits HTML document structure, rather than reasoning compositionally.", 
        "45": "but also relation types other than IsA.", 
        "46": "3 Modifiers as Functions  Formalization.", 
        "47": "In formal semantics, modification is modeled as function application.", 
        "48": "Specifically, let MH be a class label consisting of a head H, which we assume to be a common noun, preceded by a modifier M .", 
        "49": "We use J\u00b7K to represent the \u201cinterpretation function\u201d that maps a linguistic expression to its denotation in the world.", 
        "50": "The interpretation of a common noun is the set of entities2 in the universe U .", 
        "51": "They are denoted by the noun (Heim and Kratzer, 1998):\nJHK = {e \u2208 U | e is a H} (1)\nThe interpretation of a modifier M is a function that maps between sets of entities.", 
        "52": "That is, modifiers select a subset3 of the input set:\nJMK(H) = {e \u2208 H | e satisfies M} (2)\nThis formalization leaves open how one decides whether or not \u201ce satisfiesM\u201d.", 
        "53": "This nontrivial, as the meaning of a modifier can vary depending on the class it is modifying: if e is a \u201cgood student\u201d, e is not necessarily a \u201cgood person\u201d, making it difficult to model whether \u201ce satisfies good\u201d in general.", 
        "54": "We therefore reframe the above equation, so that the decision of whether \u201ce satisfies M\u201d is made by calling a binary function \u03c6M , parameterized by the class H within which e is being considered:\nJMK(H) = {e \u2208 H | \u03c6M (H, e)} (3)\nConceptually, \u03c6M captures the core \u201cmeaning\u201d of the modifier M , which is the set of properties that differentiate members of the output class MH from members of the more general input class H. This formal semantics framework has two important consequences.", 
        "55": "First, the modifier has an intrinsic \u201cmeaning\u201d.", 
        "56": "The properties entailed by the modifier are independent of the particular state of the world.", 
        "57": "This makes it possible to make inferences about \u201c1950s musician\u201d even if no\n2We use \u201centities\u201d and \u201cinstances\u201d interchangeably;\u201centities\u201d is standard terminology in linguistics.", 
        "58": "3As does virtually all previous work in information extraction, we assume that modifiers are subsective, acknowledging the limitations (Kamp and Partee, 1995).", 
        "59": "1950s musician have been observed.", 
        "60": "Second, the modifier is a function that can be applied in a truth-theoretic setting.", 
        "61": "That is, applying \u201c1950s\u201d to the set of \u201cmusicians\u201d returns exactly the set of \u201c1950s musicians\u201d.", 
        "62": "Computational Approaches.", 
        "63": "While the notion of modifiers as functions has been incorporated into computational models previously, prior work focuses on either assigning an intrinsic meaning to M or on operationalizing M in a truth-theoretic sense, but not on doing both simultaneously.", 
        "64": "For example, Young et al.", 
        "65": "(2014) focuses exclusively on the subset selection aspect of modification.", 
        "66": "That is, given a set of instances H and a modifier M , their method could return the subset MH.", 
        "67": "However, their method does not model the meaning of the modifier itself, so that, e.g., if there were no red cars in their model of the world, the phrase \u201cred cars\u201d would have no meaning.", 
        "68": "In contrast, Baroni and Zamparelli (2010) models the meaning of modifiers explicitly as functions that map between vector-space representations of nouns.", 
        "69": "However, their model focuses on similarity between class labels\u2013e.g., to say that \u201cimportant routes\u201d is similar to \u201cmajor roads\u201d\u2013and it is not obvious how the method could be operationalized in order to identify instances of those classes.", 
        "70": "A contribution of our work is to model the semantics of M intrinsically, but in a way that permits application in the model theoretic setting.", 
        "71": "We learn an explicit model of the \u201cmeaning\u201d of a modifier M relative to a head H, represented as a distribution over properties that differentiate the members of the class MH from those of the class H. We then use this representation to identify the subset of instances of H, which constitute the subclass MH.", 
        "72": "4 Learning Modifier Interpretations    4.1 Setup  For each modifier M , we would like to learn the function \u03c6M from Eq.", 
        "73": "3.", 
        "74": "Doing so makes it possible, given H and an instance e \u2208 H, to decide whether e has the properties required to be an instance of MH.", 
        "75": "In general, there is no systematic way to determine the implied relation between M and H, as modifiers can arguably express any semantic relation, given the right context (Weiskopf,\n2007).", 
        "76": "We therefore model the semantic relation between M and H as a distribution over properties that could potentially define the subclass MH \u2286 H. We will refer to this distribution as a \u201cproperty profile\u201d for M relative to H. We make the assumption that relations between M and H that are discussed more often are more likely to capture the important properties of the subclass MH.", 
        "77": "This assumption is not perfect (Section 4.4) but has given good results for paraphrasing noun phrases (Nakov and Hearst, 2013; Pas\u0327ca, 2015).", 
        "78": "Our method for learning property profiles is based on the unsupervised method proposed by Pas\u0327ca (2015), which uses query logs as a source of common sense knowledge, and rewrites noun compounds by matching MH (\u201cAmerican musicians\u201d) to queries of the form \u201cH(.\u2217)M\u201d (\u201cmusicians from America\u201d).", 
        "79": "4.2 Inputs  We assume two inputs: 1) an IsA repository, O, containing \u3008e, C\u3009 tuples where C is a category and e is an instance of C, and 2) a fact repository, D, containing \u3008s, p, o, w\u3009 tuples where s and o are noun phrases, p is a predicate, and w is a confidence that p expresses a true relation between s and o.", 
        "80": "Both O and D are extracted from a sample of around 1 billion Web documents in English.", 
        "81": "The supplementary material gives additional details.", 
        "82": "We instantiate O with an IsA repository constructed by applying Hearst patterns to the Web documents.", 
        "83": "Instances are represented as automatically-disambiguated entity mentions4 which, when possible, are resolved to Wikipedia pages.", 
        "84": "Classes are represented as (non-disambiguated) natural language strings.", 
        "85": "We instantiate D with a large repository of facts extracted using in-house implementations of ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012).", 
        "86": "The predicates are extracted as natural language strings.", 
        "87": "Subjects and objects may be either disambiguated entity references or natural language strings.", 
        "88": "Every tuple is included in both the forward and the reverse direction.", 
        "89": "E.g.", 
        "90": "\u3008jazz, perform at, venue\u3009 also appears as \u3008venue,\u2190perform at, jazz\u3009, where \u2190 is a spe-\n4\u201cEntity mentions\u201d may be individuals, like \u201cBarack Obama\u201d, but may also be concepts like \u201cjazz\u201d.", 
        "91": "cial character signifying inverted predicates.", 
        "92": "These inverted predicates simplify the following definitions.", 
        "93": "In total, O contains 1.1M tuples and D contains 30M tuples.", 
        "94": "4.3 Building Property Profiles  Properties.", 
        "95": "Let I be a function that takes as input a noun phrase MH and returns a property profile for M relative to H. We define a \u201cproperty\u201d to be a tuple of a subject, predicate and object in which the subject position5 is a wildcard, e.g.", 
        "96": "\u3008\u2217, born in,America\u3009.", 
        "97": "Any instance that fills the wildcard slot then \u201chas\u201d the property.", 
        "98": "We expand adjectival modifiers to encompass nominalized forms using a nominalization dictionary extracted from WordNet (Miller, 1995).", 
        "99": "If MH is \u201cAmerican musician\u201d and we require a tuple to have the form \u3008H, p,M,w\u3009, we will include tuples in which the third element is either \u201cAmerican\u201d or \u201cAmerica\u201d.", 
        "100": "Relating M to H Directly.", 
        "101": "We first build property profiles by taking the predicate and object from any tuple in D in which the subject is the head and the object is the modifier:\nI1(MH) = {\u3008\u3008p,M\u3009, w\u3009 | \u3008H, p,M,w\u3009 \u2208 D} (4)\nRelating M to an Instance of H. We also consider an extension in which, rather than requiring the subject to be the class label H, we require the subject to be an instance of H.\nI2(MH) = {\u3008\u3008p,M\u3009, w\u3009 | \u3008e,H\u3009 \u2208 O \u2227\u3008e, p,M,w\u3009 \u2208 D} (5)\nModifier Expansion.", 
        "102": "In practice, when building property profiles, we do not require that the object of the fact tuple match the modifier exactly, as suggested in Eq.", 
        "103": "4 and 5.", 
        "104": "Instead, we follow Pas\u0327ca (2015) and take advantage of facts involving distributionally similar modifiers.", 
        "105": "Specifically, rather than looking only at tuples in D in which the object matches M , we consider all tuples, but discount the weight proportionally to the similarity between M and the object of the tuple.", 
        "106": "5Inverse predicates capture properties in which the wildcard is conceptually the object of the relation, but occupies the subject slot in the tuple.", 
        "107": "For example, \u3008venue,\u2190perform at, jazz\u3009 captures that a \u201cjazz venue\u201d is a \u201cvenue\u201d e such that \u201cjazz performed at e\u201d.", 
        "108": "Thus, I1 is computed as below:\nI1(MH) = {\u3008\u3008p,M\u3009, w \u00d7 sim(M,N)\u3009 | \u3008H, p,N,w\u3009 \u2208 D} (6)\nwhere sim(M,N) is the cosine similarity between M and N .", 
        "109": "I2 is computed analogously.", 
        "110": "We compute sim using a vector space built from Web documents following Lin and Wu (2009); Pantel et al.", 
        "111": "(2009).", 
        "112": "We retain the 100 most similar phrases for each of\u223c10M phrases, and consider all other similarities to be 0.", 
        "113": "4.4 Analysis of Property Profiles  Table 1 provides examples of good and bad property profiles for several MHs.", 
        "114": "In general, frequent relations between M and H capture relevant properties of MH, but it is not always the case.", 
        "115": "To illustrate, the most frequently discussed relation between \u201cchild\u201d and \u201cactor\u201d is that actors have children, but this property is not indicative of the meaning of \u201cchild actor\u201d.", 
        "116": "Qualitatively, the top-ranked interpretations learned by using the head noun directly (I1, Eq.", 
        "117": "4) are very similar to those learned using instances of the head (I2, Eq.", 
        "118": "5).", 
        "119": "However, I2 returns many more properties (10 on average per MH) than I1 (just over 1 on average).", 
        "120": "Anecdotally, we see that I2 captures more specific relations than does I1.", 
        "121": "For example, for \u201cjazz musicians\u201d, both methods return \u201c* write jazz\u201d and \u201c* compose jazz\u201d, but I2 additionally returns properties like \u201c* be major creative influence in jazz\u201d.", 
        "122": "We compare I1 and I2 quantitatively in Section 6.", 
        "123": "Importantly, we do see that both I1 and I2 are capable of learning head-specific property profiles for a modifier.", 
        "124": "Table 2 provides examples.", 
        "125": "5 Class-Instance Identification  Instance finding.", 
        "126": "After finding properties that relate a modifier to a head, we turn to the task of identifying instances of fine-grained\nclasses.", 
        "127": "That is, for a given modifier M , we want to instantiate the function \u03c6M from Eq.", 
        "128": "3.", 
        "129": "In practice, rather than being a binary function that decides whether or not e is in class MH, our instantiation, \u03c6\u0302M , will return a realvalued score expressing the confidence that e is a member of MH.", 
        "130": "For notational convenience, let D(\u3008s, p, o\u3009) = w, if \u3008s, p, o, w\u3009 \u2208 D and 0 otherwise.", 
        "131": "We define \u03c6\u0302M as follows:\n\u03c6\u0302M (H, e) = \u2211\n\u3008\u3008p,o\u3009,\u03c9\u3009\u2208I(MH) \u03c9\u00d7D(\u3008e, p, o\u3009) (7)\nApplying M to H, then, is as in Eq.", 
        "132": "3 except that instead of a discrete set, it returns a scored list of candidate instances:\nJMK(H) = {\u3008e, \u03c6\u0302M (H, e)\u3009 | \u3008e,H\u3009 \u2208 O} (8)\nUltimately, we need to identify instances of arbitrary class labels, which may contain multiple modifiers.", 
        "133": "Given a class label C = M1 .", 
        "134": ".", 
        "135": ".MkH that contains a head H preceded by modifiers M1 .", 
        "136": ".", 
        "137": ".Mk, we generate a list of candidate instances by finding all instances of H that have some property to support every modifier:\nk\u22c2\ni=1\n{\u3008e, s(e)\u3009 | \u3008e, w\u3009 \u2208 JMiK(H) \u2227w > 0} (9)\nwhere s(e) is the mean6 of the scores assigned by each separate \u03c6\u0302Mi .", 
        "138": "From here on, we use Mods to refer to our method that generates lists of instances for a class using Eq.", 
        "139": "8 and 9.", 
        "140": "When \u03c6\u0302M (Eq.", 
        "141": "7) is implemented using I1, we use the name ModsH (for \u201cheads\u201d).", 
        "142": "When it is implemented using I2, we use the name ModsI (for \u201cinstances\u201d).", 
        "143": "Weakly Supervised Reranking.", 
        "144": "Eq.", 
        "145": "8 uses a naive ranking in which the weight for e \u2208MH is the product of how often e has been observed with some property and the weight of that property for the class MH.", 
        "146": "Thus, instances of H with overall higher counts in D receive high weights for every MH.", 
        "147": "We therefore train a simple logistic regression model to predict the likelihood that e belongs to MH.", 
        "148": "We use a small set of features7, including the raw weight as computed in Eq.", 
        "149": "7.", 
        "150": "For training, we sample \u3008e, C\u3009 pairs from our IsA repository O as positive examples and random pairs that were not extracted by any Hearst pattern as negative examples.", 
        "151": "We frame the task as a binary prediction of whether e \u2208 C, and use the model\u2019s confidence as the value of \u03c6\u0302M in place of the function in Eq.", 
        "152": "7.", 
        "153": "6 Evaluation    6.1 Experimental Setup  Evaluation Sets.", 
        "154": "We evaluate our models on their ability to return correct instances for arbitrary class labels.", 
        "155": "As a source of evaluation data, we use Wikipedia category pages (e.g., http://en.wikipedia.org/wiki/Category: Pakistani film actresses).", 
        "156": "These are pages in which the title is the name of the category (\u201cpakistani film actresses\u201d) and the body is a manually curated list of links to other pages that fall under the category.", 
        "157": "We measure the precision and recall of each method for discovering the instances listed on these pages given the page title (henceforth \u201cclass label\u201d).", 
        "158": "We collect the titles of all Wikipedia category pages, removing those in which the last word is capitalized or which contain fewer than three words.", 
        "159": "These heuristics are intended to retain compositional titles in which the head is a single common noun.", 
        "160": "We also remove\n6Also tried minimum, but mean gave better results.", 
        "161": "7Feature templates in supplementary material.", 
        "162": "any titles that contain links to sub-categories.", 
        "163": "This is to favor fine-grained classes (\u201cpakistani film actresses\u201d) over coarse-grained ones (\u201cfilm actresses\u201d).", 
        "164": "We perform heuristic modifier chunking in order to group together multiword modifiers (e.g., \u201cpuerto rican\u201d); for details, see supplementary material.", 
        "165": "From the resulting list of class labels, we draw two samples of 100 labels each, enforcing that no H appear as the head of more than three class labels per sample.", 
        "166": "The first sample is chosen uniformly at random (denoted UniformSet).", 
        "167": "The second (WeightedSet) is weighted so that the probability of drawing M1 .", 
        "168": ".", 
        "169": ".MkH is proportional to the total number of class labels in which H appears as the head.", 
        "170": "These different evaluation sets8 are intended to evaluate performance on the head versus the tail of class label distribution, since information retrieval methods often perform differently on different parts of the distribution.", 
        "171": "On average, there are 17 instances per category in UniformSet and 19 in WeightedSet.", 
        "172": "Table 3 gives examples of class labels.", 
        "173": "Baselines.", 
        "174": "We implement two baselines using our IsA repository (O as defined in Section 4.1).", 
        "175": "Our simplest baseline ignores modifiers altogether, and simply assumes that any instance of H is an instance of MH, regardless of M .", 
        "176": "In this case the confidence value for\n8Available at http://www.seas.upenn.edu/\u223cnlp/ resources/finegrained-class-eval.gz\n\u3008e,MH\u3009 is equivalent to that for \u3008e,H\u3009.", 
        "177": "We refer to this baseline simply as Baseline.", 
        "178": "Our second, stronger baseline uses the IsA repository directly to identify instances of the finegrained class C = M1 .", 
        "179": ".", 
        "180": ".MkH.", 
        "181": "That is, we consider e to be an instance of the class if \u3008e, C\u3009 \u2208 O, meaning the entire class label appeared in a source sentence matching some Hearst pattern.", 
        "182": "We refer to this baseline as Hearst.", 
        "183": "The weight used to rank the candidate instances is the confidence value assigned by the Hearst pattern extraction (Section 4.2).", 
        "184": "Compositional Models.", 
        "185": "As a baseline compositional model, we augment the Hearst baseline via set intersection.", 
        "186": "Specifically, for a class C = M1 .", 
        "187": ".", 
        "188": ".MkH, if each of the MiH appears in O independently, we take the instances of C to be the intersection of the instances of each of the MiH.", 
        "189": "We assign the weight of an instance e to be the sum of the weights associated with each independent modifier.", 
        "190": "We refer to this method as Hearst\u2229.", 
        "191": "It is roughly equivalent to (Pas\u0327ca, 2014).", 
        "192": "We contrast it with our proposed model, which recognizes instances of a fine-grained class by 1) assigning a meaning to each modifier in the form of a property profile and 2) checking whether a candidate instance exhibits these properties.", 
        "193": "We refer to the versions of our method as ModsH and ModsI , as described in Section 5.", 
        "194": "When relevant, we use \u201craw\u201d to refer to the version in which instances are ranked using raw weights and \u201cRR\u201d to refer to the version in which instances are ranked using logistic regression (Section 5).", 
        "195": "We also try using the proposed methods to extend rather than replace the Hearst baseline.", 
        "196": "We combine predictions by merging the ranked lists produced by each system: i.e.", 
        "197": "the score of an instance is the inverse of the sum of its ranks in each of the input lists.", 
        "198": "If an instance does not appear at all in an input list, its rank in that list is set to a large constant value.", 
        "199": "We refer to these combination systems as Hearst+ModsH and Hearst+ModsI .", 
        "200": "6.2 Results  Precision and Coverage.", 
        "201": "We first compare the methods in terms of their coverage, the number of class labels for which the method is able to find some instance, and their\nprecision, to what extent the method is able to correctly rank true instances of the class above non-instances.", 
        "202": "We report total coverage, the number of labels for which the method returns any instance, and correct coverage, the number of labels for which the method returns a correct instance.", 
        "203": "For precision, we compute the average precision (AP) for each class label.", 
        "204": "AP ranges from 0 to 1, where 1 indicates that all positive instances were ranked above all negative instances.", 
        "205": "We report mean average precision (MAP), which is the mean of the APs across all the class labels.", 
        "206": "MAP is only computed over class labels for which the method returns something, meaning methods are not punished for returning empty lists.", 
        "207": "Table 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods.", 
        "208": "Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods.", 
        "209": "In combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e.", 
        "210": "the proposed method does not cause a precision drop on classes covered by the baseline.", 
        "211": "In addition, there are many classes for which the baseline is not able to extract any instances, but the proposed method is.", 
        "212": "None of the methods can extract some of the gold instances, such as \u201cDictator perpetuo\u201d and \u201cFuror Teutonicus\u201d of the gold class \u201clatin political phrases\u201d.", 
        "213": "Table 5 also reveals that the reranking model (RR) consistently increases MAP for the proposed methods.", 
        "214": "Therefore, going forward, we only report results using the reranking model (i.e.", 
        "215": "ModsH and ModsI will refer to ModsH RR and ModsI RR, respectively).", 
        "216": "Manual Re-Annotation.", 
        "217": "It possible that true instances of a class are missing from our Wikipedia reference set, and thus that our precision scores underestimate the actual precision of the systems.", 
        "218": "We therefore manually verify the top 10 predictions of each of the systems for a random sample of 25 class labels.", 
        "219": "We choose class labels for which Hearst was able to return at least one instance, in order to ensure reliable precision\nestimates.", 
        "220": "For each of these labels, we manually check the top 10 instances proposed by\neach method to determine whether each belongs to the class.", 
        "221": "Table 6 shows the precision scores for each method computed against the original Wikipedia list of instances and against our manually-augmented list of gold instances.", 
        "222": "The overall ordering of the systems does not change, but the precision scores increase notably after re-annotation.", 
        "223": "We continue to evaluate against the Wikipedia lists, but acknowledge that reported precision is likely an underestimate of true precision.", 
        "224": "Precision-Recall Analysis.", 
        "225": "We next look at the precision-recall tradeoff in terms of the area under the curve (AUC) when each method attempts to rank the complete list of candidate instances.", 
        "226": "We take the union of all of the instances proposed by all of the methods (including the Baseline method which, given a class label M0 .", 
        "227": ".", 
        "228": ".MkH, proposes every instance of the head H as a candidate).", 
        "229": "Then, for each method, we rank this full set of candidates such that any instance returned by the method is given the score the method assigns, and every other instance is scored as 0.", 
        "230": "Table 7 reports the AUC and recall.", 
        "231": "Figure 3 plots the full ROC curves.", 
        "232": "The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candidate instances.", 
        "233": "In comparison, the proposed compositional methods make use of a larger set of sentences, and provide nonzero scores for many more candidates, resulting in a >10 point increase in AUC on both UniformSet and WeightedSet (Table 7).", 
        "234": "7 Conclusion  We have presented an approach to IsA extraction that takes advantage of the compositionality of natural language.", 
        "235": "Existing approaches often treat class labels as atomic units that must be observed in full in order to be pop-\nulated with instances.", 
        "236": "As a result, current methods are not able to handle the infinite number of classes describable in natural language, most of which never appear in text.", 
        "237": "Our method reasons about each modifier in the label individually, in terms of the properties that it implies about the instances.", 
        "238": "This approach allows us to harness information that is spread across multiple sentences, significantly increasing the number of fine-grained classes that we are able to populate.", 
        "239": "Acknowledgments  The paper incorporates suggestions on an earlier version from Susanne Riehemann.", 
        "240": "Ryan Doherty offered support in refining and accessing the fact repository used in the evaluation."
    }, 
    "document_id": "P17-1192.pdf.json"
}
