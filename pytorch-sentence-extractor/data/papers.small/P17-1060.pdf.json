{
    "abstract_sentences": {
        "1": "An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision givenK existing domains.", 
        "2": "One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daum\u00e9 III (2009).", 
        "3": "However, it is desirable to adapt without having to reestimate a global model from scratch each time a new domain with potentially new intents and slots is added.", 
        "4": "We describe a solution based on attending an ensemble of domain experts.", 
        "5": "We assume K domainspecific intent and slot models trained on respective domains.", 
        "6": "When given domain K + 1, our model uses a weighted combination of the K domain experts\u2019 feedback along with its own opinion to make predictions on the new domain.", 
        "7": "In experiments, the model significantly outperforms baselines that do not use domain adaptation and also performs better than the full retraining approach."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 643\u2013653 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1060  1 Introduction  An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains.", 
        "2": "In spoken language understanding, new domains of interest for categorizing user utterances are added on a regular basis1.", 
        "3": "For instance, we may\n1A scenario frequently arising in practice is having a request for creating a new virtual domain targeting a specific application.", 
        "4": "One typical use case is that of building natural language capability through intent and slot modeling (without actually building a domain classifier) targeting a specific application.", 
        "5": "add ORDERPIZZA domain and desire a domainspecific intent and semantic slot tagger with a limited amount of training data.", 
        "6": "Training only on the target domain fails to utilize the existing resources in other domains that are relevant (e.g., labeled data for PLACES domain with place name, location as the slot types), but naively training on the union of all domains does not work well since different domains can have widely varying distributions.", 
        "7": "Domain adaptation offers a balance between these extremes by using all data but simultaneously distinguishing domain types.", 
        "8": "A common approach for adapting to a new domain is to retrain a global model across all K + 1 domains using well-known techniques, for example the feature augmentation method of Daume\u0301 III (2009) which trains a single model that has one domaininvariant component along with K + 1 domainspecific components each of which is specialized in a particular domain.", 
        "9": "While such a global model is effective, it requires re-estimating a model from scratch on all K + 1 domains each time a new domain is added.", 
        "10": "This is burdensome particularly in our scenario in which new domains can arise frequently.", 
        "11": "In this paper, we present an alternative solution based on attending an ensemble of domain experts.", 
        "12": "We assume that we have already trained K domain-specific models on respective domains.", 
        "13": "Given a new domainK+1 with a small amount of training data, we train a model on that data alone but queries the K experts as part of the training procedure.", 
        "14": "We compute an attention weight for each of these experts and use their combined feedback along with the model\u2019s own opinion to make predictions.", 
        "15": "This way, the model is able to selectively capitalize on relevant domains much like in\n643\nstandard domain adaptation but without explicitly re-training on all domains together.", 
        "16": "In experiments, we show clear gains in a domain adaptation scenario across 7 test domains, yielding average error reductions of 44.97% for intent classification and 32.30% for slot tagging compared to baselines that do not use domain adaptation.", 
        "17": "Moreover we have higher accuracy than the full re-training approach of Kim et al.", 
        "18": "(2016c), a neural analog of Daume\u0301 III (2009).", 
        "19": "2 Related Work    2.1 Domain Adaptation  There is a venerable history of research on domain adaptation (Daume III and Marcu, 2006; Daume\u0301 III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another.", 
        "20": "In the context of NLP, a particularly successful approach is the feature augmentation method of Daume\u0301 III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge.", 
        "21": "The method is generalized to the neural paradigm by Kim et al.", 
        "22": "(2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains.", 
        "23": "In the context of SLU, Jaech et al.", 
        "24": "(2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al.", 
        "25": "(2016c) instead employed K + 1 LSTMs.", 
        "26": "Hakkani-Tu\u0308r et al.", 
        "27": "(2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent.", 
        "28": "All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between domains.", 
        "29": "This becomes difficult to scale when there is a new domain coming in at high frequency.", 
        "30": "We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts.", 
        "31": "This gives a clear computational advantage over the feature augmentation method.", 
        "32": "2.2 Spoken Language Understanding  Recently, there has been much investment on the personal digital assistant (PDA) technology in in-\ndustry (Sarikaya, 2015; Sarikaya et al., 2016).", 
        "33": "Apples Siri, Google Now, Microsofts Cortana, and Amazons Alexa are some examples of personal digital assistants.", 
        "34": "Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014).", 
        "35": "PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e.", 
        "36": "movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a).", 
        "37": "Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-Tu\u0308r et al., 2016), adversary domain training (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015).", 
        "38": "There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b).", 
        "39": "3 Method  We use an LSTM simply as a mapping \u03c6 : Rd \u00d7 Rd\u2032 \u2192 Rd\u2032 that takes an input vector x and a state vector h to output a new state vector h\u2032 = \u03c6(x, h).", 
        "40": "See Hochreiter and Schmidhuber (1997) for a detailed description.", 
        "41": "At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a character layer, a BiLSTM at word level, and feedfoward network at the output.", 
        "42": "3.1 Individual Model Architecture  Let C denote the set of character types and W the set of word types.", 
        "43": "Let \u2295 denote the vector concatenation operation.", 
        "44": "A wildly successful architecture for encoding a sentence (w1 .", 
        "45": ".", 
        "46": ".", 
        "47": "wn) \u2208 Wn is given by bidirectional LSTMs (BiLSTMs) (Schuster and Paliwal, 1997; Graves, 2012).", 
        "48": "Our model first constructs a network over an utterance closely following Lample et al.", 
        "49": "(2016).", 
        "50": "The model parameters \u0398 associated with this BiLSTM layer are\n\u2022 Character embedding ec \u2208 R25 for each c \u2208 C\n\u2022 Character LSTMs \u03c6Cf , \u03c6Cb : R25\u00d7R25 \u2192 R25\n\u2022 Word embedding ew \u2208 R100 for each w \u2208 W\n\u2022 Word LSTMs \u03c6Wf , \u03c6Wb : R150\u00d7R100 \u2192 R100\nLetw1 .", 
        "51": ".", 
        "52": ".", 
        "53": "wn \u2208 W denote a word sequence where word wi has character wi(j) \u2208 C at position j.", 
        "54": "First, the model computes a character-sensitive word representation vi \u2208 R150 as\nfCj = \u03c6 C f ( ewi(j), f C j\u22121 )\n\u2200j = 1 .", 
        "55": ".", 
        "56": ".", 
        "57": "|wi| bCj = \u03c6 C b ( ewi(j), b C j+1 ) \u2200j = |wi| .", 
        "58": ".", 
        "59": ".", 
        "60": "1 vi = f C |wi| \u2295 b C 1 \u2295 ewi\nfor each i = 1 .", 
        "61": ".", 
        "62": ".", 
        "63": "n.2 Next, the model computes\nfWi = \u03c6 W f ( vi, f W i\u22121 )\n\u2200i = 1 .", 
        "64": ".", 
        "65": ".", 
        "66": "n bWi = \u03c6 W b ( vi, b W i+1 ) \u2200i = n .", 
        "67": ".", 
        "68": ".", 
        "69": "1\nand induces a character- and context-sensitive word representation hi \u2208 R200 as\nhi = f W i \u2295 bWi (1)\nfor each i = 1 .", 
        "70": ".", 
        "71": ".", 
        "72": "n. These vectors can be used to perform intent classification or slot tagging on the utterance.", 
        "73": "Intent Classification We can predict the intent of the utterance using (h1 .", 
        "74": ".", 
        "75": ".", 
        "76": "hn) \u2208 R200 in (1) as follows.", 
        "77": "Let I denote the set of intent types.", 
        "78": "We introduce a single-layer feedforward network gi : R200 \u2192 R|I| whose parameters are denoted by \u0398i.", 
        "79": "We compute a |I|-dimensional vector\n\u00b5i = gi\n( n\u2211\ni=1\nhi\n)\nand define the conditional probability of the correct intent \u03c4 as\np(\u03c4 |h1 .", 
        "80": ".", 
        "81": ".", 
        "82": "hn) \u221d exp ( \u00b5i\u03c4 )\n(2)\n2For simplicity, we assume some random initial state vectors such as fC0 and bC|wi|+1 when we describe LSTMs.", 
        "83": "The intent classification loss is given by the negative log likelihood:\nLi ( \u0398,\u0398i ) = \u2212 \u2211\nl\nlog p ( \u03c4 (l)|h(l) ) (3)\nwhere l iterates over intent-annotated utterances.", 
        "84": "Slot Tagging We predict the semantic slots of the utterance using (h1 .", 
        "85": ".", 
        "86": ".", 
        "87": "hn) \u2208 R200 in (1) as follows.", 
        "88": "Let S denote the set of semantic types and L the set of corresponding BIO label types 3 that is, L = {B-e : e \u2208 E}\u222a{I-e : e \u2208 E}\u222a{O}.", 
        "89": "We add a transition matrix T \u2208 R|L|\u00d7|L| and a singlelayer feedforward network gt : R200 \u2192 R|L| to the network; denote these additional parameters by \u0398t.", 
        "90": "The conditional random field (CRF) tagging layer defines a joint distribution over label sequences of y1 .", 
        "91": ".", 
        "92": ".", 
        "93": "yn \u2208 L of w1 .", 
        "94": ".", 
        "95": ".", 
        "96": "wn as\np(y1 .", 
        "97": ".", 
        "98": ".yn|h1 .", 
        "99": ".", 
        "100": ".", 
        "101": "hn)\n\u221d exp ( n\u2211\ni=1\nTyi\u22121,yi \u00d7 gtyi(hi) ) (4)\nThe tagging loss is given by the negative log likelihood:\nLt ( \u0398,\u0398t ) = \u2212 \u2211\nl\nlog p ( y(l)|h(l) ) (5)\nwhere l iterates over tagged sentences in the data.", 
        "102": "Alternatively, we can optimize the local loss:\nLt\u2212loc ( \u0398,\u0398t ) = \u2212 \u2211\nl\n\u2211\ni\nlog p ( y (l) i |h (l) i )\n(6)\nwhere p(yi|hi) \u221d exp ( gtyi(hi) ) .", 
        "103": "4 Method    4.1 Domain Attention Architecture  Now we assume that for each of theK domains we have an individual model described in Section 3.1.", 
        "104": "Denote these domain experts by \u0398(1) .", 
        "105": ".", 
        "106": ".\u0398(K).", 
        "107": "We now describe our model for a new domain K + 1.", 
        "108": "Given an utterance w1 .", 
        "109": ".", 
        "110": ".", 
        "111": "wn, it uses a BiLSTM layer to induce a feature representation h1 .", 
        "112": ".", 
        "113": ".", 
        "114": "hn as specified in (1).", 
        "115": "It further invokes K domain experts \u0398(1) .", 
        "116": ".", 
        "117": ".\u0398(K) on this utterance to obtain the feature representations h(k)1 .", 
        "118": ".", 
        "119": ".", 
        "120": "h (k) n for\n3For example, to/O San/B-Source Francisco/I-Source airport/O.", 
        "121": "k = 1 .", 
        "122": ".", 
        "123": ".K.", 
        "124": "For each word wi, the model computes an attention weight for each domain k = 1 .", 
        "125": ".", 
        "126": ".K domains as\nqdoti,k = h > i h (k) (7)\nin the simplest case.", 
        "127": "We also experiment with the bilinear function\nqbii,k = h > i Bh (k) (8)\nwhere B is an additional model parameter, and also the feedforward function\nqfeedi,k = W tanh ( Uh>i + V h (k) + b1 ) + b2 (9)\nwhere U, V,W, b1, b2 are additional model parameters.", 
        "128": "The final attention weights a(1)i .", 
        "129": ".", 
        "130": ".", 
        "131": "a (1) i are obtained by using a softmax layer\nai,k = exp(qi,k)\u2211K k=1 exp(qi,k)\n(10)\nThe weighted combination of the experts\u2019 feedback is given by\nhexpertsi =\nK\u2211\nk=1\nai,kh (k) i (11)\nand the model makes predictions by using h\u03041 .", 
        "132": ".", 
        "133": ".", 
        "134": "h\u0304n where\nh\u0304i = hi \u2295 hexpertsi (12)\nThese vectors replace the original feature vectors hi in defining the intent or tagging losses.", 
        "135": "4.2 Domain Attention Variants  We also consider two variants of the domain attention architecture in Section 4.1.", 
        "136": "Label Embedding In addition to the state vectors h(1) .", 
        "137": ".", 
        "138": ".", 
        "139": "h(K) produced by K experts, we further incorporate their final (discrete) label predictions using pre-trained label embeddings.", 
        "140": "We induce embeddings ey for labels y from all domains using the method of Kim et al.", 
        "141": "(2015d).", 
        "142": "At the i-th word, we predict the most likely label y(k) under the k-th expert and compute an attention weight as\nq\u0304doti,k = h > i e y(k) (13)\nThen we compute an expectation over the experts\u2019 predictions\na\u0304i,k = exp(q\u0304i,k)\u2211K k=1 exp(q\u0304i,k)\n(14)\nhlabeli =\nK\u2211\nk=1\na\u0304i,ke y(k) i (15)\nand use it in conjunction with h\u0304i.", 
        "143": "Note that this makes the objective a function of discrete decision and thus non-differentiable, but we can still optimize it in a standard way treating it as learning a stochastic policy.", 
        "144": "Selective Attention Instead of computing attention over all K experts, we only consider the top K \u2032 \u2264 K that predict the highest label scores.", 
        "145": "We only compute attention over these K \u2032 vectors.", 
        "146": "We experiment with various values of K \u2032  5 Experiments  In this section, we describe the set of experiments conducted to evaluate the performance of our model.", 
        "147": "In order to fully assess the contribution of our approach, we also consider several baselines and variants besides our primary expert model.", 
        "148": "5.1 Test domains and Tasks  To test the effectiveness of our proposed approach, we apply it to a suite of 7 Microsoft Cortana domains with 2 separate tasks in spoken language understanding: (1) intent classification and (2) slot (label) tagging.", 
        "149": "The intent classification task is a multi-class classification problem with the goal of determining to which one of the |I| intents a user utterance belongs within a given domain.", 
        "150": "The slot tagging task is a sequence labeling problem with the goal of identifying entities and chunking of useful information snippets in a user utterance.", 
        "151": "For example, a user could say \u201creserve a table at joeys grill for thursday at seven pm for five people\u201d.", 
        "152": "Then the goal of the first task would be to classify this utterance as \u201cmake reservation\u201d intent given the places domain, and the goal of the second task would be to tag \u201cjoeys grill\u201d as restaurant, \u201cthursday\u201d as date, \u201cseven pm\u201d as time, and \u201cfive\u201d as number people.", 
        "153": "The short descriptions on the 7 test domains are shown in Table 1.", 
        "154": "As the table shows, the test domains have different granularity and diverse semantics.", 
        "155": "For each personal assistant test domain,\nwe only used 1000 training utterances to simulate scarcity of newly labeled data.", 
        "156": "The amount of development and test utterance was 100 and 10k respectively.", 
        "157": "The similarities of test domains, represented by overlapping percentage, with experts or source domains are represented in Table 2.", 
        "158": "The intent overlapping percentage ranges from 30% on FITNESS domain to 70% on EVENTS, which averages out at 51.49%.", 
        "159": "And the slots for test domains overlaps more with those of source domains ranging from 60% on TV domain to 100% on both M-TICKET and TAXI domains, which averages out at 81.69%.", 
        "160": "5.2 Experimental Setup  In testing our approach, we consider a domain adaptation (DA) scenario, where a target domain has a limited training data and the source domain has a sufficient amount of labeled data.", 
        "161": "We further consider a scenario, creating a new virtual domain targeting a specific scenario given a large inventory of intent and slot types and underlying models build for many different applications and scenarios.", 
        "162": "One typical use case is that of building natural language capability through intent and slot modeling (without actually building a domain classifier) targeting a specific application.", 
        "163": "Therefore, our experimental settings are rather different from previ-\nously considered settings for domain adaptation in two aspects:\n\u2022 Multiple source domains: In most previous works, only a pair of domains (source vs. target) have been considered, although they can be easily generalized to K > 2.", 
        "164": "Here, we experiment with K = 25 domains shown in Table 3.", 
        "165": "\u2022 Variant output: In a typical setting for domain adaptation, the label space is invariant across all domains.", 
        "166": "Here, the label space can be different in different domains, which is a more challenging setting.", 
        "167": "See Kim et al.", 
        "168": "(2015d) for details of this setting.", 
        "169": "For this DA scenario, we test whether our approach can effectively make a system to quickly generalize to a new domain with limited supervision given K existing domain experts shown in 3 .", 
        "170": "In summary, our approach is tested with 7 Microsoft Cortana personal assistant domains across 2 tasks of intent classification and slot tagging.", 
        "171": "Below shows more detail of our baselines and variants used in our experiments.", 
        "172": "Baselines: All models below use same underlying architecture described in Section 3.1\n\u2022 TARGET: a model trained on a targeted domain without DA techniques.", 
        "173": "\u2022 UNION: a model trained on the union of a targeted domain and 25 domain experts.", 
        "174": "\u2022 DA: a neural domain adaptation method of Kim et al.", 
        "175": "(2016c) which trains domain specific K LSTMs with a generic LSTM on all domain training data.", 
        "176": "Domain Experts (DE) variants: All models below are based on attending on an ensemble of 25 domain experts (DE) described in Section 4.1, where a specific set of intent and slots models are trained for each domain.", 
        "177": "We have two feedback from domain experts: (1) feature representation from LSTM, and (2) label embedding from feedfoward described in Section 4.1 and Section 4.2, respectively.", 
        "178": "\u2022 DEB: DE without domain attention mechanism.", 
        "179": "It uses the unweighted combination of first feedback from experts like bag-of-word model.", 
        "180": "\u2022 DE1: DE with domain attention with the weighted combination of the first feedbacks from experts.", 
        "181": "\u2022 DE2: DE1 with additional weighted combination of second feedbacks.", 
        "182": "\u2022 DES2: DE2 with selected attention mechanism, described in Section 4.2.", 
        "183": "In our experiments, all the models were implemented using Dynet (Neubig et al., 2017) and were trained using Stochastic Gradient Descent (SGD) with Adam (Kingma and Ba, 2015)\u2014an adaptive learning rate algorithm.", 
        "184": "We used the initial learning rate of 4\u00d7 10\u22124 and left all the other hyper parameters as suggested in Kingma and Ba (2015).", 
        "185": "Each SGD update was computed without a minibatch with Intel MKL (Math Kernel Library)4.", 
        "186": "We used the dropout regularization (Srivastava et al., 2014) with the keep probability of 0.4 at each LSTM layer.", 
        "187": "To encode user utterances, we used bidirectional LSTMs (BiLSTMs) at the character level and the word level, along with 25 dimensional character embedding and 100 dimensional word embedding.", 
        "188": "The dimension of both the input and output of the character LSTMs were 25, and the dimensions of the input and output of the word LSTMs were 1505 and 100, respectively.", 
        "189": "The dimension of the input and output of the final feedforward network for intent, and slot were 200 and the number of their corresponding task.", 
        "190": "Its activation was rectified linear unit (ReLU).", 
        "191": "To initialize word embedding, we used word embedding trained from (Lample et al., 2016).", 
        "192": "In the following sections, we report intent classification results in accuracy percentage and slot results in F1-score.", 
        "193": "To compute slot F1-score, we used the standard CoNLL evaluation script6  5.3 Results  We show our results in the DA setting where we had a sufficient labeled dataset in the 25 source domains shown in Table 3, but only 1000 labeled data in the target domain.", 
        "194": "The performance of the baselines and our domain experts DE variants are shown in Table 4.", 
        "195": "The top half of the table shows\n4https://software.intel.com/en-us/articles/intelr-mkl-andc-template-libraries\n5We concatenated last two outputs from the character LSTM and word embedding, resulting in 150 (25+25+100)\n6http://www.cnts.ua.ac.be/conll2000/chunking/output.html\nthe results of intent classification and the results of slot tagging is in the bottom half.", 
        "196": "The baseline which trained only on the target domain (TARGET) shows a reasonably good performance, yielding on average 87.7% on the intent classification and 83.9% F1-score on the slot tagging.", 
        "197": "Simply training a single model with aggregated utterance across all domains (UNION) brings the performance down to 77.4% and 75.3%.", 
        "198": "Using DA approach of Kim et al.", 
        "199": "(2016c) shows a significant increase in performance in all 7 domains, yielding on average 90.3% intent accuracy and 86.2%.", 
        "200": "The DE without domain attention (DEB) shows similar performance compared to DA.", 
        "201": "Using DE model with domain attention (DE1) shows another increase in performance, yielding on average 90.9% intent accuracy and 86.9%.", 
        "202": "The performance increases again when we use both feature representation and label embedding (DE2), yielding on average 91.4% and 88.2% and observe nearly 93.6% and 89.1% when using selective attention (DES2).", 
        "203": "Note that DES2 selects the appropriate number of experts per layer by evaluation on a development set.", 
        "204": "The results show that our expert variant approach (DES2) achieves a significant performance gain in all 7 test domains, yielding average error reductions of 47.97% for intent classification and 32.30% for slot tagging.", 
        "205": "The results suggest that our expert approach can quickly generalize to a new domain with limited supervision given K existing domains by having only a handful more data of 1k newly labeled data points.", 
        "206": "The poor performance of using the union of both source and target domain data might be due to the relatively very small size of the target domain data, overwhelmed by the data in the source domain.", 
        "207": "For example, a word such as \u201chome\u201d can be labeled as place type under the TAXI domain, but in the source domains can be labeled as either home screen under the PHONE domain or contact name under the CALENDAR domain.", 
        "208": "5.4 Training Time  The Figure 3 shows the time required for training DES2 and DA of Kim et al.", 
        "209": "(2016c).", 
        "210": "The training time for DES2 stays almost constant as the number of source domains increases.", 
        "211": "However, the training time for DA grows exponentially in the number of source domains.", 
        "212": "Specifically, when trained\nwith 1 source or expert domain, both took around a minute per epoch on average.", 
        "213": "When training with full 25 source domains, DES2 took 3 minutes per epoch while DA took 30 minutes per epoch.", 
        "214": "Since we need to iterate over all 25+1 domains to re-train the global model, the net training time ratio could be over 250.", 
        "215": "5.5 Learning Curve  We also measured the performance of our methods as a function of the number of domain experts.", 
        "216": "For each test domain, we consider all possible sizes of experts ranging from 1 to 25 and we then take the average of the resulting performances obtained from the expert sets of all different sizes.", 
        "217": "Figure 4 shows the resulting learning curves for each test domain.", 
        "218": "The overall trend is clear: as the more expert domains are added, the more the test performance improves.", 
        "219": "With ten or more expert domains added, our method starts to get saturated achiev-\ning more than 90% in accuracy across all seven domains.", 
        "220": "5.6 Attention weights  From the heatmap shown in Figure 5, we can see that the attention strength generally agrees with common sense.", 
        "221": "For example, the M-TICKET and TAXI domain selected MOVIE and PLACES as their top experts, respectively.", 
        "222": "5.7 Oracle Expert  The results in Table 5 show the intent classification accuracy of DE2 when we already have the same domain expert in the expert pool.", 
        "223": "To simulate such a situation, we randomly sampled 1,000, 100, and 100 utterances from each domain as training, development and test data, respectively.", 
        "224": "In both ALARM and HOTEL domains, the trained models only on the 1,000 training utterances (TARGET) achieved only 70.1%and 65.2% in accuracy, respectively.", 
        "225": "Whereas, with our method (DE2) applied, we reached almost the full training performance by selectively paying attention to the oracle expert, yielding 98.2% and 96.9%, respectively.", 
        "226": "This result again confirms that the behavior of the trained attention network indeed matches the semantic closeness between different domains.", 
        "227": "5.8 Selective attention  The results in Table 6 examines how the intent prediction accuracy of DES2 varies with respect to the\nnumber of experts in the pool.", 
        "228": "The rationale behind DES2 is to alleviate the downside of soft attention, namely distributing probability mass over all items even if some are bad items.", 
        "229": "To deal with such issues, we apply a hard cut-off at top k domains.", 
        "230": "From the result, a threshold at top 3 or 5 yielded better results than that of either 1 or 25 experts.", 
        "231": "This matches our common sense that their are only a few of domains that are close enough to be of help to a test domain.", 
        "232": "Thus it is advisable to find the optimal k value through several rounds of experiments on a development dataset.", 
        "233": "6 Conclusion  In this paper, we proposed a solution for scaling domains and experiences potentially to a large number of use cases by reusing existing data labeled for different domains and applications.", 
        "234": "Our solution is based on attending an ensemble of domain experts.", 
        "235": "When given a new domain, our model uses a weighted combination of domain experts\u2019 feedback along with its own opinion to make prediction on the new domain.", 
        "236": "In both intent classification and slot tagging tasks, the model significantly outperformed baselines that do not use domain adaptation and also performed better than the full re-training approach.", 
        "237": "This approach enables creation of new virtual domains through a weighted combination of domain experts\u2019 feedback reducing the need to collect and annotate the similar intent and slot types multiple times for different domains.", 
        "238": "Future work can include an extension of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well."
    }, 
    "document_id": "P17-1060.pdf.json"
}
