{
    "abstract_sentences": {
        "1": "Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection.", 
        "2": "This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account.", 
        "3": "We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 433\u2013440 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2069  1 Introduction  Anchored Packed Trees (APTs) is a recently proposed approach to distributional semantics that takes distributional composition to be a process of lexeme contextualisation (Weir et al., 2016).", 
        "2": "A lexeme\u2019s meaning, characterised as knowledge concerning co-occurrences involving that lexeme, is represented with a higher-order dependencytyped structure (the APT) where paths associated with higher-order dependencies connect vertices associated with weighted lexeme multisets.", 
        "3": "The central innovation in the compositional theory is that the APT\u2019s type structure enables the precise alignment of the semantic representation of each of the lexemes being composed.", 
        "4": "Like other countbased distributional spaces, however, it is prone to considerable data sparsity, caused by not observing all plausible co-occurrences in the given data.", 
        "5": "Recently, Kober et al.", 
        "6": "(2016) introduced a simple unsupervised algorithm to infer missing cooccurrence information by leveraging the distributional neighbourhood and ease the sparsity effect in count-based models.", 
        "7": "In this paper, we generalise distributional inference (DI) in APTs and show how precisely\nthe same mechanism that was introduced to support distributional composition, namely \u201coffsetting\u201d APT representations, gives rise to a novel form of distributional inference, allowing us to infer co-occurrences from neighbours of these representations.", 
        "8": "For example, by transforming a representation of white to a representation of \u201cthings that can be white\u201d, inference of unobserved, but plausible, co-occurrences can be based on finding near neighbours (which will be nouns) of the \u201cthings that can be white\u201d structure.", 
        "9": "This furthermore exposes an interesting connection between distributional inference and distributional composition.", 
        "10": "Our method is unsupervised and maintains the intrinsic interpretability of APTs1.", 
        "11": "2 Offset Representations  The basis of how composition is modelled in the APT framework is the way that the co-occurrences are structured.", 
        "12": "In characterising the distributional semantics of some lexeme w, rather than just recording a co-occurrence between w and w\u2032 within some context window, we follow Pado\u0301 and Lapata (2007) and record the dependency path from w to w\u2032.", 
        "13": "This syntagmatic structure makes it possible to appropriately offset the semantic representations of each of the lexemes being composed in some phrase.", 
        "14": "For example many nouns will have distributional features starting with the type amod, which cannot be observed for adjectives or verbs.", 
        "15": "Thus, when composing the adjective white with the noun clothes, the feature spaces of the two lexemes need to be aligned first.", 
        "16": "This can be achieved by offsetting one of the constituents, which we will explain in more detail in this section.", 
        "17": "We will make use of the following nota-\n1We release our code and data at https://github.", 
        "18": "com/tttthomasssss/acl2017\n433\ntion throughout this work.", 
        "19": "A typed distributional feature consists of a path and a lexeme such as in amod:white.", 
        "20": "Inverse paths are denoted by a horizontal bar above the dependency relation such as in dobj:prefer and higherorder paths are separated by a dot such as in amod.compound:dress.", 
        "21": "Offset representations are the central component in the composition process in the APT framework.", 
        "22": "Figure 1 shows the APT representations for the adjective white (left) and the APT for the noun clothes (right), as might have been observed in a text collection.", 
        "23": "Each node holds a multiset of lexemes and the anchor of an APT reflects the current perspective of a lexeme at the given node.", 
        "24": "An offset representation can be created by shifting the anchor along a given path.", 
        "25": "For example the lexeme white is at the same node as other adjectives such as black and clean, whereas nouns such as shoes or noise are typically reached via the amod edge.", 
        "26": "Offsetting in APTs only involves a change in the anchor, the underlying structure remains unchanged.", 
        "27": "By offsetting the lexeme white by amod the anchor is shifted along the amod edge, which results in creating a noun view for the adjective white.", 
        "28": "We denote the offset view of a lexeme for a given path by superscripting the offset path, for example the amod offset of the adjective white is denoted as whiteamod.", 
        "29": "The offsetting procedure changes the starting points of the paths as visible in Figure 1 between the anchors for white and whiteamod, since paths always begin at the anchor.", 
        "30": "The red dashed line in Figure 1 reflects that anchor shift.", 
        "31": "The lexeme whiteamod represents a prototypical \u201cwhite thing\u201d, that is, a noun that has been modified by the adjective white.", 
        "32": "We note that all edges in the APT space are bi-directional as exemplified in the coloured amod and amod edges in the APT for white, however for brevity we only show uni-directional edges in Figure 1.", 
        "33": "By considering the APT representations for the lexemes white and clothes in Figure 1, it becomes apparent that lexemes with different parts of speech are located in different areas of the semantic space.", 
        "34": "If we want to compose the adjective-noun phrase white clothes, we need to offset one of the two constituents to align the feature spaces in order to leverage their distributional commonalities.", 
        "35": "This can be achieved by either creating a noun offset view of white, by shift-\ning the anchor along the amod edge, or by creating an adjective offset representation of clothes by shifting its anchor along amod.", 
        "36": "In this work we follow Weir et al.", 
        "37": "(2016) and always offset the dependent in a given relation.", 
        "38": "Table 1 shows a subset of the features of Figure 1 as would be represented in a vectorised APT.", 
        "39": "Vectorising the whole APT lexicon results in a very highdimensional and sparse typed distributional space.", 
        "40": "The features for whiteamod (middle column) highlight the change in feature space caused by offsetting the adjective white.", 
        "41": "The features of the offset view whiteamod, are now aligned with the noun clothes such that the two can be composed.", 
        "42": "Composition can be performed by either selecting the union or intersection of the aligned features.", 
        "43": "2.1 Qualitative Analysis of Offset Representations  Any offset view of a lexeme is behaviourally identical to a \u201cnormal\u201d lexeme.", 
        "44": "It has an associated part of speech, a distributional representation which locates it in semantic space, and we can find neighbours for it in the same way that we find neighbours for any other lexeme.", 
        "45": "In this way, a single APT data structure is able to provide many different views of any given lexeme.", 
        "46": "These views reflect the different ways in which the lexeme is used.", 
        "47": "For example lawnsubj is the nsubj offset representation of the noun law.", 
        "48": "This lexeme is a verb and represents an action carried out by the law.", 
        "49": "This contrasts with lawdobj, which is the dobj offset representation of the noun law.", 
        "50": "It is also a verb, however represents actions done to the law.", 
        "51": "Table 2 lists the 10 nearest neighbours for a number of lexemes, offset by amod, dobj and nsubj respectively.", 
        "52": "For example, the neighbourhood of the lexeme ancient in Table 2 shows that the offset view for ancientamod is a prototypical representation of an \u201cancient thing\u201d, with neighbours easily associated with the property ancient.", 
        "53": "Furthermore, Table 2\nillustrates that nearest neighbours of offset views are often other offset representations.", 
        "54": "This means that for example actions carried out by a mother tend to be similar to actions carried out by a father or a parent.", 
        "55": "2.2 Offset Inference  Our approach generalises the unsupervised algorithm proposed by Kober et al.", 
        "56": "(2016), henceforth \u201cstandard DI\u201d, as a method for inferring missing knowledge into an APT representation.", 
        "57": "Rather than simply inferring potentially plausible, but unobserved co-occurrences from near distributional neighbours, inferences can be made involving offset APTs.", 
        "58": "For example, the adjective white can be offset so that it represents a noun \u2014 a prototypical \u201cwhite thing\u201d.", 
        "59": "This allows inferring plausible co-occurrences from other \u201cthings that can be white\u201d, such as shoes or shirts.", 
        "60": "Our algorithm therefore reflects the contextualised use of a word.", 
        "61": "This has the advantage of being able to make flexible and fine grained distinctions in the inference process.", 
        "62": "For example if the noun law is used as a subject, our algorithm allows inferring plausible co-occurrences from \u201cother actions carried out by the law\u201d.", 
        "63": "This contrasts the use of law as an object, where offset inference is able to find cooccurrences on the basis of \u201cother actions done to the law\u201d.", 
        "64": "This is a crucial advantage over the method of Kober et al.", 
        "65": "(2016) which only supports inference on uncontextualised lexemes.", 
        "66": "A sketch of how offset inference for a lexeme w works is shown in Algorithm 1.", 
        "67": "Our algorithm requires a distributional model M , an APT representation for the lexeme w for which to perform offset inference, a dependency path p, describing the offset for w, and the number of neighbours k.\nThe offset representation of w\u2032 is then enriched with the information from its distributional neighbours by some merge function.", 
        "68": "We note that if the offset path p is the empty path, we would recover the algorithm presented by Kober et al.", 
        "69": "(2016).", 
        "70": "Our algorithm is unsupervised, and agnostic to the input distributional model and the neighbour retrieval function.", 
        "71": "Algorithm 1 Offset Inference 1: procedure OFFSET INFERENCE(M , w, p, k) 2: w\u2032 \u2190 offset(w, p) 3: for all n in neighbours(M,w\u2032, k) do 4: w\u2032\u2032 \u2190 merge(w\u2032\u2032, n) 5: end for 6: return w\u2032\u2032 7: end procedure\nConnection to Distributional Composition\nAn interesting observation is the similarity between distributional inference and distributional composition, as both operations are realised by the same mechanism \u2014 an offset followed by inferring plausible co-occurrence counts for a single lexeme in the case of distributional inference, or for a phrase in the case of composition.", 
        "72": "The merging of co-occurrence dimensions for distributional inference can also be any of the operations commonly used for distributional composition such as pointwise minimum, maximum, addition or multiplication.", 
        "73": "This relation creates an interesting dynamic between distributional inference and composition when used in a complementary manner as in this work.", 
        "74": "The former can be used as a process of cooccurrence embellishment which is adding miss-\ning information, however with the risk of introducing some noise.", 
        "75": "The latter on the other hand can be used as a process of co-occurrence filtering, that is leveraging the enriched representations, while also sieving out the previously introduced noise.", 
        "76": "3 Experiments  For our experiments we re-implemented the standard DI method of Kober et al.", 
        "77": "(2016) for a direct comparison.", 
        "78": "We built an order 2 APT space on the basis of the concatenation of ukWaC, Wackypedia and the BNC (Baroni et al., 2009), pre-parsed with the Malt parser (Nivre et al., 2006).", 
        "79": "We PPMI transformed the raw co-occurrence counts prior to composition, using a negative SPPMI shift of log 5 (Levy and Goldberg, 2014b).", 
        "80": "We also experimented with composing normalised counts and applying the PPMI transformation after composition as done by Weeds et al.", 
        "81": "(2017), however found composing PPMI scores to work better for this task.", 
        "82": "We evaluate our offset inference algorithm on two popular short phrase composition benchmarks by Mitchell and Lapata (2008) and Mitchell and Lapata (2010), henceforth ML08 and ML10 respectively.", 
        "83": "The ML08 dataset consists of 120 distinct verb-object (VO) pairs and the ML10 dataset contains 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object pairs.", 
        "84": "The goal is to compare a model\u2019s similarity estimates to human provided judgements.", 
        "85": "For both tasks, each phrase pair has been rated by multiple human annotators on a scale between 1 and 7, where 7 indicates maximum similarity.", 
        "86": "Comparison with human judgements is achieved by calculating Spearman\u2019s \u03c1 between the model\u2019s similarity estimates and the scores of each human annotator individually.", 
        "87": "We performed composition by intersection and tuned the number of neighbours by a grid search over {0, 10, 30, 50, 100, 500, 1000} on the ML10 develop-\nment set, selecting 10 neighbours for NNs, 100 for ANs and 50 for VOs for both DI algorithms.", 
        "88": "We calculate statistical significance using the method of Steiger (1980).", 
        "89": "Effect of the number of neighbours Figure 2 shows the effect of the number of neighbours for AN, NN and VO phrases, using offset inference, on the ML10 development set.", 
        "90": "Interestingly, NN compounds exhibit an early saturation effect, while VOs and ANs require more neighbours for optimal performance.", 
        "91": "One explanation for the observed behaviour is that up to some threshold, the neighbours being added contribute actually missing co-occurrence events, whereas past that threshold distributional inference degrades to just generic smoothing that is simply compensating for sparsity, but overwhelming the representations with non-plausible co-occurrence information.", 
        "92": "A similar effect has also been observed by Erk and Pado (2010) in an exemplarbased model.", 
        "93": "Results Table 3 shows that both forms of distributional inference significantly outperform a baseline without DI.", 
        "94": "On average, offset inference outperforms\nthe method of Kober et al.", 
        "95": "(2016) by a statistically significant margin on both datasets.", 
        "96": "Table 4 shows that offset inference substantially outperforms comparable sparse models by Dinu et al.", 
        "97": "(2013) on ML08, achieving a new state-ofthe-art, and matches the performance of the stateof-the-art neural network model of Hashimoto et al.", 
        "98": "(2014) on ML10, while being fully interpretable.", 
        "99": "4 Related Work  Distributional inference has its roots in the work of Dagan et al.", 
        "100": "(1993, 1994), who aim to find probability estimates for unseen words in bigrams, and Schu\u0308tze (1992, 1998) who leverages the distributional neighbourhood through clustering of contexts for word-sense discrimination.", 
        "101": "Recently Kober et al.", 
        "102": "(2016) revitalised the idea for compositional distributional semantic models.", 
        "103": "Composition with distributional semantic models has become a popular research area in recent years.", 
        "104": "Simple, yet competitive methods, are based on pointwise vector addition or multiplication (Mitchell and Lapata, 2008, 2010).", 
        "105": "However, these approaches neglect the structure of the text defining composition as a commutative operation.", 
        "106": "A number of approaches proposed in the literature attempt to overcome this shortcoming by introducing weighted additive variants (Guevara, 2010, 2011; Zanzotto et al., 2010).", 
        "107": "Another popular strand of work models semantic composition on the basis of ideas arising in formal semantics.", 
        "108": "Composition in such models is usually implemented as operations on higher-order tensors (Ba-\nroni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013).", 
        "109": "Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell\u2019Arciprete, 2012; Annesi et al., 2014) as composition functions.", 
        "110": "The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pado\u0301 (2008); Gamallo and Pereira-Farin\u0303a (2017); Levy and Goldberg (2014a); Pado\u0301 and Lapata (2007); Thater et al.", 
        "111": "(2010, 2011); Weeds et al.", 
        "112": "(2014).", 
        "113": "The perhaps most popular approach in the literature to evaluating compositional distributional semantic models is to compare human word and phrase similarity judgements with similarity estimates of composed meaning representations, under the assumption that better distributional representations will perform better at these tasks (Blacoe and Lapata, 2012; Dinu et al., 2013; Erk and Pado\u0301, 2008; Hashimoto et al., 2014; Hermann and Blunsom, 2013; Kiela et al., 2014; Turney, 2012).", 
        "114": "5 Conclusion  In this paper we have introduced a novel form of distributional inference that generalises the method introduced by Kober et al.", 
        "115": "(2016).", 
        "116": "We have shown its effectiveness for semantic composition on two benchmark phrase similarity tasks where we achieved state-of-the-art performance while retaining the interpretability of our model.", 
        "117": "We have furthermore highlighted an interesting connection between distributional inference and distributional composition.", 
        "118": "In future work we aim to apply our novel method to improve modelling selectional preferences, lexical inference, and scale up to longer phrases and full sentences."
    }, 
    "document_id": "P17-2069.pdf.json"
}
