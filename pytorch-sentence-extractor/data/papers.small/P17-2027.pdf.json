{
    "abstract_sentences": {
        "1": "In this work, we propose a novel, implicitly-defined neural network architecture and describe a method to compute its components.", 
        "2": "The proposed architecture forgoes the causality assumption used to formulate recurrent neural networks and instead couples the hidden states of the network, allowing improvement on problems with complex, long-distance dependencies.", 
        "3": "Initial experiments demonstrate the new architecture outperforms both the Stanford Parser and baseline bidirectional networks on the Penn Treebank Part-ofSpeech tagging task and a baseline bidirectional network on an additional artificial random biased walk task."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 172\u2013177 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2027  1 Introduction  Feedforward neural networks were designed to approximate and interpolate functions.", 
        "2": "Recurrent Neural Networks (RNNs) were developed to predict sequences.", 
        "3": "RNNs can be \u2018unwrapped\u2019 and thought of as very deep feedforward networks, with weights shared between each layer.", 
        "4": "Computation proceeds one step at a time, like the trajectory of an ordinary differential equation when solving an initial value problem.", 
        "5": "The path of an initial value problem depends only on the current state and the current value of the forcing function.", 
        "6": "In a RNN, the analogy is the current hidden state and the current input sequence.", 
        "7": "However, in certain applications in natural language processing, especially those with long-distance dependencies or where grammar matters, sequence predic-\n\u2217This work is sponsored by the Air Force Research Laboratory under Air Force contract FA-8721-05-C-0002.", 
        "8": "Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.", 
        "9": "tion may be better thought of as a boundary value problem.", 
        "10": "Changing the value of the forcing function (analogously, of an input sequence element) at any point in the sequence will affect the values everywhere else.", 
        "11": "The bidirectional recurrent network (Schuster and Paliwal, 1997) attempts to addresses this problem by creating a network with two recurrent hidden states \u2013 one that progresses in the forward direction and one that progresses in the reverse.", 
        "12": "This allows information to flow in both directions, but each state can only consider information from one direction.", 
        "13": "In practice many algorithms require more than two passes through the data to determine an answer.", 
        "14": "We provide a novel mechanism that is able to process information in both directions, with the motivation being a program which iterates over itself until convergence.", 
        "15": "1.1 Related Work  Bidirectional, long-distance dependencies in sequences have been an issue as long as there have been NLP tasks, and there are many approaches to dealing with them.", 
        "16": "Hidden Markov models (HMMs) (Rabiner, 1989) have been used extensively for sequencebased tasks, but they rely on the Markov assumption \u2013 that a hidden variable changes its state based only on its current state and observables.", 
        "17": "In finding maximum likelihood state sequences, the Forward-Backward algorithm can take into account the entire set of observables, but the underlying model is still local.", 
        "18": "In recent years, popularity of the Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and variants such as the Gated Recurrent Unit (GRU) (Cho et al., 2014) has soared, as they enable RNNs to process long sequences without the problem of vanishing or exploding gradients (Pascanu et al., 2013).", 
        "19": "However, these models\n172\nonly allow for information/gradient information to flow in the forward direction.", 
        "20": "The Bidirectional LSTM (b-LSTM) (Graves and Schmidhuber, 2005), a natural extension of (Schuster and Paliwal, 1997), incorporates past and future hidden states via two separate recurrent networks, allowing information/gradients to flow in both directions of a sequence.", 
        "21": "This is a very loose coupling, however.", 
        "22": "In contrast to these methods, our work goes a step further, fully coupling the entire sequences of hidden states of an RNN.", 
        "23": "Our work is similar to (Finkel et al., 2005), which augments a CRF with long-distance constraints.", 
        "24": "However, our work differs in that we extend an RNN and uses NetwonKrylov (Knoll and Keyes, 2004) instead of Gibbs Sampling.", 
        "25": "2 The Implicit Neural Network (INN)    2.1 Traditional Recurrent Neural Networks  A typical recurrent neural network has a (possibly transformed) input sequence [\u03be1, \u03be2, .", 
        "26": ".", 
        "27": ".", 
        "28": ", \u03ben] and initial state hs and iteratively produces future states:\nh1 = f(\u03be1, hs) h2 = f(\u03be2, h1) .", 
        "29": ".", 
        "30": ".", 
        "31": "hn = f(\u03ben, hn\u22121)\nThe LSTM, GRU, and related variants follow this formula, with different choices for the state transition function.", 
        "32": "Computation proceeds linearly, with each next state depending only on inputs and previously computed hidden states.", 
        "33": "2.2 Proposed Architecture  In this work, we relax this assumption by allowing ht = f(\u03bet, ht\u22121, ht+1)1.", 
        "34": "This leads to an implicit set of equations for the entire sequence of hidden states, which can be thought of as a single tensor\n1A wider stencil can also be used, e.g.", 
        "35": "f(ht\u22122, ht\u22121, .", 
        "36": ".", 
        "37": ".).", 
        "38": "H: H = [h1, h2, .", 
        "39": ".", 
        "40": ".", 
        "41": ", hn]\nThis yields a system of nonlinear equations.", 
        "42": "This setup has the potential to arrive at nonlocal, whole sequence-dependent results.", 
        "43": "We also hope such a system is more \u2018stable\u2019, in the sense that the predicted sequence may drift less from the true meaning, since errors will not compound with each time step in the same way.", 
        "44": "There are many potential ways to architect a neural network \u2013 in fact, this flexibility is one of deep learning\u2019s best features \u2013 but we restrict our discussion to the structure depicted in Figure 2.", 
        "45": "In this setup, we have the following variables:\ndata X labels Y parameters \u03b8\nand functions:\ninput layer transformation \u03be = g(\u03b8,X) implicit hidden layer def.", 
        "46": "H = F (\u03b8, \u03be,H) loss function L = `(\u03b8,H, Y )\nOur implicit definition function, F , is made up of local state transitions and forms a system of nonlinear equations that require solving, denoting n as the length of the input sequence and hs, he as boundary states:\nh1 = f(hs, h2, \u03be1) .", 
        "47": ".", 
        "48": ".", 
        "49": "hi = f(hi\u22121, hi+1, \u03bei) .", 
        "50": ".", 
        "51": ".", 
        "52": "hn = f(hn\u22121, he, \u03ben)  2.3 Computing the forward pass  To evaluate the network, we must solve the equationH = F (H).", 
        "53": "We computed this via an approximate Newton solve, where we successively refine an approximation Hn of H:\nHn+1 = Hn \u2212 (I \u2212\u2207HF )\u22121(Hn \u2212 F (Hn))\nLet k be the dimension of a single hidden state.", 
        "54": "(I \u2212\u2207HF ) is a sparse matrix, since\u2207HF is zero except for k pairs of n \u00d7 n block matrices, corresponding to the influence of the left and right neighbors of each state.", 
        "55": "Because of this sparsity, we can apply Krylov subspace methods (Knoll and Keyes, 2004), specifically the BiCG-STAB method (Van der Vorst, 1992), since the system is non-symmetric.", 
        "56": "This has the added advantage of only relying on matrix-vector multiplies of the gradient of F .", 
        "57": "2.4 Gradients  In order to train the model, we perform stochastic gradient descent.", 
        "58": "We take the gradient of the loss function:\n\u2207\u03b8L = \u2207\u03b8`+\u2207H`\u2207\u03b8H\nThe gradient of the hidden units with respect to the parameters can found via the implicit definition:\n\u2207\u03b8H = \u2207\u03b8F +\u2207HF\u2207\u03b8H +\u2207\u03beF\u2207\u03b8\u03be = (I \u2212\u2207HF )\u22121 (\u2207\u03b8F +\u2207\u03beF\u2207\u03b8\u03be)\nwhere the factorization follows from the noting that\n(I \u2212\u2207HF )\u2207\u03b8H = \u2207\u03b8F +\u2207\u03beF\u2207\u03b8\u03be.", 
        "59": "The entire gradient is thus:\n\u2207\u03b8L =\u2207H`(I \u2212\u2207HF )\u22121 (\u2207\u03b8F +\u2207\u03beF\u2207\u03b8\u03be) +\u2207\u03b8`\n(1) Once again, the inverse of I \u2212\u2207HF appears, and we can compute it via Krylov subspace methods.", 
        "60": "It is worth mentioning the technique of computing parameter updates by implicit differentiation and conjugate gradients have been applied before, in the context of energy minimization models in image labeling and denoising (Domke, 2012).", 
        "61": "2.5 Transition Functions  Recall the original GRU equations (Cho et al., 2014), with slight notational modifications:\nfinal h ht = (1\u2212 zt)h\u0302t + zth\u0303t candidate h h\u0303t = tanh(Wxt + U(rth\u0302t) + b\u0303) update weight zt = \u03c3(Wzxt + Uzh\u0302t + bz) reset gate rt = \u03c3(Wrxt + Urh\u0302t + br)\nWe make the following substitution for h\u0302t (which was set to ht\u22121 in the original GRU definition):\nstate comb.", 
        "62": "h\u0302t = sht\u22121 + (1\u2212 s)ht+1 switch s = spsp+sn prev.", 
        "63": "switch sp = \u03c3(Wpxt + Upht\u22121 + bp) next switch sn = \u03c3(Wnxt + Unht+1 + bn) (2) This modification makes the architecture both implicit and bidirectional, since h\u0302t is a linear combination of previous and future hidden states.", 
        "64": "The switch variable s is determined by a competition between two sigmoidal units sp and sn, representing the contributions of the previous and next hidden states, respectively.", 
        "65": "2.6 Implementation Details  We implemented the implicit GRU structure using Theano (Bergstra et al., 2011).", 
        "66": "The product \u2207HFv for various v, required for the BiCG-STAB method, was computed via the Rop operator.", 
        "67": "In computing \u2207\u03b8L (Equation 1), we noted it is more efficient to compute \u2207H`(I \u2212\u2207HF )\u22121 first, and thus used the Lop operator.", 
        "68": "All experiments used a batch size of 20.", 
        "69": "To batch solve the linear equations, we simply solved a single, very large block diagonal system of equations: each sequence in the batch was a single block matrix, and we input the encompassing matrix into our Theano BiCG solver.", 
        "70": "(In practice the block diagonal system is represented as a 3-tensor, but it is equivalent.)", 
        "71": "In this setup, each step does receive separate update directions, but one global step length.", 
        "72": "hS and he were fixed at zero, but could be trained as parameters.", 
        "73": "In solving multiple simultaneous systems of equations, we noted some elements converged significantly faster than others.", 
        "74": "For this reason, we found it helpful to run Newton\u2019s method from two separate initializations for each element in our batch, one selected randomly and the other set to a\n\u201cone-step\u201d approximation: Hidden states of a traditional GRU were computed in both forward (hfi ) and reverse (hbi ) directions, and hi was initialized to f(hfi\u22121, h b i+1, \u03bei).", 
        "75": "If either of the two candidates converged, we took its value and stopped computing the other.", 
        "76": "We also limited both the number Newton iterations and BiCG-STAB iterations per Newton iteration to 40.", 
        "77": "3 Experiments    3.1 Biased random walks  We developed an artificial task with bidirectional sequence-level dependencies to explore the performance of our model.", 
        "78": "Our task was to find the point at which a random walk, in the spirit of the Wiener Process (Durrett, 2010), changes from a zero to nonzero mean.", 
        "79": "We trained a network to predict when the walk is no longer unbiased.", 
        "80": "We generated algorithmic data for this problem, the specifics of which are as follows: First, we chose an integer interval lengthN uniformly in the range 1 to 40.", 
        "81": "Then, we chose a (continuous) time t\u2032 \u2208 [0, N), and a direction v \u2208 Rd.", 
        "82": "We produced the input sequence xi \u2208 Rd, setting x0 = 0 and iteratively computing xi+1 = xi +N (0, 1).", 
        "83": "After time t, a bias term of b \u00b7 v was added at each time step (b\u00b7v\u00b7(t\u2032\u2212t)) for the first time step greater than t\u2032.", 
        "84": "b is a global scalar parameter.", 
        "85": "The network was fed in these elements, and asked to predict y = 0 for times t \u2264 t\u2032 and y = 1 for times t > t\u2032.", 
        "86": "For each architecture, \u03be was simply the unmodified input vectors, zero-padded to the embedding dimension size.", 
        "87": "The output was a simple binary logistic regression.", 
        "88": "We produced 50,000 random training examples, 2500 random validation examples, and 5000 random test examples.", 
        "89": "The implicit algorithm used a hidden dimension of 200, and the b-LSTM had an embedding dimension ranging from 100 to 1000. b-LSTM dimension of 300 was the point where the total number of parameters were roughly equal.", 
        "90": "The results are shown in Table 1.", 
        "91": "The b-LSTM scores reported are the maximum over sweeps from 100 to 1500 hidden dimension size.", 
        "92": "The INN outperforms the best b-LSTM in the more challenging cases where the bias size b is small.", 
        "93": "3.2 Part-of-speech tagging  We next applied our model to a real-world problem.", 
        "94": "Part-of-speech tagging fits naturally in the sequence labeling framework, and has the advantage\nof a standard dataset that we can use to compare our network with other techniques.", 
        "95": "To train a partof-speech tagger, we simply let L be a softmax layer transforming each hidden unit output into a part of speech tag.", 
        "96": "Our input encoding \u03be, is a concatenation of three sets of features, adapted from (Huang et al., 2015): first, word vectors for 39,000 case-insensitive vocabulary words; second, six additional \u2018word vector\u2019 components indicating the presence of the top-2000 most common prefixes and suffixes of words, for affix lengths 2 to 4; and finally, eight other binary features to indicate the presence of numbers, symbols, punctuation, and more rich case data.", 
        "97": "We trained the Part of Speech (POS) tagger on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993), blocks 0-18, validated on 19-21, and tested on 22-24, per convention.", 
        "98": "Training was done using stochastic gradient descent, with an initial learning rate of 0.5.", 
        "99": "The learning rate was halved if validation perplexity increased.", 
        "100": "Word vectors were of dimension 320, prefix and suffix vectors were of dimension 20.", 
        "101": "Hidden unit size was equal to feature input size, so in this case, 448.", 
        "102": "As shown in Table 2, the INN outperformed baseline GRU, bidirectional GRU, LSTM, and bLSTM networks, all with 628-dimensional hidden layers (1256 for the bidirectional architectures), The INN also outperforms the Stanford Part-ofSpeech tagger (Toutanova et al., 2003) (model wsj-0-18-bidirectional-distsim.tagger from 10-31-2016).", 
        "103": "Note that performance gains past approximately 97% are difficult due to errors/inconsistencies in the dataset, ambiguity, and complex linguistic constructions including dependencies across sentence boundaries (Manning, 2011).", 
        "104": "4 Time Complexity  The implicit experiments in this paper took approximately 3-5 days to run on a single Tesla K40, while the explicit experiments took approximately 1-3 hours.", 
        "105": "Running time of the solver is approximately nn \u00d7 nb \u00d7 tb where nn is the number of Newton iterations, nb is the number of BiCGSTAB iterations, and tb is the time for a single BiCG-STAB iteration.", 
        "106": "tb is proportional to the number of non-zero entries in the matrix (Van der Vorst, 1992), in our case n(2k2 + 1).", 
        "107": "Newton\u2019s method has second order convergence (Isaacson and Keller, 1994), and while the specific bound depends on the norm of (I \u2212\u2207HF )\u22121 and the norm of its derivatives, convergence is wellbehaved.", 
        "108": "For nb, however, we are not aware of a bound.", 
        "109": "For symmetric matrices, the Conjugate Gradient method is known to take O( \u221a \u03ba) iterations (Shewchuk et al., 1994), where \u03ba is the condition number of the matrix.", 
        "110": "However, our matrix is nonsymmetric, and we expect \u03ba to vary from problem to problem.", 
        "111": "Because of this, we empirically estimated the correlation between sequence length and total time to compute a batch of 20 hidden layer states.", 
        "112": "For the random walk experiment with b = 0.5, we found the the average run time for a given sequence length to be approximately 0.17n1.8, with r2 = 0.994.", 
        "113": "Note that the exponent would have been larger had we not truncated the number of BiCG-STAB iterations to 40, as the inner iteration frequently hit this limit for larger n. However, the average number of Newton iterations did not go above 10, indicating that exiting early from the BiCG-STAB loop did not prevent the Newton solver from converging.", 
        "114": "Run times for the other random walk experiments were very similar, indicating run time does not depend on b; However, for the POS task runtime was 0.29n1.3, with\nr2 = 0.910.", 
        "115": "5 Conclusion and Future Work  We have introduced a novel, implicitly defined neural network architecture based on the GRU and shown that it outperforms a b-LSTM on an artificial random walk task and slightly outperforms both the Stanford Parser and a baseline bidirectional network on the Penn Treebank Part-ofSpeech tagging task.", 
        "116": "In future work, we intend to consider implicit variations of other architectures, such as the LSTM, as well as additional, more challenging, and/or data-rich applications.", 
        "117": "We also plan to explore ways to speed up the computation of (I\u2212\u2207HF )\u22121.", 
        "118": "Potential speedups include approximating the hidden state values by reducing the number of Newton and/or BiCG-STAB iterations, using cached previous solutions as initial values, and modifying the gradient update strategy to keep the batch full at every Newton iteration.", 
        "119": "6 Acknowledgements  This work would not be possible without the support and funding of the Air Force Research Laboratory.", 
        "120": "We also acknowledge Nick Malyska, Elizabeth Salesky, and Jonathan Taylor at MIT Lincoln Lab for interesting technical discussions related to this work.", 
        "121": "Cleared for Public Release on 29 Jul 2016.", 
        "122": "Originator reference number: RH-16-115722.", 
        "123": "Case Number: 88ABW2016-3809."
    }, 
    "document_id": "P17-2027.pdf.json"
}
