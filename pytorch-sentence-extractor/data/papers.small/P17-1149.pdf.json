{
    "abstract_sentences": {
        "1": "Integrating text and knowledge into a unified semantic space has attracted significant research interests recently.", 
        "2": "However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities.", 
        "3": "In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base.", 
        "4": "In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense.", 
        "5": "In experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings.", 
        "6": "Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1623\u20131633 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1149\n1 Introduction\nJointly learning text and knowledge representations in a unified vector space greatly benefits many Natural Language Processing (NLP) tasks, such as knowledge graph completion (Han et al., 2016; Wang and Li, 2016), relation extraction (Weston et al., 2013), word sense disambiguation (Mancini et al., 2016), entity classification (Huang et al., 2017) and linking (Huang et al., 2015).", 
        "2": "Existing work can be roughly divided into two categories.", 
        "3": "One is encoding words and entities into a unified vector space using Deep Neural\n\u2217Corresponding author.", 
        "4": "Networks (DNN).", 
        "5": "These methods suffer from the problems of expensive training and great limitations on the size of word and entity vocabulary (Han et al., 2016; Toutanova et al., 2015; Wu et al., 2016).", 
        "6": "The other is to learn word and entity embeddings separately, and then align similar words and entities into a common space with the help of Wikipedia hyperlinks, so that they share similar representations (Wang et al., 2014; Yamada et al., 2016).", 
        "7": "However, there are two major problems arising from directly integrating word and entity embeddings into a unified semantic space.", 
        "8": "First, mention phrases are highly ambiguous and can refer to multiple entities in the common space.", 
        "9": "As shown in Figure 1, the same mention independence day (m1) can either refer to a holiday: Independence Day (US) or a film: Independence Day (film).", 
        "10": "Second, an entity often has various aliases when mentioned in various contexts, which implies a much larger size of mention vocabulary compared with entities.", 
        "11": "For example, in Figure 1, the documents d2 and d3 describes the same entity Independence Day (US) (e2) with distinct mentions: independence day and July 4th.", 
        "12": "We observe tens of millions of mentions referring to 5 millions of entities in Wikipedia.", 
        "13": "To address these issues, we propose to learn multiple embeddings for mentions inspired by the Word Sense Disambiguation (WSD) task (Reisinger and Mooney, 2010; Huang et al., 2012;\n1623\nTian et al., 2014; Neelakantan et al., 2014; Li and Jurafsky, 2015).", 
        "14": "The basic idea behind it is to consider entities in KBs that can provide a meaning repository of mentions (i.e.", 
        "15": "words or phrases) in texts.", 
        "16": "That is, each mention has one or multiple meanings, namely mention senses, and each sense corresponds to an entity.", 
        "17": "Furthermore, we assume that different mentions referring to the same entity express the same meaning and share a common mention sense embedding, which largely reduces the size of mention vocabulary to be learned.", 
        "18": "For example, the mentions Independence Day in d2 and July 4th in d3 have a common mention sense embedding during training since they refer to the same holiday.", 
        "19": "Thus, text and knowledge are bridged via mention sense.", 
        "20": "In this paper, we propose a novel MultiPrototype Mention Embedding (MPME) model, which jointly learns the representations of words, entities, and mentions at sense level.", 
        "21": "Different mention senses are distinguished by taking advantage of both textual context information and knowledge of reference entities.", 
        "22": "Following the frameworks in (Wang et al., 2014; Yamada et al., 2016), we use separate models to learn the representations for words, entities and mentions, and further align them by a unified optimization objective.", 
        "23": "Extending from skip-gram model and CBOW model, our model can be trained efficiently (Mikolov et al., 2013a,b) from a large scale corpus.", 
        "24": "In addition, we also design a language model based approach to determine the sense for each mention in a document based on multi-prototype mention embeddings.", 
        "25": "For evaluation, we first provide qualitative analysis to verify the effectiveness of MPME to bridge text and knowledge representations at the sense level.", 
        "26": "Then, separate tasks for words and entities show improvements by using our word, entity and mention representations.", 
        "27": "Finally, using entity linking as a case study, experimental results on the benchmark dataset demonstrate the effectiveness of our embedding model as well as the disambiguation method.", 
        "28": "2 Preliminaries\nIn this section, we formally define the input and output of multi-prototype mention embedding.", 
        "29": "A knowledge baseKB contains a set of entities E = {ej}, and their relations.", 
        "30": "We use Wikipedia as the given knowledge base, and organize it as a\ndirected knowledge network: nodes denote entities, and edges are outlinks from Wikipedia pages.", 
        "31": "In the directed network, we define the entities that point to ej as its neighborsN (ej), but ignore those entities that ej points to, so that the repeated computations on the same edge would be avoided if edges were undirected.", 
        "32": "A text corpus D is a set of sequential words D = {w1, \u00b7 \u00b7 \u00b7 , wi, \u00b7 \u00b7 \u00b7 , w|D|}, where wi is the ith word and |D| is the length of the word sequence.", 
        "33": "Since an entity mention ml may consist of multiple words, we define an annotated text corpus1 as D\u2032 = {x1, \u00b7 \u00b7 \u00b7 , xi, \u00b7 \u00b7 \u00b7 , x|D\u2032|}, where xi corresponds to either a word wi or a mention ml.", 
        "34": "We define the words around xi within a predefined window as its context words C(xi).", 
        "35": "An Anchor is a Wikipedia hyperlink from a mention ml linking to its entity ej , and is represented as a pair< mh, ej >\u2208 A.", 
        "36": "The anchors provide mention boundaries as well as their reference entities from Wikipedia articles.", 
        "37": "These Wikipedia articles are used as an annotated text corpus D\u2032 in this paper.", 
        "38": "Multi-Prototype Mention Embedding .", 
        "39": "Given a KB, an annotated text corpus D\u2032 and a set of anchors A, we aim to learn multi-prototype mention embedding, namely multiple sense embeddings sjl \u2208 Rk for each mention ml as well as word embeddings w and entity embeddings e. We useM\u2217l = {slj} to denote the sense set of mention ml, where each slj refers to an entity ej .", 
        "40": "Thus, the vocabulary size is reduced to a fixed number |{s\u2217j}| = |E|.", 
        "41": "We use s\u2217j to denote the shared sense of mentions referring to entity ej .", 
        "42": "Example As shown in Figure 1, Independence Day (m1) has two mention senses s11, s 1 2, and July 4th (m2) has one mention sense s22.", 
        "43": "Based on the assumption in Section 1, we have s\u22172 = s 1 2 = s 2 2 referring to entity Independence Day (US) (e2).", 
        "44": "3 An Overview of Our Method\nGiven a knowledge base KB, an annotated text corpusD\u2032 and a set of anchorsA, we aim to jointly learn word, entity and mention sense representations: w, e, s.\nAs shown in Figure 2, our framework contains two key components:\n1Generally, the mention boundary can be obtained by using NER tools like Standford NER (Finkel et al., 2005).", 
        "45": "In this paper, we use Wikipedia anchors as annotations of Wikipedia text corpus for the concentration of our main purpose.", 
        "46": "Representation Learning\nEntity Representation Learning\nText Representation Learning\nbands played it during public events,\nsuch as [[Independence Day\n(US)|July 4th]] celebrations\n\u2026 In the 1996 action film [[Independence Day (film)|Independence Day]], the United States\nmilitary uses alien technology captured \u2026\n4\n300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nACL 2016 Submission ***.", 
        "47": "Confidential review copy.", 
        "48": "DO NOT DISTRIBUTE.", 
        "49": "computing similarity between word and mention embeddings referring to that entity.", 
        "50": "3 Method In this section, we present three main components in MPME: text model, knowledge model and joint model, and then introduce the detailed information on training process.", 
        "51": "Finally, we briefly introduce the framework for entity linking.", 
        "52": "3.1 Skip-gram model capable of iterative learning; capable of learning more mention names; capable of tuning mention sense via text model; capable of NIL sense; 1. take pre-trained word and entity embeddings as input; 2. collect mention name to entity title mapping; use anchor to annotate each mention.", 
        "53": "each mention corresponds multiple sense; each sense relates to one entity title.", 
        "54": "3. given the context and the mention\u2019s sense, predict the entity; got entity title embedding.", 
        "55": "4. each title has multiple vector, each corresponds to a different entity.", 
        "56": "maintain the context cluster; the cluster role.", 
        "57": "5. text model again, use context to predict mention sense, to predict the context; also can predict a new sense, called NIL in EL tasks, future work.", 
        "58": "3.2 Text model Lw = TX t=1 logP (wt+j |wmt , si)P (si|wcontext) + TX t=1 X cjc,j 6=0 logP (wt+j |wt) (1) DX CX P (wt+j |wmt , si)P (si|wmt , wcontext)\n3.3 Knowledge model KBX NX\nP (eneighbor|ei)\n3.4 Joint model AX\nP (ej |wmt , si) + P (ej |wcontext)\neIndependence Day (film)\neIndependence Day (US)\nwm1Independence Day\nwm2Independence Day 3.5 Training 3.6 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "59": "2. align mention with entity using single prototype model.", 
        "60": "4.3 Parameter Setting 4.4 Text Evaluation 4.5 Entity Evaluation 4.6 EL evaluation 5 Related Work 6 Conclusion References Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "61": "2016.", 
        "62": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "63": "CoRR, abs/1611.04125.", 
        "64": "Hongzhao Huang, Larry Heck, and Heng Ji.", 
        "65": "2015.", 
        "66": "Leveraging deep neural networks and knowledge graphs for entity disambiguation.", 
        "67": "CoRR, abs/1504.07678.", 
        "68": "Massimiliano Mancini, Jose\u0301 Camacho-Collados, Ignacio Iacobacci, and Roberto Navigli.", 
        "69": "2016.", 
        "70": "Embedding words and senses together via joint knowledgeenhanced training.", 
        "71": "CoRR, abs/1612.02703.", 
        "72": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", 
        "73": "2013a.", 
        "74": "Efficient estimation of word representations in vector space.", 
        "75": "CoRR, abs/1301.3781.", 
        "76": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.", 
        "77": "2013b.", 
        "78": "Distributed representations of words and phrases and their compositionality.", 
        "79": "In Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "80": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 3111\u20133119.", 
        "81": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon.", 
        "82": "2015.", 
        "83": "Representing text for joint embedding of text and knowledge bases.", 
        "84": "ACL Association for Computational Linguistics.", 
        "85": "4\n300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 73 374 375 376 377 378 379 380 381 382 383 384 385 386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nACL 2016 Submission ***.", 
        "86": "Confidential review copy.", 
        "87": "DO NOT DISTRIBUTE.", 
        "88": "computing similarity between word and mention embeddings referring to that entity.", 
        "89": "3 Method In this section, we present three main components in MPME: text model, knowledge model and joint model, and then introduce the detailed information on training process.", 
        "90": "Finally, we briefly introduce the framework for entity linking.", 
        "91": "3.1 Skip-gram model capable of iterative learning; capable of learning more mention names; capable of tuning mention sense via text model; capable of NIL sense; 1. take pre-trained word and entity embeddings as input; 2. collect mention name to entity title mapping; use anchor to annotate each mention.", 
        "92": "each mention corresponds multiple sense; each sense relates to one entity title.", 
        "93": "3. giv n the context and the mentio \u2019s sense, predict the entity; got entity title embedding.", 
        "94": "4. each title has multiple vector, each corre p nds to a diff ren entity.", 
        "95": "maintain the context clust r; the clust r role.", 
        "96": "5. text model again, us context to predict mention sense, to predict the context; lso can predict a n w sense, called NIL in EL tasks, future work.", 
        "97": "3.2 Text model Lw = TX t=1 logP (wt+j |wmt , si)P (si|wcontext) + TX t=1 X cjc,j 6=0 logP (wt+j |wt) (1) DX CX P (wt+j |wmt , si)P (si|wmt , wcontext) 3.3 Knowledge model KBX NX\nP (eneighbor|ei)\n3.4 Joint model AX\nP (ej |wmt , si) + P (ej |wcontext)\neIndependence Day (film)\neIndependence Day (US)\nwm1Independence Day\nwm2Independence Day 3.5 Training 3.6 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "98": "2. align mention with entity using single prototype model.", 
        "99": "4.3 Parameter Setting 4.4 Text Evaluation 4.5 Entity Evaluation 4.6 EL ev luation 5 Related Work 6 Conclusion Refer nces Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "100": "2016.", 
        "101": "Joint representation learning of text and knowledge for knowl d e graph completion.", 
        "102": "CoRR, abs/161 .04125.", 
        "103": "Hongzhao Huang, Larry Heck, and Heng Ji.", 
        "104": "2015.", 
        "105": "Leveraging deep neural networks and knowledge graphs for entity disambiguation.", 
        "106": "CoRR, abs/1504.07678.", 
        "107": "Massimiliano Mancini, Jose\u0301 Camacho-Collados, Ignacio Iacobacci, and Roberto Navigli.", 
        "108": "2016.", 
        "109": "Embedding words and senses together via joint knowledgeenhanced training.", 
        "110": "CoRR, abs/1612.02703.", 
        "111": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", 
        "112": "2013a.", 
        "113": "Efficient estimation of word representations in vector space.", 
        "114": "CoRR, abs/1301.3781.", 
        "115": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.", 
        "116": "2013b.", 
        "117": "Distributed representations of words and phrases and their compositionality.", 
        "118": "In Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "119": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 3111\u20133119.", 
        "120": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Ch dhury, and Micha l Gamon.", 
        "121": "2015.", 
        "122": "Representing text for joint embedding of text and knowledge bases.", 
        "123": "ACL Associatio for Computational Lin uistics.", 
        "124": "Anchor\nText 4\n300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nACL 2016 Submission ***.", 
        "125": "Confidential review copy.", 
        "126": "DO NOT DISTRIBUTE.", 
        "127": "tities by modeling semantic network constructed from the given knowledge base.", 
        "128": "Joint model learns multiple mention embeddings by maximizing the probability of the mention in the context referring to target entity.", 
        "129": "Text model .", 
        "130": "Kg model .", 
        "131": "Joint model .", 
        "132": "As shown in Figure 3, for each anchor ai = (mj , ek), we firstly replace the mention name with entity title m\u21e4t via pre-defined mapping rules.", 
        "133": "Given KB, D and the mapped anchors A, we iteratively train the three models until convergence using a joint optimization objective, which will be introduced later.", 
        "134": "Though following the basic components of three models in (Wang et al., 2014; Yamada et al., 2016), MPME designs different structure in text model and joint model to combine text and knowledge in phrase level via multi-prototype mention embedding, rather than aligning between singleprototype word embeddings and entity embeddings.", 
        "135": "Actually, MPME is flexible to utilize pretrained entity embeddings from arbitrary knowledge representation model, and enjoys their advantages of different aspects in knowledge bases2.", 
        "136": "This is reasonable because we output two separately semantic vector spaces for text and knowledge respectively, while we can still obtain the relatedness between word and entity indirectly by computing similarity between word and mention embeddings referring to that entity.", 
        "137": "3 Method\nIn this section, we present three main components in MPME: text model, knowledge model and joint model, and then introduce the detailed information on training process.", 
        "138": "Finally, we briefly introduce the framework for entity linking.", 
        "139": "3.1 Skip-gram model capable of iterative learning; capable of learning more mention names; capable of tuning mention sense via text model; capable of NIL sense; 1. take pre-trained word and entity embeddings as input; 2. collect mention name to entity title mapping; use anchor to annotate each mention.", 
        "140": "each mention corresponds multiple sense; each sense relates\n2Thus, MPME only trains text model and joint model.", 
        "141": "to one entity title.", 
        "142": "3. given the context and the mention\u2019s sense, predict the entity; got entity title embedding.", 
        "143": "4. each title has mult ple vector, each corresponds to a differe t e tity.", 
        "144": "m intain the co - text cluster; the cluster role.", 
        "145": "5. text model again, use context to predict menti n se s , to predict the context; also can predict a new sense, called NIL in EL tasks, future work.", 
        "146": "3.2 Text model Lw = TX t=1 logP (wt+j |wmt , si)P (si|wcontext) + TX t=1 X cjc,j 6=0 logP (wt+j |wt) (1) DX CX P (wt+j |wmt , si)P (si|wmt , wcontext) .3 Knowledge model KBX N P (eneighbor|ei)\n3.4 Joint model\nAX P (ej |wmt , si) + P (ej |wcontext)\neIndependence Day (film)\neIndependence Day (US)\nwm1Independence Day\nwm2Independence Day\nwfilm\nwcelebrations\nwmMemorial Day\n4\n300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350 351 3 2 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nACL 2016 Submission ***.", 
        "147": "Confidential review copy.", 
        "148": "DO NOT DISTRIBUTE.", 
        "149": "tities by modeling semantic network constructed from the given knowledge base.", 
        "150": "Joint model learns multiple mention embeddings by maximizing the probability of the mention in the context referring to target entity.", 
        "151": "Text model .", 
        "152": "Kg model .", 
        "153": "Joint model .", 
        "154": "As shown in Figure 3, for each anchor ai = (mj , ek), we firstly replace the mention name with entity title m\u21e4t via pre-defined mapping rules.", 
        "155": "Given KB, D and the mapped anchors A, we iteratively train the three models until convergence using a joint optimization objective, which will be introduced later.", 
        "156": "Though following the basic compon n s of three models in (Wang et al., 2014; Yamada et al., 2016), MPME designs different structure i text model and joint model to combine text a d knowledge in phrase level via multi-prototype mention embedding, rather than aligning between singleprototype word embeddings and entity embeddings.", 
        "157": "Actually, MPME is flexible to utilize pretrained entity embeddings from arbitrary knowledge representation model, and enjoys their advantages of different aspects in knowledge bases2.", 
        "158": "This is reasonable because we output two separately semantic vector spaces for text and knowledge respectively, while we can still obtain the relatedness between ord and entity indirectly by computing similarity b tween word and mention embeddings referring to that entity.", 
        "159": "3 Method\nIn this section, we present three main components in MPME: text model, knowledge model and joint model, and then introduce the detailed information on training process.", 
        "160": "Finally, we briefly introduce the framework for entity linking.", 
        "161": "3.1 Skip-gram model capable of iterative learning; capable of learning more mention names; capable of tuning mention sense via text model; capable of NIL sense; 1. take pre-trained word and entity embeddings as input; 2. collect me tion name to entity title mapping; use anchor t nnotate each mention.", 
        "162": "each mention c rre ponds multiple sense; each sense relates\n2Thus, MPME only trains text model and joint model.", 
        "163": "to one entity title.", 
        "164": "3. given the context and the mention\u2019s sense, predict the ntity; g t ntity title emb dding.", 
        "165": "4. each title has multiple v ctor, each corresponds to a different ntity.", 
        "166": "maint in the context cluster; the cluster role.", 
        "167": "5. text model again, use context to predict mention sense, to predict the context; also can predict a new sense, called NIL in EL tasks, future work.", 
        "168": "3.", 
        "169": "Text model Lw = TX t=1 logP (wt+j |wmt , si)P (si|wcontext) + TX t=1 X cjc,j 6=0 logP (wt+j |wt) (1) DX CX P (wt+j |wmt , si)P (si|wmt , wcontext) 3.", 
        "170": "Knowledge model\nKBX NX P ( neighb r| i)\n3.4 Joint model\nAX P (ej |wmt , si) + P (ej |wcontext)\neIndependence Day (film)\neIndependence Day (US)\nwm1Independence Day\nwm2Independence Day\nwfilm\nwcelebrations\nwmMemorial Day\n5\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2016 Submission ***.", 
        "171": "Confidential review copy.", 
        "172": "DO NOT DISTRIBUTE.", 
        "173": "3.2 Skip-gram model\ng(Independence Day, )\nP (N (ej)|ej) P (ej |C(mh), tsl ) P (C(wi)|wi)P (C(mh)|tsl ,mh)\n3.3 Text model\nLw = TX\nt=1\nlogP (wt+j |wmt , si)P (si|wcontext)\n+ TX\nt=1\nX\ncjc,j 6=0 logP (wt+j |wt)\n(1)\nDX CX P (wt+j |wmt , si)P (si|wmt , wcontext)\n3.4 Knowledge model KBX NX\nP (eneighbor|ei)\n3.5 Joint model AX\nP (ej |wmt , si) + P (ej |wcontext)\n3.6 Training\n3.7 Integrating into GBDT for EL\n4 Experiment\n4.1 Data Preparation\n4.2 Baseline Methods\n1. directly align words with entity.", 
        "174": "2. align mention with entity using single prototype model.", 
        "175": "4.3 Parameter Setting\n4.4 Qualitative Analysis\n4.5 Entity Relatedness\n4.6 Word Analogy\n4.7 EL evaluation\n5 Related Work\n6 Conclusion\nReferences Antoine Bordes, Nicolas Usunier, Alberto Garc\u0131\u0301a-\nDura\u0301n, Jason Weston, and Oksana Yakhnenko.", 
        "176": "2013.", 
        "177": "Translating embeddings for modeling multirelational data.", 
        "178": "In Burges et al.", 
        "179": "(Burges et al., 2013), pages 2787\u20132795.", 
        "180": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "181": "2013.", 
        "182": "Advances in Neural Information Proc ss Systems 26: 27th Annual Conference on eural Information Processing Systems 2013.", 
        "183": "Proceedings of a meeting held December 5-8, 2013, Lake Ta e, Ne da, United States.", 
        "184": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "185": "2016.", 
        "186": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "187": "CoRR, abs/1611.04125.", 
        "188": "Hongzhao Huang, Larry Heck, and Heng Ji.", 
        "189": "2015.", 
        "190": "Leveraging deep neural networks and knowledge graphs for entity disambiguation.", 
        "191": "CoRR, abs/1504.07678.", 
        "192": "Massimiliano Mancini, Jose\u0301 Camacho-Collados, Ignacio Iacobacci, and Roberto Navigli.", 
        "193": "2016.", 
        "194": "Embedding words and senses together via joint knowledgeenhanced training.", 
        "195": "CoRR, abs/1612.02703.", 
        "196": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", 
        "197": "2013a.", 
        "198": "Efficient estimation of word representations in vector space.", 
        "199": "CoRR, abs/1301.3781.", 
        "200": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.", 
        "201": "2013b.", 
        "202": "Distributed representations of words and phrases and their compositionality.", 
        "203": "In Burges et al.", 
        "204": "(Burges et al., 2013), pages 3111\u20133119.", 
        "205": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon.", 
        "206": "2015.", 
        "207": "Representing text for joint embedding of text and knowledge bases.", 
        "208": "ACL Association for Computational Linguistics.", 
        "209": "Zhigang Wang and Juan-Zi Li.", 
        "210": "2016.", 
        "211": "Text-enhanced representation learning for knowledge graph.", 
        "212": "In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 1293\u20131299.", 
        "213": "IJCAI/AAAI Press.", 
        "214": "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.", 
        "215": "2014.", 
        "216": "Knowledge graph and text jointly embedding.", 
        "217": "In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1591\u20131601.", 
        "218": "ACL.", 
        "219": "Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier.", 
        "220": "2013.", 
        "221": "Connecting language and knowledge bases with embedding models for relation extraction.", 
        "222": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1366\u20131371.", 
        "223": "ACL.", 
        "224": "Independence Day (US)\nUnited States\nF reworks\nIndependence Day (film)\nMemorial Day\nCelebrations\nOb se\nrv ed\nb y\nPublic holidays in the United States\ncat ego\nry\nWill Smith\nst ar\nrin g\nPhiladelphia\nbo rn country\ninlink\nout link inlink\nKnowledge Base\n5\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2016 Submission ***.", 
        "225": "Confidential review copy.", 
        "226": "DO NOT DISTRIBUTE.", 
        "227": "3.2 Skip-gram model\ng(Independence Day, )\nP (N (ej)|ej) P (ej |C(mh), tsl ) e1 e2\nP (C(wi)|wi) \u00b7 P (C(mh)|tsl ,mh)\n(1)\nt1I dependence Day t2Indep nden e Day t1Memorial Day\ng(Independence Day,\nIndependence Day (US)) (2)\ng(Independence Day) g(July 4th) (3)\n3.3 Text model\nLw = TX\nt=1\nlogP (wt+j |wmt , si)P (si|wcontext)\n+ TX\nt=1\nX\ncjc,j 6=0 logP (wt+j |wt)\n(4)\nDX CX P (wt+j |wmt , si)P (si|wmt , wcontext)\n3.4 Knowledge model KBX NX\nP (eneighbor|ei)\n3.5 Joint model AX\nP (ej |wmt , si) + P (ej |wcontext)\n3.6 Training\n3.7 Integrating into GBDT for EL\n4 Experiment\n4.1 Data Preparation\n4.2 Baseline Methods\n1. directly align words with entity.", 
        "228": "2. align mention with entity using single prototype model.", 
        "229": "4.3 Parameter Setting 4.4 Qualitative Analysis 4.5 Entity Relatedness 4.6 Word Analogy 4.7 EL evaluation\n5 Related Work\n6 Conclusion\nReferences Antoine Bordes, Nicolas Usunier, Alberto Garc\u0131\u0301a-\nDura\u0301n, Jason Weston, and Oksana Yakhnenko.", 
        "230": "2013.", 
        "231": "Translating embeddings for modeling multirelati nal data.", 
        "232": "In Burges et al.", 
        "233": "(Burges et al., 2013), pages 2787\u20132795.", 
        "234": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "235": "2013.", 
        "236": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Proc ssing Systems 2013.", 
        "237": "Proceedings of a eeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "238": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "239": "2016.", 
        "240": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "241": "CoRR, abs/1611.04125.", 
        "242": "Hongzhao Huang, Larry Heck, and Heng Ji.", 
        "243": "2015.", 
        "244": "Leveraging deep neural networks and knowledge graphs for entity disambiguation.", 
        "245": "CoRR, abs/1504.07678.", 
        "246": "Massimiliano Mancini, Jose\u0301 Camacho-Collados, Ignacio Iacobacci, and Roberto Navigli.", 
        "247": "2016.", 
        "248": "Embedding words and senses together via joint knowledgeenhanced training.", 
        "249": "CoRR, abs/1612.02703.", 
        "250": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean.", 
        "251": "2013a.", 
        "252": "Efficient estimation of word representations in vector space.", 
        "253": "CoRR, abs/1301.3781.", 
        "254": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.", 
        "255": "2013b.", 
        "256": "Distributed representations of words and phrases and their compositionality.", 
        "257": "In Burges et al.", 
        "258": "(Burges et al., 2013), pages 3111\u20133119.", 
        "259": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon.", 
        "260": "2015.", 
        "261": "Representing text for joint embedding of text and knowledge bases.", 
        "262": "ACL Association for Computational Linguistics.", 
        "263": "Zhigang W ng and Juan-Zi Li.", 
        "264": "2016.", 
        "265": "Text-enhanced representation learning for knowledge graph.", 
        "266": "In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 1293\u20131299.", 
        "267": "IJCAI/AAAI Press.", 
        "268": "5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479\n480\n481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\nACL 2016 Submission ***.", 
        "269": "Confidential revi w copy.", 
        "270": "DO NOT DISTRIBUTE.", 
        "271": "3.2 Skip-gram model\ng(Independence Day, )\nP (N (ej)|ej) P (ej |C(mh), tsl ) e1 e2\nP (C(wi)|wi) \u00b7 P (C(mh)|tsl ,mh)\n(1)\nt1Independence Day t2Independence Day t1Memorial Day\ng(Independence Day,\nIndependenc Day (US)) (2)\ng(Independence Day) g(July 4th) (3)\n3.3 Text model\nLw = TX\nt=1\nlogP (wt+j |wmt , si)P (si|wcontext)\n+ TX\nt=1\nX\ncjc,j 6=0 logP (wt+j |wt)\n(4)\nDX CX\nP (wt+j |wmt , si)P (si|wmt , wcontext)\n3.4 Knowledge model KBX NX\nP (eneighbor|ei)\n3.5 Joint model AX\nP (ej |wmt , si) + P (ej |wco text)\n3.6 Training\n3.7 Integrating into GBDT for EL\n4 Experiment\n4.1 Data Preparation\n4.2 Baseline Methods\n1. directly align words with entity.", 
        "272": "2. align mention with entity using single prototype model.", 
        "273": "4.3 Parameter Setting 4.4 Qualitative Analysis 4.5 Entity Relatedness 4.6 Word Analogy 4.7 EL evaluatio\n5 Relat Work\n6 Conclusion\nReferences Antoine Bordes, Nicolas Usunier, Alberto Garc\u0131\u0301a-\nDura\u0301n, Jason Weston, and Oksana Yakhnenko.", 
        "274": "2013.", 
        "275": "Translating embeddings for modeling multirelational data.", 
        "276": "In Burges et al.", 
        "277": "(Burges et al., 2013), pages 2787\u20132795.", 
        "278": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "279": "2013.", 
        "280": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Syst ms 2013.", 
        "281": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "282": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "283": "2016.", 
        "284": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "285": "CoRR, abs/1611.04125.", 
        "286": "Hongzhao Huang, Larry Heck, and H ng Ji.", 
        "287": "2015.", 
        "288": "Leveraging deep neural networks and knowledge graphs for entity disambiguation.", 
        "289": "CoRR, abs/1504.07678.", 
        "290": "Massimiliano Mancini, Jose\u0301 Camacho-Collados, Igna-\ncio Iacobacci, and Roberto Navigli.", 
        "291": "2016.", 
        "292": "Embed-\nding words and senses together via joint knowledge-\nenhanced training.", 
        "293": "CoRR, abs/1612.02703.", 
        "294": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", 
        "295": "2013a.", 
        "296": "Efficient estimation of word representations in vector space.", 
        "297": "CoRR, abs/1301.3781.", 
        "298": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.", 
        "299": "2013b.", 
        "300": "Distributed representations of words and phrases and their compositionality.", 
        "301": "In Burges et al.", 
        "302": "(Burges et al., 2013), pages 3111\u20133119.", 
        "303": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon.", 
        "304": "2015.", 
        "305": "Representing text for joint embedding of text and knowledge bases.", 
        "306": "ACL Association for Computational Linguistics.", 
        "307": "Zhigang Wang and Juan-Zi Li.", 
        "308": "2016.", 
        "309": "Text-enhanced representation learning for knowledge graph.", 
        "310": "In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 1293\u20131299.", 
        "311": "IJCAI/AAAI Press.", 
        "312": "5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2016 Submission ***.", 
        "313": "Confidential review copy.", 
        "314": "DO NOT DISTRIBUTE.", 
        "315": "3.2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "316": "Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention senses: entity-centric sense and out-of-KB sense.", 
        "317": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "318": "When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB sense.", 
        "319": "To be concrete, each mention sense has an embedding (sense vector) tsl and a context clust r with center \u00b5(tsl ).", 
        "320": "The representation f the context is defined as the average of the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi)wj.", 
        "321": "We predict tsl , the sense of entity title tl in the m ntion < tl, C(tl) >, when observed with context C(tl) as the context cluster membership.", 
        "322": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl otherwise (5) where is a hyper-parameter and tmaxl = argmaxtsl sim(\u00b5(tsl ), C(tl)).", 
        "323": "We adopt an online non-parametric clustering procedure to learn outof-KB mention senses, which means that if the nearest distance of the context vector to sense cluster center is larger than a threshold, we create a new context cluster and a new sense vector that doesn\u2019t belong to any entity-centric senses.", 
        "324": "The cluster center is the average of all the context vectors belonging to that cluster.", 
        "325": "For the similarity metric, we use cosine in our experiments.", 
        "326": "Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the probability of observing the context words given either a wor wi or a mention se se of entity title tsl :\nLw = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nwi/t s l , , , w, , ej , e (7)\n3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words ith entity.", 
        "327": "2. align mention with ntity using single prototype model.", 
        "328": "4.3 Parameter Setting 4.4 Qualitative Analysis before conducting the experiments on the tasks, we first give qualit tive analysis of words, mentions an entities.", 
        "329": "firstly, we give th phrase embedding by its ne re t words and e titi s. next, we give quantitativ analysis on s veral tasks.", 
        "330": "4.5 Entity Rela ednes 4.6 Word Similar ty 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Relate Work 6 Conclusion References Alfred V Aho and Margaret J Corasick.", 
        "331": "1975.", 
        "332": "Efficient string matching: an aid to bibliographic search.", 
        "333": "Communications of the ACM, 18(6):333\u2013340.", 
        "334": "J-I Aoe.", 
        "335": "1989.", 
        "336": "An efficient digital search algorithm by using a double-array structure.", 
        "337": "IEEE Tra sactions on Software Engineering, 15(9):1066\u20131077.", 
        "338": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "339": "2013.", 
        "340": "Advances in Neural Information Processing Systems 26: 27th A nual Conference on Neural Information Processing Systems 2013.", 
        "341": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "342": "Xu Han, Zhiyuan Liu, and Maosong Su .", 
        "343": "2016.", 
        "344": "Joint representation learning of text and k owledge for knowledge graph completion.", 
        "345": "CoRR, abs/1611.04125.", 
        "346": "5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 51 452 453 54 455 456 457 458 459 460 461 462 463 464 465 466\n67\n468 469\n70\n471 472 473 474 475 476 477 478 479\n80\n481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\nACL 2016 Submission ***.", 
        "347": "Confidential review copy.", 
        "348": "DO NOT DISTRIBUTE.", 
        "349": "3.2.3 Text Representation Learning Giv n the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "350": "Particularly, each word has a unique vector, and each mention has multiple sense vectors including tw kinds of mention senses: entity-centric sense and out-of-KB sense.", 
        "351": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "352": "When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to disti uish existing mention senses, or create a new out-of-KB sense.", 
        "353": "To be concrete, each mention sense has an embedding (sense vector) tsl and a context cluster with center \u00b5(tsl ).", 
        "354": "The representation of the context is defined as the average of the word vectors in the context: C(wi) =\n1 |C(wi)| P wj2C(wi)wj.", 
        "355": "We predict tsl , the sense of entity title tl in the mention < tl, C(tl) >, when observed with context C(tl) as the context cluster membership.", 
        "356": "Formally, we have:\ntsl =\n\u21e2 ts+1l t max l <\ntmaxl otherwise (5)\nwhere is a hyper-parameter and tmaxl = argmaxtsl\nsim(\u00b5(tsl ), C(tl)).", 
        "357": "We adopt an online non-parametric clustering procedure to learn outof-KB mention senses, which means that if the nearest distance of the context vector to sense cluster center is larger than a threshold, we create a ew context cluster and a new sense vector that doesn\u2019t belong to any entity-centric senses.", 
        "358": "The cluster center is the average of all the context vectors belonging to that cluster.", 
        "359": "For the similarity metric, we use cosine in our experiments.", 
        "360": "Here, we extend Skip-gram model to learn word embeddings as well as mention sense embed ings by the following objective to maximize the probability of observing the context words given either a word wi or mention se s of entity tit e tsl :\nL = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nC(\u00b7) (7)\n3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "361": "2. align mention with entity using single prototype model.", 
        "362": "4.3 Parameter Setting 4.4 Qualitative Analysis before conducting the experiments on the tasks, we first give quali ative analysis of words, mentions and entities.", 
        "363": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "364": "next, we give quantitative analysis on several tasks.", 
        "365": "4.5 Entity Relatedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work\n6 Conclusion\nReferences Alfred V Aho and Margaret J Corasick.", 
        "366": "1975.", 
        "367": "Effi-\ncient string matching: an aid to bibliographic search.", 
        "368": "Communications of the ACM, 18(6):333\u2013340.", 
        "369": "J-I Aoe.", 
        "370": "1989.", 
        "371": "An efficient digital search algorithm by using a double-array structure.", 
        "372": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "373": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "374": "2013.", 
        "375": "Advances in Neural Information Processing Systems 26: 27th Annual C nference on Neural Infor ation Processing Systems 2013.", 
        "376": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "377": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "378": "2016.", 
        "379": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "380": "CoRR, abs/1611.04125.", 
        "381": "5\n0 1 2 3 4 5 6 07 08 09 0 1 2\n3 4 5 6\n17 18 19\n0 1 2 3 4 5 6\n27 28 29\n0 1 2 3 4 5 6\n37 38 39\n0 1 2 3 4 5 6 447 448 449\n0 1 2 3 4 5 6 57 58 59 0 1 2\n3 4 5 6\n67 68 69\n0 1 2 3 4 5 6 477\n78 79\n0 1 2 3 4 5 6\n87 88 89\n0 1 2 3 4 5 6 497 498 499\nACL 2016 Submission ***.", 
        "382": "Confidential review copy.", 
        "383": "DO NOT DISTRIBUTE.", 
        "384": "3.2.3 Text Representation Learning Given the ann tated text corpus, we le rn word and me tion representations simultaneously by using a multi-pr totype embedding model.", 
        "385": "Particularly, each word has a unique vector, and each mention has multiple se se vectors including two ki ds of mention senses: entity-centric sense a d o t-of-KB sense.", 
        "386": "Based n the fixed number of entity-centric enses (Section 3.1), e further learn a vary ng\nnumber of out-of-KB senses for each entity title.", 
        "387": "When encounter an ti of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the cont xt information to disti guish existing mention senses, or create a n w ut-of-KB sense.", 
        "388": "To be concrete, each men tion sens has an embeddi g (sense vector) tsl and a context cluster ith center \u00b5(tsl ).", 
        "389": "The representation of the context is d fined as the average of the word vectors in the context: C(wi) =\n1 |C(wi)| P wj2C(wi)wj.", 
        "390": "We pr dict tsl , the sense of entity title tl in th mention < tl, C(tl) >, when observ d with context C(tl) s the con ext cluster membership.", 
        "391": "Formally, we have:\ntsl =\n\u21e2 ts+1l t max l <\ntmaxl otherwise (5)\nwhere is a hyper-parameter and tmaxl = argmaxtl\nsim(\u00b5( sl ), C(tl)).", 
        "392": "We adopt an on ine non-parametric clus ering procedure to lea n outof-K m n ion enses, which m ans tha if the nearest di ta ce f the co text vector to sen e cluster c nter is larger than a thre hold, we creat a new context cluster and a new sense vector that doesn\u2019t b long t any entity-centric senses.", 
        "393": "The cluster center is the average of all the context vec-\nrs belo ing to th t clus er.", 
        "394": "For the sim larity me ric, we use cosin in our experi ents.", 
        "395": "Here, we xte d Skip-gr m model to lear word embe dings as w ll as m ntion s s embeddings by the f llowing objective to maximize the probability of observing the context words given ither a word wi or a mention sense of entity title tsl :\nLw = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nC(\u00b7) (7)\n3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experi ent 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "396": "2. align mention with entity usi g single prototyp model.", 
        "397": "4.3 Parameter Setting 4.4 Qualitative Analysis befor conduct ng experim nts on the tasks, we first give qualitative analysis of words, mentio s and entities.", 
        "398": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "399": "next, we give quanti ative analysis on several tasks.", 
        "400": "4.5 Entity Relatedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work\n6 Conclusion\nReferences Alfred V Aho and Margaret J Corasick.", 
        "401": "1975.", 
        "402": "Effi-\ncie t string matching: an aid to bibliographic search.", 
        "403": "Communications of the ACM, 18(6):333\u2013340.", 
        "404": "J-I Aoe.", 
        "405": "1989.", 
        "406": "An efficient digital search algorit m by usi g doub e-array structure.", 
        "407": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "408": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "409": "2013.", 
        "410": "Advances i Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "411": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "412": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "413": "2016.", 
        "414": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "415": "CoRR, abs/1611.04125.", 
        "416": "5\n400 401 402 03 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n50 451 452 453 454 455 456 457 458 459 60 461 462\n63\n464 465 466 467 468 469 470 471 472\n73\n474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\nACL 2016 Submission ***.", 
        "417": "Confidential review copy.", 
        "418": "DO NOT DISTRIBUTE.", 
        "419": "3.2 3 Text Representation Learning Given the annotated text corpus, we learn word and mention represen ations simultaneously by using a multi-prototype embedding model.", 
        "420": "Particularly, ach word has a unique vector, and each m nti n has multiple sense v ctors including two kinds of mention senses: entity-centric sense and out-of-KB sense.", 
        "421": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "422": "When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB sense.", 
        "423": "To be concrete, each mention sense has an embedding (sense vector) tsl a d a context cluster with center \u00b5(tsl ).", 
        "424": "The representation of the co t xt is defined as the average of the word vectors in the context: C(wi) =\n1 |C(wi)| P wj2C(wi)wj.", 
        "425": "We predict tsl , the sense of entity title tl in the ention < tl, C(tl) >, when observed with context C(tl) as the context cluster membership.", 
        "426": "Formally, we have:\ntsl =\n\u21e2 ts+1l t max l <\ntmaxl otherwise (5)\nwhere is a hyper-parameter and tmaxl = argmaxtsl\nsim(\u00b5(tsl ), C(tl)).", 
        "427": "We adopt an onlin non-parametric clustering procedure to learn outof-KB mention senses, which means that if the nearest distance of the context vector to sense cluster cent r is larger than a threshold, we create a new context cluster and a new sense vector that doesn\u2019t belong to any entity-centric senses.", 
        "428": "The cl ster c nter is the average of all the context vectors belonging to that cluster.", 
        "429": "For the similarity metric, we use cosine in our experiments.", 
        "430": "Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the probability of observing the context words given either a word wi or a mention sense of entity title tsl :\nLw = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nC(\u00b7) (7)\n.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating int GBDT for EL Experiment\n4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "431": "2. align ntion with entity using single prototype model.", 
        "432": "4.3 Parameter Setting 4.4 Qualitative Analysis before conducting the experiments on the tasks, we first give qualitativ an lysis of words, mentions and entities.", 
        "433": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "434": "next, we give qua titative analysis on several tasks.", 
        "435": "4.5 tity Relatedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 uns pervised 5 Rel ted Work\n6 Conclusion\nReferences Alfred V Aho and Margaret J Corasick.", 
        "436": "1975.", 
        "437": "Effi-\ncient string matching: an aid to bibliographic search.", 
        "438": "Communications of the ACM, 18(6):333\u2013340.", 
        "439": "J-I Aoe.", 
        "440": "1989.", 
        "441": "An efficient digital search algorithm by using a double-arr y structure.", 
        "442": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "443": "Christopher J. C. Burges, Le\u0301o Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "444": "2013.", 
        "445": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "446": "Proceedings of a meeting held December 5-8, 2013, Lak Tah e, Nev da, United States.", 
        "447": "Xu H n, Zhiyuan Liu, and Maosong Sun.", 
        "448": "2016.", 
        "449": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "450": "CoRR, abs/1611.04125.", 
        "451": "Mention Representation Learning\n5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2016 Submission ***.", 
        "452": "Confidential review copy.", 
        "453": "DO NOT DISTRIBUTE.", 
        "454": "3.2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "455": "Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention senses: entity-centric sense and out-of-KB sense.", 
        "456": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn varying umber of out-of-KB senses for each entity title.", 
        "457": "When encounter an mentio f entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use th context inf rmation to distinguish existing mention senses, or create a new out-of-KB sense.", 
        "458": "To be concrete, each mention sense has an embedding (sense vector) tsl and a context cluster with center \u00b5(tsl ).", 
        "459": "The representation of the context is defined as the average of the word vectors in the context: C(wi) = 1 |C(wi)| P j2C(wi)wj.", 
        "460": "We pr dict tsl , the sense of entity title tl in the mention < tl, C(tl) >, wh n observed with context C(tl) as the context cluster membership.", 
        "461": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl otherwise (5) where is a hyper-parameter and tmaxl argmaxtsl\nsim(\u00b5(tsl ), C(tl)).", 
        "462": "We ad pt an onl ne non-parametric clustering procedure to learn outof-KB mention senses, which means that if the nearest distance of the context vector to sense cluster center is larger than a threshold, we create a new context cluster an a new s nse vector that doesn\u2019t belong to any entity-centric senses.", 
        "463": "The cluster center is the average of all the con ext vectors belonging to that cluster.", 
        "464": "For the similarity metric, we use cosine in our experiments.", 
        "465": "Here, we xt nd Skip-gr m model to l arn word embe di gs as well as mention sense emb ddings by he following objective o maximize the probability of observing the cont xt w r s giv eith r a word wi or a mention sense of entity title tsl :\nLw = wi, l2D P (C(wi)|wi) + P (C(tl)|tl, sl )\n(6)\nwi/t s l , , , w, , ej , e (7)\n3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "466": "2. a ign mention with entity using single protot pe model.", 
        "467": "4.3 Parameter Setting 4.4 Qualitative Analysis before cond cting th experiments on the tasks, we first give qualitative analysis of words, mentions and entities.", 
        "468": "firstly, we give the phrase embedding by its nearest ords and entities.", 
        "469": "next, we give quantitative analysis on several tasks.", 
        "470": "4.5 Entity Relatedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work\n6 Conclusion\nReferences Alf ed V Aho and Margaret J Corasick.", 
        "471": "1975.", 
        "472": "Effi-\nci t stri g matching: an id to bibliographic search.", 
        "473": "Communicatio s of the ACM, 18(6):333\u2013340.", 
        "474": "J-I Aoe.", 
        "475": "1989.", 
        "476": "An efficient digital search algorithm by using double- rray structur .", 
        "477": "IEEE Transa tions on Software Engineering, 15(9):1066\u201310 7.", 
        "478": "Christopher J. C. Burges, Le\u0301on B ttou, Zoubin Ghahramani, nd Kilian Q. Weinberge , editors.", 
        "479": "2013.", 
        "480": "A - va c s in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Proc ssing Systems 2013.", 
        "481": "Proceedings of a meeti g held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "482": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "483": "2016.", 
        "484": "Joint representa ion learn ng of text and knowledge for knowledge graph completion.", 
        "485": "CoRR, abs/1611.04125.", 
        "486": "5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 49 499 ACL 2016 Submission ***.", 
        "487": "Confidential review copy.", 
        "488": "DO NOT DISTRIBUTE.", 
        "489": "3.2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "490": "Particularly, each word has a unique vector, and each mention has multiple sense v ctors including two kinds of mention senses: entity-centric sense and out-of-KB sense.", 
        "491": "Based on th fixed number of entity-c ntric senses (Section 3.1), we further learn a varying number of out- f-KB senses for each entity title.", 
        "492": "When encounter an mention of enti y ti le tl, inspired by the idea of word sense di a bigua (WSD) task, we use th context informatio to distinguish existing mention se ses, or create a new out-of-KB sense.", 
        "493": "To be concrete, each mention sense has an emb dding (sense vector) tsl and a context cluster with center \u00b5(tsl ).", 
        "494": "The representation of the context is defined as the average of the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi)wj.", 
        "495": "We predict tsl , the sense of entity title tl in the menti n < tl, C(tl) >, whe bserved with context C(tl) as the cont xt clust r membership.", 
        "496": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl otherwise (5) where is a hyper-parameter and tmaxl = argmaxtsl sim(\u00b5(tsl ), C(tl)).", 
        "497": "W adopt an online non-parametric clustering procedure to learn outof-KB mention senses, which eans t at if the nearest distance of the context vector to sense cluster center is larger tha a thr shold, we create a new context cluster and new sense vector that doesn\u2019t belong to any entity-centric senses.", 
        "498": "The cluster center is the average of all the context vectors belonging to that cluster.", 
        "499": "F r the similarity metric, we use cosine in our experiments.", 
        "500": "Here, we extend Skip-gram model to learn word embeddings as well as mention s nse mbeddings by the following objective to maximize th pr bability of observing the context words given either a word wi or a mention sense of entity title tsl : Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl ) (6) C(\u00b7) (7) 3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 I tegrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. dir ctly alig wor s wi h entity.", 
        "501": "2. align m tion with entity using single prototype model.", 
        "502": "4.3 Parameter Setting 4.4 Qualitative Analysis bef re conduc ng th xperiments on the t sks, we first giv qualit ti e analysis of words, - tions and entities.", 
        "503": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "504": "next, we give quantitativ analysis on several tasks.", 
        "505": "4.5 Entity Relatedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7 2 unsupervi ed 5 Related Work 6 Conclusion References Alfred V Aho and Margaret J Corasick.", 
        "506": "1975.", 
        "507": "Efficient string matching: an a d to biblio raphic search.", 
        "508": "Communications of the ACM, 18(6):333\u2013340.", 
        "509": "J-I Aoe.", 
        "510": "1989.", 
        "511": "An efficient digital s arch algorithm by using a double-array structure.", 
        "512": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "513": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Wei berger, editors.", 
        "514": "2013.", 
        "515": "Advances in Neural Informatio Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "516": "Proceedings of a meeti g held Decemb r 5-8, 2013, Lake Tahoe, Nevad , United States.", 
        "517": "Xu Han, Zhiyuan Liu, and Maoso g Sun.", 
        "518": "2016.", 
        "519": "J int e resentati n learning of text and kn wled for knowledge graph completio .", 
        "520": "CoRR, abs/1611.04125.", 
        "521": "5 400 401 402 403 404 405 6 7 8 9 10 11 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 34 5 6 7 8 9 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 6 7 8 9 60 61 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 84 5 6 7 8 9 490 491 492 493 494 495 496 497 498 499 ACL 2016 Submission ***.", 
        "522": "Confidential review copy.", 
        "523": "DO NOT DISTRIBUTE.", 
        "524": ".2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "525": "Particularly, each word has a unique vector, and ach mention has multiple sense vectors including two kinds of mention senses: e tity-ce tric sense and out-of-KB sense.", 
        "526": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "527": "When encounter an m tion of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context informatio to disting i h ex sting me tion enses, or create a n w out-of-KB sense.", 
        "528": "To be concr te, e ch mention s nse has an embedding (sense vec or) tsl and a co t xt cluster with center \u00b5(tsl ).", 
        "529": "Th repre entation of the context is defi ed s the vera e of the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi)wj.", 
        "530": "We predict tsl , the sense of ent ty title tl i the mention < tl, C(tl) >, when observed with context C(tl) as the context cluster membership.", 
        "531": "Fo - mally, we have: tsl = \u21e2 ts+1l t max l < tmaxl otherw se (5) where is a hyper-parameter and tmaxl = argmaxtsl sim(\u00b5(tsl ), C(tl)).", 
        "532": "We adopt an online non-parametric clustering procedure to learn outof-KB mention senses, which means that if the nearest distance of the context vector to sense cluster center is larger than a threshold, we create a new context cluster and a new sense vector that d esn\u2019t belong t any entity-centric senses.", 
        "533": "The cluster ce ter is the ave age of all the context vectors belonging to that cluster.", 
        "534": "For the similarity metric, we use cosine in our experiments.", 
        "535": "Here, xtend Skip-gram mod l to l arn word embeddings as well as mention sense mbeddings by th following objectiv to m ximize th proba bility of observing the ontext words given either a word i or a mention sense of entity itl tsl : Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, sl ) (6) C(\u00b7) (7) 3.2.4 Entity-centric Sense Representation L arning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Exp riment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "536": "2. align mention with entity using single prototype m del.", 
        "537": "4.3 Para eter Set ing 4.4 Qualitativ Analysis before conducting the experiments on the tasks, we first give qualitative analysis of words, mentions and entities.", 
        "538": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "539": "next, we give quantitative analysis on several tasks.", 
        "540": "4.5 Entity R latedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervi ed 5 Relat d Work 6 Con lusion References Alfred V Aho and Margaret J Corasick.", 
        "541": "1975.", 
        "542": "Efficient string matching: an aid to bibliographic search.", 
        "543": "Communications of the ACM, 18(6):333\u2013340.", 
        "544": "J-I Aoe.", 
        "545": "1989.", 
        "546": "An efficient digital search algorithm by using a double-array structure.", 
        "547": "EEE Transactions on Software Engine ring, 15(9):1066\u20131077.", 
        "548": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "549": "2013.", 
        "550": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "551": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United Stat s Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "552": "201 .", 
        "553": "Joint representation learning of text and knowldg for knowledge graph completion.", 
        "554": "CoRR, abs/1611.04125.", 
        "555": "5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426\n7 8 9\n30 31\n432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476\n7 8 9\n80 81\n482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499\nACL 2016 Submission ***.", 
        "556": "Confidential review copy.", 
        "557": "DO NOT DISTRIBUTE.", 
        "558": "3.2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "559": "Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention ses: ent ty-centric sense and out-of-KB sense.", 
        "560": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "561": "When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the cont xt information to distinguish existing mention senses, or create a new out-of-KB sense.", 
        "562": "To be concrete, each mention sense has an embedding (sense vector) tsl and a context cluster with center \u00b5(tsl ).", 
        "563": "The representation of th context is defined as the avera e of the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi) j.", 
        "564": "We predict tsl , the s se of entity title tl in the mention < t , C(tl) >, when observed with context C(tl) as the context cluster embership.", 
        "565": "Formally, we hav :\ntsl =\n\u21e2 ts+1l t max l <\ntmaxl otherwise (5)\nwhere is a hyper-parameter and tmaxl = argmaxtsl\nsim(\u00b5(tsl ), C(tl)).", 
        "566": "We adopt an online non-parametric clustering procedure to learn outof-KB mention senses, which eans t at if the nearest di tance of he context vector to sense c uster center is larger han a threshold, we c eate a new cont x clu ter and a new s ns vec or that doesn\u2019t belo g t any e tity-centric sense .", 
        "567": "The cluster center is the averag of all the context vectors belonging to that cluster.", 
        "568": "For the similarity metric, we use cosine in our experiments.", 
        "569": "Here, we extend Skip-gram model to learn word embe d ngs as well as mention sense embeddings by the following objective to maximize the probability of observing the context words given either a ord wi or a mention se se of enti y title tsl :\nLw = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nC(\u00b7) (7)\n3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "570": "2. align ention with entity using single prototype model.", 
        "571": "4 3 Parameter Sett 4.4 Qualitative Analysis before conducting the experiments on the tasks, we first give qualitative analysis of words, mentions and entities.", 
        "572": "firstly, we give the phrase embedding by its nearest wor s and entities.", 
        "573": "next, we give quantitative analysis on several tasks.", 
        "574": "4.5 Entity Rel tedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work\n6 Conclusi n\nR fer nces Alfred V Aho and Margaret J Corasick.", 
        "575": "1975.", 
        "576": "Effi-\ncient string matching: an a d to bibliogr phic search.", 
        "577": "Communications of the ACM, 18(6):333\u2013340.", 
        "578": "J-I Aoe.", 
        "579": "1989.", 
        "580": "An efficient digit l search algorithm by using a double-array structure.", 
        "581": "IEEE Transactions on Softw re Engine ring, 15(9):1066\u20131077.", 
        "582": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "583": "2013.", 
        "584": "Advances in Neural Information Processing Systems 26: 27t Annual Conference on Neural Information Processing Systems 2013.", 
        "585": "Proce dings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "586": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "587": "2016.", 
        "588": "Joint representation learning of text and knowledge for knowledg graph completion.", 
        "589": "CoRR, abs/1611.04125.", 
        "590": "5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 20 21 422 423 24 425 426 427 28 29 430 431 32 33 434 435\n3 3\n438 439\n40 41\n442 443 444\n45\n446 447 448\n49\n450 451 452 4 3 454 455 4 6 457 458 459 460 461 462 463 464 465 466 4 7 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 ACL 2016 Submission ***.", 
        "591": "Confidential review copy.", 
        "592": "DO NOT DISTRIBUTE.", 
        "593": "3.2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "594": "Particularly, each word has a unique vector, and each menti n has multiple sense vectors including tw kinds of mention senses: entity-centric sense and o t-of-KB sense.", 
        "595": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "596": "When encoun er an mention of entity title tl, inspir d by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing mention senses, or create a new out-of-KB se se.", 
        "597": "To be concrete, each mention sense has an embedding (sense vector) tsl an a context cluster with center \u00b5(tsl ).", 
        "598": "The epresentation of the context is defined as the average f the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi)wj.", 
        "599": "We predict tsl , the sense of entity title tl in the mention < tl, C(tl) >, when observed with context C(tl) as e c ntext cluster membership.", 
        "600": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl oth rwise (5) where is a hyp r-parameter and tmaxl = argmaxtsl sim(\u00b5(tsl ), C(tl)).", 
        "601": "We dopt an online non-parame ric cl s ring procedure to learn outof-KB mention senses, which me ns that if the near st ista ce of the context vector to sense cluster cent is larger than a threshold, we create a new context cluster and a new sense vector that do sn\u2019t bel ng to any entity-centric senses.", 
        "602": "Th clust r cent r is the average of all h context vectors belonging to that cluster.", 
        "603": "For the s milarity m tric, we use cosine in our experiments.", 
        "604": "Here, w extend Skip-gram model to learn word e beddings as well as mention se se embe dings by the f llo ing objective to maximize th probability of observing the context words given either a word wi or a mentio sen e of entity title tsl : Lw = X w ,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl ) (6) N (\u00b7) (7) 3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Join ly Training 3.3 Integrating into GBDT for EL 4 Experimen 4.1 Data Preparation 4.2 Baselin Methods 1. directly align words with entity.", 
        "605": "2. align mention with entity using single prototype model.", 
        "606": "4.3 Parameter Setti g 4.4 Qualitative Analysis before conducting the experim nts on the tasks, w first give qualitative analysis of words, m ntio s and entities.", 
        "607": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "608": "next, we give quantitative analysis on several tasks.", 
        "609": "4.5 Entity Relatedness 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work 6 Conclusion References Alfred V Aho and Margaret J Corasick.", 
        "610": "1975.", 
        "611": "Efficie t string matching: an aid to bibliographic search.", 
        "612": "Communications of the ACM, 18(6):333\u2013340.", 
        "613": "J-I Aoe.", 
        "614": "1989.", 
        "615": "An efficie t digital search algorithm by using a doubl -array structure.", 
        "616": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "617": "Christopher J. C. Bur es, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Wei berger, editors.", 
        "618": "2013.", 
        "619": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "620": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "621": "Xu H n, Zhiyuan Liu, and Maosong Sun.", 
        "622": "2016.", 
        "623": "Joint representation learning of text and knowledg for knowledge graph completion.", 
        "624": "CoRR, abs/1611.04125.", 
        "625": "5\n0 1 2 3 4 5 6 07 08 09 0 1 2 3 4 5 6 17 418 19 0 1 2 3 4 5 6 27 28 29 0 1 2\n3\n6\n37 38 39\n0 1 2 3 4 5 6 447 448 449\n0 1 2 3 4 5 6 57 58 59 0 1 2 3 4 5 6 67 68 69 0 1 2 3 4 5 6 77 78 79 0 1 2\n3 4 5 6\n87 88 89\n0 1 2 3 4 5 6 497 498 499\nACL 2016 Submission ***.", 
        "626": "Confidential review copy.", 
        "627": "DO NOT DISTRIBUTE.", 
        "628": "3.2.3 Text Representation Learning Given the ann tated text cor us, we le rn word a d me tion representations simultaneously by using a multi-pr totype embedding model.", 
        "629": "Particularly, each word has a unique vector, and ea h mention has multiple se se vectors i cludi g two ki ds of mention senses: entity-centric sense a d o t-of-KB sense.", 
        "630": "Based n the fixed numb r of ntity-ce tric senses (Section 3.1), e further learn a varying number of out-of-KB senses for each entity title.", 
        "631": "Whe encount r an ti of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the cont xt information to disti guish existing mention senses, or cre te a n w out-of-KB sense.", 
        "632": "To be c crete, each mention sens has an embeddi g (sense vector) tsl and a context cluster ith center \u00b5(tsl ).", 
        "633": "The representation of the context is d fined as the average of the word vectors in the context: C(wi) = 1 |C( i)| P wj2C(wi)wj.", 
        "634": "We pr dict sl , th se of n ity title tl in the m t on < tl, C(tl) >, when observed w th context C(tl) as the c ntext cluster membership.", 
        "635": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl oth rwise (5) where is a hyp r-paramet r and tmaxl = argmaxtsl\nsim(\u00b5(tsl ), C(tl)).", 
        "636": "We adopt an onli e non-parametric clustering procedure to learn outof-KB m ntion senses, which m ans that if the nearest dista ce f the co text vector to sense cluster c nt r is larger than a threshold, we creat new context l t r and a n w sense vector t at doesn\u2019t b long t any entity-centric senses.", 
        "637": "The clust r center is the average of all th context vectors belo ing to th t clus er.", 
        "638": "F the similarit me ric, we use cosin in our experi ents.", 
        "639": "Here, we xte d Skip-gram model to learn word e be dings as w ll as m tion s se embeddings by the f llo ing objectiv to maximize th probability of observing the context words given ither a word wi or a mention sen e of entity title tsl :\nLw = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nN (\u00b7) (7)\n3.2.4 Entity-centric Se se Representation Learni g Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experiment 4.1 Data Preparation 4.2 B seli e M thods 1. directly align words with entity.", 
        "640": "2. align mention with entity using single prototype model.", 
        "641": "4.3 Parameter Setting 4.4 Qualitative Analysis before conducting t experim nts on the tasks, w first give qualitative analysis of words, mentio s and entities.", 
        "642": "firstly, we give the phrase embedding by its nearest words and entities.", 
        "643": "next, we give quantitative analysis on several tasks.", 
        "644": "4.5 Entity Relatedness 4.6 Word Similarity 4.7 EL valu tion 4.7.1 gbdt 4.7.2 unsupervised 5 Relate Work 6 Conclusion References Alfred V Aho and Margaret J Corasick.", 
        "645": "1975.", 
        "646": "Effi-\ncie t string matching: a aid to bibliographic search.", 
        "647": "Communications of th ACM, 18(6):333\u2013340.", 
        "648": "J-I Aoe.", 
        "649": "1989.", 
        "650": "An efficient digital search algorit m by usi g doub e-array structure.", 
        "651": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "652": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors.", 
        "653": "2013.", 
        "654": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "655": "Proceed ngs of a me ting held December 5-8, 2013, Lake Tahoe, Nevada, United St tes.", 
        "656": "Xu H n, Zhiyuan Liu, and Maosong Sun.", 
        "657": "2016.", 
        "658": "Joint representation l arning of text and knowledge for knowledge graph completion.", 
        "659": "CoRR, abs/1611.04125.", 
        "660": "5\n0 1 2 3 4 5 6 07 08 09 0 1 2 3 4 5 6 17 18 19 0 1 2 4 3 4 5 6 427 28 29\n0 1 2 3\n4 5\n6\n37 38 39\n0 1 2 3 4 5 6\n47\n448 449\n0 1 2 3 4 5 6 57 8 59 0 1 2 3 4 5 6 67 68 9 0 1 2 3 4 5 6 77 78 79\n0 1 2 3 4 5 6\n87 88 89\n0 1 2 3 4 5 6 497 498 499\nACL 2016 Submissi n ***.", 
        "661": "Confidential review copy.", 
        "662": "DO NOT DISTRIBUTE.", 
        "663": "3.2.3 T xt Representatio Lear ing Give the ann tated text corpus, we le rn word and e ti re esentations simultaneously by using a multi-pr totype embedding mod l. Particularly, each word has a unique vector, and each mention has multiple se se vectors includ g wo ki ds of mention senses: entity-centric sense a d o t-of-KB se se.", 
        "664": "B s d n he fixed number of en ity-c ntric ens s (Section 3.1), e further learn a vary ng number of out-of-KB senses for each entity title.", 
        "665": "When encounter an ti of e tity title tl, inspired by the idea of word sense dis mb uati n (WSD) task, we use th cont xt information to disti guish existing mention senses, or create a n w out-of-KB sense.", 
        "666": "To be co crete, each men tio se s h s a embeddi g (sense ector) tsl and a context cluster ith center \u00b5(tsl ).", 
        "667": "The represe tation of the cont xt is defin d as th average of the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi)wj.", 
        "668": "W pr dict tsl , th se of entity title tl in the mention < tl, C(tl) >, hen obser ed with context C(tl) as the context cluster membership.", 
        "669": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl otherwise (5) here is a hyper-parameter and tmaxl = arg axtl\nsim(\u00b5( sl ), C(tl)).", 
        "670": "We adopt an on i non-param tric clus ring procedur to lea n utof-KB m n ion enses, which m ans tha if the nearest dista c of the co text vector to s n e clust r c nter is l rger than a threshold, we creat a new context clus er and a new sense vector that doesn\u2019t b long t any entity-centric se ses.", 
        "671": "The cluster center is the average of al th context vectors belo ing to th t clus er.", 
        "672": "For the similarity m ric, we use cosin in ou experi ents.", 
        "673": "Her e xte d Skip-gram m del to learn word e be dings as w ll as m tion s se embeddings by the following objective to maximize the robability of observing the context words give either a word wi or mention sense of ntity title tsl :\nLw = X\nwi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl )\n(6)\nN (\u00b7) (7)\n3.2.4 Entity-centric Sense Representation L ar ing Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Int grating i to GBDT for EL 4 Experiment 1 D t Prepara ion 2 Baseline Methods 1. di ctly align words with entity.", 
        "674": "2. ign mentio with e tit u ing ingle oto ype mo l. 4.3 Parameter Setting 4.4 Qualitative Analysis before conduct ng experim nts on the tasks, we first give qualitative analysis of w rds, mentio s and entities.", 
        "675": "firstly, we giv the phrase embedding by its nearest words and entities.", 
        "676": "next, we give quantitative analysis on several tasks.", 
        "677": "4.5 Entity Relatedne s 4.6 Word Similarity 4.7 EL evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 Related Work\n6 Conclusio\nReferences Alfred V Aho a d Margar t J Corasi k. 1975.", 
        "678": "Effi\ncie t string match ng: an id to bibliog phic earch.", 
        "679": "Communications of the ACM, 18(6):3 3\u2013340.", 
        "680": "J-I A e. 1989.", 
        "681": "An efficient digital search algorit by usi g doub e-array structure.", 
        "682": "IEEE Transactions on Software Engineering, 15(9):1066\u20131077.", 
        "683": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahrama i, and Kilian Q. Weinber er, editors.", 
        "684": "2013.", 
        "685": "Adva ces in Neur l Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013.", 
        "686": "Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "687": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "688": "2016.", 
        "689": "J int representation learning of text and knowledge for knowledge g aph completion.", 
        "690": "CoRR, abs/1611.04125.", 
        "691": "played it during public events, such as [[ ]] celebrations Mention Sense Mapping\n5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 ACL 2016 Submission ***.", 
        "692": "Confidential review copy.", 
        "693": "DO NOT DISTRIBUTE.", 
        "694": "3.2.3 Text Representation Learning Given the annotated text corpus, we learn word and mention representations simultaneously by using a multi-prototype embedding model.", 
        "695": "Particularly, each word has a unique vector, and each mention has multiple sense vectors including two kinds of mention senses: entity-c n ric sense and out-of-KB sense.", 
        "696": "Based on the fixed number of entity-centric senses (Section 3.1), we further learn a varying number of out-of-KB senses for each entity title.", 
        "697": "When encounter an mention of entity title tl, inspired by the idea of word sense disambiguation (WSD) task, we use the context information to distinguish existing ment on senses, or create new out-of-KB sense.", 
        "698": "To be concrete, each mention sense has an embedding (sense vector) tsl and a context cluster with center \u00b5(tsl ).", 
        "699": "The representation of the context is defi ed as the average of the word vectors in the context: C(wi) = 1 |C(wi)| P wj2C(wi)wj.", 
        "700": "We predict tsl , the sense of entity title tl i he mention < tl, C(tl) >, when observed with context C(tl) as the context cluster membership.", 
        "701": "Formally, we have: tsl = \u21e2 ts+1l t max l < tmaxl otherwise (5) where is a hyper-parameter an tmaxl = argmaxtsl sim(\u00b5(tsl ), C(tl)).", 
        "702": "We adopt an online non-parametric clustering procedure to learn outof-KB mention senses, which means that if the nearest distance of the context vector to sense cluster center is larger than a threshold, we create a new context cluster and a new sense vector that doesn\u2019t belong to any entity-centric senses.", 
        "703": "The cluster center is the average of all the context vectors belo ging to that cluster.", 
        "704": "For the similarity etric, we use cosine in our experiments.", 
        "705": "Here, we extend Skip-gram model to learn word embeddings as well as mention sense embeddings by the following objective to maximize the probability of observing the context words given either a word wi or a mention s nse of e tity title tsl : Lw = X wi,tl2D P (C(wi)|wi) + P (C(tl)|tl, tsl ) (6) g(July 4th, e1) (7) 3.2.4 Entity-centric Sense Representation Learning Lm = X (mh,ej)2A P (ej |C(mh), tsl ) (8) 3.2.5 Jointly Training 3.3 Integrating into GBDT for EL 4 Experi ent 4.1 Data Preparation 4.2 Baseline Methods 1. directly align words with entity.", 
        "706": "2. align mention with entity sing sin le pro otype model.", 
        "707": "4.3 Parameter Setting 4.4 Qualitative Analysis before conducting the xperiments on the tasks, w first give qualitative a alysis of word , m - tions and entities.", 
        "708": "firstly, we give the phrase emb dding by it nearest words and ent ties.", 
        "709": "next, e give quantitative analysis on several tasks.", 
        "710": "4.5 Entity Relatedness 4.6 Word Simil rity 4.7 L evaluation 4.7.1 gbdt 4.7.2 unsupervised 5 R lated Work 6 C clusion R ferenc s Alfred V Aho a d Margar t J Corasick.", 
        "711": "1975.", 
        "712": "Efficient string matching: an aid to bibliogr phic search.", 
        "713": "Communications of the ACM, 18(6):333\u2013340.", 
        "714": "J-I Aoe.", 
        "715": "1989.", 
        "716": "An efficient digita search algorithm by using a double-array structure.", 
        "717": "IEEE Transacti s on Software Eng neering, 15(9):1066\u20131077.", 
        "718": "Christopher J. C. Burges, Le\u0301on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editor .", 
        "719": "2013.", 
        "720": "Advances in Neural Informati n Processing Syste s 26: 27th Annual Conference on Neural Informat o Processing Systems 2013.", 
        "721": "Proceedings of a meeti g held December 5-8, 2013, Lake Tahoe, Nevada, United States.", 
        "722": "Xu Han, Zhiyuan Liu, and Maoso Sun.", 
        "723": "2016.", 
        "724": "Joint representation learni g of text d knowledge for knowledge graph completion.", 
        "725": "C RR, abs/1611.04125.", 
        "726": "3\n200 201 202 203 204 205 206 207 208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250 251 252 253 254 255 256 257 258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nACL 2016 Submissio ***.", 
        "727": "Confidential review copy.", 
        "728": "DO NOT DISTRIBUTE.", 
        "729": "as w ll as w rd embeddi gs w nd entit emb ddings e. Note that slj 2 m\u21e4l denotes that mention s nse f ml refer to entity ej , where m\u21e4l represents he sen e set of ml.", 
        "730": "Different mentions may share th same ention sense, deno ed as s\u21e4j .", 
        "731": "Example As shown i Figure 1, there ar two differe t mentio s \u201cIndep ndence Day\u201d m1 and \u201cJuly 4th\u201d m2 in t e docu ents.", 
        "732": "MPM is to learn two ment on senses s11, s 1 2 f r m1, and one me t on s 22 for 2.", 
        "733": "Clear y, th se t o mentions share a common s se n th last two d cum nts: the United States holid y e2, so w ve s\u21e42 = 1 2 = s 2 2.", 
        "734": "Note that w,m, s ar atur lly embedded into the s m semantic space since they re basic units in texts, and e modeling the graph tructure in KB is actually in another semantic space.", 
        "735": "3 Method\nIn this s cti n, w firs ly describe he framework of MPME, followed by the detail d inf m tion of ach key comp e t. Th n, we introduce a well designe me tion s se disambiguati n method, which c also be used for entity linking n unsupervis d way.", 
        "736": "National Day\ns\u21e4I dependence Day (film), s \u21e4 Independence Day (US)\n3.1 Framework Given KB, D and A, we are to jointly learn word, entity and mention representations: w, e, m. Serving as basic units in texts, W rd {wi} and entity title {tl} are naturally embedded into a unified semantic space, meanwhile entities {ej} are mapped to one of mention senses of its title: sl .", 
        "737": "Thus, text an knowledge are combined via the bridge of mentions.", 
        "738": "We can eas-\nily obtain the similarity between word and en-\ntity Similarity(wi, ej) by computing the similarity between word and its corresponding mention sense: Similarity(wi, f(ej)).", 
        "739": "As shown in Figure 2, our proposed MPME contains four key components: (1) Mention Sense Mapping: we map the anchor < mh, ej > A to the corresponding mention sense tsl to reduce the vocabulary to learn.", 
        "740": "(2) Entity Representation L ar ing given a knowledge base KB, we construct a knowledge network among entities, and\nlearn their embeddings so that similar entities on the graph have similar representations.", 
        "741": "(3) Text Representation Learning given text corpus D as well as the annotated anchors, we learn word and entity title embeddings by maximizing the probability f co-occurri w rds/entity titles so that si ilar words/entity titles have similar representations.", 
        "742": "(4) Mention Representation Learning given ann tated anchors tsl :< mh, ej >2 A, we learn entity title embeddings by incorporatin both contextual words embeddings and entity embeddings in order to distinguish different mention e s that has similar rep ese tations to its corresponding entity embeddings.", 
        "743": "R p esentation learning of (2), (3) and (4) ses an iterative update procedure following a unified optimization objective.", 
        "744": "The outputs of word embeddings wi and entity embeddings ej keep their own semantic space and are naturally bridged via the new learned entity title embeddings tl, which inspires u to globally optimize the robability of choosing mention s nses of all the phrases of mention names in the given document.", 
        "745": "Since each mention sense corresponds to an entity, the mentio se se disambiguation process can also be reg rded as inking entities to knowledge base in a unsupervised way, which will be detailed in Section ??.", 
        "746": "3.2 Mention Sense Mapping\nThere are two kinds of mappings: from entities to mention senses, and from mention names to mention sen es.", 
        "747": "The former is pre-defined at the very beginning.", 
        "748": "Given the knowledge Base KB, we extract entity titles {tl} and initialize with multiple mention senses, where the sense number depends on how many entities share a common title.", 
        "749": "The latter is to find possible mention senses for the given mention name, which is similar to candidate mention generation in entity linking task.", 
        "750": "Conventional candidate mention generation generally maintains a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mention name, and recognizes the mention name in text by accurate string matching.", 
        "751": "Or it firstly recognizes possible mention names in texts using NER (Named Entity Recognition) tool, and then approximately retrieves candidate entities via an information retrieval method.", 
        "752": "Since this c mponent is not key point in this paper, we adopt the first method to collect a3\n200 201 202 203 204 205 206 207 208 209 210 211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n2 1\n242\n243\n244\n245\n2 6\n247\n248\n249\n250 251 252 253 254 255 256 257 258 259 260 261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nACL 2016 Submission ***.", 
        "753": "Confidential review copy.", 
        "754": "DO NOT DISTRIBUTE.", 
        "755": "as w ll as word embeddings w and entity embeddings e. Note that slj 2 m\u21e4l denotes that mention sens of ml r fers t entity j , where m\u21e4l represents the sense set of ml.", 
        "756": "Different mentions may share he sam mentio sens , den ted as s\u21e4j .", 
        "757": "Example As s own in Figure 1, there are two differ nt me tions \u201cIndependence Day\u201d m1 and \u201cJuly 4th\u201d 2 in the documents.", 
        "758": "MPME is to learn two mentio senses s11, s 1 2 for m1, and one\nmention sens s22 f r m2.", 
        "759": "Clearly, these two mentions share common s se in the last two documents: the Unit d States holiday e2, so we have s\u21e42 = s 1 2 = s 2 2.", 
        "760": "N te that w,m, s are naturally embedde into the same semantic space since they are basic units n texts, and e modeling the gr ph structur in KB is actu lly in another semantic space.", 
        "761": "3 M t od\nI his s ction, e firstly describe the framework of MPME, followed by the d tailed information of each key component.", 
        "762": "Then, we introduce a well d sign d mentio sense disambiguation method, which can also be used for entity linking in a unsupervis d way.", 
        "763": "eNational Day\ns\u21e4Independence Day (f lm), s \u21e4 Indepe dence Day (US)\n3.1 Framework Given KB, D and A, we are to joi tly lear word, entity and me tion representations: w, e, m. Serving as basic units i texts, Word {wi} and entity title {tl} are naturally e bedded into a unified semantic space, meanwhile entities {ej} are mapped to one of mention senses of its title: tsl .", 
        "764": "Thus, text and knowledge are combined via the bridge of mentions.", 
        "765": "We can easily obtain the similarity between word and entity Similarity(wi, ej) by computing the similarity between word and its corresponding mention\nsense: Similarity(wi, f(ej)).", 
        "766": "As shown in Figure 2, our proposed MPME contains four key components: (1) Mention Sense Mapping: we map the anchor < mh, ej >2 A to the corresponding mention sense tsl to reduce the vocabulary to learn.", 
        "767": "(2) Entity Representation Learning give a knowledge base KB, we construct a knowledge network among entities, and\nlearn their embeddings so that similar entities on the graph have similar representations.", 
        "768": "(3) Text Representation Learning given text corpus D as well as the annotated anchors, we learn word and entity itle e eddings by maximizing the probability of co-occurring words/entity titles so that similar words/entity titles have similar representations.", 
        "769": "(4) Mention Representation Learning given annotated anchors tsl :< mh, ej >2 A, we learn entity title embeddings by incorporating both contextual words embeddings and entity emb ddings in order to distinguish different mention senses that has similar representations to its corresponding entity embeddings.", 
        "770": "Representation learn ng of (2), (3) and (4) uses an iterative update procedure following a unified optimization objective.", 
        "771": "Th outputs of word embeddi gs wi and entity mbeddings ej keep their own semantic space and are naturally bridged via th new learned entity title mbeddings tl, which inspires us to globally optimize the probability of choosing mention senses of all the phrases of mention names in the give document.", 
        "772": "Since each\nenti n sense corresponds to an entity, the mention sense disambiguation process can also be regarded as linking entities to knowledge base in a unsupervised way, which will be detailed in Section ??.", 
        "773": "3.2 Mention Sense Mapping\nThere are two kinds of mappings: from entities to mention senses, and from mention names to mention senses.", 
        "774": "The former is pre-defined at the very beginning.", 
        "775": "Given the knowledge Base KB, we extract entity titles {tl} and initialize with multiple mention senses, where the sense number depends on how many entities share a common title.", 
        "776": "The latter is to find possible mention senses for the given mention name, which is similar to candidate mention generation in entity linking task.", 
        "777": "Conventional candidate mention generation generally maintains a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mention name, and recognizes the mention name in text by accurate string matching.", 
        "778": "Or it firstly recognizes possible mention names in texts using NER (Named Entity Recognition) tool, and then approximately retrieves candidate entities via an information retrieval method.", 
        "779": "Since this component is not key point in this paper, we adopt the first method to collect a\noutlink\nObserved by\ncategory\n3\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nACL 2016 Submission ***.", 
        "780": "Confidential review copy.", 
        "781": "DO NOT DISTRIBUTE.", 
        "782": "KB, a xt corpu D a a set of a hors A, multiprototype e tion embedding is to learn multiple sense emb ddings sjl 2 Rk for a mention ml as well as word embeddings w and entity embeddings e. Note that slj 2 \u21e4l enote that en ion s nse f ml r fers e tity ej , where m\u21e4l repr - sents the sense s t of ml.", 
        "783": "iff rent mentions may share the same m ntion sense, denoted as s\u21e4j .", 
        "784": "Example As shown in Figure 1, there are wo different mentions \u201cIndependence Day\u201d m1 and \u201cJuly 4th\u201d m2 in the ocum ts.", 
        "785": "MPME is to learn two mention s ns s s11, s 1 2 r m1, and one mention sense s22 for m2.", 
        "786": "Clearly, these two mentions share a common sense n the last two documents: th United States holiday e2, so we have s\u21e42 = s 1 2 = s 2 2.", 
        "787": "Note that w,m, s are naturally embedd d into t e sam semantic space since they are basic units in t xts, and modeling the graph structure in KB is actually in another semantic space.", 
        "788": "3 Method\nIn this section, we firstly descri e the framew rk of MPME, followed by the det iled i f rmation f\neach key component.", 
        "789": "Then, we introduce a well\ndesigned mention sense disambiguation method, which can also be used for entity linking in a unsupervised way.", 
        "790": "3.1 Framework\nGiv n kn wledge bas KB, t xt corpus D and a set of anchors A, we are to jointly learn word, entity and mention representations: w, e, m. As shown in Figure 2, our proposed MPME contains four key components: (1) Mention Sense Mapping: given an anchor < ml, ej >, we map it to the corresponding mention sense to r duce the mention vocabulary t le embeddings.", 
        "791": "If only a mention is given, we map it to several m ntion senses that requires disambiguation (Section 3.4).", 
        "792": "(2) Entity Representation Learning based on outlinks in Wikipedia pages, we construct a knowledge network to represent the semantic relatedness among entities.", 
        "793": "And then learn entity embed ings so that similar entities on the graph have similar representations.", 
        "794": "(3) Mention Representation Learning given mapped anchors in contexts, we learn mention sense embeddings by incorporating both textual context embeddings and entity embeddings.", 
        "795": "(4) Text Representation Learning we extend skip-gram model to simultaneously learn\nword and m tion ense emb ddings on annotated text corpu D0.", 
        "796": "Following (Y mad et al., 2016), we use wikipedia articles as text corpus, and the anchors provide annotated mentions1.", 
        "797": "W jointly trai (2), (3) a d (4) by using a unified opti ization obj ctive.", 
        "798": "The outputs emb - di gs of word a d mention ar naturally in the sam seman ic sp ce since they are different units in annotated tex corpus D0 for text representation learning.", 
        "799": "Entity embeddings keep their own semantics in another vector space, be ause we only us them s answers to predi t in mention representation learning by extending Continuous BOW model, which w ll be further discussed in Section ??.", 
        "800": "s\u21e4Memorial Day\nword embed ings wi and entity mbeddings ej keep their own s mantic sp ce nd are naturally bridged via the new learned e tity title embeddings tl, which inspires us t globally optimize the probability of c oosing mention s nses f all the phrases of mention names in the given document.", 
        "801": "Since each mention se se corresponds to an\nentity, the mention sense disambiguation process\ncan also be regarded as linking entities to knowledge base in a unsupervised way, which will be detailed in Section ??.", 
        "802": "3.2 Mention Sense M pping\nThere are two kinds of mappings: from entities to mention senses, and from mention names to mention senses.", 
        "803": "The former is pre-defined at the very beginning.", 
        "804": "Given the knowledge Base KB, we extract entity titles {tl} and i itialize with multiple mention senses, where the sense number depends on how many entities share a common title.", 
        "805": "The latter is to find possible mention senses f r the given mention name, which is similar to candidate mention generation in entity linking task.", 
        "806": "Conventional candidate mention generation generally maintains a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mention name, and recognizes the mention name in text by accurate string\n1We can also annotate text corpus by using NER tool like python nltk to recognize mentions, and disambiguating its mapped mention senses as described in Section 3.4.", 
        "807": "This is an ongoing work with the goal of learning additional out-ofKB senses by self-training.", 
        "808": "In this paper, we will focus on the effectiveness of our model and the quality of three kinds of learned embeddings.", 
        "809": "3\n200 201 202 203 204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250 251 252 253 254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nACL 2016 Submission ***.", 
        "810": "Confidential review copy.", 
        "811": "DO NOT DISTRIBUTE.", 
        "812": "KB, a text orpus D a d set of a chors A, mult prototype mention em edd g is to lear m ltiple sens embeddings sjl 2 Rk for each ntio ml as well a word embeddings w a d entity embeddings e. Note hat slj 2 m\u21e4l d o es that men ion sens f ml efers to en ity ej , where \u21e4 repres nts the se s t of ml.", 
        "813": "Differ nt e ti s may share the same m ntion se s , denoted as s\u21e4j .", 
        "814": "Example As shown i Figur 1, ther ar two differe t mentions \u201cIndepe dence Day\u201d m1 and \u201cJuly 4th\u201d m2 in the do ument .", 
        "815": "MPME s o learn two mention senses s11, s 1 2 for 1, nd n m ntion s ns 22 f r m2.", 
        "816": "Clearly, th e t o m ntions share a common sense in the last two documen s: the United St es holiday e2, so we ave s\u21e42 = s 1 2 = s 2 2.", 
        "817": "Not that w, , s are naturally embedded into the sa e semantic space si ce they are basic u its i texts, and e modeling the graph structure in KB is actual y in another semantic space.", 
        "818": "3 Method\nIn this section, we firstly describe the framework of MPME, followed by th detailed inf mation of each key componen .", 
        "819": "Then, we introduce a well design d mention sense disambiguation method, which can also b use for entity linki g in a unsupervis d way.", 
        "820": "3.1 Framework\nGiven knowledge base KB, text corpus D and set of nchors A, we are to jointly learn word, entity and mention representations: w, e, m. As shown in Figure 2, our proposed MPME contains four key components: (1) Mention Sense Mapping: given an anchor < ml, ej >, we map it to the corresponding mention sense to reduce the mention vocabulary to lear mbeddings.", 
        "821": "If only a en-\ntion is given, we map it to several mention s ses\nthat requires disa biguation (Section 3.4).", 
        "822": "(2) Entity R presentation Learning based on outlinks in Wikipedia pages, we construct a knowledge network to represe t the s mantic relatedness among entities.", 
        "823": "And then learn entity embeddings so that similar entities on the graph have similar representations.", 
        "824": "(3) Mention Representation Learning given apped anc ors in contexts, we learn mention sense embeddings y incorporating both textual cont xt emb ddings a d entity embeddings.", 
        "825": "(4) Text epresentation Learning we extend skip-gram model to simultaneously learn\nword and mention ense embeddings on annotated text corpus D0.", 
        "826": "Following (Yamada e al., 201 ), we use w kipedia articles as text corpus, and the anc ors p ovide annotated mentions1.", 
        "827": "W joint y train (2), (3) and (4) by using a u ified optimization objective.", 
        "828": "The outputs embeddings of word and mention are naturally in the same semantic space since they are different units in an ot ted text corpus D0 f r text representation learning.", 
        "829": "E tity embeddings keep heir own se-\na tics i noth r ctor p ce, because we only use hem as answers to predict in mention represent tio l ni g by xtending Co tinuous BOW model, which will be further discussed in Section 3.3.4.", 
        "830": "Figur 2 shows a real example of \u201c\u201d eMemorial Day word embeddings wi and entity embeddings ej keep their own sema tic space and are naturally bridged via the new learned entity title embeddings tl, which inspires us to globally optimize the probability o hoos ng menti n senses of all the phrases of mention names in the given document.", 
        "831": "Since e ch mention sense corresponds to an entity, the mention sense disambiguation process can also be reg rde s l king entities t knowledge base in a unsupervised way, which will be detailed in Section ??.", 
        "832": "3.2 M ntion Se se Mapping There ar two kinds of mappi s: from ntities to mention senses, and from ention names to mention senses The former is pre-defined at t e very begin ing.", 
        "833": "Given the knowledge Base KB, we extract entity titles {tl} and initialize with multiple mention senses, wh re the sense number depends on how many ntities hare a common title.", 
        "834": "The latter is to find possible mention sens s f r the given mention name, which is si ilar to candidate\nmention generation in entity linking task.", 
        "835": "Conventional candidate mention generation gener lly maintain a list of pairs of mention name and entity that denotes a candidate reference in knowledge base for the mentio name, and recog-\nizes the m tion ame i text by accurat s ring m tchi .", 
        "836": "Or it firstly recognizes possible mention\n1We can also annotate text corpus by using NER tool like python nltk to recognize mentions, and disambiguating its mapped mention senses as described in Section 3.4.", 
        "837": "This is an ongoing work with th goal of learning additional out-ofKB senses by self-training.", 
        "838": "In this paper, we will focus on the effectiveness of our model and the quality of three kinds of learned embeddings.", 
        "839": "\u2026 holds annual [[Independence Day (US)| Ind pendence Day]] celebrations and other\nfestivals \u2026\n\u2026 early Confederate [[Memorial Day]] celebrations were simple, somber occasions for veterans and their families to honor the dead \u2026\n5\n400 401 402 403 404 405 406 407 408 409 41 411 412 413 414 415 416 417 418 419 420 421 422 4 3 424 425 426 427 428 4 9 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 ACL 2017 ubmission ***.", 
        "840": "Confident al Review Copy.", 
        "841": "DO NOT DISTRIBUTE.", 
        "842": "predict the context word by maximizing th f llowing objective function: Lw = X wi,ml2D0 logP (C(wi)|wi) + logP (C(ml)|s\u21e4j ) (6) whe e s\u21e4j = g(< ml, ej > i obtain d from anchors in wikipedi rticles.", 
        "843": "Thus, similar words and m tio senses will be closed in text space, such as wfilm and s\u21e4Independence Day (film), or wc lebratio s and s\u21e4I dependence Day (US) b ca e y fr quently occur i the same contexts.", 
        "844": "Simila to WDS, we maintain a context cluster for each mention sense, which can be used for disa biguation given th conte s (S ction 5).", 
        "845": "r xample, i d1 f Figure 2, h context cluster of s\u21e4 consists of all context vectors W n ncou tering ention, th c nt xt vector we also maintain a context l ster center \u00b5\u21e4j for each mention sense s\u21e4j , which is computed by aver ging all the ontext vectors belonging to the cluster.", 
        "846": "We define context vecto s the average sum f cont xt word embeddings 1 |C(wi)| P wj2C(wi)wj.", 
        "847": "T e cluster enter is helpful for inducing mentio sense n co texts.", 
        "848": "When encou ter a menti n, we ap it t a set of mentio senses, and then find th ear st on according to the distance from its context vector to each mention sense cluster cen er, which will be discussed in Section 5. d1, d , d3, s \u21e4 j , i/s \u21e4 j s\u21e4Independence Day (US) P (ej |C(ml), s\u21e4j ) P (C(wi)|wi) \u00b7 P (C( l)|s\u21e4j ) (7) 4.5 Joint Training Considering all the above representation lear ing components, we define the overall objective function as linear combinations: L = Lw + Le + Lm (8) The training of MPME is to maximize the above function, and iteratively update thre ypes of embeddings.", 
        "849": "Also, we use negative sampli g technique for efficiency (Mikolov et al., 2013a).", 
        "850": "5 Mention Sense Disambiguation MPME learns each mention with multiple sense e beddings, and each sense corr sponds t a context cluster.", 
        "851": "Given an nnotated document D0 including M mentions, and their sense sets accordng to Section ??", 
        "852": ": M\u21e4l = {slj |slj 2 g(ml),ml 2 M}.", 
        "853": "In this section, we describe how to determine the mention sense for e ch m ntion ml in the document.", 
        "854": "Based on lang age model, identifying m ntion senses in a document c n be r garded as maximizing their joint probability.", 
        "855": "However, the global optimum is xp nsive, in which each mention g ts n optimum sense, to search over the space of all men ion se ses of all menti ns in the document.", 
        "856": "Thus, we appr ximately assi n e ch mention independently: P (D0, .", 
        "857": ".", 
        "858": ".", 
        "859": ", sj , .", 
        "860": ".", 
        "861": ".", 
        "862": ", ) \u21e1 Y P (D0|slj) \u00b7 P ( lj) \u21e1 Y P (C(ml)|slj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9) where P (C(ml)|slj) is prop rtion l to cosine similarity between context v ctor a d mention sense cluster center \u00b5lj to measure the ention\u2019s local similari y, namely local prob bility.", 
        "863": "N\u0302 (ml) de ot s neighbor mentions of ml cooccurring in a piece f text (e. .", 
        "864": "a document), and P (N\u0302 (ml)|slj) is defi ed as global pr ability since it measures global coherence of neighbor m ntions.", 
        "865": "The underlying idea is to achieve consistent semantics in a pi ce of text assu ing that all entions inside it are talking about the same topic.", 
        "866": "In this paper, w regard he mention senses identified first as neighbors of the rest mentions.", 
        "867": "P (slj) denotes prior probability of a mention sense occur ing in texts proportional to the frequency of corresp ding entity in Wikipedia anchors: P (slj) = ( |Aej | |A| ) 2 [0, 1] where is a hyper-parameter to smooth the gaps between different entity frequencies, namely smoothing parameter.", 
        "868": "It controls the importance5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 41 415 416 417 418 419 420 421 422 423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Submission ** .", 
        "869": "onfidential Review Copy.", 
        "870": "DO NOT DISTRIBUTE.", 
        "871": "predict the context words by maximizing the fo - low ng o jective functi n: Lw = X wi,ml2D0 logP (C(wi)|wi) + logP (C(ml)|s\u21e4j ) (6) where s\u21e4j = g(< ml, ej >) is obtained from anchors in wikip dia articles.", 
        "872": "Thu , similar words and mention senses w l be clos d n text pace, such as wfilm and \u21e4Independence Day (film), or wcel brations and s\u21e4Independence Day (US) b ause hey frequ ntly occur in the same contexts.", 
        "873": "Similar o WDS, we intain a ont xt clust r for ac mention sense, which can be used for di - ambiguation given t e contexts (Section 5).", 
        "874": "For exampl , in d1 of Figure 2, the co text clust r of s\u21e4 consists of al context vectors When e co ering a ention, th context v cto\nwe also maintain a context clu c nt r \u00b5\u21e4j for each m ntion sen e s\u21e4j , ich i computed by ave aging all the context vectors bel ging t the clust r. We defi e co xt ector s the average sum of context word embed ings\n1 |C wi)| P wj2C(wi)wj.", 
        "875": "Th clu t r cent r s elpful f i duci g menti n sense i c nt xts.", 
        "876": "W en encoun er a mentio , we m p it o a se of e tion enses, a d then find the n arest one accordin to the distance from its con ext vect r t ach mentio s nse cluster center, hich will be discussed i Section 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j\ns\u21e4Indep ndence Day (US)\nP (ej |C(ml), s\u21e4j )\nP (C(wi)| i) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Traini g\nConsidering al th above r pr sentation lear ing compon nts, we d fine the overall objective function as linear combinations:\nL = Lw + Le + Lm (8)\nThe training of MPME is to maximize the above fu ctio , nd iterativ ly updat three types of embedding .", 
        "877": "Also, w use negative sampling technique for effici ncy (M k lov et al., 2013a).", 
        "878": "5 Men ion Sense Disambiguation MPME learns each mention with multiple sense embeddings, and each sense corresponds to a context cluster.", 
        "879": "Given an annotated document D0 including M mentions, and their sense sets according to Section ??", 
        "880": ": M\u21e4l = {slj |slj 2 g(ml),ml 2 M}.", 
        "881": "In this section, e describe how to determine the mention sense for each m tion ml in the docum nt.", 
        "882": "Based o language model, identifying mention enses in document can be regarded as maximizing their joint probab lity.", 
        "883": "However, the global opimum s xpe sive, in which ach mention gets an optimu sense, to search over the space of ll ent o sen es of all mentions in the document.", 
        "884": "Thus, we approximately assign ach mention independently:\nP (D0, .", 
        "885": ".", 
        "886": ".", 
        "887": ", slj , .", 
        "888": ".", 
        "889": ".", 
        "890": ", ) \u21e1 Y\nP (D0| lj) \u00b7 P (slj)\n\u21e1 Y P (C(ml)| lj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9)\nwhere P (C(ml)|slj) is p oportional to cosine similarity between context vector and mention sense cluster center \u00b5lj to m asur the mention\u2019s local imilarity, namely loc probabil ty.", 
        "891": "N\u0302 (ml) denotes neighbor mentions of ml co-\nccurring i a piece of text (e.g.", 
        "892": "a document), and P (N\u0302 ( l)|slj) is defined as global probability sinc i measures global coherence of neighbor mentions.", 
        "893": "The underlying idea is t achieve c nsist nt se antics in a piece of text assumi g that all ment ons inside it are talk ng about the same topic.", 
        "894": "In this paper, we regard the mention senses id ntified first as neighbors of the rest mentions.", 
        "895": "P (slj) denot s prior probability of a mention sense occurring n texts p oport onal to the frequency of c rresponding entity in Wikipedia anchors:\nP (slj) = ( |Aej | |A| )\n2 [0, 1]\nwhere is a hyper-parameter t smooth the gaps between different entity frequencies, namely smoothing parameter.", 
        "896": "It co trols the importance\n5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 48 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 ACL 2017 Submission ***.", 
        "897": "C nfidential Revie Copy.", 
        "898": "DO NOT DISTRIBUTE.", 
        "899": "predict the context words by m ximizing the following objective function: Lw = X wi,ml2D0 logP (C(wi)|wi) + logP (C(ml)|s\u21e4j ) ( ) where s\u21e4j = g(< ml, ej >) is obtained from anchors in wikipedia articles.", 
        "900": "Thus, similar w rds and me tion senses will be closed in text space, such as film and s\u21e4Independence Day (film), or wcelebrations and s\u21e4Independence Day (US) because hey frequently occur in the same contexts.", 
        "901": "Similar to WDS, we aintain a contex clu t r for each mention sense, which can be used for disambiguation given the contexts (Section 5).", 
        "902": "For example, in d1 of Figure , the context clu ter of s\u21e4 consists of all context vectors Wh n e cou t ring a mention, the context vect r we also maintain a context cluster center \u00b5\u21e4j for each mention sense s\u21e4j , which is computed by averaging all the context ve tors b longi to the cluster.", 
        "903": "We define context vector s the average sum of context word mbeddings 1 |C(wi)| P wj2C(wi) j.", 
        "904": "The cluster cent r is helpful for inducing mention sense in contexts.", 
        "905": "Wh n encounter a mention, we map it to a s t of menti n senses, and then find the near st one ccording to the distance from its context vector to each m nti n sense cluster center, which will be discussed in Sect on 5. d1, d2, d3, s \u21e4 j , wi/s \u21e4 j s\u21e4Independence Day (US) P (ej |C(ml), s\u21e4j ) P (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7) 4.5 Joint Training Considering all the above rep esentation learning components, we define the overall objective function as linear combinations: L = Lw + Le + Lm (8) The trai i g of MPME is to maximize the above function, nd iteratively update three types of mbeddings.", 
        "906": "Also, we use negative s pli g t chnique for efficiency (Mikolov t al., 2013a).", 
        "907": "5 Mention S nse Disambiguation MPME learns each me tio with multiple sense embeddings, and each s se corr sponds to a context clust r. Given an a notat d document D0 including M m ntions, and their sense sets acc rding to Section ??", 
        "908": ": M\u21e4l = { lj | lj 2 g(ml),ml 2 M}.", 
        "909": "In this i n, we d cribe how to determine the m ntio sen for e ch ntion l in the document.", 
        "910": "Based on language model, i ntifyi g me ti senses in a docum nt can be g r ed as maximizing their jo nt probabili y. H wev r, the global opti um is expensive, i which each mention gets an optimum sens , to arch over the space of all mention senses of all m ti n in th document.", 
        "911": "Thus, we approximately assign each mention independently: P (D0, .", 
        "912": ".", 
        "913": ".", 
        "914": ", sj , .", 
        "915": ".", 
        "916": ".", 
        "917": ", ) \u21e1 Y P (D0|slj) \u00b7 P (slj) \u21e1 Y P (C(ml)|slj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9) wher P (C(ml)|slj) is proport onal o cosine simlarity b tween cont xt vector and ention sense luster ce er \u00b5lj to m asur th m tion\u2019s local simi arity, namely local probabili y. N\u0302 (ml) note nei hbor mentions of ml c - occurring in a piece of text (e.g.", 
        "918": "a document , and P (N\u0302 (ml)|slj) is defined as glob l robability since it measures global coherence f neighbor mentions.", 
        "919": "The und rlyin ide is to achieve onsistent s mantics in a piece of text assu ing that all ntions inside it are talking about the sa e topic.", 
        "920": "In this paper, w regard the menti n senses identifie first as n ighbors of the rest mentions.", 
        "921": "P (slj) de tes prior probability of a menti n sense occurring in texts proportional to the frequency of corresponding entity in Wikipedia anchors: P (slj) = ( |Aej | |A| ) 2 [0, 1] where i a hyper-parameter to smooth the gaps between different tity frequencies, namely smoothing parameter.", 
        "922": "It controls the importance\n5\n400 401 402 403 404 405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 1 452 53 454 455\n456\n457\n8\n459\n60\n461\n462\n463\n464\n65\n466\n67\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Submission ***.", 
        "923": "Confidenti l Review Copy.", 
        "924": "DO NOT DISTRIBUTE.", 
        "925": "predict he c ext words by max izi g th following objective function: Lw = X\nwi,ml2D0 logP (C(wi)|wi\n+ logP (C( l)|s\u21e4j ) (6)\nwhere s\u21e4j = g(< l, j >) is obtained from a - chors in wikipedia articl s.\nThus, similar word nd menti n senses will be closed in text space, such as wfilm and s\u21e4Independ ce Day (film), or wcelebrations and s\u21e4Independence Day (US) b cause t y fr qu ntly occur in the same co text .", 
        "926": "Similar to WDS, we maintai a conte t cluster for each men on e se, w ch ca be us d for disambiguation given the cont x s (Sect on 5).", 
        "927": "For example, in d1 of Figure , the c text cluster of s\u21e4 c sists of all c ntext v ctors Wh n ncou t ring a mentio , the context vector\nwe also maintai a context cluster ce ter \u00b5\u21e4j for each ment on sense s\u21e4j , which is computed by averaging all the c ntext vector l ngi g to the cluster.", 
        "928": "We define context vector as the average sum of context wor embeddings\n1 |C(wi)| P wj2C(wi)wj.", 
        "929": "The cluster cent r is helpful for inducing mention sense in c ntexts.", 
        "930": "When encounter a mention, we map it to a set f mention senses, and n fi d the ne t one according to the distance from its context vector to each m - tion sense cluster center, which will be discussed in Sectio 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j\ns\u21e4Independence Day (US)\nP ( j |C(ml), s\u21e4j )\nP (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Training\nConsidering all the above representation learning components, we define the overall objective function as lin ar c mbinations:\nL = Lw + Le + Lm (8)\nTh a ing of MPME is t ximize th abov fu ction, nd it r tiv ly update three types of embeddings.", 
        "931": "Also, w us eg tive sampling t chnique for effici ncy (Mikolov et al., 2013a).", 
        "932": "5 Me ti S Dis mbiguatio\nMPME l arns each menti n w th multipl sense emb dings, and each sen e corr spo ds to a c - text clu er.", 
        "933": "Giv an annotat d d cument D0 includi g M m nt ons, and their sens sets according to Sectio ??", 
        "934": ": M\u21e4l = {slj |slj 2 g(ml),ml 2 M}.", 
        "935": "In this section, we escribe h w d termine the m ntion sens f e ch me tion ml in th d cument.", 
        "936": "Ba ed o languag m del, identi ying mentio e ses in a doc me t can be eg d d a maximiz-\ning their joi t pr bability.", 
        "937": "However, the global optimum is exp nsiv , i which e ch mention gets an optimum sense, to search over the space of all me tion sens s f all mentio s in t e document.", 
        "938": "Thus, we approxim tely assign each mention i - dependently:\nP (D0, .", 
        "939": ".", 
        "940": ".", 
        "941": ", slj , .", 
        "942": ".", 
        "943": ".", 
        "944": ", ) \u21e1 Y\nP (D0|slj) \u00b7 P (slj)\n\u21e1 Y P (C(ml)|slj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9)\nwhere P ( l)|slj is proportional to cosine similarity between co ext vector and m ntion sens cluster ce ter \u00b5lj to mea ure the mention\u2019s l cal similarity, namely local probability.", 
        "945": "N\u0302 (ml) denotes neighbor mentions of ml cooccurring in a piece of text (e.g.", 
        "946": "a document), and P (N\u0302 (ml)|slj) is defined as global probability since it measures global coh re c of neig b r mentions.", 
        "947": "The underlying idea is to achi ve consistent semantics in a piece of text assuming that all mentions inside it are talking about the same topic.", 
        "948": "In this paper, we regard the mention senses identifi d first as nei hbors f the rest ntions.", 
        "949": "P (slj) denotes prior pr bability of a mention\nsense occurring in t xts proportional to the fr - que cy f c rrespondi g entity in Wikipedia nchors:\nP (slj) = ( |Aej | |A| )\n2 [0, 1]\nwhere is a yper-parameter to smooth he gaps between different entity frequencies, namely smoothing paramet r It controls the im ortance5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Sub ission ***.", 
        "950": "Confiden ial R view Copy.", 
        "951": "DO NOT DISTRIBUTE.", 
        "952": "predict the ont xt word by max miz ng the following objective function: Lw = X wi,ml2D0 logP (C( i)|wi) + logP (C(ml)|s\u21e4j ) (6) here \u21e4j = g(< ml, j >) i b ain d fr m anch rs in wikipedia articles.", 
        "953": "Thus, similar words a d n sen s will be c os d n text spac , such as wfil a d s\u21e4Independenc Day (film), or w lebra s a d s\u21e4Independence Day (US) because h y freq e tly - cur in the same contex s.\nSimilar to WDS, we ain ain a co text cluster for each me t on se se, which an b u d fo disambiguation given h contexts Section 5 .", 
        "954": "F r example, in d1 f Fi ur 2, th cont xt clust r f s\u21e4 con ist f ll context vec ors Wh n ountering a mention, the context v ctor\nwe also maintain a context lus er cent r \u00b5\u21e4j fo each me ti n s ns s\u21e4j , hich is c mput d by av aging all the co text vect r belonging to the cluster.", 
        "955": "We defi c t xt ve tor as the average sum of cont xt ord edding\n1 |C(wi)| P wj2C( i)wj Th cluster center h lpful for in ucing mention sense i c ntexts.", 
        "956": "When encounter a me io , we map it t set of ment on senses, and th n find the nearest o e according to the istance from its ontext v ctor to each menti n s nse cluster c nt r, which will be discussed in Section 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j\ns\u21e4Indep dence Day (US)\nP (ej |C(ml), s\u21e4j )\nP (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Trai ing\nConsidering all the above representation learning components, we define the overall obj ive functio as lin ar combinations:\nL = Lw + Le + Lm (8)\nThe training of MPME is to aximize the bove function, and it ratively update three types of emb ddings.", 
        "957": "Also, we use negative sampling t chnique for efficie cy (Mikolov et al., 2013a).", 
        "958": "5 M n i n Sens Disambiguation MPME learn each ention with multiple sense beddi gs, and ach sense corresponds to a cont xt cluster.", 
        "959": "Give an annotated document D0 including M mentions, d t eir ense sets according to S ctio ??", 
        "960": ": M\u21e4l = {slj |slj 2 g(ml),ml 2 M}.", 
        "961": "I this section, we d scribe how to determine t e m ntion s nse f r ach me tion ml in the document.", 
        "962": "Based o language model, identifying mention sense in ocument can be r g rded as maximizi g their joint pr b bility.", 
        "963": "However, the global optimu is exp nsive, in wh ch each mention gets a opt mu sense, to search over the space of all mention senses of all m nt ns in the docume t. T us, we approxi at ly assign e c ment on independently:\nP (D0, .", 
        "964": ".", 
        "965": ".", 
        "966": ", slj , .", 
        "967": ".", 
        "968": ".", 
        "969": ", ) \u21e1 Y\nP D0|slj) \u00b7 P (slj)\n\u21e1 Y P (C( l)| j) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9)\nwhere P (C(ml)|slj) is pr p rtional to c sine similarity between c ntext vector and m ntion sense cluster center \u00b5lj t m asur the mentio \u2019s local si ilarity, nam ly local probability.", 
        "970": "N\u0302 (ml) denot s neighbor mentions of ml cooccurring in a p ece of text (e.g.", 
        "971": "a d cument), an P (N\u0302 (ml)|slj) is d fin d as global probability since t measur s global coh rence of neig bor m tion .", 
        "972": "The und rlyi g idea is to achieve consistent semantics in a piece of text assum ng that all men ions inside it ar t lking about the sa e topic.", 
        "973": "In this pap r, we regard the ention senses identified first as neighbors of the rest mentions.", 
        "974": "P (slj) denotes prior probability of a mention s nse occurring in texts proportion l to the frequency of corresponding entity in Wikipedia anchors:\nP ( lj) = (\n|Aej | |A| )\n2 [0, 1]\nwhere is a hyper-p rameter to smooth th gaps between diff rent entity frequenc es, namely smoothing parameter.", 
        "975": "It controls the importance\n5\n400 401 402 403 404 405 406 407 408 409 410 411 412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457 4 8 459 460 461 462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Submission ***.", 
        "976": "Confidential Review Copy.", 
        "977": "DO NOT DISTRIBUTE.", 
        "978": "predict the context words by maximizing the following objective function: Lw = X wi,ml2D0 logP (C(wi)|wi) + logP (C(ml)| \u21e4j ) (6) where s\u21e4j = g(< ml, ej >) is obtained from anchors in wikipedia articles.", 
        "979": "Thus, similar words and mention sens s will be closed in text space, such as wfilm and s\u21e4Independence Day (film), or wcelebrations and s\u21e4Independence Day (US) because they frequ ntly occur in the same contexts.", 
        "980": "Similar to WDS, we maintain a context cluster for each mention sense, which can be u d for di - ambiguation given the contexts (Section 5).", 
        "981": "F r example, in d1 of Figure 2, the context c uster of s\u21e4 consists of all context v c ors When en unt ring a mention, the context vector\nwe also maintain a context cluster center \u00b5\u21e4j for each mention sens s\u21e4j , hich is computed by averaging all the context vectors belonging to the cluster.", 
        "982": "W define context vector as the average sum of context word embe di gs\n1 |C(wi)| P wj2C(wi)wj.", 
        "983": "The cluster center is helpful for inducing mention sense in cont xts.", 
        "984": "When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to ea h mention sense cluster center, which will be discussed in Section 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j\ns\u21e4Independence Day (US)\nP (ej |C(ml), s\u21e4j )\nP (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Training\nConsidering all the above representation learning components, we define the overall obj ctive function as linear combinations:\nL = Lw + Le + Lm (8)\nThe training of MPME is to maximize the above function, and iterativ ly updat three types of embeddings.", 
        "985": "Also, we us nega ive ampli g t chique for efficiency (M kolov t al., 2013 ).", 
        "986": "5 M ntion S nse D sambiguation MPME lear s ch nti n w th multiple sens embed ings, and each sens corresp ds to a ontext cluster.", 
        "987": "Given an annotat d d cumen D0 including M mentions, and their sense sets according to Sectio ??", 
        "988": ": M\u21e4l = {slj |slj 2 g(ml), l 2 M}.", 
        "989": "In this section, e d s rib h w to d t rmine the entio sens f r ach men io ml in t d cument.", 
        "990": "Bas d on languag mod l, identifying m nti n senses in a d cument can be rega ded as aximizing t eir joint prob b l ty.", 
        "991": "Howeve , the gl b l optimum is xpen ive, i which each mention g ts an p imum sense, to search over p c f all mention senses of all me tions in th do ument.", 
        "992": "Thus, we app ximately s ig eac ment on independently:\nP (D0, .", 
        "993": ".", 
        "994": ".", 
        "995": ", slj , .", 
        "996": ".", 
        "997": ".", 
        "998": ", ) \u21e1 Y\nP (D0|slj) \u00b7 P ( lj)\n\u21e1 Y P (C(ml)|slj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9)\nwhere P (C(m )|slj) is pr porti al t co i e similarity betwe n context vector a mentio ense cluster center \u00b5lj to measure the nti n\u2019s cal similarity, nam ly local probability.", 
        "999": "N\u0302 (ml) denotes neighbor me ti ns of l cooccurring in a piece of text (e.g.", 
        "1000": "a docume t), and P (N\u0302 (ml)| lj) is defined s global probability since it measures global coherence of neighbor m ntions.", 
        "1001": "The underlyi idea is to achi ve consistent semantics i a piece of text assuming that all mentions inside it are talking about the same topic.", 
        "1002": "In this paper, we regard the m tion enses identified first as neighbors of the rest mentions.", 
        "1003": "P (slj) denotes prior probability of a mention sense occurring in texts proportio al to the fre-\nquency of corresponding entity in Wikipedia an-\nchors:\nP (slj) = ( |Aej | |A| )\n2 [0, ]\nwhere is a hyp r-para eter o moo the gaps between different entity frequencies, amely smoothing parameter.", 
        "1004": "It controls th importance\n5\n400 401 402 403 404 405 406 407 408 409 410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457 458 459 460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n7\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n91\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Su missio ***.", 
        "1005": "Confid tia Review Copy.", 
        "1006": "DO NOT DISTRIBUTE.", 
        "1007": "predict the context words by maximizing the following objective functi n: Lw = X wi,ml2D0 logP (C(wi)|wi) + logP (C(ml)|s\u21e4j ) (6) where s\u21e4j = g(< ml, ej >) is obtained from anchors in wikipedia articles.", 
        "1008": "Thus, similar words and mention senses will be closed in text space, such as wfilm and s\u21e4Independence Day (film), or wcelebrations and s\u21e4Independence Day (US) because they frequently occur in the same contexts.", 
        "1009": "Similar to WDS, we maintain a context cluster for each mention sense, which can be used for disambiguation given the contexts (Sectio 5).", 
        "1010": "For example, in d1 of Figure 2, the context cluster of s\u21e4 consists of all context vectors When ncountering a mention, the context vector\nwe also maintain a context cluster center \u00b5\u21e4j for each mention sense s\u21e4j , which is computed by averaging all the context vectors belonging to the cluster.", 
        "1011": "We define context vector as the average sum of context word embedd ngs\n1 |C(wi)| P wj2C(wi)wj.", 
        "1012": "The cluster center is helpful for inducing mention sense in contexts.", 
        "1013": "When encounter a mention, we map it to a set of mention senses, and then find the nearest one according to the distance from its context vector to each mention sense cluster center, which will be discussed in Section 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j\ns\u21e4Independence Day (US)\nP (ej |C(ml), s\u21e4j )\nP (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Training\nConsidering all the above representation learning components, we define the overall objective function as linear combinations:\nL = Lw + Le + Lm (8)\nThe training of MPME to aximiz the bove fun tion, d iterativ ly update t ree types of - beddings.", 
        "1014": "Also, us negativ sa pli g - niqu for effic ency (Mik lov et al., 013a).", 
        "1015": "5 Mention Sense Disa biguatio MPME learns each me tio with multipl sense mb dding , and ach sense resp n s o a con-\nt xt cluster.", 
        "1016": "Given an ann tat d docu t D0 i - cluding M mentions, and their se se ets ac rd ing to Section ??", 
        "1017": ": M\u21e4l = {slj |slj 2 g(ml),ml 2 M}.", 
        "1018": "In this secti , we describ how to d t min the mention sense for each m ntion l i the document.", 
        "1019": "Based on langu ge m d l, id n ifyi g me t on senses i a d cume t can be reg rd d as maxim zing their j int pr bability.", 
        "1020": "However, the global optimum is x ensive, in which each enti n g s an optimum sense, to sear h over the space of all mention senses of all mentions in th document.", 
        "1021": "Thus, w approximat y ssig ach e t on independ tly:\nP (D0, .", 
        "1022": ".", 
        "1023": ".", 
        "1024": ", slj , .", 
        "1025": ".", 
        "1026": ".", 
        "1027": ", ) \u21e1 Y\nP (D0|slj) \u00b7 P (slj)\n\u21e1 Y P (C(ml)| lj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9)\nwhere P (C(ml)|slj) is prop rtio al to cosi imilarity between context vector and menti n sense cluster center \u00b5lj to easure the ment on\u2019s local simi arity, na ely local probability.", 
        "1028": "N\u0302 (ml) denotes neig bor mentions of ml cooccurring in a piece of text (e.g.", 
        "1029": "a document), and P (N\u0302 (ml)|slj) is defi ed as global pr babi - ity since it measu es global coherence f neighbor mentions.", 
        "1030": "The underlying idea is to achieve consistent semantics in a pi ce of text assuming that all mentions inside it are talking ab ut the same topic.", 
        "1031": "In this paper, we regard the mention senses\nidentified first as neighbors of the rest mentions.", 
        "1032": "P (slj) denotes prior pro ability of a me tion\nsense occurri g in texts proporti n l to the fr - quency f corresponding entity in Wikipedia anchors:\nP (slj) = ( |Aej | |A| )\n2 [0, 1]\nwhere is a hyper-parameter to smooth the gaps between different entity frequencies, namel smoothing para ete .", 
        "1033": "It controls he importa ce\n5\n400 401 402 403 404 405 406 407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 454 455 456 457\n458\n459\n460\n461\n462\n463\n46\n465\n466\n467\n468\n469\n470\n471\n47\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Su mission ***.", 
        "1034": "Confid ntial Review Copy.", 
        "1035": "DO NOT DISTRIBUTE.", 
        "1036": "predict the context words by maximizing the following objective functi n: Lw = X wi,ml2D0 logP (C(wi)|wi) + logP (C(ml)|s\u21e4j ) (6)\nwhere s\u21e4j = g(< ml, ej >) is obtained from anchors in wikipedia articles.", 
        "1037": "Thus, similar words and mention senses will be closed in text space, such as wfilm and s\u21e4Independence Day (film), or wcelebrations and s\u21e4Independence Day (US) because they frequently occur in the same contexts.", 
        "1038": "Similar to WDS, we maintain a co text cluster for each mention sense, which can be used for d sambiguation given the contexts (Section 5).", 
        "1039": "For example, in d1 of Figure 2, the context clu ter of s\u21e4 consists of all context vectors When encountering a mention, the context vector\nwe also maintain a context cluster center \u00b5\u21e4j for each mention sense s\u21e4j , which is computed by averaging all the context v ctors belonging to the cluster.", 
        "1040": "We define c ntext vector s the average sum of context word embeddings\n1 |C(wi)| P wj2C(wi)wj.", 
        "1041": "The cluster c nter is helpful for inducing mention sense in contexts.", 
        "1042": "When encounter a mention, we map it to a set of mention senses, and then find the nearest on accordi g to the distance from its context vector to each mention sense cluster center, which will be discussed in Section 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j\ns\u21e4Independence Day (US)\nP (ej |C(ml), s\u21e4j )\nP (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Training\nConsidering all the above representation learning components, we define the overall objective function as linear combinations:\nL = Lw + Le + Lm (8)\nThe training of MPME is t m xi iz the above functio , and iteratively update thr e type of - beddings.", 
        "1043": "Als , w use neg tive sampli g t chnique for efficie cy (Mikolov et al., 2013a).", 
        "1044": "5 Me tion Sens Disambigu tion\nMPME l arns each mention with multiple sense embeddings, and each sense corr sp nds t a co - text cluster.", 
        "1045": "Given an annotated d cument D0 incl d ng M m ntions, and their ns sets ccording to Section ??", 
        "1046": ": M\u21e4l = {slj |slj 2 g(ml),ml 2 M}.", 
        "1047": "In this section, e describ how to d termin the mention sense for each mention l i t d - ument.", 
        "1048": "Based on language mod l, identifying mention senses in a document can be r g rd d as maxi izing their joint probability.", 
        "1049": "However, the global optimum is expensive, in hich each mentio gets an optimum sense, to search over the space of all mention senses of all me tions i the document.", 
        "1050": "Thus, we approximately assign ea h menti n independently:\nP (D0, .", 
        "1051": ".", 
        "1052": ".", 
        "1053": ", slj , .", 
        "1054": ".", 
        "1055": ".", 
        "1056": ", ) \u21e1 Y\nP (D0|slj) \u00b7 P (slj)\n\u21e1 Y P (C(ml)|slj) \u00b7 P ( \u02c6 (ml)| lj) \u00b7 P (slj) (9)\nwhere P (C(ml)|slj) is proportional to c sin simil rity bet een context vector nd mention sense cluster center \u00b5lj to measur the me ti \u2019s l cal similarity, namely local probability.", 
        "1057": "N\u0302 (ml) denotes eighb r m ntions f ml cooccurring in a piece of tex (e.g.", 
        "1058": "a document), and P (N\u0302 (ml)|slj) is defined as global probability since it measur global cohe e e of n ighbor mentions.", 
        "1059": "The underlying idea is to ach eve consistent manti s a piece of text assuming ha\nall mentions inside it are talking about the sam\nopi .", 
        "1060": "In this pap r, w regard the mention senses identified first as neighbors of the rest mentions.", 
        "1061": "P (slj) denotes prior probability of a mention sense occurring in texts proportional to the frequency of rresponding ntity in Wikipedia anchors:\nP (slj) = ( |Aej | |A| )\n2 [0, 1]\nwhere is a hyper-parameter t s ooth the gaps between different entity frequencies, namely smoothing parameter.", 
        "1062": "It controls the importance\n5\n400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450 451 452 453 45 455 456 457 458 459 60 61 462 463 464 465 466 467 468 6\n7\n471\n472\n473\n474\n475\n476\n477\n478\n79\n80\n481\n482\n483\n48\n485\n486\n487\n88\n89\n490\n491\n492\n493\n494\n495\n496\n497\n98\n99\nACL 2017 Submissi n ***.", 
        "1063": "Confidential Review Copy.", 
        "1064": "DO NOT DISTRIBUTE.", 
        "1065": "predict the context words by maximizing the following objective function: Lw = X wi, l2D0 logP (C(wi)|wi) + logP (C(ml)|s\u21e4j ) (6) where s\u21e4j = g(< ml, ej >) is obtained from anchors in wikipedia articles.", 
        "1066": "Thus, similar words and mention senses will be closed in text space, such a wfilm and s\u21e4Independence Day (film), or wcelebrations and s\u21e4Independence Day (US) because they fr quently occur in the same contexts.", 
        "1067": "Si ilar to WDS, we maintain a context cluster for e ch mention sense, which can be used for disambiguation given the contexts (Section 5).", 
        "1068": "For example, in d1 f Figure 2, the co text cluster of s\u21e4 consists of all context vec ors When encountering a mention, the context vector\nwe also maintain a context cluster center \u00b5\u21e4j for each mention sense s\u21e4j , which is computed by averaging all the ontext vectors belongi g to the cluster.", 
        "1069": "We d fi e context vector as the average sum of context word embeddings\n1 |C(wi)| P wj2C(wi)wj.", 
        "1070": "The cluster center is helpful for inducing mention sense in c nt xts.", 
        "1071": "When encounter a mention, we map it to a set of mention senses, an then fi d the nearest one accordi g to the distance from its context vector to each mention sense clust r c nter, which will be discuss d in Section 5.\nd1, d2, d3, s \u21e4 j , wi/s \u21e4 j , e3\ns\u21e4Independ nce Day (US)\nP (ej |C(ml), s\u21e4j )\nP (C(wi)|wi) \u00b7 P (C(ml)|s\u21e4j ) (7)\n4.5 Joint Training\nConsidering all the above representati n learni g components, we define the overall objective function as linear combinations:\nL = Lw + Le + Lm (8)\nThe training f MPME is to maximize the ab ve function, and it ratively update three types of em beddings.", 
        "1072": "Also, we use negative sampling technique for effici n y (Mikol v et al., 2013a).", 
        "1073": "5 Mention Sens Disambiguation MPME learns each mention ith multiple sen embeddings, and each se se correspo ds to a context cluster.", 
        "1074": "Giv n an a not t d docume t D0 i - cluding M mentions, an th ir sen s ts cc rd i g to Section ??", 
        "1075": ": M\u21e4 = {slj | lj 2 g( l), l 2 M}.", 
        "1076": "In this section, we describe how t determine the mention ense for each mentio ml in the cument.", 
        "1077": "Based on language model, ide tifying mention senses in a d cument can be r g ded as aximizi g their joi t probability.", 
        "1078": "However, the global optimum is expensive, in which each me tion gets an optimum sense, to search over the space of all mention s ses f all mentions i the ocum nt.", 
        "1079": "Thus, w approxim t ly assign e ch enti n independently:\nP (D0, .", 
        "1080": ".", 
        "1081": ".", 
        "1082": ", slj , .", 
        "1083": ".", 
        "1084": ".", 
        "1085": ", ) \u21e1 Y\nP (D0|slj) \u00b7 P (slj)\n\u21e1 Y P (C(ml)|slj) \u00b7 P (N\u0302 (ml)|slj) \u00b7 P (slj) (9)\nwh re P (C(ml)|slj) is p oportion l t cosi e similarity bet een cont x vector and menti n sense clu ter center \u00b5lj to mea ure th mention\u2019s l cal similarity, nam ly local probability.", 
        "1086": "N\u0302 (ml) d n tes neighb r mentions of ml cooccurring in a piece of text (e.g.", 
        "1087": "a ocume t), and P (N\u0302 (ml)|slj) is defined as global probability since it measures gl bal c her nce of ighbor men ions.", 
        "1088": "The underlying idea is to achiev consist nt semantics in a pi ce of text assuming that all mentions inside it are talki g about the ame topic.", 
        "1089": "In this paper, we r gard the m ntion senses identifi d first as neighbors of th rest e tions.", 
        "1090": "P (slj) d notes p ior pr bability of a mention sense occurring in texts proportiona to th frequency of corres onding entity in Wikipedia anchors:\nP (slj) = ( |Aej | |A| )\n2 [0, 1]\nwher is a hyper-par meter to smooth the gaps between different entity frequencies, namely smo thing aramete .", 
        "1091": "It controls th importance\nK owledge Space\nText Spac\nFigure 2: Framework f Multi-Prototype M ntion Embedding odel.", 
        "1092": "Mention Sens Mapping To reduce th size of the mention vocabulary, each mention is mapped to a set of shared mention senses accordin to a predefined dictionary.", 
        "1093": "We build th dictionary y c llecting entity-mention pairs < l, ej > from Wikipedia anchors and page titl s, then cr - e mention senses if there is a different entity.", 
        "1094": "The\nsense number of a mention depends on how many different entity-mention pairs it is involv d.\nFormally, we have: M\u2217l = g(ml) = \u22c3 g(< ml, ej >) = {s\u2217j}, where g(\u00b7) denotes the mapping function from an entity mention to its mention sense given an anchor.", 
        "1095": "We directly use the anchors contained in the annotated text corpus D \u2032 for training.", 
        "1096": "As Figure 2 shows, we re-\nplace the anchor <July 4th, Independe ce Day (US)> with the corresponding mention sense: s\u2217Independence Day (US).", 
        "1097": "Representation Learning Using KB, A and D\u2032 as input, we design three separate models and a unified optimization objective to jointly learn entity, word and mention sense representations into two semantic spaces.", 
        "1098": "As shown in the knowledge space in Figure 2, entity embeddings can reflect their relatedness in the network.", 
        "1099": "For example, Independence Day (US) (e1) and Memorial Day (e3) are close to each other because they share some common neighbors, such as United States and Public holidays in the United States.", 
        "1100": "Word and mention embeddings are learned in\nthe same semantic space.", 
        "1101": "As two basic units in D\u2032, their emb ddi gs repr sent their di ribut d semanti s i texts.", 
        "1102": "For xa ple, ention I dependence Day a d word celebrations co-occur frequently w en it refers to the holiday: Independence Day (US), thus they have sim lar representations.", 
        "1103": "Without disambiguating th mention sen es, some words, such as film will also shar similar representatio s as Independence Day.", 
        "1104": "Besid s, by introducing entity embeddings into our MPME framework, the knowledge information ill also be distilled i to mentio sense embedding , so that the menti n sense Memorial Day will be similar as Independence Day (US).", 
        "1105": "Mention Sense Disambiguation According to our predefined dictionary, each mention has been mapped to more than one se ses, and learned with multiple embedding vectors.", 
        "1106": "Co equently, to induce the correct sense for a mention within a context is critical in the usage of the multiprototype embeddings, especially in an unsupervised way.", 
        "1107": "Formally, given an annotated document D\u2032, we determine one sense s\u0302\u2217j \u2208 M\u2217l for each mention ml \u2208 D\u2032, where s\u0302\u2217j is the correct sense.", 
        "1108": "Based on language model, we design a mention sense disambiguation method without using any supervision that takes into account three aspects: 1) sense prior denotes how dominant the sense is, 2) local context information reflects how semantically appropriate the sense is in the context, and\n3) global mention information denotes how semantically consistent the sense is with the neighbor mentions.", 
        "1109": "To better utilize the context information, we maintain a context cluster for each mention sense during training, which will be detailed in Section 4.4.", 
        "1110": "Since each mention sense corresponds to an entity in the given KB, the disambiguation method is equivalent to entity linking.", 
        "1111": "Thus, text and knowledge base is bridged via the multiprototype mention embeddings.", 
        "1112": "We will give more analysis in Section 6.4.", 
        "1113": "4 Representation Learning\nDistributional representation learning plays an increasing important role in many fileds (Bengio et al., 2013; Zhang et al., 2017, 2016) due to its effectiveness for dimensionality reduction and addressing sparseness issue.", 
        "1114": "For NLP tasks, this trends has been accelerated by the Skip-gram and CBOW models (Mikolov et al., 2013a,b) due to its efficiency and remarkable semantic compositionality of embedding vectors.", 
        "1115": "In this section, we first briefly introduce the Skip-gram and CBOW models, and then extend them to three variants for the word, mention and entity representation learning.", 
        "1116": "4.1 Skip-Gram and CBOW model\nThe basic idea of the Skip-gram and CBOW models is to model the predictive relations among sequential words.", 
        "1117": "Given a sequence of words D, the optimization objective of Skip-gram model is to use the current word to predict its context words by maximizing the average log probability:\nL = \u2211\nwi\u2208D\n\u2211\nwo\u2208C(wi) logP (wo|wi) (1)\nIn contrast, CBOW model aims to predict the current word given its context words:\nL = \u2211\nwi\u2208D logP (wi|C(wi)) (2)\nFormally, the conditional probability P (wo|wi) is defined using a softmax function:\nP (wo|wi) = exp(wi \u00b7wo)\u2211\nwo\u2208D exp(wi \u00b7wo) (3)\nwhere wi,wo denote the input and output word vectors during training.", 
        "1118": "Furthermore, these two\nmodels can be accelerated by using hierarchical softmax or negative sampling (Mikolov et al., 2013a,b).", 
        "1119": "4.2 Entity Representation Learning Given a knowledge base KB, we aim to learn entity embeddings by modeling \u201ccontextual\u201d entities, so that the entities sharing more common neighbors tend to have similar representations.", 
        "1120": "Therefore, we extend Skip-gram model to a network by maximizing the log probability of being a neighbor entity.", 
        "1121": "Le = \u2211\nej\u2208E logP (N (ej)|ej) (4)\nClearly, the neighbor entities serve a similar role as the context words in Skip-gram model.", 
        "1122": "As shown in Figure 2, entity Memorial Day (e3) also share two common neighbors of United States and Public holidays in the United States with entity Independence Day (US), thus their embeddings are close in the Knowledge Space.", 
        "1123": "These entity embeddings will be later used to learn mention representations.", 
        "1124": "4.3 Mention Representation Learning As mentioned above, the textual context information and reference entities are helpful to distinguish different senses for a mention.", 
        "1125": "Thus, given an anchor < ml, ej > and its context words C(ml), we combine mention sense embeddings with its context word embeddings to predict the reference entity by extending CBOW model.", 
        "1126": "The objective function is as follows:\nLm = \u2211\n<ml,ej>\u2208A logP (ej |C(ml), s\u2217j ) (5)\nwhere s\u2217j = g(< ml, ej >).", 
        "1127": "Thus, if two mentions refer to similar entities and share similar contexts, they tend to be close in semantic vector space.", 
        "1128": "Take Figure 1 as an example again, mentions Independence Day and Memorial Day refer to similar entities Independence Day (US) (e1) and Memorial Day (e2), they also share some similar context words, such as celebrations in documents d2, d3, so their sense embeddings are close to each other in the text space.", 
        "1129": "4.4 Text Representation Learning Instead of directly using a word or a mention to predict the context words, we incorporate mention\nsense to joint optimize word and sense representations, which can avoid some noise introduced by ambiguous mentions.", 
        "1130": "For example, in Figure 2, without identifying the mention Independence Day as the holiday or the film, various dissimilar context words such as the words celebrations and film in documents d1, d2 will share similar semantics, which will further affect the performance of entity representations during joint training.", 
        "1131": "Given the annotated corpus D\u2032, we use a word wi or a mention sense s\u2217j to predict the context words by maximizing the following objective function:\nLw = \u2211\nwi,ml\u2208D\u2032 logP (C(wi)|wi)\n+ logP (C(ml)|s\u2217j ) (6)\nwhere s\u2217j = g(< ml, ej >) is obtained from anchors in Wikipedia articles.", 
        "1132": "Thus, words and mention senses will share the same vector space, where similar words and mention senses are close to each other, such as celebrations and Independence Day (US) because they frequently occur in the same contexts.", 
        "1133": "Similar to WDS, we maintain a context cluster for each mention sense, which can be used for mention sense disambiguation (Section 5).", 
        "1134": "The context cluster of a mention sense s\u2217j contains all the context vectors of its mentionml.", 
        "1135": "We compute context vector of ml by averaging the sum of its context word embeddings: 1|C(ml)| \u2211 wj\u2208C(ml)wj.", 
        "1136": "Further, the center of a context cluster \u00b5\u2217j is defined as the average of context vectors of all mentions which refer to the sense.", 
        "1137": "These context clusters will be later used to disambiguate the sense of a given mention with its contexts.", 
        "1138": "4.5 Joint Training Considering all of the above representation learning components, we define the overall objective function as linear combinations:\nL = Lw + Le + Lm (7) The goal of training MPME is to maximize the\nabove function, and iteratively update three types of embeddings.", 
        "1139": "Also, we use negative sampling technique for efficiency (Mikolov et al., 2013a).", 
        "1140": "MPME shares the same entity representation learning method with (Yamada et al., 2016), but\nthe role of entities in the entire framework as well as mention representation learning is different in three aspects.", 
        "1141": "First, we focus on learning embeddings for mentions, not merely words as in (Yamada et al., 2016).", 
        "1142": "Clearly, MPME is more natural to integrate text and knowledge base.", 
        "1143": "Second, we propose to learn multiple embeddings for each mention denoting its different meanings.", 
        "1144": "Third, we prefer to use both mentions and context words to predict entities, so that the distribution of entities will help improve word embeddings, meanwhile, avoid being hurt if we force entity embeddings to satisfy word embeddings during training (Wang et al., 2014).", 
        "1145": "We will give more analysis in experiments.", 
        "1146": "5 Mention Sense Disambiguation\nAs mentioned in Section 3, we induce a correct sense s\u0302\u2217j \u2208 M\u2217l for each mention ml in an annotated document D\u2032.", 
        "1147": "We regard this problem from the perspective of language model that maximizes a joint probability of all mention senses contained in the document.", 
        "1148": "However, the global optimum is expensive with a time complexity of O(|M||M\u2217l |).", 
        "1149": "Thus, we approximately identify each mention sense independently:\nP (D\u2032, .", 
        "1150": ".", 
        "1151": ".", 
        "1152": ", s\u2217j , .", 
        "1153": ".", 
        "1154": ".", 
        "1155": ", ) \u2248 \u220f\nP (D\u2032|s\u2217j ) \u00b7 P (s\u2217j )\n\u2248 \u220f P (C(ml)|s\u2217j ) \u00b7 P (N\u0302 (ml)|s\u2217j ) \u00b7 P (s\u2217j ) (8)\nwhere P (C(ml)|s\u2217j ), local context information (Section 3), denotes the probability of the local contexts of ml given its mention sense s\u2217j .", 
        "1156": "we define it proportional to the cosine similarity between the current context vector and the sense context cluster center \u00b5\u2217j as described in Section 4.4.", 
        "1157": "It measures how likely a mention sense occurring together with current context words.", 
        "1158": "For example, given the mention sense Independence Day (film), word film is more likely to appear within the context than the word celebrations.", 
        "1159": "P (N\u0302 (ml)|slj), global mention information, denotes the probability of the contextual mentions of ml given its sense slj , where N\u0302 (ml) is the collection of the neighbor mentions occurring together with ml in a predefined context window.", 
        "1160": "We define it proportional to the cosine similarity between mention sense embeddings and the neighbor mention vector, which is computed similar to\ncontext vector: \u2211 1 |N\u0302 (ml)| s\u0302lj, where s\u0302 l j is the correct sense for ml.", 
        "1161": "Considering there are usually multiple mentions\nin a document to be disambiguated.", 
        "1162": "The mentions disambiguated first will be helpful for inducing the senses of the rest mentions.", 
        "1163": "That is, how to choose the mentions disambiguated first will influence the performance.", 
        "1164": "Intuitively, we adopt two orders similar to (Chen et al., 2014): 1) L2R (left to right) induces senses for all the mentions in the document following natural order that varies according to language, normally from left to right in the sequence.", 
        "1165": "2) S2C (simple to complex) denotes that we determine the correct sense for those mentions with fewer senses, which makes the problem easier.", 
        "1166": "Global mention information assumes that there should be consistent semantics in a context window, and measures whether all neighbor mentions are related.", 
        "1167": "For instance, two mentions Memorial Day and Independence Day occur in the same document.", 
        "1168": "If we already know that Memorial Day denotes a holiday, then obviously Independence Day has higher probability of being a holiday than a film.", 
        "1169": "P (s\u2217j ), sense prior, is a prior probability of sense s\u2217j indicating how possible it occurs without considering any additional information.", 
        "1170": "We define it proportional to the frequency of sense s\u2217j in Wikipedia anchors:\nP (s\u2217j ) = ( |As\u2217j | |A| ) \u03b3 \u03b3 \u2208 [0, 1]\nwhere As\u2217j is the set of anchors annotated with s\u2217j , and \u03b3 is a smoothing hyper-parameter to control the impact of prior on the overall probability, which is set by experiments (Section 6.4).", 
        "1171": "6 Experiment\nSetup We choose Wikipedia, the March 2016 dump, as training corpus, which contains nearly 75 millions of anchors, 180 millions of edges among entities and 1.8 billions of tokens after preprocessing.", 
        "1172": "We then train MPME2 for 1.5 millions of words, 5 millions of entities and 1.7 millions of mentions.", 
        "1173": "The entire training process in 10 iterations costs nearly 8 hours on the server with 64 core CPU and 188GB memory.", 
        "1174": "2Our main code for MPME can be found in https://github.com/TaoMiner/bridgeGap.", 
        "1175": "We use the default settings in word2vec3, and set our embedding dimension as 200 and context window size as 5.", 
        "1176": "For each positive example, we sample 5 negative examples4.", 
        "1177": "Baseline Methods As far as we know, this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison.", 
        "1178": "We use the method in (Yamada et al., 2016) as a baseline, marked as ALIGN5, because (1) this is the most similar work that directly aligns word and entity embeddings.", 
        "1179": "(2) it achieves the state-of-the-art performance in entity linking task.", 
        "1180": "To investigate the effect of multi-prototype, we degrade our method to single-prototype as another baseline, which means to use one sense to represent all mentions with the same phrase, namely Single-Prototype Mention Embedding (SPME).", 
        "1181": "For example, SPME only learns one unique sense vector for Independence Day whatever it denotes a holiday or a film.", 
        "1182": "6.1 Qualitative Analysis\nWe use cosine similarity to measure the similarity of two vectors, and present the top 5 nearest words and entities for two most popular senses of the mention Independence Day.", 
        "1183": "Because ALIGN is incapable of dealing with multiple words, we only present the results of SPME and MPME.", 
        "1184": "As shown in Figure 1, without considering mention sense, the mention Independence Day can only show a dominant holiday sense based on SPME and ignore all other senses.", 
        "1185": "Instead, MPME successfully learns two clear and distinct senses.", 
        "1186": "For the sense Independence Day (US), all of its nearest words and entities, such as parades, celebrations, and Memorial Day, are holiday related, while for another sense Independence Day (film), its nearest words and entities, like robocop and The Terminator, are all science fiction films.", 
        "1187": "The results verify the effectiveness of our framework in learning mention embeddings at the sense level.", 
        "1188": "3https://code.google.com/archive/p/word2vec/ 4We tested different parameters (e.g.", 
        "1189": "window size of 10 and dimension of 500) which achieve similar results, and report the current settings considering program runtime efficiency.", 
        "1190": "5We carefully re-implemented ALIGN and used the same shared parameters in our model for fairly comparison.", 
        "1191": "However, we failed to fully reproduce the positive result in the original paper, meanwhile the authors are unable to release their code.", 
        "1192": "6.2 Entity Relatedness\nTo evaluate the quality of entity embeddings, we conduct experiments using the dataset which is designed for measuring entity relatedness (Ceccarelli et al., 2013; Huang et al., 2015; Yamada et al., 2016).", 
        "1193": "The dataset contains 3,314 entities, and each mention has 91 candidate entities on average with gold-standard labels indicating whether they are semantically related.", 
        "1194": "We compute cosine similarity between entity embeddings to measure their relatedness, and rank them in a descending order.", 
        "1195": "To evaluate the ranking quality, we use two standard metrics: normalized discounted cumulative gain (NDCG) (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002) and mean average precision (MAP) (Schu\u0308tze, 2008).", 
        "1196": "We design another baseline method: Entity2vec, which learns entity embeddings using the method described in Section 4.2, without joint training with word and mention sense embeddings.", 
        "1197": "As shown in Table 2, ALIGN achieves lower performance than Entity2vec, because it doesn\u2019t consider the mention phrase ambiguity and yields lots of noise when forcing entity embeddings to satisfy word embeddings and aligning them into the unified space.", 
        "1198": "For example, the entity Gente (magazine) should be more relevant to the entity France, the place where its company locates.", 
        "1199": "However, ALIGN mixed various meanings of mention Gente (e.g., the song) and ranked some bands higher (e.g., entity Poolside (band)).", 
        "1200": "SPME also doesn\u2019t consider the ambiguity of\nmentions but achieves comparative results with Entity2vec.", 
        "1201": "We analyze the reasons and find that, it can avoid some noise by using word embeddings to predict entities.", 
        "1202": "MPME outperforms all the other methods, which demonstrates that the unambiguous textual information is helpful to refine the entity embeddings.", 
        "1203": "6.3 Word Analogical Reasoning\nFollowing (Mikolov et al., 2013a; Wang et al., 2014), we use the word analogical reasoning task to evaluate the quality of word embeddings.", 
        "1204": "The dataset consists of 8,869 semantic questions (\u201cParis\u201d:\u201cFrance\u201d::\u201cRome\u201d:?", 
        "1205": "), and 10,675 syntactic questions (e.g., \u201csit\u201d:\u201csitting\u201d::\u201cwalk\u201d:?).", 
        "1206": "We solve it by finding the closest word vector w?", 
        "1207": "to wFrance\u2212wParis+wRome according to cosine similarity.", 
        "1208": "We compute accuracy for top 1 nearest word to measure the performance.", 
        "1209": "We also adopt Word2vec6 as an additional baseline method, which provides a standard to measure the impact from other components on word embeddings.", 
        "1210": "Table 3 shows the results.", 
        "1211": "We can see that ALIGN, SPME and MPME, achieve higher performance in dealing with semantic questions, because relations among entities (e.g., country-capital relation for entity France and Paris) enhance the semantics in word embeddings through jointly training.", 
        "1212": "On the other hand, their performance for syntactic questions is weakened because more accurate semantics yields a bias to predict semantic relations even though given a syntactic query.", 
        "1213": "For example, given the query \u201cpleasant\u201d:\u201cunpleasant\u201d::\u201cpossibly\u201d:?, our\n6https://code.google.com/archive/p/word2vec/\nmodel tends to return the word (e.g., probably) highly semantical related to query words, such as possibly, instead of the syntactical similar word impossibly.", 
        "1214": "In this scenario, we are more concerned about semantic task to incorporate knowledge of reference entities into word embeddings, and this issue could be tackled, to some extent, by using syntactic tool like stemming.", 
        "1215": "The word embeddings of MPME achieve the best performance for semantic questions mainly because (1) text representation learning has better generalization ability due to the larger size of training examples than entities (e.g., 1.8b v.s.", 
        "1216": "0.18b) as well as relatively smaller size of vocabulary (e.g., 1.5m v.s.", 
        "1217": "5m).", 
        "1218": "(2) unambiguous mention embeddings capture both textual context information and knowledge, and thus enhance word and entity embeddings.", 
        "1219": "6.4 A Case Study: Entity Linking\nEntity linking is a core NLP task of identifying the reference entity for mentions in texts.", 
        "1220": "The main difficulty lies in the ambiguity of various entities sharing the same mention phrase.", 
        "1221": "Previous work addressed this issue by taking advantage of the similarity between words and entities (FrancisLandau et al., 2016; Sun et al., 2015), and/or the relations among entities (Thien Huu Nguyen, 2016; Cao et al., 2015).", 
        "1222": "Therefore, we use entity linking as a case study for a comprehensive measurement of the multi-prototype mention embeddings.", 
        "1223": "Given mentions in a text, entity linking aims to link them to a predefined knowledge base.", 
        "1224": "One of the main challenges in this task is the ambiguity of entity mentions.", 
        "1225": "We use the public dataset AIDA created by (Hoffart et al., 2011), which includes 1,393 documents and 27,816 mentions referring to Wikipedia entries.", 
        "1226": "The dataset has been divided into 946, 216 and 231 documents for the purpose of training, developing and testing.", 
        "1227": "Following (Pershina et al., 2015; Yamada et al., 2016), we use a publicly available dictionary to generate candidate entities and mention senses.", 
        "1228": "For evaluation, we rank the candidate entities for each mention and report both standard micro (aggregates over all mentions) and macro (aggregates over all documents) precision over top-ranked entities.", 
        "1229": "Supervised Entity Linking Yamada et al.", 
        "1230": "(2016) designed a list of fea-\ntures for each mention and candidate entity pair.", 
        "1231": "By incorporating these features into a supervised learning-to-rank algorithm, Gradient Boosting Regression Tree (GBRT), each pair is assigned a relevance score indicating whether they should be linked to each other.", 
        "1232": "Following their recommended parameters, we set the number of trees as 10,000, the learning rate as 0.02 and the maximum depth of the decision tree as 4.", 
        "1233": "Based on word and entity embeddings learned by ALIGN, the key features in (Yamada et al., 2016) are from two aspects: (1) the cosine similarity between context words and candidate entity, and (2) the coherence among \u201ccontextual\u201d entities in the same document.", 
        "1234": "To evaluate the performance of multi-prototype mention embeddings, we incorporate the following features into GBDT for comparison: (1) the cosine similarity between the current context vector and the sense context cluster center \u00b5\u2217j , which denotes how likely the mention sense refers to the candidate entity, (2) the cosine similarity between the current context vector and the mention sense embeddings.", 
        "1235": "As shown in Table 4, we can see that ALIGN performs better than SPME.", 
        "1236": "This is because SPME learns word embeddings and entity embeddings in separate semantic spaces, and fails to measure the similarity between context words and candidate entities.", 
        "1237": "However, MPME computes the similarity between context words with mention sense instead of entities, thus achieves the best performance, which also demonstrates the high quality of the mention sense embeddings.", 
        "1238": "Unsupervised Entity Linking Linking a mention to a specific entity equals to\ndisambiguating mention senses since each candidate entity corresponds to a mention sense.", 
        "1239": "As described in Section 5, we disambiguate senses in two orders: (1) L2R (from left to right), and (2) S2C (from simple to complex).", 
        "1240": "We evaluate our unsupervised disambiguation methods on the entire AIDA dataset.", 
        "1241": "To be fair, we choose the state-of-the-art unsupervised methods, which are proposed in (Hoffart et al., 2011; Alhelbawy and Gaizauskas, 2014; Cucerzan, 2007;\nTable 5: Performance of Unsupervised Methods\nCucerzan Kulkarni Hoffart Shirakawa Alhelbawy MPME (L2R) MPME (S2C)\nMicro P@1 0.510 0.729 0.818 0.823 0.842 0.882 0.885 Macro P@1 0.437 0.767 0.819 0.830 0.875 0.875 0.890\nKulkarni et al., 2009; Masumi Shirakawa and Nishio, 2011) using the same dataset.", 
        "1242": "Table 5 shows the results.", 
        "1243": "We can see that our two methods outperform all other methods.", 
        "1244": "MPME (L2R) is more efficient and easy to apply, while MPME (S2C) slightly outperforms it because the additional step of ranking mentions according to their candidates number guarantees a higher disambiguation performance for those simple mentions, which consequently help disambiguate those complex mentions through global mention information in Equation 8.", 
        "1245": "We analyze the results and observe a disambiguation bias to popular senses.", 
        "1246": "For example, there are three mentions in the sentence \u201cJapan began the defence of their Asian Cup I title with a lucky 2-1 win against Syria in a Group C championship match on Friday\u201d, where the country name Japan and Syria actually denote their national football teams, while the football match name Asian Cup I has little ambiguity.", 
        "1247": "Compared to the team, the sense of country occurs more frequently and has a dominant prior, which greatly affects the disambiguation.", 
        "1248": "By incorporating local context information and global mention information, both the context words (e.g., defence or match) and the neighbor mentions (e.g., Asian Cup I) provide us enough clues to identify a soccer related mention sense instead of the country.", 
        "1249": "Influence of Smoothing Parameter As mentioned above, a mention sense may possess a dominant prior and greatly affect the disambiguation.", 
        "1250": "So we introduce a smoothing parameter \u03b3 to control its importance to the overall probability.", 
        "1251": "Figure 3 shows the linking accuracy under different values of \u03b3 on the dataset of AIDA.", 
        "1252": "\u03b3 = 0 indicates we don\u2019t use any prior knowledge, and \u03b3 = 1 indicates the case without smoothing parameter.", 
        "1253": "We can see that both micro and macro accuracy decrease a lot if we don\u2019t use the parameter (\u03b3 = 1).", 
        "1254": "Only using local and global probabilities for disambiguation (\u03b3 = 0) achieves a comparable performance when \u03b3 = 0.05, both accuracy reach their peaks, which is optimal and default value in our experiments.", 
        "1255": "7 Conclusions and Future Work\nIn this paper, we propose a novel Multi-Prototype Mention Embedding model that jointly learns word, entity and mention sense embeddings.", 
        "1256": "These mention senses capture both textual context information and knowledge from reference entities, and provide an efficient approach to disambiguate mention sense in text.", 
        "1257": "We conduct a series of experiments to demonstrate that multiprototype mention embedding improves the quality of both word and entity representations.", 
        "1258": "Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve the state-of-the-art.", 
        "1259": "In the future, we will improve the scalability of our model and learn multi-prototype embeddings for the mentions without reference entities in a knowledge base, and introduce compositional approaches to model the internal structures of multiword mentions.", 
        "1260": "8 Acknowledgement\nThis work is supported by NSFC Key Program (No.", 
        "1261": "61533018), 973 Program (No.", 
        "1262": "2014CB340504), Fund of Online Education Research Center, Ministry of Education (No.", 
        "1263": "2016ZD102), Key Technologies Research and Development Program of China (No.", 
        "1264": "2014BAK04B03), NSFC-NRF (No.", 
        "1265": "61661146007) and the U.S. DARPA LORELEI Program No.", 
        "1266": "HR0011-15-C-0115.", 
        "1267": "References Ayman Alhelbawy and Robert J Gaizauskas.", 
        "1268": "2014.", 
        "1269": "Graph ranking for collective named entity disambiguation.", 
        "1270": "In ACL (2).", 
        "1271": "pages 75\u201380.", 
        "1272": "Yoshua Bengio, Aaron Courville, and Pascal Vincent.", 
        "1273": "2013.", 
        "1274": "Representation learning: A review and new perspectives.", 
        "1275": "IEEE transactions on pattern analysis and machine intelligence 35(8):1798\u20131828.", 
        "1276": "Yixin Cao, Juanzi Li, Xiaofei Guo, Shuanhu Bai, Heng Ji, and Jie Tang.", 
        "1277": "2015.", 
        "1278": "Name list only?", 
        "1279": "target entity disambiguation in short texts.", 
        "1280": "In EMNLP.", 
        "1281": "pages 654\u2013664.", 
        "1282": "https://doi.org/10.18653/v1/D15-1077.", 
        "1283": "Diego Ceccarelli, Claudio Lucchese, Salvatore Orlando, Raffaele Perego, and Salvatore Trani.", 
        "1284": "2013.", 
        "1285": "Learning relatedness measures for entity linking.", 
        "1286": "In Proceedings of the 22nd ACM international conference on Information & Knowledge Management.", 
        "1287": "ACM, pages 139\u2013148.", 
        "1288": "Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.", 
        "1289": "2014.", 
        "1290": "A unified model for word sense representation and disambiguation.", 
        "1291": "In EMNLP.", 
        "1292": "Citeseer, pages 1025\u2013 1035. https://doi.org/10.3115/v1/D14-1110.", 
        "1293": "Silviu Cucerzan.", 
        "1294": "2007.", 
        "1295": "Large-scale named entity disambiguation based on wikipedia data .", 
        "1296": "Jenny Rose Finkel, Trond Grenager, and Christopher Manning.", 
        "1297": "2005.", 
        "1298": "Incorporating non-local information into information extraction systems by gibbs sampling.", 
        "1299": "In Proceedings of the 43rd annual meeting on association for computational linguistics.", 
        "1300": "Association for Computational Linguistics, pages 363\u2013 370.", 
        "1301": "Matthew Francis-Landau, Greg Durrett, and Dan Klein.", 
        "1302": "2016.", 
        "1303": "Capturing semantic similarity for entity linking with convolutional neural networks.", 
        "1304": "In Proceedings of NAACL-HLT .", 
        "1305": "pages 1256\u20131261.", 
        "1306": "https://doi.org/10.18653/v1/N16-1150.", 
        "1307": "Xu Han, Zhiyuan Liu, and Maosong Sun.", 
        "1308": "2016.", 
        "1309": "Joint representation learning of text and knowledge for knowledge graph completion.", 
        "1310": "CoRR abs/1611.04125.", 
        "1311": "Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen Fu\u0308rstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum.", 
        "1312": "2011.", 
        "1313": "Robust disambiguation of named entities in text.", 
        "1314": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing.", 
        "1315": "Association for Computational Linguistics, pages 782\u2013792.", 
        "1316": "Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng.", 
        "1317": "2012.", 
        "1318": "Improving word representations via global context and multiple word prototypes.", 
        "1319": "In Proc.", 
        "1320": "ACL.", 
        "1321": "Hongzhao Huang, Larry Heck, and Heng Ji.", 
        "1322": "2015.", 
        "1323": "Leveraging deep neural networks and knowledge\ngraphs for entity disambiguation.", 
        "1324": "arXiv preprint arXiv:1504.07678 .", 
        "1325": "Lifu Huang, Jonathan May, Xiaoman Pan, Heng Ji, Xiang Ren, Jiawei Han, Lin Zhao, and James A Hendler.", 
        "1326": "2017.", 
        "1327": "Liberal entity extraction: Rapid construction of fine-grained entity typing systems.", 
        "1328": "Big Data 5(1):19\u201331.", 
        "1329": "Kalervo Ja\u0308rvelin and Jaana Keka\u0308la\u0308inen.", 
        "1330": "2002.", 
        "1331": "Cumulated gain-based evaluation of ir techniques.", 
        "1332": "ACM Transactions on Information Systems (TOIS) 20(4):422\u2013446.", 
        "1333": "Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti.", 
        "1334": "2009.", 
        "1335": "Collective annotation of wikipedia entities in web text.", 
        "1336": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining.", 
        "1337": "ACM, pages 457\u2013466.", 
        "1338": "Jiwei Li and Dan Jurafsky.", 
        "1339": "2015.", 
        "1340": "Do multi-sense embeddings improve natural language understanding?", 
        "1341": "In Proc.", 
        "1342": "EMNLP.", 
        "1343": "https://doi.org/10.18653/v1/D151200.", 
        "1344": "Massimiliano Mancini, Jose\u0301 Camacho-Collados, Ignacio Iacobacci, and Roberto Navigli.", 
        "1345": "2016.", 
        "1346": "Embedding words and senses together via joint knowledgeenhanced training.", 
        "1347": "CoRR abs/1612.02703.", 
        "1348": "Haixun Wang Yangqiu Song Zhongyuan Wang Kotaro Nakayama Takahiro Hara Masumi Shirakawa and Shojiro Nishio.", 
        "1349": "2011.", 
        "1350": "Entity disambiguation based on a. technical report.", 
        "1351": "In Technical Report MSR-TR2011-125.", 
        "1352": "Microsoft Research.", 
        "1353": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.", 
        "1354": "2013a.", 
        "1355": "Efficient estimation of word representations in vector space.", 
        "1356": "CoRR abs/1301.3781.", 
        "1357": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.", 
        "1358": "2013b.", 
        "1359": "Distributed representations of words and phrases and their compositionality.", 
        "1360": "In NIPS.", 
        "1361": "pages 3111\u20133119.", 
        "1362": "Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum.", 
        "1363": "2014.", 
        "1364": "Efficient non-parametric estimation of multiple embeddings per word in vector space.", 
        "1365": "In Proc.", 
        "1366": "EMNLP.", 
        "1367": "https://doi.org/10.3115/v1/D14-1113.", 
        "1368": "Maria Pershina, Yifan He, and Ralph Grishman.", 
        "1369": "2015.", 
        "1370": "Personalized page rank for named entity disambiguation.", 
        "1371": "In HLT-NAACL.", 
        "1372": "pages 238\u2013243.", 
        "1373": "Joseph Reisinger and Raymond J Mooney.", 
        "1374": "2010.", 
        "1375": "Multi-prototype vector-space models of word meaning.", 
        "1376": "In Proc.", 
        "1377": "NAACL.", 
        "1378": "Hinrich Schu\u0308tze.", 
        "1379": "2008.", 
        "1380": "Introduction to information retrieval.", 
        "1381": "In Proceedings of the international communication of association for computing machinery conference.", 
        "1382": "Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang.", 
        "1383": "2015.", 
        "1384": "Modeling mention, context and entity with neural networks for entity disambiguation.", 
        "1385": "In IJCAI.", 
        "1386": "pages 1333\u20131339.", 
        "1387": "Nicolas Fauceglia Mariano Rodriguez-Muro Oktie Hassanzadeh Alfio Massimiliano Gliozzo Mohammad Sadoghi Thien Huu Nguyen.", 
        "1388": "2016.", 
        "1389": "Joint learning of local and global features for entity linking via neural networks.", 
        "1390": "In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan.", 
        "1391": "pages 2310\u2013 2320.", 
        "1392": "Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu.", 
        "1393": "2014.", 
        "1394": "A probabilistic model for learning multi-prototype word embeddings.", 
        "1395": "In COLING.", 
        "1396": "Kristina Toutanova, Danqi Chen, Patrick Pantel, Pallavi Choudhury, and Michael Gamon.", 
        "1397": "2015.", 
        "1398": "Representing text for joint embedding of text and knowledge bases.", 
        "1399": "ACL Association for Computational Linguistics https://doi.org/10.18653/v1/D15-1174.", 
        "1400": "Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.", 
        "1401": "2014.", 
        "1402": "Knowledge graph and text jointly embedding.", 
        "1403": "In Proc.", 
        "1404": "EMNLP.", 
        "1405": "https://doi.org/10.3115/v1/D14-1167.", 
        "1406": "Zhigang Wang and Juan-Zi Li.", 
        "1407": "2016.", 
        "1408": "Text-enhanced representation learning for knowledge graph.", 
        "1409": "In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence.", 
        "1410": "Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier.", 
        "1411": "2013.", 
        "1412": "Connecting language and knowledge bases with embedding models for relation extraction.", 
        "1413": "In Proc.", 
        "1414": "ACL.", 
        "1415": "Jiawei Wu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun.", 
        "1416": "2016.", 
        "1417": "Knowledge representation via joint learning of sequential text and knowledge graphs.", 
        "1418": "CoRR .", 
        "1419": "Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji.", 
        "1420": "2016.", 
        "1421": "Joint learning of the embedding of words and entities for named entity disambiguation.", 
        "1422": "In Proc.", 
        "1423": "CoNLL.", 
        "1424": "https://doi.org/10.18653/v1/K16-1025.", 
        "1425": "Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-Seng Chua.", 
        "1426": "2017.", 
        "1427": "Visual translation embedding network for visual relation detection.", 
        "1428": "arXiv preprint arXiv:1702.08319 .", 
        "1429": "Hanwang Zhang, Xindi Shang, Wenzhuo Yang, Huan Xu, Huanbo Luan, and Tat-Seng Chua.", 
        "1430": "2016.", 
        "1431": "Online collaborative learning for open-vocabulary visual classifiers.", 
        "1432": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", 
        "1433": "pages 2809\u20132817."
    }, 
    "document_id": "P17-1149.pdf.json"
}
