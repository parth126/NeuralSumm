{
    "abstract_sentences": {
        "1": "This paper tackles the task of event detection (ED), which involves identifying and categorizing events.", 
        "2": "We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches.", 
        "3": "In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms.", 
        "4": "In specific, we systematically investigate the proposed model under the supervision of different attention strategies.", 
        "5": "Experimental results show that our approach advances state-ofthe-arts and achieves the best F1 score on ACE 2005 dataset."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1789\u20131798 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1164\nThis paper tackles the task of event detection (ED), which involves identifying and categorizing events.", 
        "2": "We argue that arguments provide significant clues to this task, but they are either completely ignored or exploited in an indirect manner in existing detection approaches.", 
        "3": "In this work, we propose to exploit argument information explicitly for ED via supervised attention mechanisms.", 
        "4": "In specific, we systematically investigate the proposed model under the supervision of different attention strategies.", 
        "5": "Experimental results show that our approach advances state-ofthe-arts and achieves the best F1 score on ACE 2005 dataset.", 
        "6": "1 Introduction  In the ACE (Automatic Context Extraction) event extraction program, an event is represented as a structure comprising an event trigger and a set of arguments.", 
        "7": "This work tackles event detection (ED) task, which is a crucial part of event extraction (EE) and focuses on identifying event triggers and categorizing them.", 
        "8": "For instance, in the sentence \u201cHe died in the hospital\u201d, an ED system is expected to detect a Die event along with the trigger word \u201cdied\u201d.", 
        "9": "Besides, the task of EE also includes event argument extraction (AE), which involves event argument identification and role classification.", 
        "10": "In the above sentence, the arguments of the event include \u201cHe\u201d(Role = Person) and \u201chospital\u201d(Role = Place).", 
        "11": "However, this paper does not focus on AE and only tackles the former task.", 
        "12": "According to the above definitions, event arguments seem to be not essentially necessary to ED.", 
        "13": "However, we argue that they are capable of providing significant clues for identifying and categorizing events.", 
        "14": "They are especially useful for ambiguous trigger words.", 
        "15": "For example, consider a sentence in ACE 2005 dataset:\nMohamad fired Anwar, his former protege, in 1998.", 
        "16": "In this sentence, \u201cfired\u201d is the trigger word and the other bold words are event arguments.", 
        "17": "The correct type of the event triggered by \u201cfired\u201d in this case is End-Position .", 
        "18": "However, it might be easily misidentified as Attack because \u201cfired\u201d is a multivocal word.", 
        "19": "In this case, if we consider the phrase \u201cformer protege\u201d, which serves as an argument (Role = Position) of the target event, we would have more confidence in predicting it as an End-Position event.", 
        "20": "Unfortunately, most existing methods performed event detection individually, where the annotated arguments in training set are totally ignored (Ji and Grishman, 2008; Gupta and Ji, 2009; Hong et al., 2011; Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a,b; Nguyen and Grishman, 2016).", 
        "21": "Although some joint learning based methods have been proposed, which tackled event detection and argument extraction simultaneously (Riedel et al., 2009; Li et al., 2013; Venugopal et al., 2014; Nguyen et al., 2016), these approaches usually only make remarkable improvements to AE, but insignificant to ED.", 
        "22": "Table 1 illustrates our observations.", 
        "23": "Li et al.", 
        "24": "(2013) and Nguyen et al.", 
        "25": "(2016) are state-of-the-art joint models in symbolic and embedding methods for event extraction, respectively.", 
        "26": "Compared with state-of-the-art pipeline systems, both join-\n1789\nt methods achieved remarkable improvements on AE (over 1.9 points), whereas achieved insignificant improvements on ED (less than 0.2 points).", 
        "27": "The symbolic joint method even performed worse (67.5 vs. 68.3) than pipeline system on ED.", 
        "28": "We believe that this phenomenon may be caused by the following two reasons.", 
        "29": "On the one hand, since joint methods simultaneously solve ED and AE, methods following this paradigm usually combine the loss functions of these two tasks and are jointly trained under the supervision of annotated triggers and arguments.", 
        "30": "However, training corpus contains much more annotated arguments than triggers (about 9800 arguments and 5300 triggers in ACE 2005 dataset) because each trigger may be along with multiple event arguments.", 
        "31": "Thus, the unbalanced data may cause joint models to favor AE task.", 
        "32": "On the other hand, in implementation, joint models usually pre-predict several potential triggers and arguments first and then make global inference to select correct items.", 
        "33": "When pre-predicting potential triggers, almost all existing approaches do not leverage any argument information.", 
        "34": "In this way, ED does hardly benefit from the annotated arguments.", 
        "35": "By contrast, the component for pre-prediction of arguments always exploits the extracted trigger information.", 
        "36": "Thus, we argue that annotated arguments are actually used for AE, not for ED in existing joint methods, which is also the reason we call it an indirect way to use arguments for ED.", 
        "37": "Contrast to joint methods, this paper proposes to exploit argument information explicitly for ED.", 
        "38": "We have analyzed that arguments are capable of providing significant clues to ED, which gives us an enlightenment that ar-\nguments should be focused on when performing this task.", 
        "39": "Therefore, we propose a neural network based approach to detect events in texts.", 
        "40": "And in the proposed approach, we adopt a supervised attention mechanism to achieve this goal, where argument words are expected to acquire more attention than other words.", 
        "41": "The attention value of each word in a given sentence is calculated by an operation between the current word and the target trigger candidate.", 
        "42": "Specifically, in training procedure, we first construct gold attentions for each trigger candidate based on annotated arguments.", 
        "43": "Then, treating gold attentions as the supervision to train the attention mechanism, we learn attention and event detector jointly both in supervised manner.", 
        "44": "In testing procedure, we use the ED model with learned attention mechanisms to detect events.", 
        "45": "In the experiment section, we systematically conduct comparisons on a widely used benchmark dataset ACE20051.", 
        "46": "In order to further demonstrate the effectiveness of our approach, we also use events from FrameNet (FN) (F. Baker et al., 1998) as extra training data, as the same as Liu et al.", 
        "47": "(2016a) to alleviate the data-sparseness problem for ED to augment the performance of the proposed approach.", 
        "48": "The experimental results demonstrate that the proposed approach is effective for ED task, and it outperforms state-of-the-art approaches with remarkable gains.", 
        "49": "To sum up, our main contributions are: (1) we analyze the problem of joint models on the task of ED, and propose to use the annotated argument information explicitly for this task.", 
        "50": "(2) to achieve this goal, we introduce a supervised attention based ED model.", 
        "51": "Furthermore, we systematically investigate different attention strategies for the proposed model.", 
        "52": "(3) we improve the performance of ED and achieve the best performance on the widely used benchmark dataset ACE 2005.", 
        "53": "2 Task Description  The ED task is a subtask of ACE event evaluations where an event is defined as a specific occurrence involving one or more participants.", 
        "54": "Event extraction task requires certain specified types of events, which are mentioned\n1https://catalog.ldc.upenn.edu/LDC2006T06\nin the source language data, be detected.", 
        "55": "We firstly introduce some ACE terminologies to facilitate the understanding of this task:\nEntity: an object or a set of objects in one of the semantic categories of interests.", 
        "56": "Entity mention: a reference to an entity (typically, a noun phrase).", 
        "57": "Event trigger: the main word that most clearly expresses an event occurrence.", 
        "58": "Event arguments: the mentions that are involved in an event (participants).", 
        "59": "Event mention: a phrase or sentence within which an event is described, including the trigger and arguments.", 
        "60": "The goal of ED is to identify event triggers and categorize their event types.", 
        "61": "For instance, in the sentence \u201cHe died in the hospital\u201d, an ED system is expected to detect a Die event along with the trigger word \u201cdied\u201d.", 
        "62": "The detection of event arguments \u201cHe\u201d(Role = Person) and \u201chospital\u201d(Role = Place) is not involved in the ED task.", 
        "63": "The 2005 ACE evaluation included 8 super types of events, with 33 subtypes.", 
        "64": "Following previous work, we treat these simply as 33 separate event types and ignore the hierarchical structure among them.", 
        "65": "3 The Proposed Approach  Similar to existing work, we model ED as a multi-class classification task.", 
        "66": "In detail, given a sentence, we treat every token in that sentence as a trigger candidate, and our goal is to classify each of these candidates into one of 34 classes (33 event types plus an NA class).", 
        "67": "In our approach, every word along with its context, which includes the contextual words and entities, constitute an event trigger candidate.", 
        "68": "Figure 1 describes the architecture of the proposed approach, which involves two components: (i) Context Representation Learning (CRL), which reveals the representation of both contextual words and entities via attention mechanisms; (ii) Event Detector (ED), which assigns an event type (including the NA type) to each candidate based on the learned contextual representations.", 
        "69": "3.1 Context Representation Learning  In order to prepare for Context Representation Learning (CRL), we limit the context to a fixed length by trimming longer sen-\ntences and padding shorter sentences with a special token when necessary.", 
        "70": "Let n be the fixed length and w0 be the current candidate trigger word, then its contextual words Cw is [w\u2212 n\n2 , w\u2212 n 2 +1, ..., w\u22121, w1, ..., wn 2 \u22121, wn 2 ]2, and its contextual entities, which is the corresponding entity types (including an NA type) of Cw, is [e\u2212 n\n2 , e\u2212 n 2 +1, ..., e\u22121, e1, ..., en 2 \u22121, en 2 ].", 
        "71": "For convenience, we use w to denote the current word, [w1, w2, ..., wn] to denote the contextual words Cw and [e1, e2, ..., en] to denote the contextual entities Ce in figure 1.", 
        "72": "Note that, both w, Cw and Ce mentioned above are originally in symbolic representation.", 
        "73": "Before entering CRL component, we transform them into real-valued vector by looking up word embedding table and entity type embedding table.", 
        "74": "Then we calculate attention vectors for both contextual words and entities by performing operations between the current word w and its contexts.", 
        "75": "Finally, the contextual words representation cw and contextual entities representation ce are formed by the weighted sum of the corresponding embeddings of each word and entity in Cw and Ce, respectively.", 
        "76": "We will give the details in the fol-\n2The current candidate trigger word w0 is not included in the context.", 
        "77": "lowing subsections.", 
        "78": "3.1.1 Word Embedding Table  Word embeddings learned from a large amount of unlabeled data have been shown to be able to capture the meaningful semantic regularities of words (Bengio et al., 2003; Erhan et al., 2010).", 
        "79": "This paper uses the learned word embeddings as the source of basic features.", 
        "80": "Specifically, we use the Skip-gram model (Mikolov et al., 2013) to learn word embeddings on the NYT corpus3.", 
        "81": "3.1.2 Entity Type Embedding Table  The ACE 2005 corpus annotated not only events but also entities for each given sentence.", 
        "82": "Following existing work (Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015), we exploit the annotated entity information in our ED system.", 
        "83": "We randomly initialize embedding vector for each entity type (including the NA type) and update it in training procedure.", 
        "84": "3.1.3 Representation Learning  In this subsection, we illustrate our proposed approach to learn representations of both contextual words and entities, which serve as inputs to the following event detector component.", 
        "85": "Recall that, we use the matrix Cw and Ce to denote contextual words and contextual entities, respectively.", 
        "86": "As illustrated in figure 1, the CRL component needs three inputs: the current candidate trigger word w, the contextual words Cw and the contextual entities Ce.", 
        "87": "Then, two attention vectors, which reflect different aspects of the context, are calculated in the next step.", 
        "88": "The contextual word attention vector \u03b1w is computed based on the current word w and its contextual words Cw.", 
        "89": "We firstly transform each word wk (including w and every word in Cw) into a hidden representation wk by the following equation:\nwk = f(wk Ww) (1)\nwhere f(\u00b7) is a non-linear function such as the hyperbolic tangent, andWw is the transformation matrix.", 
        "90": "Then, we use the hidden representations to compute the attention value for each\n3https://catalog.ldc.upenn.edu/LDC2008T19\nword in Cw:\n\u03b1kw = exp(w wTk )\u2211 i exp(w wTi )\n(2)\nThe contextual entity attention vector \u03b1e is calculated with a similar method to \u03b1w.", 
        "91": "\u03b1ke = exp(we eTk )\u2211 i exp(we eTi )\n(3)\nNote that, we do not use the entity information of the current candidate token to compute the attention vector.", 
        "92": "The reason is that only a small percentage of true event triggers are entities4.", 
        "93": "Therefore, the entity type of a candidate trigger is meaningless for ED.", 
        "94": "Instead, we use we, which is calculated by transforming w from the word space into the entity type space, as the attention source.", 
        "95": "We combine \u03b1w and \u03b1e to obtain the final attention vector, \u03b1 = \u03b1w+\u03b1e.", 
        "96": "Finally, the contextual words representation cw and the contextual entities representation ce are formed by weighted sum of Cw and Ce, respectively:\ncw = Cw\u03b1 T (4)\nce = Ce\u03b1 T (5)  3.2 Event Detector  As illustrated in figure 1, we employ a threelayer (an input layer, a hidden layer and a softmax output layer) Artificial Neural Networks (ANNs) (Hagan et al., 1996) to model the ED task, which has been demonstrated very effective for event detection by Liu et al.", 
        "97": "(2016a).", 
        "98": "3.2.1 Basic ED Model  Given a sentence, as illustrated in figure 1, we concatenate the embedding vectors of the context (including contextual words and entities) and the current candidate trigger to serve as the input to ED model.", 
        "99": "Then, for a given input sample x, ANN with parameter \u03b8 outputs a vector O, where the i-th value oi of O is the confident score for classifying x to the i-th event type.", 
        "100": "To obtain the conditional probability p(i|x, \u03b8), we apply a softmax operation over all event types:\np(i|x, \u03b8) = e oi\n\u2211m k=1 e ok (6)\n4Only 10% of triggers in ACE 2005 are entities.", 
        "101": "Given all of our (suppose T) training instances (x(i); y(i)), we can then define the negative loglikelihood loss function:\nJ(\u03b8) = \u2212 T\u2211\ni=1\nlog p(y(i)|x(i), \u03b8) (7)\nWe train the model by using a simple optimization technique called stochastic gradient descent (SGD) over shuffled mini-batches with the Adadelta rule (Zeiler, 2012).", 
        "102": "Regularization is implemented by a dropout (Kim, 2014; Hinton et al., 2012) and L2 norm.", 
        "103": "3.2.2 Supervised Attention  In this subsection, we introduce supervised attention to explicitly use annotated argument information to improve ED.", 
        "104": "Our basic idea is simple: argument words should acquire more attention than other words.", 
        "105": "To achieve this goal, we first construct vectors using annotated arguments as the gold attentions.", 
        "106": "Then, we employ them as supervision to train the attention mechanism.", 
        "107": "Constructing Gold Attention Vectors\nOur goal is to encourage argument words to obtain more attention than other words.", 
        "108": "To achieve this goal, we propose two strategies to construct gold attention vectors:\nS1: only pay attention to argument words.", 
        "109": "That is, all argument words in the given context obtain the same attention, whereas other words get no attention.", 
        "110": "For candidates without any annotated arguments in context (such as negative samples), we force all entities to average the whole attention.", 
        "111": "Figure 2 illustrates the details, where \u03b1\u2217 is the final gold attention vector.", 
        "112": "S2: pay attention to both arguments  and the words around them.", 
        "113": "The assumption is that, not only arguments are important\nto ED, the words around them are also helpful.", 
        "114": "And the nearer a word is to arguments, the more attention it should obtain.", 
        "115": "Inspired by Mi et al.", 
        "116": "(2016), we use a gaussian distribution g(\u00b7) to model the attention distribution of words around arguments.", 
        "117": "In detail, given an instance, we first obtain the raw attention vector \u03b1 in the same manner as S1 (see figure 2).", 
        "118": "Then, we create a new vector \u03b1 \u2032 with all points initialized with zero, and for each \u03b1i = 1, we update \u03b1 \u2032 by the following algorithm:\nAlgorithm 1: Updating \u03b1 \u2032\nfor k \u2208 {\u2212w, ..., 0, ..., w} do \u03b1\n\u2032 i+k = \u03b1 \u2032 i+k + g(|k|, \u00b5, \u03c3)\nend\nwhere w is the window size of the attention mechanism and \u00b5, \u03c3 are hyper-parameters of the gaussian distribution.", 
        "119": "Finally, we normalize \u03b1 \u2032 to obtain the target attention vector \u03b1\u2217.", 
        "120": "Similar with S1, we treat all entities in the context as arguments if the current candidate does not has any annotated arguments (such as netative samples).", 
        "121": "Jointly Training ED and Attention Given the gold attention \u03b1\u2217 (see subsection 3.2.2) and the machine attention \u03b1 produced by our model (see subsection 3.1.3), we employ the square error as the loss function of attentions:\nD(\u03b8) = T\u2211\ni=1\nn\u2211\nj=1\n(\u03b1\u2217ij \u2212 \u03b1ij)2 (8)\nCombining equation 7 and equation 8, we define the joint loss function of our proposed model as follows:\nJ \u2032 (\u03b8) = J(\u03b8) + \u03bbD(\u03b8) (9)\nwhere \u03bb is a hyper-parameter for trade-off between J and D. Similar to basic ED model, we minimize the loss function J \u2032 (\u03b8) by using SGD over shuffled mini-batches with the Adadelta update rule.", 
        "122": "4 Experiments    4.1 Dataset and Experimental Setup  Dataset\nWe conducted experiments on ACE 2005 dataset.", 
        "123": "For the purpose of comparison, we fol-\nlowed the evaluation of (Li et al., 2013; Chen et al., 2015; Liu et al., 2016b): randomly selected 30 articles from different genres as the development set, and subsequently conducted a blind test on a separate set of 40 ACE 2005 newswire documents.", 
        "124": "We used the remaining 529 articles as our training set.", 
        "125": "Hyper-parameter Setting\nHyper-parameters are tuned on the development dataset.", 
        "126": "We set the dimension of word embeddings to 200, the dimension of entity type embeddings to 50, the size of hidden layer to 300, the output size of word transformation matrix Ww in equation 1 to 200, the batch size to 100, the hyper-parameter for the L2 norm to 10\n\u22126 and the dropout rate to 0.6.", 
        "127": "In addition, we use the standard normal distribution to model attention distributions of words around arguments, which means that \u00b5 = 0.0, \u03c3 = 1.0, and the window size is set to 3 (see Subsection 3.2.2).", 
        "128": "The hyper-parameter \u03bb in equation 9 is various for different attention strategies, we will give its setting in the next section.", 
        "129": "4.2 Correctness of Our Assumption  In this section, we conduct experiments on ACE 2005 corpus to demonstrate the correctness of our assumption that argument information is crucial to ED.", 
        "130": "To achieve this goal, we design a series of systems for comparison.", 
        "131": "ANN is the basic event detection model, in which the hyper-parameter \u03bb is set to 0.", 
        "132": "This system does not employ argument information and computes attentions without supervision (see Subsection 3.1.3).", 
        "133": "ANN-ENT assigns \u03bb with 0, too.", 
        "134": "The difference is that it constructs the attention vector \u03b1 by forcing all entities in the context to average the attention instead of computing it in the manner introduced in Subsection 3.1.3.", 
        "135": "Since all arguments are entities, this system is designed to investigate the effects of entities.", 
        "136": "ANN-Gold1 uses the gold attentions constructed by strategy S1 in both training and testing procedure.", 
        "137": "ANN-Gold2 is akin to ANN-Gold1, but uses the second strategy to construct its gold attentions.", 
        "138": "Note that, in order to avoid the interference of attention mechanisms, the last two systems\nare designed to use argument information (via gold attentions) in both training and testing procedure.", 
        "139": "Thus both ANN-Gold1 and ANNGold2 assign \u03bb with 0.", 
        "140": "Table 2 compares these systems on ACE 2005 corpus.", 
        "141": "From the table, we observe that systems with argument information (the last two systems) significantly outperform systems without argument information (the first two systems), which demonstrates that argument information is very useful for this task.", 
        "142": "Moreover, since all arguments are entities, for preciseness we also investigate that whether ANN-Gold1/2 on earth benefits from entities or arguments.", 
        "143": "Compared with ANN-ENT (revising that this system only uses entity information), ANN-Gold1/2 performs much better, which illustrates that entity information is not enough and further demonstrates that argument information is necessary for ED.", 
        "144": "4.3 Results on ACE 2005 Corpus  In this section, we conduct experiments on ACE 2005 corpus to demonstrate the effectiveness of the proposed approach.", 
        "145": "Firstly, we introduce systems implemented in this work.", 
        "146": "ANN-S1 uses gold attentions constructed by strategy S1 as supervision to learn attention.", 
        "147": "In our experiments, \u03bb is set to 1.0.", 
        "148": "ANN-S2 is akin to ANN-S1, but use strategy S2 to construct gold attentions and the hyper-parameter \u03bb is set to 5.0.", 
        "149": "These two systems both employ supervised attention mechanisms.", 
        "150": "For comparison, we use an unsupervised-attention system ANN as our baseline, which is introduced in Subsection 4.2.", 
        "151": "In addition, we select the following state-ofthe-art methods for comparison.", 
        "152": "1).", 
        "153": "Li\u2019s joint model (Li et al., 2013) extracts events based on structure prediction.", 
        "154": "It is the best structure-based system.", 
        "155": "2).", 
        "156": "Liu\u2019s PSL (Liu et al., 2016b) employs both latent local and global information for event detection.", 
        "157": "It is the best-reported featurebased system.", 
        "158": "3).", 
        "159": "Liu\u2019s FN-Based approach (Liu et al., 2016a) leverages the annotated corpus of FrameNet to alleviate data sparseness problem of ED based on the observation that frames in FN are analogous to events in ACE.", 
        "160": "4).", 
        "161": "Ngyen\u2019s joint model (Nguyen et al., 2016) employs a bi-directional RNN to jointly extract event triggers and arguments.", 
        "162": "It is the best-reported representation-based joint approach proposed on this task.", 
        "163": "5).", 
        "164": "Skip-CNN (Nguyen and Grishman, 2016) introduces the non-consecutive convolution to capture non-consecutive k-grams for event detection.", 
        "165": "It is the best reported representation-based approach on this task.", 
        "166": "Table 3 presents the experimental results on ACE 2005 corpus.", 
        "167": "From the table, we make the following observations:\n1).", 
        "168": "ANN performs unexpectedly poorly, which indicates that unsupervised-attention mechanisms do not work well for ED.", 
        "169": "We believe the reason is that the training data of ACE 2005 corpus is insufficient to train a precise attention in an unsupervised manner, considering that data sparseness is an important issue of ED (Zhu et al., 2014; Liu et al., 2016a).", 
        "170": "2).", 
        "171": "With argument information employed via supervised attention mechanisms, both ANN-S1 and ANN-S2 outperform ANN with remarkable gains, which illustrates the effec-\ntiveness of the proposed approach.", 
        "172": "3).", 
        "173": "ANN-S2 outperforms ANN-S1, but the latter achieves higher precision.", 
        "174": "It is not difficult to understand.", 
        "175": "On the one hand, strategy S1 only focuses on argument words, which provides accurate information to identify event type, thus ANN-S1 could achieve higher precision.", 
        "176": "On the other hand, S2 focuses on both arguments and words around them, which provides more general but noised clues.", 
        "177": "Thus, ANN-S2 achieves higher recall with a little loss of precision.", 
        "178": "4).", 
        "179": "Compared with state-of-the-art approaches, our method ANN-S2 achieves the best performance.", 
        "180": "We also perform a t-test (p 6 0.05), which indicates that our method significantly outperforms all of the compared methods.", 
        "181": "Furthermore, another noticeable advantage of our approach is that it achieves much higher precision than state-of-the-arts.", 
        "182": "4.4 Augmentation with FrameNet  Recently, Liu et al.", 
        "183": "(2016a) used events automatically detected from FN as extra training data to alleviate the data-sparseness problem for event detection.", 
        "184": "To further demonstrate the effectiveness of the proposed approach, we also use the events from FN to augment the performance of our approach.", 
        "185": "In this work, we use the events published by Liu et al.", 
        "186": "(2016a)5 as extra training data.", 
        "187": "However, their data can not be used in the proposed approach without further processing, because it lacks of both argument and entity information.", 
        "188": "Figure 3 shows several examples of this data.", 
        "189": "Processing of Events from FN\nLiu et al.", 
        "190": "(2016a) detected events from FrameNet based on the observation that frames in FN are analogous to events in ACE\n5https://github.com/subacl/acl16\n(lexical unit of a frame \u2194 trigger of an event, frame elements of a frame \u2194 arguments of an event).", 
        "191": "All events they published are also frames in FN.", 
        "192": "Thus, we treat frame elements annotated in FN corpus as event arguments.", 
        "193": "Since frames generally contain more frame elements than events, we only use core6 elements in this work.", 
        "194": "Moreover, to obtain entity information, we use RPI Joint Information Extraction System7 (Li et al., 2013, 2014; Li and Ji, 2014) to label ACE entity mentions.", 
        "195": "Experimental Results\nWe use the events from FN as extra training data and keep the development and test datasets unchanged.Table 4 presents the experimental results.", 
        "196": "From the results, we observe that:\n1).", 
        "197": "With extra training data, ANN achieves significant improvements on F1 measure (66.7 vs. 65.0).", 
        "198": "This result, to some extent, demonstrates the correctness of our assumption that the data sparseness problem is the reason that causes unsupervised attention mechanisms to be ineffective to ED.", 
        "199": "2).", 
        "200": "Augmented with external data, both ANN-S1 and ANN-S2 achieve higher recall with a little loss of precision.", 
        "201": "This is to be expected.", 
        "202": "On the one hand, more positive training samples consequently make higher recall.", 
        "203": "On the other hand, the extra event samples are automatically extracted from FN, thus false-positive samples are inevitable to be involved, which may result in hurting the precision.", 
        "204": "Anyhow, with events from FN, our approach achieves higher F1 score.", 
        "205": "6FrameNet classifies frame elements into three groups: core, peripheral and extra-thematic.", 
        "206": "7http://nlp.cs.rpi.edu/software/  5 Related Work  Event detection is an increasingly hot and challenging research topic in NLP.", 
        "207": "Generally, existing approaches could roughly be divided into two groups.", 
        "208": "The first kind of approach tackled this task under the supervision of annotated triggers and entities, but totally ignored annotated arguments.", 
        "209": "The majority of existing work followed this paradigm, which includes feature-based methods and representationbased methods.", 
        "210": "Feature-based methods exploited a diverse set of strategies to convert classification clues (i.e., POS tags, dependency relations) into feature vectors (Ahn, 2006; Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Gupta and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Liu et al., 2016b).", 
        "211": "Representation-based methods typically represent candidate event mentions by embeddings and feed them into neural networks (Chen et al., 2015; Nguyen and Grishman, 2015; Liu et al., 2016a; Nguyen and Grishman, 2016).", 
        "212": "The second kind of approach, on the contrast, tackled event detection and argument extraction simultaneously, which is called joint approach (Riedel et al., 2009; Poon and Vanderwende, 2010; Li et al., 2013, 2014; Venugopal et al., 2014; Nguyen et al., 2016).", 
        "213": "Joint approach is proposed to capture internal and external dependencies of events, including trigger-trigger, argument-argument and trigger-argument dependencies.", 
        "214": "Theoretically, both ED and AE are expected to benefit from joint methods because triggers and arguments are jointly considered.", 
        "215": "However, in practice, existing joint methods usually only make remarkable improvements to AE, but insignificant to ED.", 
        "216": "Different from them, this work investigates the exploitation of argument information to improve the performance of ED.", 
        "217": "6 Conclusions  In this work, we propose a novel approach to model argument information explicitly for ED via supervised attention mechanisms.", 
        "218": "Besides, we also investigate two strategies to construct gold attentions using the annotated arguments.", 
        "219": "To demonstrate the effectiveness of the proposed method, we systematically conduc-\nt a series of experiments on the widely used benchmark dataset ACE 2005.", 
        "220": "Moreover, we also use events from FN to augment the performance of the proposed approach.", 
        "221": "Experimental results show that our approach outperforms state-of-the-art methods, which demonstrates that the proposed approach is effective for event detection.", 
        "222": "Acknowledgments  This work was supported by the Natural Science Foundation of China (No.", 
        "223": "61533018) and the National Basic Research Program of China (No.", 
        "224": "2014CB340503).", 
        "225": "And this research work was also supported by Google through focused research awards program."
    }, 
    "document_id": "P17-1164.pdf.json"
}
