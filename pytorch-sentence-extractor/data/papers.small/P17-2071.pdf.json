{
    "abstract_sentences": {
        "1": "This paper introduces the concept of temporal word analogies: pairs of words which occupy the same semantic space at different points in time.", 
        "2": "One well-known property of word embeddings is that they are able to effectively model traditional word analogies (\u201cword w1 is to word w2 as word w3 is to word w4\u201d) through vector addition.", 
        "3": "Here, I show that temporal word analogies (\u201cwordw1 at time t\u03b1 is like word w2 at time t\u03b2\u201d) can effectively be modeled with diachronic word embeddings, provided that the independent embedding spaces from each time period are appropriately transformed into a common vector space.", 
        "4": "When applied to a diachronic corpus of news articles, this method is able to identify temporal word analogies such as \u201cRonald Reagan in 1987 is like Bill Clinton in 1997\u201d, or \u201cWalkman in 1987 is like iPod in 2007\u201d."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 448\u2013453 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2071  1 Background  The meanings of utterances change over time, due both to changes within the linguistic system and to changes in the state of the world.", 
        "2": "For example, the meaning of the word awful has changed over the past few centuries from something like \u201caweinspiring\u201d to something more like \u201cvery bad\u201d, due to a process of semantic drift.", 
        "3": "On the other hand, the phrase president of the United States has meant different things at different points in time due to the fact that different people have occupied that same position at different times.", 
        "4": "These are very different types of changes, and the latter may not even be considered a linguistic phenomenon, but both types of change are relevant to the concept of temporal word analogies.", 
        "5": "I define a temporal word analogy (TWA) as a pair of words which occupy a similar semantic space at different points in time.", 
        "6": "For example, assuming that there is a semantic space associated with \u201cPresident of the USA\u201d, this space was occupied by Ronald Reagan in the 1980s, and by Bill Clinton in the 1990s.", 
        "7": "So a temporal analogy holds: \u201cRonald Reagan in 1987 is like Bill Clinton in 1997\u201d.", 
        "8": "Distributional semantics methods, particularly vector-space models of word meanings, have been employed to study both semantic change and word analogies, and as such are well-suited for the task of identifying TWAs.", 
        "9": "The principle behind these models, that the meaning of words can be captured by looking at the contexts in which they appear (i.e.", 
        "10": "other words), is not a recent idea, and is generally attributed to Harris (1954) or Firth (1957).", 
        "11": "The modern era of applying this principle algorithmically began with latent semantic analysis (LSA) (Landauer and Dumais, 1997), and the recent explosion in popularity of word embeddings is largely due to the very effective word2vec neural network approach to computing word embeddings (Mikolov et al., 2013a).", 
        "12": "In these types of vector space models (VSMs), the meaning of a word is represented as a multi-dimensional vector, and semantically-related words tend to have vectors that relate to one another in regular ways (e.g.", 
        "13": "by occupying nearby points in the vector space).", 
        "14": "One factor in word embeddings\u2019 recent popularity is their eye-catching ability to model word analogies using vector addition, as in the well-known example king + man \u2212 woman = queen (Mikolov et al., 2013b).", 
        "15": "Sagi et al.", 
        "16": "(2011) were the first to advocate the use of distributional semantics methods (specifically LSA) to automate and quantify large-scale studies of semantic change, in contrast to a more traditional approach in which a researcher inspects\n448\na handful of selected words by hand.", 
        "17": "While not using a VSM approach, Mihalcea and Nastase (2012) used context words as features to perform \u201cword epoch disambiguation\u201d, effectively capturing changes in word meanings over time.", 
        "18": "And several recent papers have combined neural embedding methods with large-scale diachronic corpora (e.g.", 
        "19": "the Google Books corpus) to study changes in word meanings over time.", 
        "20": "Kim et al.", 
        "21": "(2014) measured the drift (as cosine similarity) of a word\u2019s vector over time to identify words like cell and gay whose meaning changed over the past 100 years.", 
        "22": "Kulkarni et al.", 
        "23": "(2015) used a similar approach, combined with word frequencies and changepoint detection, to plot a word\u2019s movement through different lexical neighborhoods over time.", 
        "24": "Most recently, Hamilton et al.", 
        "25": "(2016) employed this methodology to discover and support two laws of semantic change, noting that words with higher frequency or higher levels of polysemy are more likely to experience semantic changes.", 
        "26": "While the study of word meanings over time using diachronic text corpora is a relatively niche subject with little commercial applicability, it has recently gained attention in the broader computational linguistics community.", 
        "27": "A 2015 SemEval task was dedicated to Diachronic Text Evaluation (Popescu and Strapparava, 2015); while systems submitted to the task successfully predicted the date of a text using traditional machine learning algorithms (Szymanski and Lynch, 2015), none of the submissions employed distributional semantics methods.", 
        "28": "Also in 2015, the president of the ACL singled out recent work (cited in the previous paragraph) using word embeddings to study semantic change as valuable examples of \u201cmore scientific uses of distributed representations and Deep Learning\u201d in computational linguistics (Manning, 2015).", 
        "29": "The present work is inspired by this line of research, and is a continuation on the same theme with a twist: whereas past work has investigated specific words whose meanings have changed over time, the present work investigates specific meanings whose words have changed over time.", 
        "30": "2 Method for Discovering Temporal Word Analogies  In general, work using diachronic word embeddings to study semantic change follows a common procedure: first, train independent VSMs for\neach time period in the diachronic corpus; second, transform those VSMs into a common space; and finally, compare a word\u2019s vectors from different VSMs to identify patterns of change over time.", 
        "31": "This paper follows a similar methodology.", 
        "32": "Algorithm 1 Calculating temporal word analogies VA \u2190Word2V ec(CorpusA) VB \u2190Word2V ec(CorpusB) T \u2190 FitTransform(VB, VA) VB\u2217 \u2190 ApplyTransform(T, VB) vec1 \u2190 LookupV ector(VA, word1) vec2 \u2190 NearestNeighbor(vec1, VB\u2217) word2 \u2190 LookupWord(VB\u2217 , vec2)\nThe general process of calculating a TWA is given in Algorithm 1.", 
        "33": "For example, if Corpus A is the 1987 texts, and Corpus B is the 1997 texts, and word1 is reagan, then word2 will be clinton.", 
        "34": "Crucially, it is only by applying the transformation to the B vector space that it becomes possible to directly compare the B\u2217 space with vectors from theA space.", 
        "35": "The Python code used in this paper to implement this method is available to download.1\n1.", 
        "36": "Training Independent VSMs: The data used in this analysis is a corpus of New York Times articles spanning the years 1987 through 2007, containing roughly 50 million words per year.", 
        "37": "The texts were lowercased and tokenized, and common bigrams were re-tokenized by a single pass of word2phrase.", 
        "38": "A separate embedding model was trained for each year in the corpus using word2vec with default-like parameters.2 This resulted in 21 independent vector space models, each with a vocabulary of roughly 100k words.", 
        "39": "2.", 
        "40": "Aligning Independent VSMs: Because training word embeddings typically begins with a random initial state, and the training itself is stochastic, the embeddings from different runs (even on the same dataset) are not comparable with one another.", 
        "41": "(Many properties of the embedding space, such as the distances between points, are consistent, but the actual values of the vectors are random.)", 
        "42": "This poses a challenge to work on diachronic word embeddings, which requires the ability to compare the vectors of the same word in different, independently-trained, VSMs.", 
        "43": "Previous work has employed different approaches to this problem:\n1https://github.com/tdszyman/twapy.", 
        "44": "2Example parameters: CBOW model, vector size 200,\nwindow size 8, negative samples 5.", 
        "45": "Non-random initialization.", 
        "46": "In this approach, used by Kim et al.", 
        "47": "(2014), the values for the VSM are not randomly initialized, but instead are initialized with the values from a previously-trained model, e.g.", 
        "48": "training of the 1988 model would begin with values from the 1987 model.", 
        "49": "Local linear regression.", 
        "50": "This approach, used by Kulkarni et al.", 
        "51": "(2015), assumes that two VSMs are equivalent under a linear transformation, and that most words\u2019 meanings do not change over time.", 
        "52": "A linear regression model is fit to a sample of vectors from the neighborhood of a word (hence \u201clocal\u201d), minimizing the mean squared error, i.e.", 
        "53": "minimizing the distance between the two vectors of a given word in the two vector spaces.", 
        "54": "A potential drawback of this approach is that it must be applied separately for each focal word.", 
        "55": "Orthogonal Procrustes.", 
        "56": "This approach, used by Hamilton et al.", 
        "57": "(2016), is similar to linear regression in that it aims to learn a transformation of one embedding space onto another, minimizing the distance between pairs of points, but uses a different mathematical method and is applied globally to the full space.", 
        "58": "A thorough investigation into the relative benefits of the different methods listed above would be a valuable contribution to future work in this area.", 
        "59": "In the present work, I take a global linear regression approach, broadly similar to that used by Kulkarni et al.", 
        "60": "(2015).", 
        "61": "However, I use a large sample of points to fit the model, and I apply the transformation globally to the entire VSM.", 
        "62": "Experiments showed that the accuracy of the transformation (measured by the mean Euclidean distance between pairs of points) increases as the sample size increases: using 10% of the total vocabulary shared by the two models (i.e.", 
        "63": "roughly 10k out of 100k tokens) produces good results, but there is little reason not to use all of the points (perhaps excluding the specific words that are the target of study, although even this does not make much practical difference in the outputs).", 
        "64": "3.", 
        "65": "Solving Temporal Word Analogies: Once the independent VSMs have been transformed, it is possible to compare vectors from one VSM with vectors from another VSM.", 
        "66": "Solving a TWA is then simply a matter of loading the vector of word w1 from the VSM VA, and then finding the point in the transformed VSM V \u22172 closest to that vector.", 
        "67": "That point corresponds to word w2, the solution to the analogy.", 
        "68": "It is also possible to align a series of\nVSMs to a single \u201croot\u201d VSM, and thereby trace the analogues of a word over time.", 
        "69": "The results of applying this method are discussed next.", 
        "70": "3 Example Outputs and Analysis  Using the New York Times corpus, each of the 20 VSMs from 1998 to 2007 were aligned to the 1987 VSM, making it possible to trace a series of analogies over time (e.g.", 
        "71": "\u201cWhich words in 1988, 1989, 1990 ... are like reagan in 1987?\u201d).", 
        "72": "A set of illustrative words from 1987 was chosen, and their vectors from the 1987 VSM were extracted.", 
        "73": "Then, for each subsequent year, the nearest word in that years\u2019 transformed VSM was found.", 
        "74": "The outputs are displayed in Table 1.", 
        "75": "One way to interpret Table 1 is to think of each column as representing a single semantic concept, and the words in that column illustrate the different words embodying that concept at different points in time.", 
        "76": "So column 1 represents \u201cPresident of the USA\u201d and column 2 represents \u201cmayor of New York City\u201d.", 
        "77": "The outputs of the TWA system perfectly reflect the names of the people holding these offices; likely due to the fact that these concepts are discussed in the corpus with high frequency and a well-defined lexical neighborhood (e.g.", 
        "78": "White House, Oval Office, president).", 
        "79": "While the other columns do not produce quite as clean analogies, they do tell interesting stories.", 
        "80": "The breakup of the Soviet Union is visible in the transition from soviet to russian in 1992, and later that concept (something like \u201cgeopolitical foe of the United States\u201d) is taken up in the 2000s by North Korea and Iran, two members of George W. Bush\u2019s \u201cAxis of Evil\u201d .", 
        "81": "Changes in technology can be observed, with the 1987 vector for Walkman (representing something like \u201cportable listening device\u201d) passing through CD player and MP3 player ultimately to iPod in 2007.", 
        "82": "Cultural changes can also be observed: the TWA \u201cyuppie in 1987 is like hipster in 2003\u201d, is validated by reports in the media (Bergstein, 2016).", 
        "83": "It is not easy to pin a precise meaning to each of these columns, and the \u201cright\u201d answer to any given TWA is to some degree a subjective judgment.", 
        "84": "Any given entity may fill multiple roles at once: which role should be the focus of the analogy?", 
        "85": "Each vector in the VSM can be thought of as combining multiple components: for example, the vectors for reagan include a component having to do with Ronald Reagan himself (based on words\nrelating to his personal attributes or names of family members) as well as a component having to do with the presidency (based on words like president, veto or White House).", 
        "86": "The analogies based on the 1987 reagan vector produce the names of other presidents over time (as in Table 1); however, if the 1999 reagan vector is used as the starting point, then 17 of the 20 analogies produced are either reagan or ronald reagan.", 
        "87": "This illustrates how the vector from 1999 contains a stronger component of the individual man rather than the presidency, due to the change in how he was written about when no longer in office.", 
        "88": "Similarly, the Iran Contra vector can be viewed as a mixture of \u2018the Iran Contra crisis itself\u201d and a more generic \u201cWhite House scandal\u201d concept.", 
        "89": "This second component causes Clinton-era scandals like Whitewater and Lewinsky to briefly appear in that space, while the first causes Iran Contra to continue to appear over time.", 
        "90": "4 Evaluation  Standard evaluation sets of word analogies exist and are commonly used as a measure of the quality of word embeddings (but see Linzen (2016) for why this can be problematic and misleading).", 
        "91": "No data set of manually-verified TWAs currently exists, so a small evaluation set was assembled by hand: ten TWA categories were selected which could be expected to be both newsworthy and unambiguous, and values for each year in the corpus\nwere identified using encyclopedic sources.", 
        "92": "When all pairs of years are considered, this yields a total of 4,200 individual TWAs.", 
        "93": "This data set, including the prediction outputs, is available online.", 
        "94": "For comparison, a baseline system which always predicts the output w2 to be the same as the input w1 was implemented.", 
        "95": "(A system based on word co-occurrence counts was also implemented, but produced no usable output.3) Table 2 shows the accuracy of the embedding-based system and the baseline for each category.", 
        "96": "Accuracy is determined by exact match, so mayor giuliani is incorrect for giuliani.", 
        "97": "Some categories are clearly much more difficult than others: prediction accuracy is 96% for \u201cPresident of the USA\u201d, but less than 1% for \u201cBest Actress\u201d.", 
        "98": "The names of Oscar Best Actress winners change every year with very little repetition, and it may be that an actress\u2019 role as an award winner only constitutes a small part of her overall news coverage.", 
        "99": "Accuracy is a useful metric, but it is not necessarily the best way to evaluate TWAs.", 
        "100": "Due to the nature of the data (the U.S. President, for example, only changes every four or eight years), the baseline system works quite well when the time depth of the analogy (\u03b4t = |t\u03b1 \u2212 t\u03b2|) is small.", 
        "101": "However,\n3The co-occurrence matrices were expensive to construct due to the volume of data, and despite efforts to smooth the distributions, the analogy outputs were noisy, dominated by low-frequency tokens with relatively few non-zero components and high cosine similarities.", 
        "102": "But it is possible that with more careful engineering this approach could be effective.", 
        "103": "as time depth increases, its accuracy drops sharply, while the embedding-based method remains effective, as illustrated in Figure 1.", 
        "104": "And even when the embedding-based system makes the \u201cwrong\u201d prediction, the output may still be insightful for data exploration, which is a more likely application for this method rather than prediction.", 
        "105": "The analogies evaluated here have the benefits of being easy to compile and evaluate, but they represent only one specific subset of TWAs.", 
        "106": "Other, less-clearly-defined, types of analogies (like the yuppie and walkman examples) would require a less rigid (and more expensive), form of evaluation, such as obtaining human acceptability judgments of the automatically-produced analogies.", 
        "107": "5 Conclusion  In this paper I have presented the concept of temporal word analogies and a method for identifying them using diachronic word embeddings.", 
        "108": "The method presented here is effective at solving TWAs, as shown in the evaluation, but its greater strength may be as a tool for data exploration.", 
        "109": "The small set of examples included in this paper illustrate political and social changes that unfold over time, and in the hands of users with diverse corpora and research questions, many more interesting analogies would likely be discovered using this same method.", 
        "110": "The method presented in this paper is not the only way that TWAs could be predicted.", 
        "111": "If the VSMs could somehow be trained jointly, rather than independently, this would eliminate the need for transformation and the noise it introduces.", 
        "112": "Or perhaps it is sufficient to look at lexical neighborhoods, rather the vectors themselves.", 
        "113": "One limitation of the embedding approach is that it re-\nquires vast amounts of data from each time period: tens of millions of words are required to train a model with word2vec.", 
        "114": "This makes it impractical for many historical corpora, which tend to be much smaller than the corpus used here.", 
        "115": "If a simpler, count-based approach could be made to work, this might be more applicable to smaller corpora.", 
        "116": "A method which incorporates word frequency (Kulkarni et al., 2015) might be effective at identifying when one word drops from common usage and another word appears.", 
        "117": "And assigning a measure of confidence to the proposed TWAs could help automatically identify meaningful analogies from the vast combinations of words and years that exist.", 
        "118": "In a domain where large quantities of real-time text are available, this method could potentially be applied as a form of event detection, identifying new entrants into a semantic space.", 
        "119": "And the same method described here could potentially be applied to other, non-diachronic, types of corpora.", 
        "120": "For example, given corpora of British English and American English, this methodology might be used to identify dialectal analogies, e.g.", 
        "121": "\u201celevator in US English is like lift in British English.\u201d Indeed, this general approach of comparing words in multiple embedding spaces could have many applications outside of diachronic linguistics.", 
        "122": "Acknowledgments  I would like to thank all the participants at the Insight Centre for Data Analytics special interest group meeting on NLP at NUI Galway for their encouraging and insightful feedback.", 
        "123": "Thanks also to the anonymous ACL reviewers, for encouraging the addition of a quantitative evaluation.", 
        "124": "This work was partially supported by Science Foundation Ireland through the Insight Centre for Data Analytics under grant number SFI/12/RC/2289."
    }, 
    "document_id": "P17-2071.pdf.json"
}
