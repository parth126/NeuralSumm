{
    "abstract_sentences": {
        "1": "We present a deep neural architecture that parses sentences into three semantic dependency graph formalisms.", 
        "2": "By using efficient, nearly arc-factored inference and a bidirectional-LSTM composed with a multi-layer perceptron, our base system is able to significantly improve the state of the art for semantic dependency parsing, without using hand-engineered features or syntax.", 
        "3": "We then explore two multitask learning approaches\u2014one that shares parameters across formalisms, and one that uses higher-order structures to predict the graphs jointly.", 
        "4": "We find that both approaches improve performance across formalisms on average, achieving a new state of the art.", 
        "5": "Our code is open-source and available at https://github.com/ Noahs-ARK/NeurboParser."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2037\u20132048 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1186  1 Introduction  Labeled directed graphs are a natural and flexible representation for semantics (Copestake et al., 2005; Baker et al., 2007; Surdeanu et al., 2008; Banarescu et al., 2013, inter alia).", 
        "2": "Their generality over trees, for instance, allows them to represent relational semantics while handling phenomena like coreference and coordination.", 
        "3": "Even syntactic formalisms are moving toward graphs (de Marneffe et al., 2014).", 
        "4": "However, full semantic graphs can be expensive to annotate, and efforts are fragmented across competing semantic theories, leading to a limited number of annotations in any one formalism.", 
        "5": "This makes learning to parse more difficult, especially for powerful but data-hungry machine learning techniques like neural networks.", 
        "6": "In this work, we hypothesize that the overlap among theories and their corresponding represen-\nLast week , shareholders took their money and ran .", 
        "7": "arg1\nloc\ntop\narg1 poss\narg2\n_and_c\narg1\n(a) DM\narg1\ntop\narg1 arg1\narg2 coord\narg1arg1\ncoord\nLast week , shareholders took their money ran .and (b) PAS\nLast week , shareholders took their money and ran .", 
        "8": "rstr twhen\ntop\nact app\npat conj\nact\ntop\ntwhen conj\n(c) PSD\nFigure 1: An example sentence annotated with the three semantic formalisms of the broad-coverage semantic dependency parsing shared tasks.", 
        "9": "tations can be exploited using multitask learning (Caruana, 1997), allowing us to learn from more data.", 
        "10": "We use the 2015 SemEval shared task on Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2015) as our testbed.", 
        "11": "The shared task provides an English-language corpus with parallel annotations for three semantic graph representations, described in \u00a72.", 
        "12": "Though the shared task was designed in part to encourage comparison between the formalisms, we are the first to treat SDP as a multitask learning problem.", 
        "13": "As a strong baseline, we introduce a new system that parses each formalism separately (\u00a73).", 
        "14": "It uses a bidirectional-LSTM composed with a multi-layer perceptron to score arcs and predicates, and has efficient, nearly arc-factored inference.", 
        "15": "Experiments show it significantly improves on state-of-the-art methods (\u00a73.4).", 
        "16": "We then present two multitask extensions (\u00a74.2\n2037\nand \u00a74.3), with a parameterization and factorization that implicitly models the relationship between multiple formalisms.", 
        "17": "Experiments show that both techniques improve over our basic model, with an additional (but smaller) improvement when they are combined (\u00a74.5).", 
        "18": "Our analysis shows that the improvement in unlabeled F1 is greater for the two formalisms that are more structurally similar, and suggests directions for future work.", 
        "19": "Finally, we survey related work (\u00a75), and summarize our contributions and findings (\u00a76).", 
        "20": "2 Broad-Coverage Semantic Dependency Parsing (SDP)  First defined in a SemEval 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al.", 
        "21": "(2015), the broad-coverage semantic depency parsing (SDP) task is centered around three semantic formalisms whose annotations have been converted into bilexical dependencies.", 
        "22": "See Figure 1 for an example.", 
        "23": "The formalisms come from varied linguistic traditions, but all three aim to capture predicate-argument relations between content-bearing words in a sentence.", 
        "24": "While at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al., 2013).", 
        "25": "They abstract over different syntactic realizations of the same or similar meaning (e.g., \u201cShe gave me the ball.\u201d vs. \u201cShe gave the ball to me.\u201d).", 
        "26": "Conversely, they attempt to distinguish between different senses even when realized in similar syntactic forms (e.g., \u201cI baked in the kitchen.\u201d vs. \u201cI baked in the sun.\u201d).", 
        "27": "Structurally, they are labeled directed graphs whose vertices are tokens in the sentence.", 
        "28": "This is in contrast to AMR whose vertices are abstract concepts, with no explicit alignment to tokens, which makes parsing more difficult (Flanigan et al., 2014).", 
        "29": "Their arc labels encode broadly-\napplicable semantic relations rather than being tailored to any specific downstream application or ontology.1 They are not necessarily trees, because a token may be an argument of more than one predicate (e.g., in \u201cJohn wants to eat,\u201d John is both the wanter and the would-be eater).", 
        "30": "Their analyses may optionally leave out non\u2013contentbearing tokens, such as punctuation or the infinitival \u201cto,\u201d or prepositions that simply mark the type of relation holding between other words.", 
        "31": "But when restricted to content-bearing tokens (including adjectives, adverbs, etc.", 
        "32": "), the subgraph is connected.", 
        "33": "In this sense, SDP provides a whole-sentence analysis.", 
        "34": "This is in contrast to PropBank-style SRL, which gives an analysis of only verbal and nominal predicates (Palmer et al., 2005).", 
        "35": "Semantic dependency graphs also tend to have higher levels of nonprojectivity than syntactic trees (Oepen et al., 2014).", 
        "36": "Sentences with graphs containing cycles have been removed from the dataset by the organizers, so all remaining graphs are directed acyclic graphs.", 
        "37": "Table 1 summarizes some of the dataset\u2019s high-level statistics.", 
        "38": "Formalisms.", 
        "39": "Following the SemEval shared tasks, we consider three formalisms.", 
        "40": "The DM (DELPH-IN MRS) representation comes from DeepBank (Flickinger et al., 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000).", 
        "41": "LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al., 2005).", 
        "42": "The PAS (Predicate-Argument Structures) representation is extracted from the Enju Treebank, which consists of automatic parses from the Enju HPSG parser (Miyao, 2006).", 
        "43": "PAS annotations are also available for the Penn Chinese Treebank (Xue et al., 2005).", 
        "44": "The PSD (Prague Semantic Dependencies) representation is extracted from the tectogrammatical layer of the Prague Czech-English Dependency Treebank (Hajic\u030c et al., 2012).", 
        "45": "PSD annotations are also available for a Czech translation of the WSJ Corpus.", 
        "46": "In this work, we train and evaluate only on English annotations.", 
        "47": "Of the three, PAS follows syntax most closely, and prior work has found it the easiest to predict.", 
        "48": "PSD has the largest set of labels, and parsers\n1This may make another disambiguation step necessary to use these representations in a downstream task, but there is evidence that modeling semantic composition separately from grounding in any ontology is an effective way to achieve broad coverage (Kwiatkowski et al., 2013).", 
        "49": "have significantly lower performance on it (Oepen et al., 2015).", 
        "50": "3 Single-Task SDP  Here we introduce our basic model, in which training and prediction for each formalism is kept completely separate.", 
        "51": "We also lay out basic notation, which will be reused for our multitask extensions.", 
        "52": "3.1 Problem Formulation  The output of semantic dependency parsing is a labeled directed graph (see Figure 1).", 
        "53": "Each arc has a label from a predefined set L, indicating the semantic relation of the child to the head.", 
        "54": "Given input sentence x, let Y(x) be the set of possible semantic graphs over x.", 
        "55": "The graph we seek maximizes a score function S:\ny\u0302 = arg max y\u2208Y(x) S(x, y), (1)\nWe decompose S into a sum of local scores s for local structures (or \u201cparts\u201d) p in the graph:\nS(x, y) = \u2211\np\u2208y s(p).", 
        "56": "(2)\nFor notational simplicity, we omit the dependence of s on x.", 
        "57": "See Figure 2a for examples of local structures.", 
        "58": "s is a parameterized function, whose parameters (denoted \u0398 and suppressed here for clarity) will be learned from the training data (\u00a73.3).", 
        "59": "Since we search over every possible labeled graph (i.e., considering each labeled arc for each pair of words), our approach can be considered a graph-based (or all-pairs) method.", 
        "60": "The models presented in this work all share this common graph-based approach, differing only in the set of structures they score and in the parameterization of the scoring function s. This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014).", 
        "61": "3.2 Basic Model  Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016).", 
        "62": "It borrows heavily from the neural arc-scoring architectures in those works, but decodes with a different algorithm under slightly different constraints.", 
        "63": "3.2.1 Basic Structures  Our basic model factors over three types of structures (p in Equation 2): \u2022 predicate, indicating a predicate word, de-\nnoted i\u2192\u00b7; \u2022 unlabeled arc, representing the existence of\nan arc from a predicate to an argument, denoted i\u2192j; \u2022 labeled arc, an arc labeled with a semantic\nrole, denoted i `\u2192 j.", 
        "64": "Here i and j are word indices in a given sentence, and ` indicates the arc label.", 
        "65": "This list corresponds to the most basic structures used by Martins and Almeida (2014).", 
        "66": "Selecting an output y corresponds precisely to selecting which instantiations of these structures are included.", 
        "67": "To ensure the internal consistency of predictions, the following constraints are enforced during decoding: \u2022 i\u2192\u00b7 if and only if there exists at least one j\nsuch that i\u2192j; \u2022 If i\u2192j, then there must be exactly one label `\nsuch that i `\u2192 j. Conversely, if not i\u2192j, then there must not exist any i `\u2192 j;\nWe also enforce a determinism constraint (Flanigan et al., 2014): certain labels must not appear on more than one arc emanating from the same token.", 
        "68": "The set of deterministic labels is decided based on their appearance in the training set.", 
        "69": "Notably, we do not enforce that the predicted graph is connected or spanning.", 
        "70": "If not for the predicate and determinism constraints, our model would be arc-factored, and decoding could be done for each i, j pair independently.", 
        "71": "Our structures do overlap though, and we employ AD3 (Martins et al., 2011) to find the highest-scoring internally consistent semantic graph.", 
        "72": "AD3 is an approximate discrete optimization algorithm based on dual decomposition.", 
        "73": "It can be used to decode factor graphs over discrete variables when scored structures overlap, as is the case here.", 
        "74": "3.2.2 Basic Scoring  Similarly to Kiperwasser and Goldberg (2016), our model learns representations of tokens in a sentence using a bi-directional LSTM (BiLSTM).", 
        "75": "Each different type of structure (predicate, unlabeled arc, labeled arc) then shares these same BiLSTM representations, feeding them into a multilayer perceptron (MLP) which is specific to the structure type.", 
        "76": "We present the architecture slightly differently from prior work, to make the transition to the multitask scenario (\u00a74) smoother.", 
        "77": "In our presentation, we separate the model into a function \u03c6 that represents the input (corresponding to the BiLSTM and the initial layers of the MLPs), and a function \u03c8 that represents the output (corresponding to the final layers of the MLPs), with the scores given by their inner product.2\nDistributed input representations.", 
        "78": "Long shortterm memory networks (LSTMs) are a variant of recurrent neural networks (RNNs) designed to alleviate the vanishing gradient problem in RNNs (Hochreiter and Schmidhuber, 1997).", 
        "79": "A bi-directional LSTM (BiLSTM) runs over the sequence in both directions (Schuster and Paliwal, 1997; Graves, 2012).", 
        "80": "Given an input sentence x and its corresponding part-of-speech tag sequence, each token is mapped to a concatenation of its word embedding vector and POS tag vector.", 
        "81": "Two LSTMs are then run in opposite directions over the input vector sequence, outputting the concatenation of the two hidden vectors at each position i: hi = [\u2212\u2192 h i; \u2190\u2212 h i ] (we omit hi\u2019s dependence on x and its own parameters).", 
        "82": "hi can be thought of as an encoder that contextualizes each token conditioning on all of its context, without any Markov assumption.", 
        "83": "h\u2019s parameters are learned jointly with the rest of the model (\u00a73.3); we refer the readers to Cho (2015) for technical details.", 
        "84": "The input representation \u03c6 of a predicate structure depends on the representation of one word:\n\u03c6(i\u2192\u00b7) = tanh ( Cpredhi + bpred ) .", 
        "85": "(3a)\n2For clarity, we present single-layer BiLSTMs and MLPs, while in practice we use two layers for both.", 
        "86": "For unlabeled arc and labeled arc structures, it depends on both the head and the modifier (but not the label, which is captured in the distributed output representation):\n\u03c6(i\u2192j) = tanh ( CUA [ hi;hj ] + bUA ) , (3b)\n\u03c6(i `\u2192 j) = tanh ( CLA [ hi;hj ] + bLA ) .", 
        "87": "(3c)\nDistributed output representations.", 
        "88": "NLP researchers have found that embedding discrete output labels into a low dimensional real space is an effective way to capture commonalities among them (Srikumar and Manning, 2014; Hermann et al., 2014; FitzGerald et al., 2015, inter alia).", 
        "89": "In neural language models (Bengio et al., 2003; Mnih and Hinton, 2007, inter alia) the weights of the output layer could also be regarded as an output embedding.", 
        "90": "We associate each first-order structure p with a d-dimensional real vector\u03c8(p) which does not depend on particular words in p. Predicates and unlabeled arcs are each mapped to a single vector:\n\u03c8(i\u2192\u00b7) = \u03c8pred, (4a) \u03c8(i\u2192j) = \u03c8UA, (4b)\nand each label gets a vector:\n\u03c8(i `\u2192 j) = \u03c8LA(`).", 
        "91": "(4c)\nScoring.", 
        "92": "Finally, we use an inner product to score first-order structures:\ns(p) = \u03c6(p) \u00b7\u03c8(p).", 
        "93": "(5) Figure 3 illustrates our basic model\u2019s architecture.", 
        "94": "3.3 Learning  The parameters of the model are learned using a max-margin objective.", 
        "95": "Informally, the goal is to learn parameters for the score function so that the gold parse is scored over every incorrect parse with a margin proportional to the cost of the incorrect parse.", 
        "96": "More formally, let D = { (xi, yi) }N i=1 be the training set consisting of N pairs of sentence xi and its gold parse yi.", 
        "97": "Training is then the following `2-regularized empirical risk minimization problem:\nmin \u0398\n\u03bb 2 \u2016\u0398\u20162 + 1 N\nN\u2211\ni=1\nL ( xi, yi; \u0398 ) , (6)\nwhere \u0398 is all parameters in the model, and L is the structured hinge loss:\nL ( xi, yi; \u0398 ) = max y\u2208Y(xi) { S ( xi, y ) + c ( y, yi )}\n\u2212 S ( xi, yi ) .", 
        "98": "(7)\nc is a weighted Hamming distance that trades off between precision and recall (Taskar et al., 2004).", 
        "99": "Following Martins and Almeida (2014), we encourage recall over precision by using the costs 0.6 for false negative arc predictions and 0.4 for false positives.", 
        "100": "3.4 Experiments  We evaluate our basic model on the English dataset from SemEval 2015 Task 18 closed track.3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from \u00a700\u201319 of the WSJ corpus, 1,692 development sentences from \u00a720, 1,410 sentences from \u00a721 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data.", 
        "101": "The closed track differs from the open and gold tracks in that it does not allow access to any syntactic analyses.", 
        "102": "In the open track, additional machine generated syntactic parses are provided, while the gold-track gives access to various goldstandard syntactic analyses.", 
        "103": "Our model is evaluated with closed track data; it does not have access to any syntactic analyses during training or test.", 
        "104": "We refer the readers to \u00a74.4 for implementation details, including training procedures, hyperparameters, pruning techniques, etc..\n3http://sdp.delph-in.net 4Paired bootstrap, p < 0.05 after Bonferroni correction.", 
        "105": "Empirical results.", 
        "106": "As our model uses no explicit syntactic information, the most comparable models to ours are two state-of-the-art closed track systems due to Du et al.", 
        "107": "(2015) and Almeida and Martins (2015).", 
        "108": "Du et al.", 
        "109": "(2015) rely on graphtree transformation techniques proposed by Du et al.", 
        "110": "(2014), and apply a voting ensemble to wellstudied tree-oriented parsers.", 
        "111": "Closely related to ours is Almeida and Martins (2015), who used rich, hand-engineered second-order features and AD3 for inference.", 
        "112": "Table 2 compares our basic model to both baseline systems (labeled F1 score) on SemEval 2015 Task 18 test data.", 
        "113": "Scores of those systems are repeated from the official evaluation results.", 
        "114": "Our basic model significantly outperforms the best published results with a 1.1% absolute improvement on the in-domain test set and 1.6% on the out-ofdomain test set.", 
        "115": "4 Multitask SDP  We introduce two extensions to our single-task model, both of which use training data for all three formalisms to improve performance on each formalism\u2019s parsing task.", 
        "116": "We describe a firstorder model, where representation functions are enhanced by parameter sharing while inference is kept separate for each task (\u00a74.2).", 
        "117": "We then introduce a model with cross-task higher-order structures that uses joint inference across different tasks (\u00a74.3).", 
        "118": "Both multitask models use AD3 for decoding, and are trained with the same marginbased objective, as in our single-task model.", 
        "119": "4.1 Problem Formulation  We will use an additional superscript t \u2208 T to distinguish the three tasks (e.g., y(t), \u03c6(t)), where T = {DM,PAS,PSD}.", 
        "120": "Our task is now to predict three graphs {y(t)}t\u2208T for a given input sentence x. Multitask SDP can also be understood as parsing x into a single unified multigraph y =\u22c3 t\u2208T y\n(t).", 
        "121": "Similarly to Equations 1\u20132, we decompose y\u2019s score S(x, y) into a sum of local scores for local structures in y, and we seek a multigraph y\u0302 that maximizes S(x, y).", 
        "122": "4.2 Multitask SDP with Parameter Sharing  A common approach when using BiLSTMs for multitask learning is to share the BiLSTM part of the model across tasks, while training specialized classifiers for each task (S\u00f8gaard and Goldberg, 2016).", 
        "123": "In this spirit, we let each task keep its own specialized MLPs, and explore two variants of our model that share parameters at the BiLSTM level.", 
        "124": "The first variant consists of a set of task-specific BiLSTM encoders as well as a common one that is shared across all tasks.", 
        "125": "We denote it FREDA.", 
        "126": "FREDA uses a neural generalization of \u201cfrustratingly easy\u201d domain adaptation (Daume\u0301 III, 2007; Kim et al., 2016), where one augments domainspecific features with a shared set of features to capture global patterns.", 
        "127": "Formally, let {h(t)}t\u2208T denote the three task-specific encoders.", 
        "128": "We introduce another encoder h\u0303 that is shared across all tasks.", 
        "129": "Then a new set of input functions {\u03c6(t)}t\u2208T can be defined as in Equations 3a\u20133c, for example:\n\u03c6(t)(i `\u2192 j) = tanh ( C\n(t) LA\n[ h\n(t) i ;h (t) j ;\nh\u0303i; h\u0303j ] + b (t) LA ) .", 
        "130": "(8)\nThe predicate and unlabeled arc versions are analogous.", 
        "131": "The output representations {\u03c8(t)} remain task-specific, and the score is still the inner product between the input representation and the output representation.", 
        "132": "The second variant, which we call SHARED, uses only the shared encoder h\u0303, and doesn\u2019t use task-specific encoders {h(t)}.", 
        "133": "It can be understood as a special case of FREDA where the dimensions of the task-specific encoders are 0.", 
        "134": "4.3 Multitask SDP with Cross-Task Structures  In syntactic parsing, higher-order structures have commonly been used to model interactions be-\ntween multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia).", 
        "135": "Llu\u0131\u0301s et al.", 
        "136": "(2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles.", 
        "137": "Similarly, we use higher-order structures across tasks instead of within tasks.", 
        "138": "In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures.", 
        "139": "Higher-order structure scoring.", 
        "140": "Borrowing from Lei et al.", 
        "141": "(2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder structures (i.e., arcs) p is made up of.", 
        "142": "This approach builds on and extends the parameter sharing techniques in \u00a74.2.", 
        "143": "It can either follow FREDA or SHARED to get the input representations for first-order structures.", 
        "144": "We first introduce basic tensor notation.", 
        "145": "The order of a tensor is the number of its dimensions.", 
        "146": "The outer product of two vectors forms a secondorder tensor (matrix) where [u\u2297 v]i,j = uivj .", 
        "147": "We denote the inner product of two tensors of the same dimensions by \u3008\u00b7, \u00b7\u3009, which first takes their element-wise product, then sums all the elements in the resulting tensor.", 
        "148": "For example, let p be a labeled third-order structure, including one labeled arc from each of the three different tasks: p = {p(t)}t\u2208T .", 
        "149": "Intuitively, s(p) should capture every pairwise interaction between the three input and three output representations of p. Formally, we want the score function to include a parameter for each term in the outer product of the representation vectors: s(p) = \u2329 W, \u2297\nt\u2208T\n( \u03c6(t) ( p(t) ) \u2297\u03c8(t) ( p(t) ))\u232a , (9)\nwhere W is a sixth-order tensor of parameters.6\nWith typical dimensions of representation vectors, this leads to an unreasonably large number of\n5In the future we hope to model structures over larger motifs, both across and within tasks, to potentially capture when an arc in one formalism corresponds to a path in another formalism, for example.", 
        "150": "6This is, of course, not the only way to model interactions between several representations.", 
        "151": "For instance, one could concatenate them and feed them into another MLP.", 
        "152": "Our preliminary experiments in this direction suggested that it may be less effective given a similar number of parameters, but we did not run full experiments.", 
        "153": "parameters.", 
        "154": "Following Lei et al.", 
        "155": "(2014), we upperbound the rank of W by r to limit the number of parameters (r is a hyperparameter, decided empirically).", 
        "156": "Using the fact that a tensor of rank at most r can be decomposed into a sum of r rank-1 tensors (Hitchcock, 1927), we reparameterize W to enforce the low-rank constraint by construction:\nW =\nr\u2211\nj=1\n\u2297\nt\u2208T\n([ U\n(t) LA ] j,: \u2297 [ V (t) LA ] j,: ) , (10)\nwhere U(t)LA,V (t) LA \u2208 Rr\u00d7d are now our parameters.", 
        "157": "[\u00b7]j,: denotes the jth row of a matrix.", 
        "158": "Substituting this back into Equation 9 and rearranging, the score function s(p) can then be rewritten as: r\u2211\nj=1\n\u220f\nt\u2208T\n[ U\n(t) LA\u03c6\n(t) ( p(t) )]\nj\n[ V\n(t) LA\u03c8\n(t) ( p(t) )]\nj .", 
        "159": "(11) We refer readers to Kolda and Bader (2009) for mathematical details.", 
        "160": "For labeled higher-order structures our parameters consist of the set of six matrices, {U(t)LA} \u222a {V(t)LA}.", 
        "161": "These parameters are shared between second-order and third-order labeled structures.", 
        "162": "Labeled second-order structures are scored as Equation 11, but with the product extending over only the two relevant tasks.", 
        "163": "Concretely, only four of the representation functions are used rather than all six, along with the four corresponding matrices from {U(t)LA} \u222a {V (t) LA}.", 
        "164": "Unlabeled crosstask structures are scored analogously, reusing the same representations, but with a separate set of parameter matrices {U(t)UA} \u222a {V (t) UA}.", 
        "165": "Note that we are not doing tensor factorization; we are learning U(t)LA,V (t) LA,U (t) UA, and V (t) UA directly, and W is never explicitly instantiated.", 
        "166": "Inference and learning.", 
        "167": "Given a sentence, we use AD3 to jointly decode all three formalisms.7 The training objective used for learning is the sum of the losses for individual tasks.", 
        "168": "4.4 Implementation Details  Each input token is mapped to a concatenation of three real vectors: a pre-trained word vector; a randomly-initialized word vector; and a randomlyinitialized POS tag vector.8 All three are updated\n7Joint inference comes at a cost; our third-order model is able to decode roughly 5.2 sentences (i.e., 15.5 task-specific graphs) per second on a single Xeon E5-2690 2.60GHz CPU.", 
        "169": "8There are minor differences in the part-of-speech data provided with the three formalisms.", 
        "170": "For the basic models, we\nduring training.", 
        "171": "We use 100-dimensional GloVe (Pennington et al., 2014) vectors trained over Wikipedia and Gigaword as pre-trained word embeddings.", 
        "172": "To deal with out-of-vocabulary words, we apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special unksymbol with probability \u03b11+#(w) , where #(w) is the count of w in the training set.", 
        "173": "Models are trained for up to 30 epochs with Adam (Kingma and Ba, 2015), with \u03b21 = \u03b22 = 0.9, and initial learning rate \u03b70 = 10\u22123.", 
        "174": "The learning rate \u03b7 is annealed at a rate of 0.5 every 10 epochs (Dozat and Manning, 2017).", 
        "175": "We apply early-stopping based on the labeled F1 score on the development set.9 We set the maximum number of iterations of AD3 to 500 and round decisions when it doesn\u2019t converge.", 
        "176": "We clip the `2 norm of gradients to 1 (Graves, 2013; Sutskever et al., 2014), and we do not use mini-batches.", 
        "177": "Randomly initialized parameters are sampled from a uniform distribution over[ \u2212 \u221a 6/(dr + dc), \u221a 6/(dr + dc) ] , where dr and dc are the number of the rows and columns in the matrix, respectively.", 
        "178": "An `2 penalty of \u03bb = 10\u22126 is applied to all weights.", 
        "179": "Other hyperparameters are summarized in Table 3.", 
        "180": "We use the same pruner as Martins and Almeida (2014), where a first-order feature-rich unlabeled pruning model is trained for each task, and arcs with posterior probability below 10\u22124 are discarded.", 
        "181": "We further prune labeled structures that appear less than 30 times in the training set.", 
        "182": "In the development set, about 10% of the arcs remain after pruning, with a recall of around 99%.", 
        "183": "use the POS tags provided with the respective dataset; for the multitask models, we use the (automatic) POS tags provided with DM.", 
        "184": "9Micro-averaged labeled F1 for the multitask models.", 
        "185": "4.5 Experiments  Experimental settings.", 
        "186": "We compare four multitask variants to the basic model, as well as the two baseline systems introduced in \u00a73.4.", 
        "187": "\u2022 SHARED1 is a first-order model.", 
        "188": "It uses a sin-\ngle shared BiLSTM encoder, and keeps the inference separate for each task.", 
        "189": "\u2022 FREDA1 is a first-order model based on \u201cfrus-\ntratingly easy\u201d parameter sharing.", 
        "190": "It uses a shared encoder as well as task-specific ones.", 
        "191": "The inference is kept separate for each task.", 
        "192": "\u2022 SHARED3 is a third-order model.", 
        "193": "It follows\nSHARED1 and uses a single shared BiLSTM encoder, but additionally employs cross-task structures and inference.", 
        "194": "\u2022 FREDA3 is also a third-order model.", 
        "195": "It com-\nbines FREDA1 and SHARED3 by using both \u201cfrustratingly easy\u201d parameter sharing and cross-task structures and inference.", 
        "196": "In addition, we also examine the effects of syntax by comparing our models to the state-of-the-art open track system (Almeida and Martins, 2015).10\nMain results overview.", 
        "197": "Table 4a compares our models to the best published results (labeled F1 score) on SemEval 2015 Task 18 in-domain test set.", 
        "198": "Our basic model improves over all closed track entries in all formalisms.", 
        "199": "It is even with the best open track system for DM and PSD, but improves on PAS and on average, without making use of any syntax.", 
        "200": "Three of our four multitask variants further improve over our basic model; SHARED1\u2019s differences are statistically insignificant.", 
        "201": "Our best models (SHARED3, FREDA3) outperform the previous state-of-the-art closed track system by 1.7% absolute F1, and the best open track system by 0.9%, without the use of syntax.", 
        "202": "We observe similar trends on the out-of-domain test set (Table 4b), with the exception that, on PSD, our best-performing model\u2019s improvement over the open-track system of Almeida and Martins (2015) is not statistically significant.", 
        "203": "The extent to which we might benefit from syntactic information remains unclear.", 
        "204": "With automatically generated syntactic parses, Almeida and Martins (2015) manage to obtain more than 1% absolute improvements over their closed track en-\n10Kanerva et al.", 
        "205": "(2015) was the winner of the gold track, which overall saw higher performance than the closed and open tracks.", 
        "206": "Since gold-standard syntactic analyses are not available in most realistic scenarios, we do not include it in this comparison.", 
        "207": "try, which is consistent with the extensive evaluation by Zhang et al.", 
        "208": "(2016), but we leave the incorporation of syntactic trees to future work.", 
        "209": "Syntactic parsing could be treated as yet another output task, as explored in Llu\u0131\u0301s et al.", 
        "210": "(2013) and in the transition-based frameworks of Henderson et al.", 
        "211": "(2013) and Swayamdipta et al.", 
        "212": "(2016).", 
        "213": "Effects of structural overlap.", 
        "214": "We hypothesized that the overlap between formalisms would enable multitask learning to be effective; in this section we investigate in more detail how structural overlap affected performance.", 
        "215": "By looking at undirected overlap between unlabeled arcs, we discover that modeling only arcs in the same direction may have been a design mistake.", 
        "216": "DM and PAS are more structurally similar to each other than either is to PSD.", 
        "217": "Table 5 compares the structural similarities between the three for-\nmalisms in unlabeled F1 score (each formalism\u2019s gold-standard unlabeled graph is used as a prediction of each other formalism\u2019s gold-standard unlabeled graph).", 
        "218": "All three formalisms have more than 50% overlap when ignoring arcs\u2019 directions, but considering direction, PSD is clearly different; PSD reverses the direction about half of the time it shares an edge with another formalism.", 
        "219": "A concrete example can be found in Figure 1, where DM and PAS both have an arc from \u201cLast\u201d to \u201cweek,\u201d while PSD has an arc from \u201cweek\u201d to \u201cLast.\u201d\nWe can compare FREDA3 to FREDA1 to isolate the effect of modeling higher-order structures.", 
        "220": "Table 6 shows performance on the development data in both unlabeled and labeled F1.", 
        "221": "We can see that FREDA3\u2019s unlabeled performance improves on DM and PAS, but degrades on PSD.", 
        "222": "This supports our hypothesis, and suggests that in future work, a more careful selection of structures to model might lead to further improvements.", 
        "223": "5 Related Work  We note two important strands of related work.", 
        "224": "Graph-based parsing.", 
        "225": "Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia).", 
        "226": "Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning\nhave been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia).", 
        "227": "Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016).", 
        "228": "Multitask learning in NLP.", 
        "229": "There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors.", 
        "230": "Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks.", 
        "231": "Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing.", 
        "232": "Ammar et al.", 
        "233": "(2016) and Guo et al.", 
        "234": "(2016) explored parameter sharing for multilingual parsing.", 
        "235": "Johansson (2013) and Kshirsagar et al.", 
        "236": "(2015) applied ideas from domain adaptation to multitask learning.", 
        "237": "Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daume\u0301 III, 2007).", 
        "238": "6 Conclusion  We showed two orthogonal ways to apply deep multitask learning to graph-based parsing.", 
        "239": "The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms.", 
        "240": "Without using syntactic parsing, these approaches outperform even state-of-the-art semantic dependency parsing systems that use syntax.", 
        "241": "Because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise.", 
        "242": "In future work we hope to explore cross-task scoring and inference for tasks where parallel annotations are not available.", 
        "243": "Our code is opensource and available at https://github.", 
        "244": "com/Noahs-ARK/NeurboParser.", 
        "245": "Acknowledgements  We thank the Ark, Maxwell Forbes, Luheng He, Kenton Lee, Julian Michael, and Jin-ge Yao for their helpful comments on an earlier version of this draft, and the anonymous reviewers for their valuable feedback.", 
        "246": "This work was supported by NSF grant IIS-1562364 and DARPA grant FA8750-122-0342 funded under the DEFT program."
    }, 
    "document_id": "P17-1186.pdf.json"
}
