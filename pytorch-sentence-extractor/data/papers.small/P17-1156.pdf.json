{
    "abstract_sentences": {
        "1": "Domain adaptation is an important technology to handle domain dependence problem in sentiment analysis field.", 
        "2": "Existing methods usually rely on sentiment classifiers trained in source domains.", 
        "3": "However, their performance may heavily decline if the distributions of sentiment features in source and target domains have significant difference.", 
        "4": "In this paper, we propose an active sentiment domain adaptation approach to handle this problem.", 
        "5": "Instead of the source domain sentiment classifiers, our approach adapts the general-purpose sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain.", 
        "6": "A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain.", 
        "7": "Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1701\u20131711 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1156  1 Introduction  Sentiment classification is widely known as a domain-dependent problem (Liu, 2012; Pang and Lee, 2008; Blitzer et al., 2007; Pan et al., 2010).", 
        "2": "This is because different domains usually have many different sentiment expressions.", 
        "3": "For example, \u201clengthy\u201d and \u201cboring\u201d are popularly used in Book domain to express negative sentiment.", 
        "4": "However, they are rare in Kitchen appliance domain.", 
        "5": "Moreover, the same word or phrase may convey\n\u2217Corresponding author.", 
        "6": "different sentiments in different domains.", 
        "7": "For instance, \u201cunpredictable\u201d is frequently used to express positive sentiment in Movie domain (e.g., \u201cThe plot of this movie is fun and unpredictable\u201d).", 
        "8": "However, it tends to be used as a negative word in Kitchen appliance domain (e.g., \u201cEven holding heat is unpredictable.", 
        "9": "It is just terrible!\u201d).", 
        "10": "Thus, every domain has many domain-specific sentiment expressions, which cannot be captured by other domains.", 
        "11": "The performance of directly applying a general sentiment classifier or a sentiment classifier trained in other domains to target domain is usually suboptimal.", 
        "12": "Since there are a large number of domains in user-generated content, it is impractical to manually annotate enough samples for each domain to train an accurate domain-specific sentiment classifier.", 
        "13": "Thus, sentiment domain adaptation, which transfers the sentiment classifier trained in a source domain with sufficient labeled data to a target domain with no or scarce labeled data, has been widely studied (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011; Glorot et al., 2011).", 
        "14": "Existing sentiment domain adaptation methods are mainly based on transfer learning techniques.", 
        "15": "Many of them try to learn a new feature representation to augment or replace the original feature space in order to reduce the gap of sentiment feature distributions between source and target domains (Pan et al., 2010; Glorot et al., 2011).", 
        "16": "For example, Blitzer et al.", 
        "17": "(2007) proposed to learn a latent representation for domain-specific words from both source and target domains by using pivot features as bridge.", 
        "18": "The advantage of these methods is that no labeled data in target domain is needed.", 
        "19": "However, when the distributions of sentiment features in source and target domains have significant difference, the performance of domain adaptation will heavily decline (Li et al., 2013).", 
        "20": "In some cases, the performance of adaptation is even lower than\n1701\nthat without adaptation, which is usually known as negative transfer (Pan and Yang, 2010).", 
        "21": "In this paper, we propose an active sentiment domain adaptation approach to handle this problem by incorporating both general sentiment information and a small number of actively selected labeled samples from target domain.", 
        "22": "More specifically, in our approach the general sentiment information extracted from sentiment lexicons is adapted to target domain using domain-specific sentiment similarities among words.", 
        "23": "The general sentiment information is regarded as a \u201cbackground\u201d domain to transfer.", 
        "24": "The word similarities are extracted from unlabeled samples of target domain using both syntactic rules and co-occurrence patterns.", 
        "25": "Then we actively select and annotate a small number of informative samples from target domain in an active learning manner.", 
        "26": "These labeled samples are incorporated into our approach to improve the performance of sentiment domain adaptation.", 
        "27": "A unified model is proposed to incorporate different types of sentiment information to train sentiment classifier for target domain.", 
        "28": "Extensive experiments were conducted on benchmark datasets.", 
        "29": "The experimental results show that our approach can train accurate sentiment classifiers and reduce the manual annotation effort.", 
        "30": "2 Related Work    2.1 Sentiment Domain Adaptation  Sentiment classification is well known as a highly domain-dependent task, and domain adaptation is widely studied in sentiment analysis field to handle this problem (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011; Glorot et al., 2011).", 
        "31": "Existing sentiment domain adaptation methods are mainly based on transfer learning technique (Pan and Yang, 2010), where sentiment classifiers are trained in one or multiple source domains with sufficient labeled samples, and then applied to target domain where there is no or only scarce labeled samples.", 
        "32": "In order to reduce the gap of sentiment feature distributions between source and target domains, many sentiment domain adaptation methods try to learn a new feature representation to augment or replace the original feature space.", 
        "33": "For example, Pan et al.", 
        "34": "(2010) proposed a sentiment domain adaptation method based on spectral feature alignment (SFA) algorithm.", 
        "35": "They first manually selected several domain-independent features and computed the associations between domain-\nspecific features and domain-independent features.", 
        "36": "After that they built a bipartite graph where domain-independent and domain-specific features were regarded as two types of nodes.", 
        "37": "Then domain-specific features were grouped into several clusters using spectral clustering algorithm.", 
        "38": "These clusters were used to augment the original feature representations.", 
        "39": "Glorot et al.", 
        "40": "(2011) proposed a sentiment domain adaptation method based on a deep learning technique, i.e., Stacked Denoising Autoencoders.", 
        "41": "They learned the parameters of neural networks using unlabeled samples from both source and target domains, and used the hidden nodes of the neural networks as the latent feature representations of both domains.", 
        "42": "Then they trained sentiment classifiers using source domain labeled data in this new feature space and applied it to target domain.", 
        "43": "The advantage of these sentiment domain adaptation methods is that they do not rely on the labeled data in target domain.", 
        "44": "However, they have a common shortcoming, i.e., when the distributions of sentiment features in source and target domains have significant difference, the performance of domain adaptation will heavily decline (Li et al., 2013).", 
        "45": "In some cases, negative transfer may happen (Blitzer et al., 2007; Li et al., 2013), which means the performance of adaptation is worse than that without adaptation (Pan and Yang, 2010).", 
        "46": "Different from many existing sentiment domain adaptation methods, in our approach we adapt the general sentiment information in sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode.", 
        "47": "Since the sentiment words in generalpurpose sentiment lexicons usually convey consistent sentiment polarities in different domains, and the actively selected labeled samples contain rich domain-specific sentiment information of target domain, our approach can effectively reduce the risk of negative transfer.", 
        "48": "The usefulness of labeled samples from target domain in sentiment domain adaptation has been observed by previous research works (Choi and Cardie, 2009; Chen et al., 2011; Li et al., 2013; Wu et al., 2016).", 
        "49": "For example, Choi and Cardie (2009) proposed to adapt a sentiment lexicon to a specific domain by exploiting both the relations among words which co-occur in the same sentiment expressions and the relations between words and labeled sentiment expressions.", 
        "50": "However, the\nlabeled samples used in these methods are randomly selected, while in our approach we actively select informative samples from target domain to annotate.", 
        "51": "Thus, our approach has the potential to reduce the manual annotation effort.", 
        "52": "2.2 Active Learning  Active learning is a useful technique in scenarios where unlabeled data is abundant but their labels are difficult or expensive to obtain (Tong and Koller, 2002; Settles, 2010).", 
        "53": "By actively selecting informative samples to label, active learning can effectively reduce the annotation effort, and improve the classification performance with limited budget (Li et al., 2012).", 
        "54": "An important problem in active learning is how to evaluate the informativeness of unlabeled samples (Fu et al., 2013).", 
        "55": "Different methods have been applied to select informative samples, such as uncertainty sampling (Zhu et al., 2010; Yang et al., 2015), queryby-committee (Freund et al., 1997; Li et al., 2013) and so on.", 
        "56": "In our approach, uncertainty combined with density is used to measure the informativeness of samples.", 
        "57": "A major difference between our approach and existing active learning methods is that in existing methods the parameters of the initial classifier are either initialized as zero (CesaBianchi et al., 2006) or learned from a set of randomly selected samples (Settles, 2010).", 
        "58": "In contrast, the initial sentiment classifier in our approach is constructed by adapting the general sentiment information to target domain via the domain-specific sentiment similarities among words.", 
        "59": "There are a few works that apply active learning methods to sentiment domain adaptation task (Rai et al., 2010; Li et al., 2013).", 
        "60": "For example, Rai et al.", 
        "61": "(2010) proposed an online active learning algorithm for sentiment domain adaptation.", 
        "62": "They started with a sentiment classifier trained on the labeled samples of a source domain.", 
        "63": "Then they sequentially selected informative samples in target domain to annotate with a probability positively related to classification uncertainty.", 
        "64": "The newly annotated samples were used to update the sentiment classifier in an online learning manner.", 
        "65": "Li et al.", 
        "66": "(2013) proposed another active learning method for cross-domain sentiment classification.", 
        "67": "In their method they trained two sentiment classifiers, one on the labeled samples of source domain, and the other one on the labeled samples of target domain.", 
        "68": "Then query-by-committee strategy was used to se-\nlect the informative instances from target domain.", 
        "69": "Different from these methods, our approach does not rely on the labeled data of source domains.", 
        "70": "Instead, in our approach the general sentiment information in sentiment lexicons is actively adapted to target domain, which usually has better generalization ability in various domains than the sentiment classifier trained in a source domain.", 
        "71": "In addition, our approach can incorporate the domainspecific sentiment similarities among words mined from unlabeled samples of target domain, which are not considered in these methods.", 
        "72": "3 Active Sentiment Domain Adaptation    3.1 Notations  First we introduce several notations that will be used in remaining part of this paper.", 
        "73": "Denote the general sentiment information extracted from a general-purpose sentiment lexicon as p \u2208 RD\u00d71, where D is the vocabulary size.", 
        "74": "If the ith word is labeled as positive (or negative) in the sentiment lexicon, then pi = +1 (or pi = \u22121).", 
        "75": "Otherwise, pi = 0.", 
        "76": "Following many previous works in sentiment classification field (Blitzer et al., 2007; Pan et al., 2010), here we select linear classifier as sentiment classifier, and denote the linear classification model as w \u2208 RD\u00d71.", 
        "77": "We use f(xi, yi,w) to represent the loss of classifying the ith labeled sample in target domain under the classification model w, where f is the classification loss function, xi \u2208 RD\u00d71 is the feature vector of this sample and yi is its sentiment label.", 
        "78": "In this paper we focus on binary sentiment classification and yi \u2208 {+1,\u22121}.", 
        "79": "In addition, we select log loss for f .", 
        "80": "Thus, f(xi, yi,w) = log(1+ exp(\u2212yiwTxi)).", 
        "81": "Besides, we use S \u2208 RD\u00d7D to represent the sentiment similarities among words extracted from unlabeled samples of target domain.", 
        "82": "3.2 Domain-Specific Sentiment Similarities  Next we introduce the extraction of domainspecific sentiment similarities among words from unlabeled samples of target domain.", 
        "83": "Two types of similarities are extracted in this paper.", 
        "84": "The first one is based on syntactic rules, which is inspired by (Hatzivassiloglou and McKeown, 1997; Huang et al., 2014; Wu and Huang, 2016).", 
        "85": "If two words have the same POS-tag such as adjective, verb, and adverb, and they are connected by coordinating conjunction \u201cand\u201d in the same sentence, then we regard they convey the same sentiment polarity.", 
        "86": "In\naddition, if two words are connected by adversative conjunction \u201cbut\u201d and have the same POS-tag, then they are assumed to have opposite sentiment polarities.", 
        "87": "Denote Sr \u2208 RD\u00d7D as the sentiment similarities extracted from unlabeled samples according to syntactic rules, and the similarity score between words i and j is defined as:\nSri,j = Nsi,j \u2212Noi,j\nNsi,j +N o i,j + \u03b11\n, (1)\nwhere N si,j and N o i,j are the frequencies of words i and j having the same or opposite sentiments respectively according to the syntactic rules, and \u03b11 is a positive smoothing factor.", 
        "88": "If two words have much higher frequency of sharing the same sentiment than opposite sentiments, then they will have a larger positive sentiment similarity score.", 
        "89": "Note that Sri,j can be negative according to Eq.", 
        "90": "(1).", 
        "91": "Here we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to zero.", 
        "92": "The range of Sri,j is [0, 1].", 
        "93": "The second type of sentiment similarities are extracted according to the co-occurrence patterns among words.", 
        "94": "It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016).", 
        "95": "In this paper, we compute the co-occurrence between words in the context of document.", 
        "96": "DenoteD as the set of all documents, and N id as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as:\nSci,j =\n\u2211 d\u2208Dmin{N id, N\nj d}\u2211\nd\u2208Dmax{N id, N j d}+ \u03b12\n, (2)\nwhere \u03b12 is a positive smoothing parameter.", 
        "97": "If two words frequently co-occur with each other in many documents, then they will have a high sentiment similarity score according to Eq.", 
        "98": "(2).", 
        "99": "The range of Sci,j is also [0, 1].", 
        "100": "Denote S\nc \u2208 RD\u00d7D as the set of all sentiment similarities extracted according to co-occurrence patterns.", 
        "101": "The sentiment similarities extracted according to syntactic rules are usually of high accuracy.", 
        "102": "However, their coverage is limited, because the word pairs detected by these syntactic rules are sparse.", 
        "103": "In contrast, the coverage of sentiment similarities extracted from co-occurrence patterns is\nquite wide because document is a long context, while their accuracies are not as high as the similarities based on syntactic rules.", 
        "104": "Thus, we propose to combine these two types of sentiment similarities to obtain a balance between accuracy and coverage.", 
        "105": "Denote S \u2208 RD\u00d7D as the final sentiment similarities among words, and Si,j = \u03b8Sri,j + (1\u2212 \u03b8)Sci,j , where \u03b8 \u2208 [0, 1] is the combination coefficient.", 
        "106": "In this paper we set \u03b8 to 0.5, which means that we regard these two types of sentiment similarities as equally important.", 
        "107": "3.3 Initial Sentiment Classifier Construction  In this section, we introduce the construction of the initial sentiment classifier to start the active learning process.", 
        "108": "Existing active learning methods usually randomly select a set of unlabeled samples to annotate and then train the initial classifier on them (Settles, 2010).", 
        "109": "However, these randomly selected samples may be redundant and not informative enough.", 
        "110": "In this paper, we propose to build the initial sentiment classifier by adapting the general sentiment information to target domain via domain-specific sentiment similarities as follows:\nw0 = argmin w \u2212\nD\u2211\ni=1\npiwi + \u03b1\nD\u2211\ni=1\n\u2211 j 6=i Si,j(wi \u2212 wj)2,\n(3)\nwhere w0 \u2208 RD\u00d71 is the initial sentiment classifier, \u03b1 is a positive regularization coefficient, pi is the prior sentiment polarity of word i in sentiment lexicons, and Si,j is the sentiment similarity score between words i and j. Eq.", 
        "111": "(3) is motivated by (Bengio et al., 2006), and the quadratic cost criterion is equivalent to label propagation.", 
        "112": "In Eq.", 
        "113": "(3), \u2212\u2211Di=1 piwi means that if a word i is labeled as a positive (or negative) word in a generalpurpose sentiment lexicon, i.e., pi > 0 (or pi < 0), then we constrain that its sentiment weight in the sentiment classifier is also positive (or negative).", 
        "114": "Otherwise, a penalty will be added to the objective function.", 
        "115": "In addition, \u2211D i=1 \u2211 j 6=i Si,j(wi \u2212 wj)2 represents that if two words share high sentiment similarity, then we constrain they have similar sentiment weights in sentiment classifier.", 
        "116": "For example, if we find that \u201cgreat\u201d and \u201ceasy\u201d have high sentiment similarities in Kitchen appliances domain (e.g., \u201cThis is a great pan and easy to wash\u201d), and \u201cgreat\u201d is a positive sentiment word in many sentiment lexicons, then we can infer that \u201ceasy\u201d may also be a positive sentiment word in this domain by propagating the sentiment information from\n\u201cgreat\u201d to \u201ceasy\u201d.", 
        "117": "In this way, the general sentiment information can be adapted to many domainspecific sentiment expressions in target domain.", 
        "118": "3.4 Query Strategy  Active learning methods iteratively select the most informative instances to label and add them to the training set (Settles, 2010).", 
        "119": "Thus, an important issue in these methods is how to measure the informativeness of unlabeled samples.", 
        "120": "In this paper, we select classification uncertainty as the informativeness measure, which has been proven effective in many active learning methods (Zhu et al., 2010; Yang et al., 2015).", 
        "121": "Since we focus on binary sentiment classification and the classification loss function is log loss, the classification uncertainty of an unlabeled instance x is defined as:\nU(x) = 1\u2212 \u2223\u2223\u2223\u22231\u2212\n2\n1 + exp(\u2212wTx)\n\u2223\u2223\u2223\u2223 , (4)\nwhere w is the linear sentiment classification model.", 
        "122": "The range of U(x) is [0, 1].", 
        "123": "If |wTx| is large, which means that current sentiment classifier is confident in classifying this instance, then the uncertainty of x (i.e., U(x)) will be low.", 
        "124": "If |wTx| is close to 0, then the sentiment classifier is very uncertain about this instance, probably because the sentiment expressions in this instance are not covered by current sentiment classifier, and the uncertainty of the instance x will be high.", 
        "125": "In this case, annotating this instance and adding it to the training set are beneficial, because it can provide the information of unknown sentiment expressions and has the potential to quickly improve the quality of target domain sentiment classifier.", 
        "126": "However, many researchers have found that unlabeled instances with high uncertainties can be outliers, whose labels are useless and even misleading (Settles, 2010; Zhu et al., 2010).", 
        "127": "Thus, here we combine uncertainty with representativeness to avoid outliers.", 
        "128": "Density is proven to be an effective measure of representativeness in active learning methods (Zhu et al., 2010; Hajmohammadi et al., 2015).", 
        "129": "Here we use the k-nearest neighbour based density proposed by Zhu et al.", 
        "130": "(2010) as the representativeness measure, which is formulated as:\nD(x) = 1\nk\n\u2211\nxi\u2208N (x)\nxTxi \u2016x\u20162 \u00b7 \u2016xi\u20162 , (5)\nwhere N (x) is the set of k most similar instances of x.", 
        "131": "The final informativeness score of an unlabeled sample is a linear combination of uncertainty\nand density which is formulated as follows:\nI(x) = \u03b7(t)U(x) + (1\u2212 \u03b7(t))D(x), (6)\nwhere \u03b7(t) \u2208 [0, 1] is the combination coefficient at the tth iteration.", 
        "132": "In this paper, we select a monotonically increasing function for \u03b7(t), i.e., \u03b7(t) = 1\n1+exp(2\u2212 4t T ) , where T is the total number\nof iterations.", 
        "133": "It means that at initial iterations we put more emphasis on instances with high representativeness, because the initial sentiment classifier built by adapting the general sentiment information via the domain-specific sentiment similarities is relatively weak, and we prefer to select instances with more popular sentiment expressions to annotate.", 
        "134": "As more and more labeled samples are added to the training set and the sentiment classifier becomes stronger, we gradually focus on more difficult instances, i.e., those having higher classification uncertainty scores.", 
        "135": "3.5 Active Domain Adaptation  Based on previous discussions, in this section we introduce the complete procedure of our active sentiment domain adaptation (ASDA) approach.", 
        "136": "Different from existing sentiment domain adaptation methods which rely on the sentiment classifier trained in source domains to transfer, in our approach we regard the general sentiment information in sentiment lexicons as the \u201cbackground\u201d domain and adapt it to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode.", 
        "137": "First, we build an initial sentiment classifier according to Eq.", 
        "138": "(3) by adapting the general sentiment information to target domain using the domainspecific sentiment similarities among words mined from unlabeled samples of target domain.", 
        "139": "Second, we compute the density of each unlabeled sample in U according to Eq.", 
        "140": "(5).", 
        "141": "Then we repeat following steps until the annotation budget has run out.", 
        "142": "First, we compute the uncertainty of each unlabeled sample in U according to Eq.", 
        "143": "(4), and further we compute their informativeness by combining uncertainty with density according to Eq.", 
        "144": "(6).", 
        "145": "Next, we select the unlabeled sample with the highest informativeness from U and manually annotate its sentiment polarity.", 
        "146": "Then we add it to the set of labeled samples L and remove it from U .", 
        "147": "After that we retrain the sentiment classifier for target domain based on the general sentiment information p, the labeled samples L, and the domain-\nspecific sentiment similarities S as follows:\nargmin w \u2212\nD\u2211\ni=1\npiwi + \u03b1\nD\u2211\ni=1\n\u2211 j 6=i Si,j(wi \u2212 wj)2\n+ \u03b2 \u2211\nxi\u2208L log(1 + exp(\u2212yiwTxi)) + \u03bb\u2016w\u201622,\n(7)\nwhere \u03b1, \u03b2, and \u03bb are nonnegative coefficients.", 
        "148": "By the term \u2212\u2211Di=1 piwi we constrain that the target domain sentiment classifier learned by our approach is consistent with the general sentiment information.", 
        "149": "Through this way, the general sentiment information extracted from sentiment lexicons can be adapted to target domain.", 
        "150": "The term\u2211D\ni=1 \u2211 j 6=i Si,j(wi \u2212 wj)2 is motivated by label propagation (Bengio et al., 2006).", 
        "151": "If two words tend to have high sentiment similarity with each other according to many unlabeled samples of target domain, then we constrain that their sentiment weights in the target domain sentiment classifier are also similar.", 
        "152": "The term\n\u2211 xi\u2208L log(1 +\nexp(\u2212yiwTxi)) means that we hope to minimize the empirical classification loss on labeled samples of target domain.", 
        "153": "By this term the sentiment information in the labeled samples is incorporated into the learning of target domain sentiment classifier.", 
        "154": "The L2-norm regularization term is introduced to control model complexity.", 
        "155": "The sentiment classifier trained in Eq.", 
        "156": "(7) is further used at the next iteration of active sentiment domain adaptation until all the budget of manual annotation has been used.", 
        "157": "Then we obtain the final sentiment classifier of target domain.", 
        "158": "The complete algorithm of our active sentiment domain adaptation (ASDA) approach is summarized in Algorithm 1.", 
        "159": "Algorithm 1 Active sentiment domain adaptation.", 
        "160": "1: Input: The set of unlabeled samples U , the general sen-\ntiment information p, the domain-specific sentiment similarities S, and the total annotation budget N .", 
        "161": "2: Output: Target domain sentiment classifier w. 3: Train the initial sentiment classifier w0 (Eq.", 
        "162": "(3)).", 
        "163": "4: Compute the density of each sample xi in U (Eq.", 
        "164": "(5)).", 
        "165": "5: Initialize the set of labeled samples L = \u2205, the iteration\nnumber t = 0, and the sentiment classifier w = w0.", 
        "166": "6: while t < N do 7: t = t+ 1.", 
        "167": "8: Compute the uncertainty score of each sample xi in U (Eq.", 
        "168": "(4)).", 
        "169": "9: Compute the informativeness score of each sample\nxi in U (Eq.", 
        "170": "(6)).", 
        "171": "10: Select x\u2217 from U which has the highest informativeness score.", 
        "172": "11: Annotate x\u2217 and obtain its sentiment label y.", 
        "173": "12: L = L+ {x\u2217, y}, U = U \u2212 x\u2217.", 
        "174": "13: Update sentiment classifier w according to Eq.", 
        "175": "(7).", 
        "176": "14: end while  4 Experiments    4.1 Datasets  The dataset used in our experiments is the Amazon product review dataset1 collected by Blitzer et al.", 
        "177": "(2007), which is widely used in sentiment analysis and domain adaptation research (Pan et al., 2010; Bollegala et al., 2011).", 
        "178": "This dataset contains product reviews in four domains, i.e., Book, DVD, Electronics, and Kitchen appliances.", 
        "179": "In each domain, 1,000 positive and 1,000 negative reviews as well as a large number of unlabeled samples are included.", 
        "180": "The detailed statistics of this dataset are summarized in Table 1.", 
        "181": "Following many previous works (Blitzer et al., 2007; Bollegala et al., 2011), unigrams and bigrams were used to build feature vectors in our experiments.", 
        "182": "We randomly split the labeled samples in each domain into two parts with equal size.", 
        "183": "The first part was used as test data, and the second part was used as the pool of \u201cunlabeled\u201d samples to perform active learning.", 
        "184": "The general sentiment information was extracted from Bing Liu\u2019s sentiment lexicon2 (Hu and Liu, 2004), which is one of the state-of-the-art general-purpose sentiment lexicons.", 
        "185": "The domain-specific sentiment similarities among words were extracted from the large-scale unlabeled samples.", 
        "186": "The total number of samples actively selected by our approach to annotate was set to 100.", 
        "187": "The values of \u03b1, \u03b2, and \u03bb were set to 0.1, 1, and 1 respectively.", 
        "188": "We repeated each experiment 10 times independently and the average results were reported.", 
        "189": "4.2 Algorithm Effectiveness  First we conducted several experiments to explore the effectiveness of our active sentiment domain adaptation (ASDA) approach.", 
        "190": "We hope to answer two questions via these experiments: 1) whether the domain-specific sentiment similarities among words mined from unlabeled samples of target\n1https://www.cs.jhu.edu/\u02dcmdredze/ datasets/sentiment/\n2https://www.cs.uic.edu/liub/FBS/ sentiment-analysis.html\ndomain are useful for adapting the general sentiment information to target domain; 2) whether a small number of samples which are actively selected and annotated in target domain can help improve the domain adaptation performance.", 
        "191": "In our experiments, we implemented different versions of our ASDA approach using different combinations of sentiment information.", 
        "192": "The first one is Lexicon, which means only using the general sentiment information and no domain adaptation is conducted.", 
        "193": "It serves as a baseline.", 
        "194": "The second one is Lexicon+SentiSim, which means adapting general sentiment information to target domain using domain-specific sentiment similarities, but labeled samples of target domain are not incorporated.", 
        "195": "The third one is Lexicon+SentiSim+Label, which is the complete ASDA approach.", 
        "196": "The experimental results are summarized in Fig.", 
        "197": "1.", 
        "198": "According to Fig.", 
        "199": "1, the performance of Lexicon is suboptimal.", 
        "200": "This is because the general sentiment lexicons cannot capture the domain-specific sentiment expressions in target domain (Choi and Cardie, 2009).", 
        "201": "Lexicon+SentiSim performs significantly better than Lexicon, which validates that the sentiment similarities among words extracted from unlabeled samples of target domain contain rich domain-specific sentiment information, and can help propagate the general sentiment information to many domain-specific sentiment expressions.", 
        "202": "Besides, after incorporating a small number of labeled samples which are actively selected and annotated by our approach in an active learning mode, the performance of our sentiment dom-\nain adaptation approach is significantly improved.", 
        "203": "This is because although these labeled samples are in limited size and cannot cover all the sentiment expressions in target domain, they can provide sentiment information of popular domain-specific sentiment expressions, which can be propagated to other sentiment expressions in target domain during the domain adaptation process.", 
        "204": "Thus, above experimental results validate the effectiveness of our approach.", 
        "205": "We also conducted several experiments to verify the advantage of the actively selected samples over randomly selected samples and validate the effectiveness of our active learning algorithm.", 
        "206": "We also compared the dynamic weighting scheme for combining uncertainty and density with the constant weighting scheme.", 
        "207": "The experimental results are summarized in Fig.", 
        "208": "2.", 
        "209": "According to Fig.", 
        "210": "2, our approach with actively selected samples performs better than that with randomly selected samples.", 
        "211": "It indicates that these actively selected samples are more informative than randomly selected samples for sentiment domain adaptation.", 
        "212": "In addition, our approach with dynamic weighting scheme in combining uncertainty and density outperforms that with constant weighting scheme, which implies that it is beneficial to emphasize representative samples at initial iterations and gradually focus on difficult samples at later iterations.", 
        "213": "Thus, the experimental results validate the effectiveness of our active learning algorithm.", 
        "214": "4.3 Performance Evaluation  In this section we conducted experiments to evaluate the performance of our approach by comparing it with several baseline methods.", 
        "215": "The methods to be compared include: 1) MPQA and BingLiu, using two state-of-the-art sentiment lexicons,\ni.e., MPQA (Wilson et al., 2005) and Bing Liu\u2019s lexicon (Hu and Liu, 2004) for sentiment classification following the suggestions in (Hu and Liu, 2004); 2) SVM, LS, and LR, three popular supervised sentiment classification methods, i.e., support vector machine (Pang et al., 2002), least squares (Hu et al., 2013) and logistic regression (Wu et al., 2015); 3) ZIAL, the zero initialized active learning method (Cesa-Bianchi et al., 2006); 4) LIAL, the active learning method initialized by randomly selected labeled data (Settles, 2010); 5) SCL and SFA, two famous sentiment domain adaptation methods proposed in (Blitzer et al., 2007) and (Pan et al., 2010) respectively; 6) ILP, adapting sentiment lexicons to target domain via integer linear programming (Choi and Cardie, 2009); 7) AODA, the active online domain adaptation method (Rai et al., 2010); 8) ALCD, the active learning method for cross-domain sentiment classification (Li et al., 2013); 9) ASDA, our active sentiment domain adaptation approach.", 
        "216": "For above methods, if labeled target domain samples are needed in training, the number of labeled samples was set to 100, and if source domain labeled samples are needed in training, the number of labeled samples was set to 1,000.", 
        "217": "The parameters in baseline methods were tuned via cross-validation.", 
        "218": "The experimental results are summarized in Table 2.", 
        "219": "According to Table 2, the performance of directly applying sentiment lexicons to target domain is suboptimal.", 
        "220": "This is because there are many domain-specific sentiment expressions that are not covered by these general-purpose sentiment lexicons (Choi and Cardie, 2009).", 
        "221": "In addition, the performance of supervised sentiment classification methods such as SVM, LS, and LR is also\nlimited, because the labeled samples for training are extremely scarce.", 
        "222": "The active learning methods such as ZIAL (Cesa-Bianchi et al., 2006) and LIAL (Settles, 2010) perform relatively better, because they can actively select informative samples to annotate and learn.", 
        "223": "Our approach can outperform both of them.", 
        "224": "This is because besides the labeled samples, our approach also adapts the general sentiment information in sentiment lexicons to target domain and incorporates it into the learning of target domain sentiment classifier.", 
        "225": "Our approach also performs better than state-of-the-art domain adaptation methods such as SCL (Blitzer et al., 2007) and SFA (Pan et al., 2010).", 
        "226": "It implies that a small number of actively selected labeled samples from target domain are beneficial for sentiment domain adaptation.", 
        "227": "ILP (Choi and Cardie, 2009) tries to adapt a sentiment lexicon to target domain, which is similar with our approach.", 
        "228": "ILP relies on labeled samples to extract the relations among words and relations between words and sentiment expressions.", 
        "229": "However, labeled samples in target domain are usually limited and the sentiment information in many unlabeled samples is not exploited in ILP.", 
        "230": "Thus, our approach can outperform it.", 
        "231": "Similar with our approach, AODA (Rai et al., 2010) and ALCD (Li et al., 2013) also apply active learning to domain adaptation.", 
        "232": "The major difference is that in our approach the general sentiment information extracted from sentiment lexicons is adapted to target domain, while in AODA and ALCD the sentiment classifier trained in source domains is transferred.", 
        "233": "The superior performance of our approach implies that the general sentiment information has better generalization ability than the sentiment classifier trained in a specific source domain, and is more suitable for sentiment domain adaptation.", 
        "234": "We further conducted several experiments to validate the advantage of our approach in training accurate sentiment classifier for target domain with only a few labeled samples.", 
        "235": "We varied the annotation budget, i.e., the number of labeled samples, from 100 to 1,000.", 
        "236": "The learning curve of our ASDA approach in Book domain is shown in Fig.", 
        "237": "3.", 
        "238": "We also included a purely supervised sentiment classification method, i.e., SVM, in Fig.", 
        "239": "3 as a baseline for comparison.", 
        "240": "Fig.", 
        "241": "3 shows that our ASDA approach can consistently outperform SVM when the same number of labeled samples are used.", 
        "242": "The performance advantage of our approach is more significant when labeled samples are scarce.", 
        "243": "For example, the performance of our approach with only 200 labeled samples is similar to SVM with more than 800 labeled samples.", 
        "244": "Thus, the experimental results validate that by adapting the general sentiment information to target domain and selecting the most informative samples to annotate and learn, our approach can effectively reduce the manual annotation effort, and can train accurate sentiment classifier for target domain with much less labeled samples.", 
        "245": "4.4 Parameter Analysis  In this section, we conducted several experiments to explore the influence of parameter settings on the performance of our approach.", 
        "246": "\u03b1 and \u03b2 are the two most important parameters in our approach, which control the relative importance of domainspecific sentiment similarities and the actively selected samples in training sentiment classifier for target domain.", 
        "247": "The experimental results of parameters \u03b1 and \u03b2 are summarized in Fig.", 
        "248": "4.", 
        "249": "According to Fig.", 
        "250": "4, when \u03b1 and \u03b2 are too small, the performance of our approach is not optimal.", 
        "251": "This is because the useful sentiment information in domain-specific sentiment similarities mined from unlabeled samples and the actively\nselected labeled samples of target domain is not fully exploited.", 
        "252": "Thus, the performance of our approach improves when these parameters increase from a small value.", 
        "253": "However, when these parameters become too large, the performance of our approach starts to decline.", 
        "254": "This is because when \u03b2 is too large the sentiment classifier learned by our approach is mainly decided by the limited labeled samples, and the general sentiment information extracted from sentiment lexicons is not fully exploited.", 
        "255": "When \u03b1 is too large, the information in domain-specific sentiment similarities is overemphasized, and many different words will have nearly the same sentiment weights.", 
        "256": "Thus, the performance of our approach in these scenarios is also not optimal.", 
        "257": "A moderate value of \u03b1 and \u03b2 is most suitable for our approach.", 
        "258": "5 Conclusion  In this paper we present an active sentiment domain adaptation approach to train accurate sentiment classifier for target domain with less labeled samples.", 
        "259": "In our approach, the general sentiment information in sentiment lexicons is adapted to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode.", 
        "260": "Both classification uncertainty and density are considered when selecting informative samples to label.", 
        "261": "In addition, we extract domain-specific sentiment similarities among words from unlabeled samples of target domain based on both syntactic rules and cooccurrence patterns, and incorporate them into the domain adaptation process to propagate the general sentiment information to many domain-specific sentiment words in target domain.", 
        "262": "We also propose a unified model to incorporate different types of sentiment information to train sentiment classifier for target domain.", 
        "263": "Experimental results on benchmark datasets show that our approach can train accurate sentiment classifier and at same time reduce the manual annotation effort.", 
        "264": "Acknowledgements  This research is supported by the Key Research Project of the Ministry of Science and Technology of China (Grant no.", 
        "265": "2016YFB0800402) and the Key Program of National Natural Science Foundation of China (Grant nos.", 
        "266": "U1536201, U1536207, and U1405254)."
    }, 
    "document_id": "P17-1156.pdf.json"
}
