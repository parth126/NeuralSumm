{
    "abstract_sentences": {
        "1": "In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of RNN.", 
        "2": "In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales.", 
        "3": "Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated.", 
        "4": "In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged.", 
        "5": "Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580\u2013586 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2092\nIn typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of RNN.", 
        "2": "In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales.", 
        "3": "Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated.", 
        "4": "In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged.", 
        "5": "Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", 
        "6": "1 Introduction  Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005).", 
        "7": "This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015).", 
        "8": "\u2217Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah\u2019s Ark Lab.", 
        "9": "The decoder state stores translation information at different granularities, determining which segment should be expressed (phrasal), and which word should be generated (lexical), respectively.", 
        "10": "However, due to the extensive existence of multiword phrases and expressions, the varying speed of the lexical component is much faster than the phrasal one.", 
        "11": "As in the generation of \u201cthe French Republic\u201d, the lexical component in the decoder will change thrice, each of which for a separate word.", 
        "12": "But the phrasal component may only change once.", 
        "13": "The inconsistent varying speed of the two components may cause translation errors.", 
        "14": "Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation.", 
        "15": "Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-jussa\u0300 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016).", 
        "16": "However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007).", 
        "17": "We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling).", 
        "18": "At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated with-\n1In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words.", 
        "19": "580\nout attention.", 
        "20": "The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary gate automatically.", 
        "21": "In this way, we incorporate soft phrases into NMT, which makes the model flexible at capturing both global reordering of phrases and local translation inside phrases.", 
        "22": "Our model has following benefits:\n1.", 
        "23": "The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible.", 
        "24": "2.", 
        "25": "Our model recognizes phrase structures explicitly.", 
        "26": "Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words.", 
        "27": "3.", 
        "28": "Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars.", 
        "29": "4.", 
        "30": "Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart.", 
        "31": "Experiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously.", 
        "32": "2 Standard Neural Machine Translation Model  Generally, neural machine translation system directly models the conditional probability of the translation y word by word (Bahdanau et al., 2014).", 
        "33": "Formally, given an input sequence x = [x1, x2, .", 
        "34": ".", 
        "35": ".", 
        "36": ", xJ ], and the previously generated sequence y<t = [y1, y2, .", 
        "37": ".", 
        "38": ".", 
        "39": ", yt\u22121], the probability of next target word yt is\nP (yt|x) = softmax(f(eyt\u22121 , st, ct)) (1) where f(\u00b7) is a non-linear function, eyt\u22121 is the embedding of yt\u22121; st is the decode state at the time step t, which is computed by\nst = g(st\u22121, eyt\u22121 , ct) (2)\nHere g(\u00b7) is a transition function of decoder RNN.", 
        "40": "ct is the context vector computed by\nct = J\u2211\nj=1\nATT(st\u22121, hj) \u00b7 hj = J\u2211\nj=1\n\u03b1t,j \u00b7 hj (3)\nwhere ATT is an attention operation, which outputs alignment distribution \u03b1:\n\u03b1t,j = exp(et,j)\u2211Tx\nk=1 exp(et,k) (4)\net,j = v T a tanh(Wast\u22121 + Uahj) (5)\nand h is the annotation of x from a bi-directional RNNs.", 
        "41": "The training objective is to maximize the likelihood of the training data.", 
        "42": "Beam search is adopted for decoding.", 
        "43": "3 Chunk-Based Bi-Scale Neural Machine Translation Model  Instead of the word-based decoder, we propose to use a chunk-based bi-scale decoder, which generates translation hierarchically with chunk and word time-scales, as shown in Figure 1.", 
        "44": "Intuitively, we firstly generate a chunk state with the attention model, which extracts the source context for the current phrasal scope.", 
        "45": "Then we generate multiple lexical words based on the same chunk state, which does not require attention operations.", 
        "46": "The boundary of a chunk is determined by a boundary gate, which decides whether to update the chunk state or not at each step.", 
        "47": "Formally, the probability of next word yt is\nP (yt|x) = softmax(f(eyt\u22121 , st, pt)) (6) st = g(st\u22121, eyt\u22121 , pt) (7)\nhere pt is the chunk state at step t. Compared with Equations 1 and 2, the generation of target word is based on the chunk state instead of the context vector ct produced by the attention model.", 
        "48": "Since a chunk may correspond to multiple words, we employ a boundary gate bt to decide the boundary of each chunk:\np(bt) = softmax(st\u22121, eyt\u22121) (8)\nbt will be 0 or 1, where 1 denotes this is the boundary of a new chunk while 0 denotes not.", 
        "49": "Two different operations would be executed:\npt = { pt\u22121, bt = 0 (COPY) g(pt\u22121, ept\u22121 , pct), bt = 1 (UPDATE)\nIn the COPY operation, the chunk state is kept the same as the previous step.", 
        "50": "In the UPDATE operation, ept\u22121 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016):\nept\u22121 = m(st\u22121, eyt\u22121)\u2212m(st\u2032 , eyt\u2032 ) (9)\nhere t\u2032 is the boundary of last chunk and m(\u00b7) is a linear function.", 
        "51": "pct is the context vector for chunk pt, which is calculated by a chunk attention model:\npct =\nTs\u2211\nj=1\nATT(pt\u22121, hj) \u00b7 hj (10)\nThe chunk attention model differs from the standard word attention model (i.e., Equation 3) at: 1) it reads chunk state pt\u22121 rather than word state st\u22121, and 2) it is only executed at boundary of each chunk rather than at each decoding step.", 
        "52": "In this way, our model only extracts source context once for a chunk, and the words in one chunk will share the same context for word generation.", 
        "53": "The chunk attention mechanism adds a constrain that target words in the same chunk shares the same source context.", 
        "54": "Training To encourage the proposed model to learn reasonable chunk state, we add two additional objectives in training: Chunk Tag Prediction: For each chunk, we predict the probability of its tag P (lk|x) = softmax ( f(pt, ept , ct) ) , where lk is the syntactic tag of the k-th chunk such as NP (noun phrase) and VP (verb phrase), and t is time step of its boundary.", 
        "55": "Chunk Boundary Prediction: At each decoding step, we predict the probability of chunk boundary P (bt|x) = softmax(st\u22121, eyt\u22121).", 
        "56": "Accordingly, given a set of training examples {[xn,yn]}Nn=1, the new training objective is\nJ(\u03b8, \u03b3) = arg max\nN\u2211\nn=1\n{ log P (yn|xn)\n+ log P (ln|xn) + log P (bn|xn) } (11)\nwhere ln and bn are chunk tag sequence and boundary sequence on yn, respectively.", 
        "57": "4 Experiments  We carry out experiments on a Chinese-English translation task.", 
        "58": "Our training data consists of 1.16M2 sentence pairs extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively.", 
        "59": "We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets.", 
        "60": "We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set.", 
        "61": "The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al.", 
        "62": "(2015).", 
        "63": "We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002).", 
        "64": "In training, we limit the source and target vocabularies to the most frequent 30K words.", 
        "65": "We train each model with the sentences of length up to 50 words.", 
        "66": "Sizes of the chunk representation and chunk hidden state are set to 1000.", 
        "67": "All the other settings are the same as in Bahdanau et al.", 
        "68": "(2014).", 
        "69": "4.1 Results on Chinese-English  We list the BLEU score of our proposed model in Table 1, comparing with Moses (Koehn et al., 2007) and dl4mt3 (Bahdanau et al., 2014), which are state-of-the-art models of SMT and NMT, respective.", 
        "70": "For Moses, we use the default configuration with a 4-gram language model trained on the target portion of the training data.", 
        "71": "For dl4mt, we also report the results (dl4mt-2) by using two\n23LDC2002E18, LDC2003E14, the Hansards portion of LDC2004T08, and LDC2005T06.", 
        "72": "3https://github.com/nyu-dl/ dl4mt-tutorial\ndecoder layers (Wu et al., 2016) for better comparison.", 
        "73": "As shown in Table 1, our proposed model outperforms different baselines on all sets, which verifies that the chunk-based bi-scale decoder is effective for NMT.", 
        "74": "Our model gives a 1.6 BLEU score improvement upon the standard NMT baseline (dl4mt).", 
        "75": "We conduct experiment with dl4mt-2 to see whether the neural NMT system can model the bi-scale components with different varying speeds automatically.", 
        "76": "Surprisingly, we find that dl4mt-2 obtains lower BLEU scores than dl4mt.", 
        "77": "We speculate that the more complex model dl4mt2 may need more training data for obtaining reasonable results.", 
        "78": "Effectiveness of Chunk Attention As described in Section 3, we propose to use the chunk attention to replace the word level attention in our model, in which the source context extracted by the chunk attention will be used for the corresponding word generations in the chunk.", 
        "79": "We also report the result of our model using conventional word attention for comparison.", 
        "80": "As shown in Table 2, our model with the chunk attention gives higher BLEU score than the word attention.", 
        "81": "Intuitively, we think chunks are more specific in semantics, thus could extract more specific source context for translation.", 
        "82": "The chunk attention could be considered as a compromise approach between encoding the whole source sentence into decoder without attention (Sutskever et al., 2014) and utilizing word level attention at each step (Bahdanau et al., 2014).", 
        "83": "We also draw the figure of alignments by chunk attention (Figure 2), from which we can see that our chunk attention model can well explore the alignments from phrases to words.", 
        "84": "Predictions of the Chunk Boundary and Chunk Label We also compute predicted accuracies of chunk boundaries and chunk labels on the autochunked development and testing data (Table 3).", 
        "85": "We find that the chunk boundary could be predicted well, with an average accuracy of 89%, which shows that our model could capture the phrasal boundary information in the translation process.", 
        "86": "However, our model could not predict chunk labels as well as chunk boundaries.", 
        "87": "We speculate that more syntactic context features should be added to improve the performance of predicting chunk labels.", 
        "88": "Subjective Evaluation Following Tu et al.", 
        "89": "(2016, 2017a,b), we also compare our model with the dl4mt baseline by subjective evaluation.", 
        "90": "Two human evaluators are asked to evaluate the translations of 100 source sentences randomly sampled from the test sets without knowing which system\nthe translation is translated by.", 
        "91": "The human evaluator is asked to give 4 scores: adequacy score and fluency score, which are between 0 and 5, the larger, the better; under-translation score and overtranslation score, which are set to 1 when under or over translation errors occurs, otherwise set to 0.", 
        "92": "We list the averaged scores in Table 5.", 
        "93": "We find that our proposed model improves the dl4mt baseline on both the translation adequacy and fluency aspects.", 
        "94": "Specifically, the over translation error rate drops by 6%, which confirms the assumption in the introduction that splitting the fast and slow varying components in different time-scales could help alleviate the over translation errors.", 
        "95": "4.2 Results on German-English  We evaluate our model on the WMT15 translation task from German to English.", 
        "96": "We find that our proposed chunk-based NMT model also obtains considerable accuracy improvements on GermanEnglish.", 
        "97": "However, the BLEU score gains are not as significant as on Chinese-English.", 
        "98": "We speculate that the difference between Chinese and English is larger than German and English.", 
        "99": "The chunk-based NMT model may be more useful for bilingual data with bigger difference.", 
        "100": "5 Related Work  NMT with Various Granularities.", 
        "101": "A line of previous work propose to utilize other granularities besides words for NMT.", 
        "102": "By further exploiting the character level (Ling et al., 2015; Costajussa\u0300 and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016), or the sub-word level (Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc\u0131\u0301a-Mart\u0131\u0301nez et al., 2016) information, the corresponding NMT models capture the infor-\nmation inside the word and alleviate the problem of unknown words.", 
        "103": "While most of them focus on decomposing words into characters or sub-words, our work aims at composing words into phrases.", 
        "104": "Incorporating Syntactic Information in NMT Syntactic information has been widely used in SMT (Liu et al., 2006; Marton and Resnik, 2008; Shen et al., 2008), and a lot of previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016).", 
        "105": "Shi et al.", 
        "106": "(2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly.", 
        "107": "Luong et al.", 
        "108": "(2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results.", 
        "109": "Eriguchi et al.", 
        "110": "(2016) propose a string-totree NMT system by end-to-end training.", 
        "111": "Different to previous work, we try to incorporate the syntactic information in the target side of NMT.", 
        "112": "Ishiwatari et al.", 
        "113": "(2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages.", 
        "114": "Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate.", 
        "115": "6 Conclusion  We propose a chunk-based bi-scale decoder for neural machine translation, in which way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged.", 
        "116": "Experiments show that our proposed model outperforms the standard attention-based neural machine translation baseline.", 
        "117": "Future work includes abandoning labeled chunk data, adopting reinforcement learning to explore the boundaries of phrase automatically (Mou et al., 2016).", 
        "118": "Our code is released on https: //github.com/zhouh/chunk-nmt.", 
        "119": "Acknowledge\nWe would like to thank the anonymous reviewers for their insightful comments.", 
        "120": "We also thank Lili Mou for helpful discussion and Hongjie Ji, Zhenting Yu, Xiaoxue Hou and Wei Zou for their help in data preparation and subjective evaluation.", 
        "121": "This work was partially founded by the Natural Science Foundation of China (61672277, 71503124) and the China National 973 project 2014CB340301."
    }, 
    "document_id": "P17-2092.pdf.json"
}
