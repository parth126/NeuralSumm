{
    "abstract_sentences": {
        "1": "Chunks (or phrases) once played a pivotal role in machine translation.", 
        "2": "By using a chunk rather than a word as the basic translation unit, local (intrachunk) and global (inter-chunk) word orders and dependencies can be easily modeled.", 
        "3": "The chunk structure, despite its importance, has not been considered in the decoders used for neural machine translation (NMT).", 
        "4": "In this paper, we propose chunk-based decoders for NMT, each of which consists of a chunk-level decoder and a word-level decoder.", 
        "5": "The chunklevel decoder models global dependencies while the word-level decoder decides the local word order in a chunk.", 
        "6": "To output a target sentence, the chunk-level decoder generates a chunk representation containing global information, which the wordlevel decoder then uses as a basis to predict the words inside the chunk.", 
        "7": "Experimental results show that our proposed decoders can significantly improve translation performance in a WAT \u201916 Englishto-Japanese translation task."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1901\u20131912 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1174  1 Introduction  Neural machine translation (NMT) performs endto-end translation based on a simple encoderdecoder model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b) and has now overtaken the classical, complex statistical machine translation (SMT) in terms of performance and simplicity (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016).", 
        "2": "In NMT, an encoder first maps a source sequence into vector representations and \u2217Contribution during internship at Microsoft Research.", 
        "3": "!", 
        "4": "\"# $ %&\"'()**'+ ,+**-+.", 
        "5": "(*(&/01(/2\na decoder then maps the vectors into a target sequence (\u00a7 2).", 
        "6": "This simple framework allows researchers to incorporate the structure of the source sentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b).", 
        "7": "Most of the NMT models, however, still rely on a sequential decoder based on a recurrent neural network (RNN) due to the difficulty in capturing the structure of a target sentence that is unseen during translation.", 
        "8": "With the sequential decoder, however, there are two problems to be solved.", 
        "9": "First, it is difficult to model long-distance dependencies (Bahdanau et al., 2015).", 
        "10": "A hidden state ht in an RNN is only conditioned by its previous output yt\u22121, previous hidden state ht\u22121, and current input xt.", 
        "11": "This makes it difficult to capture the dependencies between an older output yt\u2212N if they are too far from the current output.", 
        "12": "This problem can become more serious when the target sequence becomes longer.", 
        "13": "For example, in Figure 1, when we translate the English sentence into the Japanese one, after the decoder predicts the content word \u201c\u5e30\u3063 (go back)\u201d, it has to predict four function words \u201c\u3066 (suffix)\u201d, \u201c\u3057\u307e\u3044 (perfect tense)\u201d, \u201c\u305f\u3044 (desire)\u201d, and \u201c\u3068 (to)\u201d before predicting the next content word \u201c\u601d\u3063 (feel)\u201d.", 
        "14": "In such a case, the decoder is required to capture the longer dependencies in a target sentence.", 
        "15": "Another problem with the sequential decoder is that it is expected to cover multiple possible word orders simply by memorizing the local word se-\n1901\nquences in the limited training data.", 
        "16": "This problem can be more serious in free word-order languages such as Czech, German, Japanese, and Turkish.", 
        "17": "In the case of the example in Figure 1, the order of the phrase \u201c\u65e9\u304f (early)\u201d and the phrase \u201c\u5bb6\u3078 (to home)\u201d is flexible.", 
        "18": "This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order.", 
        "19": "In the past, chunks (or phrases) were utilized to handle the above problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010).", 
        "20": "By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence.", 
        "21": "This makes it easy to capture the longer dependencies in a target sentence.", 
        "22": "The order of words in a chunk is relatively fixed while that in a sentence is much more flexible.", 
        "23": "Thus, modeling intra-chunk (local) word orders and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages.", 
        "24": "In this paper, we refine the original RNN decoder to consider chunk information in NMT.", 
        "25": "We propose three novel NMT models that capture and utilize the chunk structure in the target language (\u00a7 3).", 
        "26": "Our focus is the hierarchical structure of a sentence: each sentence consists of chunks, and each chunk consists of words.", 
        "27": "To encourage an NMT model to capture the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1).", 
        "28": "Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2).", 
        "29": "Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3).", 
        "30": "We evaluate the three models on the WAT \u201916 English-to-Japanese translation task (\u00a7 4).", 
        "31": "The experimental results show that our best model outperforms the best single NMT model reported in WAT \u201916 (Eriguchi et al., 2016b).", 
        "32": "Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework.", 
        "33": "2 Preliminaries: Attention-based Neural Machine Translation  In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models.", 
        "34": "2.1 Neural Machine Translation  An NMT model usually consists of two connected neural networks: an encoder and a decoder.", 
        "35": "After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence.", 
        "36": "The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b).", 
        "37": "While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder.", 
        "38": "Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper.", 
        "39": "A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi\u22121:\nhi = GRU(hi\u22121,xi).", 
        "40": "(1)\nThe function GRU(\u00b7) is calculated as\nri = \u03c3(Wrxi +Urhi\u22121 + br), (2)\nzi = \u03c3(Wzxi +Uzhi\u22121 + bz), (3)\nh\u0303i = tanh(Wxi +U(ri hi\u22121 + b)), (4) hi = (1\u2212 zi) h\u0303i + zi hi\u22121, (5)\nwhere vectors ri and zi are reset gate and update gate, respectively.", 
        "41": "While the former gate allows the model to forget the previous states, the latter gate decides how much the model updates its content.", 
        "42": "All the W s and Us, or the bs above are trainable matrices or vectors.", 
        "43": "\u03c3(\u00b7) and denote the sigmoid function and element-wise multiplication operator, respectively.", 
        "44": "In this simple model, we train a GRU function that encodes a source sentence {x1, \u00b7 \u00b7 \u00b7 , xI} into a single vector hI .", 
        "45": "At the same time, we jointly train another GRU function that decodes hI to the target sentence {y1, \u00b7 \u00b7 \u00b7 , yJ}.", 
        "46": "Here, the j-th word in the\ntarget sentence yj can be predicted with this decoder GRU and a nonlinear function g(\u00b7) followed by a softmax layer, as\nc = hI , (6)\nsj = GRU(sj\u22121, [yj\u22121; c]), (7)\ns\u0303j = g(yj\u22121, sj , c), (8) P (yj |y<j ,x) = softmax(s\u0303j), (9)\nwhere c is a context vector of the encoded sentence and sj is a hidden state of the decoder GRU.", 
        "47": "Following Bahdanau et al.", 
        "48": "(2015), we use a mini-batch stochastic gradient descent (SGD) algorithm with ADADELTA (Zeiler, 2012) to train the above two GRU functions (i.e., the encoder and the decoder) jointly.", 
        "49": "The objective is to minimize the cross-entropy loss of the training data D, as\nJ = \u2211\n(x,y)\u2208D \u2212 logP (y|x).", 
        "50": "(10)  2.2 Attention Mechanism for Neural Machine Translation  To use all the hidden states of the encoder and improve the translation performance of long sentences, Bahdanau et al.", 
        "51": "(2015) proposed using an attention mechanism.", 
        "52": "In the attention model, the context vector is not simply the last encoder state hI but rather the weighted sum of all hidden states of the bidirectional GRU, as follows:\ncj = I\u2211\ni=1\n\u03b1jihi.", 
        "53": "(11)\nHere, the weight \u03b1ji decides how much a source word xi contributes to the target word yj .", 
        "54": "\u03b1ji is computed by a feedforward layer and a softmax layer as\neji = v \u00b7 tanh(Wehi +Uesj + be), (12)\n\u03b1ji = exp(eji)\u2211J\nj\u2032=1 exp(ej\u2032i) , (13)\nwhere We, Ue are trainable matrices and the v, be are trainable vectors.1 In a decoder using the attention mechanism, the obtained context vector cj in each time step replaces cs in Eqs.", 
        "55": "(7) and (8).", 
        "56": "An illustration of the NMT model with the attention mechanism is shown in Figure 2.", 
        "57": "The attention mechanism is expected to learn alignments between source and target words, and plays a similar role to the translation model in phrase-based SMT (Koehn et al., 2003).", 
        "58": "3 Neural Machine Translation with Chunk-based Decoder  Taking non-sequential information such as chunks (or phrases) structure into consideration has proved helpful for SMT (Watanabe et al., 2003; Koehn et al., 2003) and EBMT (Kim et al., 2010).", 
        "59": "Here, we focus on two important properties of chunks (Abney, 1991): (1) The word order in a chunk is almost always fixed, and (2) A chunk consists of a few (typically one) content words surrounded by zero or more function words.", 
        "60": "To fully utilize the above properties of a chunk, we propose modeling the intra-chunk and the inter-chunk dependencies independently with a \u201cchunk-by-chunk\u201d decoder (See Figure 3).", 
        "61": "In the standard word-by-word decoder described in \u00a7 2, a target word yj in the target sentence y is predicted by taking the previous outputs y<j and the source sentence x as input:\nP (y|x) = J\u220f\nj=1\nP (yj |y<j ,x), (14)\nwhere J is the length of the target sentence.", 
        "62": "Not 1We choose this implementation following (Luong et al., 2015b), while (Bahdanau et al., 2015) use sj\u22121 instead of sj in Eq.", 
        "63": "(12).", 
        "64": "!", 
        "65": "\"#$%& '(&)*+$,-./0*1& .", 
        "66": "\"**$2+3\"*& 4\u00a756'7\nassuming any structural information of the target language, the sequential decoder has to memorize long dependencies in a sequence.", 
        "67": "To release the model from the pressure of memorizing the long dependencies over a sentence, we redefine this problem as the combination of a word prediction problem and a chunk generation problem:\nP (y|x) = K\u220f\nk=1\n  P (ck|c<k,x) Jk\u220f\nj=1\nP (yj |y<j , ck,x)    ,\n(15) whereK is the number of chunks in the target sentence and Jk is the length of the k-th chunk (see Figure 3).", 
        "68": "The first term represents the generation probability of a chunk ck and the second term indicates the probability of a word yj in the chunk.", 
        "69": "We model the former term as a chunk-level decoder and the latter term as a word-level decoder.", 
        "70": "As demonstrated later in \u00a7 4, both K and Jk are much shorter than the sentence length J , which is why our decoders do not have to capture the long dependencies like the standard decoder does.", 
        "71": "In the above formulation, we model the information of words and their orders in a chunk.", 
        "72": "No matter which language we target, we can assume that a chunk usually consists of some content words and function words, and the word order in the chunk is almost always fixed (Abney, 1991).", 
        "73": "Although our idea can be used in several languages, the optimal network architecture could depend on the word order of the target language.", 
        "74": "In this work, we design models for lan-\nguages in which content words are followed by function words, such as Japanese and Korean.", 
        "75": "The details of our models are described in the following sections.", 
        "76": "3.1 Model 1: Basic Chunk-based Decoder  The model described in this section is the basis of our proposed decoders.", 
        "77": "It consists of two parts: a chunk-level decoder (\u00a7 3.1.1) and a word-level decoder (\u00a7 3.1.2).", 
        "78": "The part drawn in black solid lines in Figure 4 illustrates the architecture of Model 1.", 
        "79": "3.1.1 Chunk-level Decoder  Our chunk-level decoder (see Figure 3) outputs a chunk representation.", 
        "80": "The chunk representation contains the information about words that should be predicted by the word-level decoder.", 
        "81": "To generate the representation of the k-th chunk s\u0303 (c) k , the chunk-level decoder (see the bottom layer in Figure 4) takes the last states of the word-level decoder s(w)k\u22121,Jk\u22121 and updates its hidden state s (c) k as:\ns (c) k = GRU(s (c) k\u22121, s (w) k\u22121,Jk\u22121), (16) s\u0303 (c) k = Wcs (c) k + bc.", 
        "82": "(17)\nThe obtained chunk representation s\u0303(c)k continues to be fed into the word-level decoder until it outputs all the words in the current chunk.", 
        "83": "3.1.2 Word-level Decoder  Our word-level decoder (see Figure 4) differs from the standard sequential decoder described in \u00a7 2 in\nthat it takes the chunk representation s\u0303(c)k as input:\ns (w) k,j = GRU(s (w) k,j\u22121, [s\u0303 (c) k ;yk,j\u22121; ck,j\u22121]), (18)\ns\u0303 (w) k,j = g(yk,j\u22121, s (w) k,j , ck,j), (19)\nP (yk,j |y<j ,x) = softmax(s\u0303(w)k,j ).", 
        "84": "(20)\nIn a standard sequential decoder, the hidden state iterates over the length of a target sentence and then generates an end-of-sentence token.", 
        "85": "In other words, its hidden layers are required to memorize the long-term dependencies and orders in the target language.", 
        "86": "In contrast, in our word-level decoder, the hidden state iterates only over the length of a chunk and then generates an end-of-chunk token.", 
        "87": "Thus, our word-level decoder is released from the pressure of memorizing the long (interchunk) dependencies and can focus on learning the short (intra-chunk) dependencies.", 
        "88": "3.2 Model 2: Inter-Chunk Connection  The second term in Eq.", 
        "89": "(15) only iterates over one chunk (j = 1 to Jk).", 
        "90": "This means that the last state and the last output of a chunk are not being fed into the word-level decoder at the next time step (see the black part in Figure 4).", 
        "91": "In other words, s (w) k,1 in Eq.", 
        "92": "(18) is always initialized before generating the first word in a chunk.", 
        "93": "This may have a bad influence on the word-level decoder because it cannot access any previous information at the first word of each chunk.", 
        "94": "To address this problem, we add new connections to Model 1 between the first state in a chunk and the last state in the previous chunk, as\ns (w) k,1 = GRU(s (w) k\u22121,Jk\u22121 , [s\u0303 (c) k ;yk\u22121,Jk\u22121 ; ck\u22121,Jk\u22121 ]).", 
        "95": "(21) The dashed blue arrows in Figure 4 illustrate the\nadded inter-chunk connections.", 
        "96": "3.3 Model 3: Word-to-Chunk Feedback  The chunk-level decoder in Eq.", 
        "97": "(16) is only conditioned by s(w)k\u22121,Jk\u22121 , the last word state in each chunk (see the black part in Figure 4).", 
        "98": "This may affect the chunk-level decoder because it cannot memorize what kind of information has already been generated by the word-level decoder.", 
        "99": "The information about the words in a chunk should not be included in the representation of the next chunk; otherwise, it may generate the same chunks multiple times, or forget to translate some words in the source sentence.", 
        "100": "To encourage the chunk-level decoder to memorize the information about the previous outputs more carefully, we add feedback states to our chunk-level decoder in Model 2.", 
        "101": "The feedback state in the chunk-level decoder is updated at every time step j(> 1) in k-th chunk, as\ns (c) k,j = GRU(s (c) k,j\u22121, s (w) k,j ).", 
        "102": "(22)\nThe red part in Figure 4 illustrate the added feedback states and their connections.", 
        "103": "The connections in the thick black arrows are replaced with the dotted red arrows in Model 3.", 
        "104": "4 Experiments    4.1 Setup  Data To examine the effectiveness of our decoders, we chose Japanese, a free word-order language, as the target language.", 
        "105": "Japanese sentences are easy to break into well-defined chunks (called bunsetsus (Hashimoto, 1934) in Japanese).", 
        "106": "For example, the accuracy of bunsetsu-chunking on newspaper articles is reported to be over 99% (Murata et al., 2000; Yoshinaga and Kitsuregawa, 2014).", 
        "107": "The effect of chunking errors in training the decoder can be suppressed, which means we can accurately evaluate the potential of our method.", 
        "108": "We used the English-Japanese training corpus in the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016), which was provided in WAT \u201916.", 
        "109": "To remove inaccurate translation pairs, we extracted the first two million out of the 3 million pairs following the setting that gave the best performances in WAT \u201915 (Neubig et al., 2015).", 
        "110": "Preprocessings For Japanese sentences, we performed tokenization using KyTea 0.4.72 (Neubig et al., 2011).", 
        "111": "Then we performed bunsetsuchunking with J.DepP 2015.10.053 (Yoshinaga and Kitsuregawa, 2009, 2010, 2014).", 
        "112": "Special endof-chunk tokens were inserted at the end of the chunks.", 
        "113": "Our word-level decoders described in \u00a7 3 will stop generating words after each endof-chunk token.", 
        "114": "For English sentences, we performed the same preprocessings described on the WAT \u201916 Website.4 To suppress having possible\n2http://www.phontron.com/kytea/ 3http://www.tkl.iis.u-tokyo.ac.jp/\n\u02dcynaga/jdepp/ 4http://lotus.kuee.kyoto-u.ac.jp/WAT/ baseline/dataPreparationJE.html\nchunking errors affect the translation quality, we removed extremely long chunks from the training data.", 
        "115": "Specifically, among the 2 million preprocessed translation pairs, we excluded sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sentence is larger than 64 (3% of whole data); (2) The maximum length of a chunk in the target sentence is larger than 8 (14% of whole data); and (3) The maximum number of chunks in the target sentence is larger than 20 (3% of whole data).", 
        "116": "Table 1 shows the details of the extracted data.", 
        "117": "Postprocessing To perform unknown word replacement (Luong et al., 2015a), we built a bilingual English-Japanese dictionary from all of the three million translation pairs.", 
        "118": "The dictionary was extracted with the MGIZA++ 0.7.05 (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.", 
        "119": "Model Architecture Any encoder can be combined with our decoders.", 
        "120": "In this work, we adopted a single-layer bidirectional GRU (Cho et al., 2014b; Bahdanau et al., 2015) as the encoder to focus on confirming the impact of the proposed decoders.", 
        "121": "We used single layer GRUs for the wordlevel decoder and the chunk-level decoder.", 
        "122": "The vocabulary sizes were set to 40k for source side and 30k for target side, respectively.", 
        "123": "The conditional probability of each target word was computed with a deep-output (Pascanu et al., 2014) layer with maxout (Goodfellow et al., 2013) units following (Bahdanau et al., 2015).", 
        "124": "The maximum number of output chunks was set to 20 and the maximum length of a chunk was set to 8.", 
        "125": "Training Details The models were optimized using ADADELTA following (Bahdanau et al., 2015).", 
        "126": "The hyperparameters of the training procedure were fixed to the values given in Table 2.", 
        "127": "Note that the learning rate was halved when the BLEU score on the development set did not in-\n5https://github.com/moses-smt/mgiza\ncrease for 30,000 batches.", 
        "128": "All the parameters were initialized randomly with Gaussian distribution.", 
        "129": "It took about a week to train each model with an NVIDIA TITAN X (Pascal) GPU.", 
        "130": "Evaluation Following the WAT \u201916 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models.", 
        "131": "The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.16 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.17 (Isozaki et al., 2010).", 
        "132": "Following Cho et al.", 
        "133": "(2014a), we performed beam search8 with length-normalized log-probability to decode target sentences.", 
        "134": "We saved the trained models that performed best on the development set during training and used them to evaluate the systems with the test set.", 
        "135": "Baseline Systems The baseline systems and the important hyperparamters are listed in Table 3.", 
        "136": "Eriguchi et al.", 
        "137": "(2016a)\u2019s baseline system (the first line in Table 3) was the best single (w/o ensembling) word-based NMT system that were reported in WAT \u201916.", 
        "138": "For a more fair evaluation, we also reimplemented a standard attention-based NMT system that uses exactly the same encoder, training procedure, and the hyperparameters as our proposed models, but has a word-based decoder.", 
        "139": "We trained this system on the training data without chunk segmentations (the second line in Table 3) and with chunk segmentations given by J.DepP (the third line in Table 3).", 
        "140": "The chunked corpus fed to the third system is exactly the same as the training data of our proposed systems (sixth to eighth lines in Table 3).", 
        "141": "In addition, we also include the Tree-to-Sequence models (Eriguchi et al., 2016a,b) (the fourth and fifth lines in Table 3) to compare the impact of capturing the structure in the source language and that in\n6http://www.statmt.org/moses/ 7http://www.kecl.ntt.co.jp/icl/lirg/\nribes/index.html 8Beam size is set to 20.\nthe target language.", 
        "142": "Note that all systems listed in Table 3, including our models, are single models without ensemble techniques.", 
        "143": "4.2 Results  Proposed Models vs. Baselines Table 3 shows the experimental results on the ASPEC test set.", 
        "144": "We can observe that our best model (Model 3) outperformed all the single NMT models reported in WAT \u201916.", 
        "145": "The gain obtained by switching Wordbased decoder to Chunk-based decoder (+0.93 BLEU and +1.01 RIBES) is larger than the gain obtained by switching word-based encoder to Treebased encoder (+0.27 BLEU and +0.06 RIBES).", 
        "146": "This result shows that capturing the chunk structure in the target language is more effective than capturing the syntax structure in the source language.", 
        "147": "Compared with the character-based NMT model (Eriguchi et al., 2016a), our Model 3 performed better by +5.74 BLEU score and +2.84 RIBES score.", 
        "148": "One possible reason for this is that using a character-based model rather than a wordbased model makes it more difficult to capture long-distance dependencies because the length of a target sequence becomes much longer in the character-based model.", 
        "149": "Comparison between Baselines Among the five baselines, our reimplementation without chunk segmentations (the second line in Table 3) achieved the best BLEU score while the Eriguchi et al.", 
        "150": "(2016b)\u2019s system (the fourth line in Table 3) achieved the best RIBES score.", 
        "151": "The most probable reasons for the superiority of our reimplementation over the Eriguchi et al.", 
        "152": "(2016a)\u2019s word-based baseline (the first line in Table 3) is that the dimensions of word embeddings and hidden states in our systems are higher than theirs.", 
        "153": "Feeding chunked training data to our baseline system (the third line in Table 3) instead of a normal data caused bad effects by \u22120.62 BLEU score and by \u22120.33 RIBES score.", 
        "154": "We evaluated the chunking ability of this system by comparing the positions of end-of-chunk tokens generated by this system with the chunk boundaries obtained by J.DepP.", 
        "155": "To our surprise, this word-based decoder could output chunk separations as accurate as our proposed Model 3 (both systems achieved F1-score> 97).", 
        "156": "The results show that even a standard word-based decoder has the ability to predict chunk boundaries if they are given in training data.", 
        "157": "However, it is difficult for the word-based decoder to utilize the chunk information to improve the translation quality.", 
        "158": "Decoding Speed Although the chunk-based decoder runs 2x slower than our word-based decoder, it is still practically acceptable (6 sentences per second).", 
        "159": "The character-based decoder (the fifth line in Table 3) is less time-consuming mainly because of its small vocabulary size (|Vtrg| = 3k).", 
        "160": "Chunk-level Evaluation To confirm that our models can capture local (intra-chunk) and global (inter-chunk) word orders well, we evaluated the translation quality at the chunk level.", 
        "161": "First, we performed bunsetsu-chunking on the reference translations in the test set.", 
        "162": "Then, for both reference translations and the outputs of our systems, we combined all the words in each chunk into a single token to regard a chunk as the basic translation unit instead of a word.", 
        "163": "Finally, we computed the chunk-based BLEU (C-BLEU) and RIBES\n9Tree-to-Seq models are tested on CPUs instead of GPUs.", 
        "164": "10http://lotus.kuee.kyoto-u.ac.jp/WAT/\nevaluation\n(C-RIBES).", 
        "165": "The results are listed in Table 4.", 
        "166": "For the word-based decoder (the first line in Table 4), we performed bunsetsu-chunking by J.DepP on its outputs to obtain chunk boundaries.", 
        "167": "As another baseline (the second line in Table 4), we used the chunked sentences as training data instead of performing chunking after decoding.", 
        "168": "The results show that our models (Model 2 and Model 3) outperform the word-based decoders in both C-BLEU and C-RIBES.", 
        "169": "This indicates that our chunk-based decoders can produce more correct chunks in a more correct order than the word-based models.", 
        "170": "Qualitative Analysis To clarify the qualitative difference between the word-based decoder and our chunk-based decoders, we show translation examples in Figure 5.", 
        "171": "Words in blue and red respectively denote correct translations and wrong translations.", 
        "172": "The word-based decoder (our implementation) has completely dropped the translation of \u201cby oneself.\u201d On the other hand, Model 1 generated a slightly wrong translation \u201c\u81ea\u5206\u306e\u6280\u8853\u3092\u7fd2\u5f97\u3059\u308b\u3053\u3068 (to master own technique).\u201d In addition, Model 1 has made another serious word-order error \u201c\u7279\u5225\u306a\u8abf\u6574 (special adjustment).\u201d These results suggest that Model 1 can capture longer dependencies in a long sequence than the word-based decoder.", 
        "173": "However, Model 1 is not good at modeling global word order because it cannot access enough information\nabout previous outputs.", 
        "174": "The weakness of modeling word order was overcome in Model 2 thanks to the inter-chunk connections.", 
        "175": "However, Model 2 still suffered from the errors of function words: it still generates a wrong chunk \u201c\u7279\u5225\u306a (special)\u201d instead of the correct one \u201c\u7279\u5225\u306b (specially)\u201d and a wrong chunk \u201c\u3088\u308b\u201d instead of \u201c\u3088\u308a.\u201d Although these errors seem trivial, such mistakes with function words bring serious changes of sentence meaning.", 
        "176": "However, all of these problems have disappeared in Model 3.", 
        "177": "This phenomenon supports the importance of the feedback states to provide the decoder with a better ability to choose more accurate words in chunks.", 
        "178": "5 Related Work  Much work has been done on using chunk (or phrase) structure to improve machine translation quality.", 
        "179": "The most notable work involved phrasebased SMT (Koehn et al., 2003), which has been the basis for a huge amount of work on SMT for more than ten years.", 
        "180": "Apart from this, Watanabe et al.", 
        "181": "(2003) proposed a chunk-based translation model that generates output sentences in a chunkby-chunk manner.", 
        "182": "The chunk structure is effective not only for SMT but also for example-based machine translation (EBMT).", 
        "183": "Kim et al.", 
        "184": "(2010) proposed a chunk-based EBMT and showed that using chunk structures can help with finding better word alignments.", 
        "185": "Our work is different from theirs in that our models are based on NMT, but not SMT or EBMT.", 
        "186": "The decoders in the above studies can model the chunk structure by storing chunk pairs in a large table.", 
        "187": "In contrast, we do that by individually training a chunk generation model and a word prediction model with two RNNs.", 
        "188": "While most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential informa-\ntion into NMT (Eriguchi et al., 2016b; Su et al., 2017).", 
        "189": "Eriguchi et al.", 
        "190": "(2016b) use a Tree-based LSTM (Tai et al., 2015) to encode input sentence into context vectors.", 
        "191": "Given a syntactic tree of a source sentence, their tree-based encoder encodes words from the leaf nodes to the root nodes recursively.", 
        "192": "Su et al.", 
        "193": "(2017) proposed a lattice-based encoder that considers multiple tokenization results while encoding the input sentence.", 
        "194": "To prevent the tokenization errors from propagating to the whole NMT system, their attice-based encoder can utilize multiple tokenization results.", 
        "195": "These works focus on the encoding process and propose better encoders that can exploit the structures of the source language.", 
        "196": "In contrast, our work focuses on the decoding process to capture the structure of the target language.", 
        "197": "The encoders described above and our proposed decoders are complementary so they can be combined into a single network.", 
        "198": "Considering that our Model 1 described in \u00a7 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multi-layer RNNs to capture hierarchical structures in data.", 
        "199": "Hierarchical RNNs are used for various NLP tasks such as machine translation (Luong and Manning, 2016), document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016).", 
        "200": "In particular, Li et al.", 
        "201": "(2015) and Luong and Manning (2016) use hierarchical encoder-decoder models, but not for the purpose of learning syntactic structures of target sentences.", 
        "202": "Li et al.", 
        "203": "(2015) build hierarchical models at the sentence-word level to obtain better document representations.", 
        "204": "Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem.", 
        "205": "In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation.", 
        "206": "In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014).", 
        "207": "Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers.", 
        "208": "A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step.", 
        "209": "In contrast, our Model 3 has a different connection at each time step.", 
        "210": "Before it gen-\nerates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder.", 
        "211": "However, after generating a chunk representation, the connection is to be reversed to feed back the information from the word-level decoder to the chunk-level decoder.", 
        "212": "By switching the connections between two layers, our model can capture the chunk structure explicitly.", 
        "213": "This is the first work that proposes decoders for NMT that can capture plausible linguistic structures such as chunk.", 
        "214": "Finally, we noticed that (Zhou et al., 2017) (which is accepted at the same time as this paper) have also proposed a chunk-based decoder for NMT.", 
        "215": "Their good experimental result on Chinese to English translation task also indicates the effectiveness of \u201cchunk-by-chunk\u201d decoders.", 
        "216": "Although their architecture is similar to our Model 2, there are several differences: (1) they adopt chunk-level attention instead of word-level attention; (2) their model predicts chunk tags (such as noun phrase), while ours only predicts chunk boundaries; and (3) they employ a boundary gate to decide the chunk boundaries, while we do that by simply having the model generate end-of-chunk tokens.", 
        "217": "6 Conclusion  In this paper, we propose chunk-based decoders for NMT.", 
        "218": "As the attention mechanism in NMT plays a similar role to the translation model in phrase-based SMT, our chunk-based decoders are intended to capture the notion of chunks in chunkbased (or phrase-based) SMT.", 
        "219": "We utilize the chunk structure to efficiently capture long-distance dependencies and cope with the problem of free word-order languages such as Japanese.", 
        "220": "We designed three models that have hierarchical RNNlike architectures, each of which consists of a word-level decoder and a chunk-level decoder.", 
        "221": "We performed experiments on the WAT \u201916 Englishto-Japanese translation task and found that our best model outperforms the strongest baselines by +0.93 BLEU score and by +0.57 RIBES score.", 
        "222": "In future work, we will explore the optimal structures of chunk-based decoder for other free word-order languages such as Czech, German, and Turkish.", 
        "223": "In addition, we plan to combine our decoder with other encoders that capture language structure, such as a hierarchical RNN (Luong and Manning, 2016), a Tree-LSTM (Eriguchi et al., 2016b), or an order-free encoder, such as a CNN (Kalchbrenner and Blunsom, 2013).", 
        "224": "Acknowledgements  This research was partially supported by the Research and Development on Real World Big Data Integration and Analysis program of the Ministry of Education, Culture, Sports, Science and Technology (MEXT) and RIKEN, Japan, and by the Chinese National Research Fund (NSFC) Key Project No.", 
        "225": "61532013 and National China 973 Project No.", 
        "226": "2015CB352401.", 
        "227": "The authors appreciate Dongdong Zhang, Shuangzhi Wu, and Zhirui Zhang for the fruitful discussions during the first and second authors were interns at Microsoft Research Asia.", 
        "228": "We also thank Masashi Toyoda and his group for letting us use their computing resources.", 
        "229": "Finally, we thank the anonymous reviewers for their careful reading of our paper and insightful comments."
    }, 
    "document_id": "P17-1174.pdf.json"
}
