{
    "abstract_sentences": {
        "1": "We propose a novel geolocation prediction model using a complex neural network.", 
        "2": "Our model unifies text, metadata, and user network representations with an attention mechanism to overcome previous ensemble approaches.", 
        "3": "In an evaluation using two open datasets, the proposed model exhibited a maximum 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against previous models.", 
        "4": "We further analyzed several intermediate layers of our model, which revealed that their states capture some statistical characteristics of the datasets."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1260\u20131272 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1116  1 Introduction  Social media sites have become a popular source of information to analyze current opinions of numerous people.", 
        "2": "Many researchers have worked to realize various automated analytical methods for social media because manual analysis of such vast amounts of data is difficult.", 
        "3": "Geolocation prediction is one such analytical method that has been studied widely to predict a user location or a document location.", 
        "4": "Location information is crucially important information for analyses such as disaster analysis (Sakaki et al., 2010), disease analysis (Culotta, 2010), and political analysis (Tumasjan et al., 2010).", 
        "5": "Such information is also useful for analyses such as sentiment analysis (Mart\u0131\u0301nez-Ca\u0301mara et al., 2014) and user attribute analysis (Rao et al., 2010) to undertake detailed region-specific analyses.", 
        "6": "Geolocation prediction has been performed for Wikipedia (Overell, 2009), Flickr (Serdyukov et al., 2009; Crandall et al., 2009), Facebook (Backstrom et al., 2010), and Twitter (Cheng et al., 2010; Eisenstein et al., 2010).", 
        "7": "Among these sources, Twitter is often preferred because of its characteristics, which are suited for geolocation prediction.", 
        "8": "First, some tweets include geotags, which are useful as ground truth locations.", 
        "9": "Secondly, tweets include metadata such as timezones and self-declared locations that can facilitate geolocation prediction.", 
        "10": "Thirdly, a user network is obtainable by consideration of the interaction between two users as a network link.", 
        "11": "Herein, we propose a neural network model to tackle geolocation prediction in Twitter.", 
        "12": "Past studies have combined text, metadata, and user network information with ensemble approaches (Han et al., 2013, 2014; Rahimi et al., 2015a; Jayasinghe et al., 2016) to achieve state-of-the-art performance.", 
        "13": "Our model combines text, metadata, and user network information using a complex neural network.", 
        "14": "Neural networks have recently shown effectiveness to capture complex representations combining simpler representations from large-scale datasets (Goodfellow et al., 2016).", 
        "15": "We intend to obtain unified text, metadata, and user network representations with an attention mechanism (Bahdanau et al., 2014) that is superior to the earlier ensemble approaches.", 
        "16": "The contributions of this paper are the following:\n1.", 
        "17": "We propose a neural network model that learns unified text, metadata, and user network representations with an attention mechanism.", 
        "18": "2.", 
        "19": "We show that the proposed model outperforms the previous ensemble approaches in two open datasets.", 
        "20": "3.", 
        "21": "We analyze some components of the proposed model to gain insight into the unification processes of the model.", 
        "22": "Our model specifically emphasizes geolocation prediction in Twitter to use benefits derived from the characteristics described above.", 
        "23": "However, our\n1260\nmodel can be readily extended to other social media analyses such as user attribute analysis and political analysis, which can benefit from metadata and user network information.", 
        "24": "In subsequent sections of this paper, we explain the related works in four perspectives in Section 2.", 
        "25": "The proposed neural network model is described in Section 3 along with two open datasets that we used for evaluations in Section 4.", 
        "26": "Details of an evaluation are reported in Section 5 with discussions in Section 6.", 
        "27": "Finally, Section 7 concludes the paper with some future directions.", 
        "28": "2 Related Works    2.1 Text-based Approach  Probability distributions of words over locations have been used to estimate the geolocations of users.", 
        "29": "Maximum likelihood estimation approaches (Cheng et al., 2010, 2013) and language modeling approaches minimizing KL-divergence (Wing and Baldridge, 2011; Kinsella et al., 2011; Roller et al., 2012) have succeeded in predicting user locations using word distributions.", 
        "30": "Topic modeling approaches to extract latent topics with geographical regions (Eisenstein et al., 2010, 2011; Hong et al., 2012; Ahmed et al., 2013) have also been explored considering word distributions.", 
        "31": "Supervised machine learning methods with word features are also popular in text-based geolocation prediction.", 
        "32": "Multinomial Naive Bayes (Han et al., 2012, 2014; Wing and Baldridge, 2011), logistic regression (Wing and Baldridge, 2014; Han et al., 2014), hierarchical logistic regression (Wing and Baldridge, 2014), and a multilayer neural network with stacked denoising autoencoder (Liu and Inkpen, 2015) have realized geolocation prediction from text.", 
        "33": "A semi-supervised machine learning approach by Cha et al.", 
        "34": "(2015) has also been produced using a sparse-coding and dictionary learning.", 
        "35": "2.2 User-network-based Approach  Social media often include interactions of several kinds among users.", 
        "36": "These interactions can be regarded as links that form a network among users.", 
        "37": "Several studies have used such user network information to predict geolocation.", 
        "38": "Backstrom et al.", 
        "39": "(2010) introduced a probabilistic model to predict the location of a user using friendship information in Facebook.", 
        "40": "Friend and follower information in Twitter were used to predict user locations with a most frequent friend algorithm\n(Davis Jr. et al., 2011), a unified descriptive model (Li et al., 2012b), location-based generative models (Li et al., 2012a), dynamic Bayesian networks (Sadilek et al., 2012), a support vector machine (Rout et al., 2013), and maximum likelihood estimation (McGee et al., 2013).", 
        "41": "Mention information in Twitter is also used with label propagation models (Jurgens, 2013; Compton et al., 2014) and an energy and social local coefficient model (Kong et al., 2014).", 
        "42": "Jurgens et al.", 
        "43": "(2015) compared nine user-network-based approaches targeting Twitter, controlling data conditions.", 
        "44": "2.3 Metadata-based Approach  Metadata such as location fields are useful as effective clues to predict geolocation.", 
        "45": "Hecht et al.", 
        "46": "(2011) reported that decent accuracy of geolocation prediction can be achieved using location fields.", 
        "47": "Approaches to combine metadata with texts are also proposed to extend text-based approaches.", 
        "48": "Combinatory approaches such as a dynamically weighted ensemble method (Mahmud et al., 2012), polygon stacking (Schulz et al., 2013), stacking (Han et al., 2013, 2014), and average pooling with a neural network (Miura et al., 2016) have strengthened geolocation prediction.", 
        "49": "2.4 Combinatory Approach Extending User-network-based Approach  Several attempts have been made to combine usernetwork-based approaches with other approaches.", 
        "50": "A text-based approach with logistic regression was combined with label propagation approaches to enhance geolocation prediction (Rahimi et al., 2015a,b, 2016).", 
        "51": "Jayasinghe et al.", 
        "52": "(2016) combined nine components including text-based approaches, metadata-based approaches, and a usernetwork-based approach with a cascade ensemble method.", 
        "53": "2.5 Comparisons with Proposed Model  A model we propose in Section 3 which combines text, metadata, and user network information with a neural network, can be regarded as an alternative to approaches using text and metadata (Mahmud et al., 2012; Schulz et al., 2013; Han et al., 2013, 2014; Miura et al., 2016), approaches with text and user network information (Rahimi et al., 2015a,b), and an approach with text, metadata, and user network information (Jayasinghe et al., 2016).", 
        "54": "In Section 5, we demonstrate that our model outperforms earlier models.", 
        "55": "In terms of machine learning methods, our model is a neural network model that shares some similarity with previous neural network models (Liu and Inkpen, 2015; Miura et al., 2016).", 
        "56": "Our model and these previous models have two key differences.", 
        "57": "First, our model integrates user network information along with other information.", 
        "58": "Secondly, our model combines text and metadata with an attention mechanism (Bahdanau et al., 2014).", 
        "59": "3 Model    3.1 Proposed Model  Figure 1 presents an overview of our model: a complex neural network for classification with a city as a label.", 
        "60": "For each user, the model accepts inputs of messages, a location field, a description field, a timezone, linked users, and the cities of linked users.", 
        "61": "User network information is incorporated by city embeddings and user embeddings of linked users.", 
        "62": "User embeddings are introduced along with city embeddings because linked users with city information1 are limited.", 
        "63": "We chose to let the model learn geolocation representations of linked users directly via user embeddings.", 
        "64": "The model can be\n1City information are provided by a dataset.", 
        "65": "The detail of the city information is explained in Section 4.\nbroken down to several components, details of which are described in Section 3.1.1\u20133.1.4.", 
        "66": "3.1.1 Text Component  We describe the text component of the model, which is the \u201cTEXT\u201d section in Figure 1.", 
        "67": "Figure 2 presents an overview of the text component.", 
        "68": "The component consists of a recurrent neural network (RNN) (Graves, 2012) layer and attention layers.", 
        "69": "An input of the component is a timeline of a user, which consists of messages in a time sequence.", 
        "70": "As an implementation of RNN, we used Gated Recurrent Unit (GRU) (Cho et al., 2014) with a bidirectional setting.", 
        "71": "In the RNN layer, word embeddings x of a message are processed with the following transition functions:\nzt = \u03c3 (W zxt + U zht\u22121 + bz) (1)\nrt = \u03c3 (W rxt + U rht\u22121 + br) (2)\nh\u0303t = tanh (W hxt + Uh (rt \u2299 ht\u22121) + bh) (3)\nht = (1\u2212 zt)\u2299 ht\u22121 + zt \u2299 h\u0303t (4)\nwhere zt is an update gate, rt is a reset gate, h\u0303t is a candidate state, ht is a state, W z,W r,W h,U z,U r,Uh are weight matrices, bz, br, bh are bias vectors, \u03c3 is a logistic sigmoid function, and \u2299 is an element-wise multiplication operator.", 
        "72": "The bi-directional GRU outputs \u2212\u2192 h\nand \u2190\u2212 h are concatenated to form g where gt =\u2212\u2192\nht\u2225 \u2190\u2212 ht and are passed to the first attention layer AttentionM.", 
        "73": "AttentionM computes a message representation m as a weighted sum of gt with weight \u03b1t:\nm = \u2211\nt\n\u03b1tgt (5)\n\u03b1t = exp ( vT\u03b1ut ) \u2211\nt exp (v T \u03b1ut)\n(6)\nut = tanh (W \u03b1gt + b\u03b1) (7)\nwhere v\u03b1 is a weight vector, W \u03b1 is a weight matrix, and b\u03b1 a bias vector.", 
        "74": "ut is an attention context vector calculated from gt with a single fullyconnected layer (Eq.", 
        "75": "7).", 
        "76": "ut is normalized with softmax to obtain \u03b1t as a probability (Eq.", 
        "77": "6).", 
        "78": "The message representation m is passed to the second attention layer AttentionTL to obtain a timeline representation from message representations.", 
        "79": "3.1.2 Text and Metadata Component  We describe text and metadata components of the model, which is the \u201cTEXT&META\u201d section in Figure 1.", 
        "80": "This component considers the following three types of metadata along with text: location a text field in which a user is allowed to write the user location freely, description a text field a user can use for self-description, and timezone a selective field fromwhich a user can choose a timezone.", 
        "81": "Note that certain percentages of these fields are not available2, and unknown tokens are used for inputs in such cases.", 
        "82": "2Han et al.", 
        "83": "(2014) reported missing percentages of 19% for location, 24% for description, and 25%for timezone.", 
        "84": "We process location fields and description fields similarly to messages using an RNN layer and an attention layer.", 
        "85": "Because there is only one location and one description per user, a second attention layer is not required, as it is in the text component.", 
        "86": "We also chose to share word embeddings among the messages, the location, and the description processes because these inputs are all textual information.", 
        "87": "For the timezone, an embedding is assigned for each timezone value.", 
        "88": "A processed timeline representation, a location representation, and a description representation are then passed to the attention layer AttentionU with a timezone representation.", 
        "89": "AttentionU combines these four representations and outputs a user representation.", 
        "90": "This combination is done as in AttentionTL with four representations as g1 .", 
        "91": ".", 
        "92": ".", 
        "93": "g4 in Eq.", 
        "94": "5.", 
        "95": "3.1.3 User Network Component  We describe the user network component of the model, which is the \u201cUSERNET\u201d section in Figure 1.", 
        "96": "Figure 3 presents an overview of the user network component.", 
        "97": "The model has two inputs linked cities and linked users.", 
        "98": "Users connected with a user network are extracted as linked users.", 
        "99": "We treat their cities3 as linked cities.", 
        "100": "Linked cities and linked users are assigned with city embeddings c and user embeddings a respectively.", 
        "101": "c and a are then processed to output p = c \u2295 a, where \u2295 is an element-wise addition operator.", 
        "102": "p is then passed to the subsequent attention layer AttentionN to obtain a user network representa-\n3A user with city information implies that the user is included in a training set.", 
        "103": "tion as in AttentionU.", 
        "104": "3.1.4 Model Output  An output of the text and metadata component and an output of the mention network component are further passed to the final attention layer AttentionUN to obtain a merged user representation as in AttentionU.", 
        "105": "The merged user representation is then connected to labels with a fully connected layer FCUN.", 
        "106": "3.2 Sub-models of the Proposed Model  SUB-NN-TEXT We prepare a sub-model SUBNN-TEXT by adding FCU and FCUN to the text component.", 
        "107": "This sub-model can be considered as a variant of a neural network model by Yang et al.", 
        "108": "(2016), which learns a representation of hierarchical text.", 
        "109": "SUB-NN-UNET We prepare a sub-model SUBNN-UNET by connecting the text component and the user network component with FCU, AttentionUN, and FCUN.", 
        "110": "This model can be regarded as a model that uses text and user network information.", 
        "111": "SUB-NN-META We prepare a sub-model SUBNN-META by adding FCU and FCUN to the metadata component.", 
        "112": "This model is a text-metabased model that uses text and metadata.", 
        "113": "4 Data    4.1 Dataset Specifications  TwitterUS The first dataset we used is TwitterUS assembled by Roller et al.", 
        "114": "(2012), which consists of 429K training users, 10K development users, and 10K test users in a North American region.", 
        "115": "The ground truth location of a user is set to the first geotag of the user in the dataset.", 
        "116": "We\ncollected TwitterUS tweets using TwitterAPI to reconstruct TwitterUS to obtain metadata along with text.", 
        "117": "Up to date versions in November\u2013December 2016 were used for the metadata4.", 
        "118": "We additionally assigned city centers to ground truth geotags using the city category of Han et al.", 
        "119": "(2012) to make city prediction possible in this dataset.", 
        "120": "TwitterUS (train) in Table 1 presents some properties related to the TwitterUS training set.", 
        "121": "W-NUT The second dataset we used is W-NUT, a user-level dataset of the geolocation prediction shared task of W-NUT 2016 (Han et al., 2016).", 
        "122": "The dataset consists of 1M training users, 10K development users, and 10K test users.", 
        "123": "The ground truth location of a user is decided by majority voting of the closest city center.", 
        "124": "Like in TwitterUS, we obtained metadata and texts using TwitterAPI.", 
        "125": "Up to date versions in August\u2013September 2016 were used for the metadata.", 
        "126": "W-NUT (train) in Table 1 presents some properties related to the WNUT training set.", 
        "127": "4.2 Construction of the User Network  We construct mention networks (Jurgens, 2013; Compton et al., 2014; Rahimi et al., 2015a,b) from datasets as user networks.", 
        "128": "To do so, we follow the approach of Rahimi et al.", 
        "129": "(2015a) and Rahimi et al.", 
        "130": "(2015b) who use uni-directional mention to set edges of a mention network.", 
        "131": "An edge is set between the two users nodes if a user mentions another user.", 
        "132": "The number of unidirectional mention edges for TwitterUS and WNUT can be found in Table 1.", 
        "133": "The uni-directional setting results to large numbers of edges, which often are computationally expensive to process.", 
        "134": "We restricted edges to satisfy one of the following conditions to reduce the size: (1) both users have ground truth locations or (2) one user has a ground truth location and another user is mentioned 5 times or more in a training set.", 
        "135": "The number of reduced-edges with these conditions in TwitterUS and W-NUT can be confirmed in Table 1.", 
        "136": "5 Evaluation    5.1 Implemented Baselines    5.1.1 LR  LR is an l1-regularized logistic regression model with k-d tree regions (Roller et al., 2012) used\n4TwitterAPI returns the current version of metadata even for an old tweet.", 
        "137": "in Rahimi et al.", 
        "138": "(2015a).", 
        "139": "The model uses tfidf weighted bag-of-words unigrams for features.", 
        "140": "This model is simple, but it has shown state-ofthe-art performance in cases when only text is available.", 
        "141": "5.1.2 MADCEL-B-LR  MADCEL-B-LR, a model presented by (Rahimi et al., 2015a), combines LR with Modified Adsorption (MAD) (Talukdar and Crammer, 2009).", 
        "142": "MAD is a graph-based label propagation algorithm that optimizes an objective with a prior term, a smoothness term, and an uninformativeness term.", 
        "143": "LR is combined with MAD by introducing LR results as dongle nodes to MAD.", 
        "144": "This model includes an algorithm for the construction of a mention network.", 
        "145": "The algorithm removes celebrity users5 and collapses a mention network6.", 
        "146": "We use binary edges for user network edges because they performed slightly better than weighted edges by accuracy@161 metric in Rahimi et al.", 
        "147": "(2015a).", 
        "148": "5.1.3 LR-STACK  LR-STACK is an ensemble learning model that combines four LR classifiers (LR-MSG, LR-LOC, LR-DESC, LR-TZ) with an l2-regularized logistic regression meta-classifier (LR-2ND).", 
        "149": "LR-MSG, LR-LOC, LR-DESC, and LR-TZ respectively use messages, location fields, description fields, and timezones as their inputs.", 
        "150": "This model is similar to the stacking (Wolpert, 1992) approach taken in Han et al.", 
        "151": "(2013) and Han et al.", 
        "152": "(2014), which showed superior performance compared to a feature concatenation approach.", 
        "153": "The model takes the following three steps to combine text and metadata: Step 1 LR-MSG, LRLOC, LR-DESC, and LR-TZ are trained using a training set, Step 2 the outputs of the four classifiers on the training set are obtained with 10-fold cross validation, and Step 3 LR-2ND is trained using the outputs of the four classifiers.", 
        "154": "5.1.4 MADCEL-B-LR-STACK  MADCEL-B-LR-STACK is a combined model of MADCEL-B-LR and LR-STACK.", 
        "155": "LR-STACK results are introduced as dongle nodes to MAD instead of LR results to combine text, metadata, and network information.", 
        "156": "5Users with more than t unique mentions.", 
        "157": "6Users not included in training users or test users are removed and disconnected edges with the removals are converted to direct edges.", 
        "158": "5.2 Model Configurations    5.2.1 Text Processor  We applied a lower case conversion, a unicode normalization, a Twitter user name normalization, and a URL normalization for text pre-processing.", 
        "159": "The pre-processed text is then segmented using Twokenizer (Owoputi et al., 2013) to obtain words.", 
        "160": "5.2.2 Pre-training of Embeddings  We pre-trained word embeddings using messages, location fields, and description fields of a training set using fastText (Bojanowski et al., 2016) with the skip-gram algorithm.", 
        "161": "We also pre-trained user embeddings using the non-reduced mention network described in Section 4.2 of a training set with LINE (Tang et al., 2015).", 
        "162": "The detail of pre-training parameters are described in Appendix A.1.", 
        "163": "5.2.3 Neural Network Optimization  We chose an objective function of our models to cross-entropy loss.", 
        "164": "l2 regularization was applied to the RNN layers, the attention context vectors, and the FC layers of our models to avoid overfitting.", 
        "165": "The objective function was minimized through stochastic gradient descent over shuffled mini-batches with Adam (Kingma and Ba, 2014).", 
        "166": "5.2.4 Model Parameters  The layers and the embeddings in our models have unit size and embedding dimension parameters.", 
        "167": "Our models and the baseline models have regularization parameter \u03b1, which is sensitive to a dataset.", 
        "168": "The baseline models have additional k-d tree bucket size c, celebrity threshold t, and MAD parameters \u00b51, \u00b52, and \u00b53, which are also data sensitive.", 
        "169": "We chose optimal values for these parameters in terms of accuracy with a grid search using the development sets of TwitterUS and W-NUT.", 
        "170": "Details of the parameter selection strategies and the selected values are described in Appendix A.2.", 
        "171": "5.2.5 Metrics  We evaluate the models in the following four commonly used metrics in geolocation prediction: accuracy the percentage of correctly predicted cities, accuracy@161 a relaxed accuracy that takes prediction errors within 161 km as correct predictions, median error distance median value of error distances in predictions, and mean error distance mean value of error distances in predictions.", 
        "172": "5.3 Result Performance on TwitterUS  Table 2 presents results of our models and the implemented baseline models on TwitterUS.", 
        "173": "We also list values from earlier reports (Han et al., 2012; Wing and Baldridge, 2014; Rahimi et al., 2015a,b, 2016) to make our results readily comparable with past reported values.", 
        "174": "We performed some statistical significance tests among model pairs that share the same inputs.", 
        "175": "The values in the Sign.", 
        "176": "Test ID column of Table 2 represent the IDs of these pairs.", 
        "177": "As a preparation of statistical significance tests, accuracies, accuracy@161s, and error distances of each test user were calculated for each model pair.", 
        "178": "Twosided Fisher-Pittman Permutation tests were used for testing accuracy and accuracy@161.", 
        "179": "Mood\u2019s median test was used for testing error distance in terms of median.", 
        "180": "Paired t-tests were used for testing error distance in terms of mean.", 
        "181": "We confirmed the significance of improvements\nin accuracy@161 and mean distance error for all of our models.", 
        "182": "Three of our models also improved in terms of accuracy.", 
        "183": "Especially, the proposed model achieved a 2.8% increase in accuracy and a 2.4% increase in accuracy@161 against the counterpart baseline model MADCEL-B-LRSTACK.", 
        "184": "One negative result we found was the median error distance between SUB-NN-META and LR-STACK.", 
        "185": "The baseline model LR-STACK performed 4.5 km significantly better than our model.", 
        "186": "Performance on W-NUT Table 3 presents the results of our models and the implemented baseline models on W-NUT.", 
        "187": "As for TwitterUS, we listed values from Miura et al.", 
        "188": "(2016) and Jayasinghe et al.", 
        "189": "(2016).", 
        "190": "We tested the significance of these results in the same way as we did for TwitterUS.", 
        "191": "We confirmed significant improvement in the four metrics for all of our models.", 
        "192": "The proposed model achieved a 4.8% increase in accuracy and a\n6.6% increase in accuracy@161 against the counterpart baseline model MADCEL-B-LR-STACK.", 
        "193": "The accuracy is 3.8% higher against the previously reported best value (Jayasinghe et al., 2016) which combined texts, metadata, and user network information with an ensemble method.", 
        "194": "6 Discussion    6.1 Analyses of Attention Probabilities    6.1.1 Unification Strategies  In the evaluation, the proposed model has implicitly shown effectiveness at unifying text, metadata, and user network representations through improvements in the four metrics.", 
        "195": "However, details of the unification processes are not clear from the model outputs because they are merely the probabilities of estimated locations.", 
        "196": "To gain insight into the unification processes, we analyzed the states of two attention layers: AttentionU and AttentionUN in Figure 1.", 
        "197": "Figure 4 presents the estimated probability density functions (PDFs) of the four input representations for AttentionU.", 
        "198": "These PDFs are estimated with kernel density estimation from the development sets of TwitterUS and W-NUT, where all four representations are available.", 
        "199": "From the PDFs, it is apparent that the model assigns higher probabilities to time line representations than to other three representations in TwitterUS compared to W-NUT.", 
        "200": "This finding is reasonable because timelines in TwitterUS consist of more tweets (tweet/user in Table 1) and are likely to be more informative than in W-NUT.", 
        "201": "Figure 5 presents the estimated PDFs of user network representations for AttentionUN.", 
        "202": "These\nPDFs are estimated from the development sets of TwitterUS and W-NUT, where both input representations are available.", 
        "203": "Strong preference of network representation for TwitterUS against WNUT is found in the PDFs.", 
        "204": "This finding is intuitive because TwitterUS has substantially more user network edges (reduced-edge/user in Table 1) than W-NUT, which is likely to benefit more from user network information.", 
        "205": "6.1.2 Attention Patterns  We further analyzed the proposed model by clustering attention probabilities to capture typical attention patterns.", 
        "206": "For each user, we assigned six attention probabilities of AttentionU and AttentionUN as features for a clustering.", 
        "207": "A kmeans clustering was performed over these users with 9 clusters.", 
        "208": "The clustering clearly separated the users to 5 clusters for TwitterUS users and 4 clusters for W-NUT users.", 
        "209": "We extracted typical users of each cluster by selecting the closest users of the cluster centroids.", 
        "210": "Figure 6 shows a clustering result and the attention probabilities of these users.", 
        "211": "These attention probabilities can be considered as typical attention patterns of the proposed model and match with the previously estimated PDFs.", 
        "212": "For example, cluster 2 and 3 represent an attention pattern that processes users by balancing the representations of locations along with the representations of timelines.", 
        "213": "Additionally, the location probabilities in this pattern are in the right tail region of the location PDF.", 
        "214": "6.2 Limitations of Proposed Model    6.2.1 City Prediction  The evaluation produced improvements in most of our models in the four metrics.", 
        "215": "One exception we found was the median distance error between SUB-NN-META and LR-STACKING in TwitterUS.", 
        "216": "Because the median distance error of SUB-NN-META was quite low (46.8 km), we\nmeasured the performance of an oracle model where city predictions are all correct (accuracy of 100%) in the test set.", 
        "217": "Table 4 denotes this oracle performance.", 
        "218": "The oracle mean error distance is 31.4 km.", 
        "219": "Its standard deviation is 30.1.", 
        "220": "Note that ground truth locations of TwitterUS are geotags and will not exactly match the oracle city centers.", 
        "221": "These oracle values imply that the current median error distances are close to the lower bound of the city classification approach and that they are difficult to improve.", 
        "222": "6.2.2 Errors with High Confidences  The proposed model still contains 28\u201330% errors even in accuracy@161.", 
        "223": "A qualitative analysis of errors with high confidences was performed to investigate cases that the model fails.", 
        "224": "We found two common types of error in the error analysis.", 
        "225": "The first is a case when a location field is incorrect due to a reason such as a house move.", 
        "226": "For example, the model predicted \u201cHong Kong\u201d for a user with a location field of \u201cHong Kong\u201d but has the gold location of \u201cToronto\u201d.", 
        "227": "The second is a case when a user tweets a place name of a travel.", 
        "228": "For example, the model predicted \u201cSan Francisco\u201d for a user who tweeted about a travel to \u201cSan Francisco\u201d but has the gold location of \u201cBoston\u201d.", 
        "229": "These two types of error are difficult to handle with the current architecture of the proposed model.", 
        "230": "The architecture only supports single location field which disables the model to track location changes.", 
        "231": "The architecture also treats each\ntweet independently which forbids the model to express a temporal state like traveling.", 
        "232": "7 Conclusion  As described in this paper, we proposed a complex neural network model for geolocation prediction.", 
        "233": "The model unifies text, metadata, and user network information.", 
        "234": "The model achieved the maximum of a 3.8% increase in accuracy and a maximum of 6.6% increase in accuracy@161 against several previous state-of-the-art models.", 
        "235": "We further analyzed the states of several attention layers, which revealed that the probabilities assigned to timeline representations and user network representations match to some statistical characteristics of datasets.", 
        "236": "As future works of this study, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling.", 
        "237": "Additionally, we plan to apply the proposed model to other social media analyses such as gender analysis and age analysis.", 
        "238": "In these analyses, metadata like location fields and timezones may not be effective like in geolocation prediction.", 
        "239": "However, a user network is known to include various user attributes information including gender and age (McPherson et al., 2001) which suggests the unification of text and user network information to result in a success as in geolocation prediction.", 
        "240": "Acknowledgments  We would like to thank the members of Okumura\u2013 Takamura Group at Tokyo Institute of Technology for having insightful discussions about user profiling models in social media.", 
        "241": "We would also like to thank the anonymous reviewer for their comments to improve this paper.", 
        "242": "A Supplemental Materials  A.1 Parameters of Embedding Pre-training Word embeddings were pre-trained with the parameters of learning rate=0.025, window size=5, negative sample size=5, and epoch=5.", 
        "243": "User embeddings were pre-trained with the parameters of initial learning rate=0.025, order=2, negative sample size=5, and training sample size=100M.", 
        "244": "A.2 Model Parameters and Parameter Selection Strategies\nUnit Sizes, Embedding Dimensions, and a Max Tweet Number The layers and the embeddings in our models have unit size and embedding dimension parameters.", 
        "245": "We also restricted the maximum number of tweets per user for TwitterUS to reduce memory footprints.", 
        "246": "Table 5 shows the values for these parameters.", 
        "247": "Smaller values were set for TwitterUS because TwitterUS is approximately 2.6 times larger in terms of tweet number.", 
        "248": "It was computationally expensive to process TwiiterUS in the same settings as W-NUT.", 
        "249": "Regularization Parameters and Bucket Sizes We chose optimal values of \u03b1 using a grid search with the development sets of TwitterUS and WNUT.", 
        "250": "The range of \u03b1 was set as the following: \u03b1 \u2208 {1e\u22124, 5e\u22125, 1e\u22125, 5e\u22126, 1e\u22126, 5e\u22127, 1e\u22127, 5e\u22128, 1e\u22128}.", 
        "251": "We also chose optimal values of c using grid search with the development sets of TwitterUS and W-NUT for the baseline models.", 
        "252": "The range of c was set as the following for TwitterUS: c \u2208 {50, 100, 150, 200, 250, 300, 339}.", 
        "253": "The following was set for W-NUT: c \u2208 {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 2500, 3000, 3028}.", 
        "254": "Table 6 presents selected values of \u03b1 and c. For LR-STACK and MADCEl-B-LR-STACK, different parameters of \u03b1 and c were selected for each logistic regression classifier.", 
        "255": "MAD Parameters and Celebrity Threshold The MAD parameters \u00b51, \u00b52, and \u00b53 and celebrity threshold t were also chosen using grid search with the development sets of TwitterUS and WNUT.", 
        "256": "The ranges of \u00b51, \u00b52, and \u00b53 were set as the following: \u00b51 \u2208 {1.0}, \u00b52 \u2208 {0.001, 0.01, 0.1, 1.0, 10.0}, \u00b53 \u2208 {0.0, 0.001, 0.01, 0.1, 1.0, 10.0}.", 
        "257": "The range of t for TwitterUS was set as t \u2208 {2, .", 
        "258": ".", 
        "259": ".", 
        "260": ", 16}.", 
        "261": "The range of t for W-NUT was set\nas t \u2208 {2, .", 
        "262": ".", 
        "263": ".", 
        "264": ", 6}.", 
        "265": "Table 6 presents selected values of \u00b51, \u00b52, \u00b53, and t."
    }, 
    "document_id": "P17-1116.pdf.json"
}
