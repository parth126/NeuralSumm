{
    "abstract_sentences": {
        "1": "Extracting time expressions from free text is a fundamental task for many applications.", 
        "2": "We analyze time expressions from four different datasets and find that only a small group of words are used to express time information and that the words in time expressions demonstrate similar syntactic behaviour.", 
        "3": "Based on the findings, we propose a type-based approach named SynTime1 for time expression recognition.", 
        "4": "Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related token regular expressions.", 
        "5": "On the types we design general heuristic rules to recognize time expressions.", 
        "6": "In recognition, SynTime first identifies time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions.", 
        "7": "As a lightweight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text from different domains and different text types.", 
        "8": "Experiments on benchmark datasets and tweets data show that SynTime outperforms state-of-the-art methods."
    }, 
    "body_sentences": {
        "1": "  Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 420\u2013429 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1039  1 Introduction  Time expression plays an important role in information retrieval and many applications in natural language processing (Alonso et al., 2011; Campos et al., 2014).", 
        "2": "Recognizing time expressions from free text has attracted considerable attention since last decade (Verhagen et al., 2007, 2010; UzZaman et al., 2013).", 
        "3": "1Source: https://github.com/zhongxiaoshi/syntime\nWe analyze time expressions in four datasets: TimeBank (Pustejovsky et al., 2003b), Gigaword (Parker et al., 2011), WikiWars (Mazur and Dale, 2010), and Tweets.", 
        "4": "From the analysis we make four findings about time expressions.", 
        "5": "First, most time expressions are very short, with 80% of time expressions containing no more than three tokens.", 
        "6": "Second, at least 91.8% of time expressions contain at least one time token.", 
        "7": "Third, the vocabulary used to express time information is very small, with a small group of keywords.", 
        "8": "Finally, words in time expressions demonstrate similar syntactic behaviour.", 
        "9": "All the findings relate to the principle of least effort (Zipf, 1949).", 
        "10": "That is, people tend to act under the least effort in order to minimize the cost of energy at both individual level and collective level to language usage (Zipf, 1949).", 
        "11": "Time expression is part of language and acts as an interface of communication.", 
        "12": "Short expressions, occurrence, small vocabulary, and similar syntactic behaviour all reduce the cost of energy required to communicate.", 
        "13": "According to the findings we propose a typebased approach named SynTime (\u2018Syn\u2019 stands for syntactic) to recognize time expressions.", 
        "14": "Specifically, we define three main token types, namely time token, modifier, and numeral, to group timerelated token regular expressions.", 
        "15": "Time tokens are the words that explicitly express time information, such as time units (e.g., \u2018year\u2019).", 
        "16": "Modifiers modify time tokens; they appear before or after time tokens, e.g., \u2018several\u2019 and \u2018ago\u2019 in \u2018several years ago.\u2019 Numerals are ordinals and numbers.", 
        "17": "From free text SynTime first identifies time tokens, then recognizes modifiers and numerals.", 
        "18": "Naturally, SynTime is a rule-based tagger.", 
        "19": "The key difference between SynTime and other rulebased taggers lies in the way of defining token types and the way of designing rules.", 
        "20": "The definition of token type in SynTime is inspired by part-\n420\nof-speech in which \u201clinguists group some words of language into classes (sets) which show similar syntactic behaviour.\u201d (Manning and Schutze, 1999) SynTime defines token types for tokens according to their syntactic behaviour.", 
        "21": "Other rulebased taggers define types for tokens based on their semantic meaning.", 
        "22": "For example, SUTime defines 5 semantic modifier types, such as frequency modifiers;2 while SynTime defines 5 syntactic modifier types, such as modifiers that appear before time tokens.", 
        "23": "(See Section 4.1 for details.)", 
        "24": "Accordingly, other rule-based taggers design deterministic rules based on their meanings of tokens themselves.", 
        "25": "SynTime instead designs general rules on the token types rather than on the tokens themselves.", 
        "26": "For example, our general rules do not work on tokens \u2018February\u2019 nor \u20181989\u2019 but on their token types \u2018MONTH\u2019 and \u2018YEAR.\u2019 That is why we call SynTime a type-based approach.", 
        "27": "More importantly, other rule-based taggers design rules in a fixed method, including fixed length and fixed position.", 
        "28": "In contrast, SynTime designs general rules in a heuristic way, based on the idea of boundary expansion.", 
        "29": "The general heuristic rules are quite light-weight that it makes SynTime much more flexible and expansible, and leads SynTime to run in real time.", 
        "30": "The heuristic rules are designed on token types and are independent of specific tokens, SynTime therefore is independent of specific domains, specific text types, and even specific languages that consist of specific tokens.", 
        "31": "In this paper, we test SynTime on specific domains and specific text types in English.", 
        "32": "(The test for other languages needs only to construct a collection of token regular expressions in the target language under our defined token types.)", 
        "33": "Specifically, we evaluate SynTime against three state-of-the-art methods (i.e., HeidelTime, SUTime, and UWTime) on three datasets: TimeBank, WikiWars, and Tweets.3 TimeBank and Tweets are comprehensive datasets while WikiWars is a specific domain dataset about war; TimeBank and WikiWars are the datasets in formal text while Tweets dataset is in informal text.", 
        "34": "Experiments show that SynTime achieves comparable results on WikiWars dataset, and significantly outperforms the three state-of-the-art baselines on TimeBank and Tweets\n2 https://github.com/stanfordnlp/CoreNLP/tree/\nmaster/src/edu/stanford/nlp/time/rules 3Gigaword dataset is not used in our experiments because the labels in the dataset are not the ground truth labels but instead are automatically generated by other taggers.", 
        "35": "datasets.", 
        "36": "More importantly, SynTime achieves the best recalls on all three datasets and exceptionally good results on Tweets dataset.", 
        "37": "To sum up, we make the following contributions.", 
        "38": "\u2022 We analyze time expressions from four\ndatasets and make four findings.", 
        "39": "The findings provide evidence in terms of time expression for the principle of least effort (Zipf, 1949).", 
        "40": "\u2022 We propose a time tagger named SynTime\nto recognize time expressions using syntactic token types and general heuristic rules.", 
        "41": "SynTime is independent of specific tokens, and therefore independent of specific domains, specific text types, and specific languages.", 
        "42": "\u2022 We conduct experiments on three datasets,\nand the results demonstrate the effectiveness of SynTime against state-of-the-art baselines.", 
        "43": "2 Related Work  Many research works on time expression identification are reported in TempEval exercises (Verhagen et al., 2007, 2010; UzZaman et al., 2013).", 
        "44": "The task is divided into two subtasks: recognition and normalization.", 
        "45": "Rule-based Time Expression Recognition.", 
        "46": "Rule-based time taggers like GUTime, HeidelTime, and SUTime, predefine time-related words and rules (Verhagen et al., 2005; Stro\u0308tgen and Gertz, 2010; Chang and Manning, 2012).", 
        "47": "HeidelTime (Stro\u0308tgen and Gertz, 2010) hand-crafts rules with time resources like weekdays and months, and leverages language clues like part-of-speech to identify time expression.", 
        "48": "SUTime (Chang and Manning, 2012) designs deterministic rules using a cascade finite automata (Hobbs et al., 1997) on regular expressions over tokens (Chang and Manning, 2014).", 
        "49": "It first identifies individual words, then expands them to chunks, and finally to time expressions.", 
        "50": "Rule-based taggers achieve very good results in TempEval exercises.", 
        "51": "SynTime is also a rule-based tagger while its key difference from other rule-based taggers is that between the rules and the tokens it introduces a layer of token type; its rules work on token types and are independent of specific tokens.", 
        "52": "Moreover, SynTime designs rules in a heuristic way.", 
        "53": "Machine Learning based Method.", 
        "54": "Machine learning based methods extract features from the text and apply statistical models on the features for recognizing time expressions.", 
        "55": "Example features\ninclude character features, word features, syntactic features, semantic features, and gazetteer features (Llorens et al., 2010; Filannino et al., 2013; Bethard, 2013).", 
        "56": "The statistical models include Markov logic network, logistic regression, support vector machines, maximum entropy, and conditional random fields (Llorens et al., 2010; UzZaman and Allen, 2010; Filannino et al., 2013; Bethard, 2013).", 
        "57": "Some models obtain good performance, and even achieve the highest F1 of 82.71% on strict match in TempEval-3 (Bethard, 2013).", 
        "58": "Outside TempEval exercises, Angeli et al.", 
        "59": "leverage compositional grammar and employ a EMstyle approach to learn a latent parser for time expression recognition (Angeli et al., 2012).", 
        "60": "In the method named UWTime, Lee et al.", 
        "61": "handcraft a combinatory categorial grammar (CCG) (Steedman, 1996) to define a set of lexicon with rules and use L1-regularization to learn linguistic context (Lee et al., 2014).", 
        "62": "The two methods explicitly use linguistic information.", 
        "63": "In (Lee et al., 2014), especially, CCG could capture rich structure information of language, similar to the rule-based methods.", 
        "64": "Tabassum et al.", 
        "65": "focus on resolving the dates in tweets, and use distant supervision to recognize time expressions (Tabassum et al., 2016).", 
        "66": "They use five time types and assign one of them to each word, which is similar to SynTime in the way of defining types over tokens.", 
        "67": "However, they focus only on the type of date, while SynTime recoginizes all the time expressions and does not involve learning and runs in real time.", 
        "68": "Time Expression Normalization.", 
        "69": "Methods in TempEval exercises design rules for time expression normalization (Verhagen et al., 2005; Stro\u0308tgen and Gertz, 2010; Llorens et al., 2010; UzZaman and Allen, 2010; Filannino et al., 2013; Bethard, 2013).", 
        "70": "Because the rule systems have high similarity, Llorens et al.", 
        "71": "suggest to construct a large knowledge base as a public resource for the task (Llorens et al., 2012).", 
        "72": "Some researchers treat the normalization process as a learning task and use machine learning methods (Lee et al., 2014; Tabassum et al., 2016).", 
        "73": "Lee et al.", 
        "74": "(Lee et al., 2014) use AdaGrad algorithm (Duchi et al., 2011) and Tabassum et al.", 
        "75": "(Tabassum et al., 2016) use a loglinear algorithm to normalize time expressions.", 
        "76": "SynTime focuses only on the recognition task.", 
        "77": "The normalization could be achieved by using methods similar to the existing rule systems, because they are highly similar (Llorens et al., 2012).", 
        "78": "3 Time Expression Analysis    3.1 Dataset  We conduct an analysis on four datasets: TimeBank, Gigaword, WikiWars, and Tweets.", 
        "79": "TimeBank (Pustejovsky et al., 2003b) is a benchmark dataset in TempEval series (Verhagen et al., 2007, 2010; UzZaman et al., 2013), consisting of 183 news articles.", 
        "80": "Gigaword (Parker et al., 2011) is a large automatically labelled dataset with 2,452 news articles and used in TempEval-3.", 
        "81": "WikiWars dataset is derived from Wikipedia articles about wars (Mazur and Dale, 2010).", 
        "82": "Tweets is our manually annotated dataset with 942 tweets of which each contains at least one time expression.", 
        "83": "Table 1 summarizes the datasets.", 
        "84": "3.2 Finding  From the four datasets, we analyze their time expressions and make four findings.", 
        "85": "We will see that despite the four datasets vary in corpus sizes, in text types, and in domains, their time expressions demonstrate similar characteristics.", 
        "86": "Finding 1 Time expressions are very short.", 
        "87": "More than 80% of time expressions contain no more than three words and more than 90% contain no more than four words.", 
        "88": "Figure 1 plots the length distribution of time expressions.", 
        "89": "Although the texts are collected from different sources (i.e., news articles, Wikipedia articles, and tweets) and vary in sizes, the length\nof time expressions follow a similar distribution.", 
        "90": "In particular, the one-word time expressions range from 36.23% in WikiWars to 62.91% in Tweets.", 
        "91": "In informal communication people tend to use words in minimum length to express time information.", 
        "92": "The third column in Table 2 reports the average length of time expressions.", 
        "93": "On average, time expressions contain about two words.", 
        "94": "Finding 2 More than 91% of time expressions contain at least one time token.", 
        "95": "The second column in Table 2 reports the percentage of time expressions that contain at least one time token.", 
        "96": "We find that at least 91.81% of time expressions contain time token(s).", 
        "97": "(Some time expressions have no time token but depend on other time expressions; in \u20182 to 8 days,\u2019 for example, \u20182\u2019 depends on \u20188 days.\u2019) This suggests that time tokens account for time expressions.", 
        "98": "Therefore, to recognize time expressions, it is essential to recognize their time tokens.", 
        "99": "Finding 3 Only a small group of time-related keywords are used to express time information.", 
        "100": "From the time expressions in all four datasets, we find that the group of keywords used to express time information is small.", 
        "101": "Table 3 reports the number of distinct words and of distinct time tokens.", 
        "102": "The words/tokens are manually normalized before counting and their variants are ignored.", 
        "103": "For example, \u2018year\u2019 and \u20185yrs\u2019 are counted as one token \u2018year.\u2019 Numerals in the counting are ignored.", 
        "104": "Despite the four datasets\nvary in sizes, domains, and text types, the numbers of their distinct time tokens are comparable.", 
        "105": "Across the four datasets, the number of distinct words is 350, about half of the simply summing of 675; the number of distinct time tokens is 123, less than half of the simply summing 282.", 
        "106": "Among the 123 distinct time tokens, 45 appear in all the four datasets, and 101 appear in at least two datasets.", 
        "107": "This indicates that time tokens, which account for time expressions, are highly overlapped across the four datasets.", 
        "108": "In other words, time expressions highly overlap at their time tokens.", 
        "109": "Finding 4 POS information could not distinguish time expressions from common words, but within time expressions, POS tags can help distinguish their constituents.", 
        "110": "For each dataset we list the top 10 POS tags that appear in time expressions, and their percentages over the whole text.", 
        "111": "Among the 40 tags (10 \u00d7 4 datasets), 37 have percentage lower than 20%; other 3 are CD.", 
        "112": "This indicates that POS could not provide enough information to distinguish time expressions from common words.", 
        "113": "However, the most common POS tags in time expressions are NN*, JJ, RB, CD, and DT.", 
        "114": "Within time expressions, the time tokens usually have NN* and RB, the modifiers have JJ and RB, and the numerals have CD.", 
        "115": "This finding indicates that for the time expressions, their similar constituents behave in similar syntactic way.", 
        "116": "When seeing this, we realize that this is exactly how linguists define part-of-speech for language.4 The definition of POS for language inspires us to define a syntactic type system for the time expression, part of language.", 
        "117": "The four findings all relate to the principle of least effort (Zipf, 1949).", 
        "118": "That is, people tend to act with least effort so as to minimize the cost of energy at both individual and collective levels to the language usage (Zipf, 1949).", 
        "119": "Time expression is part of language and acts as an interface of communication.", 
        "120": "Short expressions, occurrence, small vocabulary, and similar syntactic behaviour all reduce the cost of energy required to communicate.", 
        "121": "To summarize: on average, a time expression contains two tokens of which one is time token and the other is modifier/numeral, and the size of time tokens is small.", 
        "122": "To recognize a time expression, therefore, we first recognize the time token, then recognize the modifier/numeral.", 
        "123": "4\u201clinguists group some words of language into classes (sets) which show similar syntactic behaviour.\u201d (Manning and Schutze, 1999)  4 SynTime: Syntactic Token Types and General Heuristic Rules  SynTime defines a syntactic type system for the tokens of time expressions, and designs heuristic rules working on the token types.", 
        "124": "Figure 2 shows the layout of SynTime, consisting of three levels: Token level, type level, and rule level.", 
        "125": "Token types at the type level group the tokens of time expressions.", 
        "126": "Heuristic rules lie at the rule level, working on token types rather than on tokens themselves.", 
        "127": "That is why the heuristic rules are general.", 
        "128": "For example, the heuristic rules do not work on tokens \u20181989\u2019 nor \u2018February,\u2019 but on their token types \u2018YEAR\u2019 and \u2018MONTH.\u2019 The heuristic rules are only relevant to token types, and are independent of specific tokens.", 
        "129": "For this reason, our token types and heuristic rules are independent of specific domains, specific text types, and even specific languages that consist of specific tokens.", 
        "130": "In this paper, we test SynTime on specific domain (i.e., war domain) and specific text types (i.e., formal text and informal text) in English.", 
        "131": "The test for other languages simply needs to construct a set of token regular expressions in the target language under our defined token types.", 
        "132": "Figure 3 shows the overview of SynTime in practice.", 
        "133": "Shown on the left-hand side, SynTime is initialized with regular expressions over tokens.", 
        "134": "After initialization, SynTime can be directly applied on text.", 
        "135": "On the other hand, SynTime can be easily expanded by simply adding the time-related token regular expressions from training text under each defined token type.", 
        "136": "The expansion enables SynTime to recognize time expressions in text from different domains and different text types.", 
        "137": "Shown on the right-hand side of Figure 3, SynTime recognizes time expression through three main steps.", 
        "138": "In the first step, SynTime identifies\ntime tokens from the POS-tagged raw text.", 
        "139": "Then around the time tokens SynTime searches for modifiers and numerals to form time segments.", 
        "140": "In the last step, SynTime transforms the time segments to time expressions.", 
        "141": "4.1 SynTime Construction  We define a syntactic type system for time expression, specifically, 15 token types for time tokens, 5 token types for modifiers, and 1 token type for numeral.", 
        "142": "Token types to tokens is like POS tags to words; for example, \u2018February\u2019 has a POS tag of NNP and a token type of MONTH.", 
        "143": "Time Token.", 
        "144": "We define 15 token types for the time tokens and use their names similar to Joda-Time classes:5 DECADE (-), YEAR (-), SEASON (5), MONTH (12), WEEK (7), DATE (-), TIME (-), DAY TIME (27), TIMELINE (12), HOLIDAY (20), PERIOD (9), DURATION (-), TIME UNIT (15), TIME ZONE (6), and ERA (2).", 
        "145": "Number in \u2018()\u2019 indicates the number of distinct tokens in this token type.", 
        "146": "\u2018-\u2019 indicates that this token type involves changing digits and cannot be counted.", 
        "147": "Modifier.", 
        "148": "We define 3 token types for the modifiers according to their possible positions relative to time tokens.", 
        "149": "Modifiers that appear before time tokens are PREFIX (48); modifiers after time tokens are SUFFIX (2).", 
        "150": "LINKAGE (4) link two time\n5 http://www.joda.org/joda-time/\ntokens.", 
        "151": "Besides, we define 2 special modifier types, COMMA (1) for comma \u2018,\u2019 and IN ARTICLE (2) for indefinite articles \u2018a\u2019 and \u2018an.\u2019\nTimeML (Pustejovsky et al., 2003a) and TimeBank (Pustejovsky et al., 2003b) do not treat most prepositions like \u2018on\u2019 as a part of time expressions.", 
        "152": "Thus SynTime does not collect those prepositions.", 
        "153": "Numeral.", 
        "154": "Number in time expressions can be a time token e.g., \u201810\u2019 in \u2018October 10, 2016,\u2019 or a modifier e.g., \u201810\u2019 in \u201810 days.\u2019 We define NUMERAL (-) for the ordinals and numbers.", 
        "155": "SynTime Initialization.", 
        "156": "The token regular expressions for initializing SynTime are collected from SUTime,6 a state-of-the-art rule-based tagger that achieved the highest recall in TempEval3 (Chang and Manning, 2012, 2013).", 
        "157": "Specifically, we collect from SUTime only the tokens and the regular expressions over tokens, and discard its other rules of recognizing full time expressions.", 
        "158": "4.2 Time Expression Recognition  On the token types, SynTime designs a small set of heuristic rules to recognize time expressions.", 
        "159": "The recognition process includes three main steps: (1) time token identification, (2) time segment identification, and (3) time expression extraction.", 
        "160": "4.2.1 Time Token Identification  Identifying time tokens is simple, through matching of string and regular expressions.", 
        "161": "Some words might cause ambiguity.", 
        "162": "For example, \u2018May\u2019 could be a modal verb, or the fifth month of year.", 
        "163": "To filter out the ambiguous words, we use POS information.", 
        "164": "In implementation, we use Stanford POS Tagger;7 and the POS tags for matching the instances of token types in SynTime are based on our Finding 4 in Section 3.2.", 
        "165": "Besides time tokens are identified, in this step, individual token is assigned with one token type of either modifier or numeral if it is matched with token regular expressions.", 
        "166": "In the next two steps, SynTime works on those token types.", 
        "167": "4.2.2 Time Segment Identification  The task of time segment identification is to search the surrounding of each time token identified in previous step for modifiers and numerals, then gather the time token with its modifiers and numerals to form a time segment.", 
        "168": "The searching is\n6 https://github.com/stanfordnlp/CoreNLP/tree/\nmaster/src/edu/stanford/nlp/time/rules 7 http://nlp.stanford.edu/software/tagger.shtml\nPREFIX/the PREFIX/last TIME_UNIT/week \u2026 said WEEK/Friday s1 s2\ne1 s1\n(a) Stand-alone time segment to time expression\ns1 s2\ns1\nPREFIX/the NUMERAL/third TIME_UNIT/quarter PREFIX/of YEAR/1984\n(b) Merge adjacent time segments\ns1 s2\ns1\nMONTH/January NUMERAL/13 YEAR/1951\n(c) Merge overlapping time segments\ns1 s2\ns1\nMONTH/June NUMERAL/30 COMMA/, YEAR/1990\n(d) Merge overlapping time segments\ns1 s2\nNUMERAL/8 LINKAGE/to NUMERAL/20 TIME_UNIT/days\nunder simple heuristic rules in which the key idea is to expand the time token\u2019s boundaries.", 
        "169": "At first, each time token is a time segment.", 
        "170": "If it is either a PERIOD or DURATION, then no need to further search.", 
        "171": "Otherwise, search its left and its right for modifiers and numerals.", 
        "172": "For the left searching, if encounter a PREFIX or NUMERAL or IN ARTICLE, then continue searching.", 
        "173": "For the right searching, if encounter a SUFFIX or NUMERAL, then continue searching.", 
        "174": "Both the left and the right searching stop when reaching a COMMA or LINKAGE or a non-modifier/numeral word.", 
        "175": "The left searching does not exceed the previous time token; the right searching does not exceed the next time token.", 
        "176": "A time segment consists of exactly one time token, and zero or some modifiers/numerals.", 
        "177": "A special kind of time segments do not contain any time token; they depend on other time segments next to them.", 
        "178": "For example, in \u20188 to 20 days,\u2019 \u2018to 20 days\u2019 is a time segment, and \u20188 to\u2019 forms a dependent time segment.", 
        "179": "(See Figure 4(e).)", 
        "180": "4.2.3 Time Expression Extraction  The task of time expression extraction is to extract time expressions from the identified time segments in which the core step is to determine whether to merge two adjacent or overlapping time segments into a new time segment.", 
        "181": "We scan the time segments in a sentence from beginning to the end.", 
        "182": "A stand-alone time segment is a time expression.", 
        "183": "(See Figure 4(a).)", 
        "184": "The focus is to deal with two or more time segments that are adjacent or overlapping.", 
        "185": "If two time segments s1 and s2 are adjacent, merge them to form a new time segment s1.", 
        "186": "(See Figure 4(b).)", 
        "187": "Consider that s1 and s2 overlap at a shared boundary.", 
        "188": "According to our time segment identification, the shared boundary could be a modifier or a numeral.", 
        "189": "If the word at the shared boundary is neither a COMMA nor a LINKAGE, then merge s1 and s2.", 
        "190": "(See Figure 4(c).)", 
        "191": "If the word is a LINKAGE, then extract s1 as a time expression and continue scanning.", 
        "192": "When the shared boundary is a COMMA, merge s1 and s2 only if the COMMA\u2019s previous token and its next token satisfy the three conditions: (1) the previous token is a time token or a NUMERAL; (2) the next token is a time token; and (3) the token types of the previous token and of the next token are not the same.", 
        "193": "(See Figure 4(d).)", 
        "194": "Although Figure 4 shows the examples as token types together with the tokens, we should note that the heuristic rules only work on the token types.", 
        "195": "After the extraction step, time expressions are exported as a sequence of tokens from the sequence of token types.", 
        "196": "4.3 SynTime Expansion  SynTime could be expanded by simply adding new words under each defined token type without changing any rule.", 
        "197": "The expansion requires the words to be added to be annotated manually.", 
        "198": "We apply the initial SynTime on the time expressions from training text and list the words that are not covered.", 
        "199": "Whether the uncovered words are added to SynTime is manually determined.", 
        "200": "The rule for determination is that the added words can not cause ambiguity and should be generic.", 
        "201": "WikiWars dataset contains a few examples like this: \u2018The time Arnold reached Quebec City.\u2019 Words in this example are extremely descriptive, and we do not collect them.", 
        "202": "In tweets, on the other hand, people may use abbreviations and informal variants; for example, \u20182day\u2019 and \u2018tday\u2019 are popular spellings of \u2018today.\u2019 Such kind of abbreviations and informal variants will be collected.", 
        "203": "According to our findings, not many words are used to express time information, the manual addition of keywords thus will not cost much.", 
        "204": "In addition, we find that even in tweets people tend\nto use formal words.", 
        "205": "In the Twitter word clusters trained from 56 million English tweets,8 the most often used words are the formal words, and their frequencies are much greater than the informal words\u2019.", 
        "206": "The cluster of \u2018today,\u20199 for example, its most often use is the formal one, \u2018today,\u2019 which appears 1,220,829 times; while its second most often use \u20182day\u2019 appears only 34,827 times.", 
        "207": "The low rate of informal words (e.g., about 3% in \u2018today\u2019 cluster) suggests that even in informal environment the manual keyword addition costs little.", 
        "208": "5 Experiments  We evaluate SynTime against three state-of-theart baselines (i.e., HeidelTime, SUTime, and UWTime) on three datasets (i.e., TimeBank, WikiWars, and Tweets).", 
        "209": "WikiWars is a specific domain dataset about war; TimeBank and WikiWars are the datasets in formal text while Tweets dataset is in informal text.", 
        "210": "For SynTime we report the results of its two versions: SynTime-I and SynTime-E. SynTime-I is the initial version, and SynTime-E is the expanded version of SynTime-I.", 
        "211": "5.1 Experiment Setting  Datasets.", 
        "212": "We use three datasets of which TimeBank and WikiWars are benchmark datasets whose details are shown in Section 3.1; Tweets is our manually labeled dataset that are collected from Twitter.", 
        "213": "For Tweets dataset, we randomly sample 4000 tweets and use SUTime to tag them.", 
        "214": "942 tweets of which each contains at least one time expression.", 
        "215": "From the remaining 3,058 tweets, we randomly sample 500 and manually annotate them, and find that only 15 tweets contain time expressions.", 
        "216": "We therefore roughly consider that SUTime misses about 3% time expressions in tweets.", 
        "217": "Two annotators then manually annotate the 942 tweets with discussion to final agreement according to the standards of TimeML and TimeBank.", 
        "218": "We finally get 1,127 manually labeled time expressions.", 
        "219": "For the 942 tweets, we randomly sample 200 tweets as test set, and the rest 742 as training set, because a baseline UWTime requires training.", 
        "220": "Baseline Methods.", 
        "221": "We compare SynTime with methods: HeidelTime (Stro\u0308tgen and Gertz, 2010), SUTime (Chang and Manning, 2012), and UW-\n8 http://www.cs.cmu.edu/\u02dcark/TweetNLP/cluster_\nviewer.html 9 http://www.cs.cmu.edu/\u02dcark/TweetNLP/paths/ 01111110010.html\nTime (Lee et al., 2014).", 
        "222": "HeidelTime and SUTime both are rule-based methods, and UWTime is a learning method.", 
        "223": "When training UWTime on Tweets, we try two settings: (1) train with only Tweets training set; (2) train with TimeBank and Tweets training set.", 
        "224": "The second setting achieves slightly better result and we report that result.", 
        "225": "Evaluation Metrics.", 
        "226": "We follow TempEval-3 and use their evaluation toolkit10 to report Precision, Recall, and F1 in terms of strict match and relaxed match (UzZaman et al., 2013).", 
        "227": "5.2 Experiment Result  Table 4 reports the overall performance.", 
        "228": "Among the 18 measures, SynTime-I and SynTime-E achieve 12 best results and 13 second best results.", 
        "229": "Except the strict match on WikiWars dataset, both SynTime-I and SynTime-E achieve F1 above 91%.", 
        "230": "For the relaxed match on all three datasets, SynTime-I and SynTime-E achieve recalls above 92%.", 
        "231": "The high recalls are consistent with our finding that at least 91.81% of time expressions contain time token(s).", 
        "232": "(See Table 2.)", 
        "233": "This indicates that SynTime covers most of time tokens.", 
        "234": "On Tweets dataset, SynTime-I and SynTime-E achieve exceptionally good performance.", 
        "235": "Their F1 reach 91.74% with 11.37% improvement in strict match and 95.87% with 6.33% improvement in re-\n10 http://www.cs.rochester.edu/\u02dcnaushad/tempeval3/\ntools.zip\nlaxed match.", 
        "236": "The reasons are that in informal environment people tend to use time expressions in minimum length, (62.91% of one-word time expressions in Tweets; see Figure 1.)", 
        "237": "the size of time keywords is small, (only 60 distinct time tokens; see Table 3.)", 
        "238": "and even in tweets people tend to use formal words.", 
        "239": "(See Section 4.3 for our finding from Twitter word clusters.)", 
        "240": "For precision, SynTime achieves comparable results in strict match and performs slightly poorer in relaxed match.", 
        "241": "5.2.1 SynTime-I vs. Baseline Methods  On TimeBank dataset, SynTime-I achieves F1 of 92.09% in strict match and of 94.96% in relaxed match.", 
        "242": "On Tweets, SynTime-I achieves 91.74% and 95.87%, respectively.", 
        "243": "It outperforms all the baseline methods.", 
        "244": "The reason is that for the rulebased time taggers, their rules are designed in a fixed way, lacking flexibility.", 
        "245": "For example, SUTime could recognize \u20181 year\u2019 but not \u2018year 1.\u2019 For the machine learning based methods, some of the features they used actually hurt the modelling.", 
        "246": "Time expressions involve quite many changing numbers which in themselves affect the pattern recognition.", 
        "247": "For example, it is difficult to build connection between \u2018May 22, 1986\u2019 and \u2018February 01, 1989\u2019 at the level of word or of character.", 
        "248": "One suggestion is to consider a type-based learning method that could use type information.", 
        "249": "For example, the above two time expressions refer to the same pattern of \u2018MONTH NUMERAL COMMA\nYEAR\u2019 at the level of token type.", 
        "250": "POS is a kind of type information.", 
        "251": "But according to our analysis, POS could not distinguish time expressions from common words.", 
        "252": "Features need carefully designing.", 
        "253": "On WikiWars, SynTime-I achieves competitive results in both matches.", 
        "254": "Time expressions in WikiWars include lots of prepositions and quite a few descriptive time expressions.", 
        "255": "SynTime could not fully recognize such kinds of time expressions because it follows TimeML and TimeBank.", 
        "256": "5.2.2 SynTime-E vs. SynTime-I  Table 5 lists the number of time tokens and modifiers added to SynTime-I to get SynTime-E.\nOn TimeBank and Tweets datasets, only a few tokens are added, the corresponding results are affected slightly.", 
        "257": "This confirms that the size of time words is small, and that SynTime-I covers most of time words.", 
        "258": "On WikiWars dataset, relatively more tokens are added, SynTime-E performs much better than SynTime-I, especially in recall.", 
        "259": "It improves the recall by 3.25% in strict match and by 2.98% in relaxed match.", 
        "260": "This indicates that with more words added from specific domains (e.g., WikiWars dataset about war), SynTime can significantly improve the performance.", 
        "261": "5.3 Limitations  SynTime assumes that words are tokenized and POS tagged correctly.", 
        "262": "In reality, however, the tokenized and tagged words are not that perfect, due to the limitation of used tools.", 
        "263": "For example, Stanford POS Tagger assigns VBD to the word \u2018sat\u2019 in \u2018friday or sat\u2019 while whose tag should be NNP.", 
        "264": "The incorrect tokens and POS tags affect the result.", 
        "265": "6 Conclusion and future work  We conduct an analysis on time expressions from four datasets, and find that time expressions in general are very short and expressed by a small vocabulary, and words in time expressions demonstrate similar syntactic behavior.", 
        "266": "Our findings provide evidence in terms of time expression for the principle of least effort (Zipf, 1949).", 
        "267": "Inspired by\npart-of-speech, based on the findings, we define a syntactic type system for the time expression, and propose a type-based time expression tagger, named by SynTime.", 
        "268": "SynTime defines syntactic token types for tokens and on the token types it designs general heuristic rules based on the idea of boundary expansion.", 
        "269": "Experiments on three datasets show that SynTime outperforms the stateof-the-art baselines, including rule-based time taggers and machine learning based time tagger.", 
        "270": "Because our heuristic rules are quite simple, SynTime is light-weight and runs in real time.", 
        "271": "Our token types and heuristic rules are independent of specific tokens, SynTime therefore is independent of specific domains, specific text types, and even specific languages that consist of specific tokens.", 
        "272": "In this paper, we test SynTime on specific domains and specific text types in English.", 
        "273": "The test for other languages needs only to construct a collection of token regular expressions in the target language under our defined token types.", 
        "274": "Time expression is part of language and follows the principle of least effort.", 
        "275": "Since language usage relates to human habits (Zipf, 1949; Chomsky, 1986; Pinker, 1995), we might expect that humans would share some common habits, and therefore expect that other parts of language would more or less follow the same principle.", 
        "276": "In the future we will try our analytical method on other parts of language.", 
        "277": "Acknowledgments  The authors would like to thank the three anonymous reviewers for their insightful comments and constructive suggestions.", 
        "278": "This research is mainly supported by the Singapore Ministry of Education Research Fund MOE2014-T2-2-066."
    }, 
    "document_id": "P17-1039.pdf.json"
}
